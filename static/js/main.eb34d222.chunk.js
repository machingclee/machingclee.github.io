(this["webpackJsonpgithub.io.source"]=this["webpackJsonpgithub.io.source"]||[]).push([[0],Array(292).concat([function(n){n.exports=JSON.parse('[{"content":"\\n##### Routing in This Blog Page\\n\\nThe routing of this react application is controlled by the following single router.ts file:\\n\\ntypescript\\ninterface IRouter {\\n path: string;\\n component: () => JSX.Element;\\n exact?: boolean;\\n routes?: IRouter[];\\n}\\n\\nconst routers: IRouter[] = [\\n {\\n path: navRoutes.ROOT,\\n component: MainContent,\\n routes: [\\n {\\n path: navRoutes.ABOUT,\\n component: About,\\n exact: true,\\n },\\n {\\n path: navRoutes.SKILLS,\\n component: Skills,\\n exact: true,\\n },\\n ...{\\n path: ${navRoutes.BLOG},\\n component: Blog,\\n exact: false,\\n },\\n ],\\n },\\n];\\n\\n\\nNote that for a sub-routing of navRoutes.BLOG (/blog) to work, exact: false is important.\\n\\nMainContent is the largest container where we want to switch from one component to another.\\n\\nI used to add another Switch in Blog component to achive this sub-routing, but recently I find a much convenient way. I call this a lazy routing since it can really achive routing in an extremely lazy but cleaner way by using useRouteMatch.\\n\\nThe Blog component now has the following structure:\\n\\ntypescript\\nexport default () => {\\n\\n ...\\n\\n const match = useRouteMatch<{ matchedArticleTitle: string }>(\\n ${navRoutes.BLOG}/:matchedArticleTitle\\n );\\n const { matchedArticleTitle } = match?.params || { matchedArticleTitle: \\"\\" };\\n\\n ...\\n\\n const activeBlogArticleTitle = useAppSelector(\\n (state: TRootState) => state.app.blog.activeBlogArticleTitle\\n );\\n const selectedArticle = articles.find(\\n md => md.title === activeBlogArticleTitle\\n );\\n\\n ...\\n\\n useEffect(() => {\\n dispatch(appActions.updateBlog(\\n { activeBlogArticleTitle: matchedArticleTitle }\\n ));\\n }, [matchedArticleTitle]);\\n\\n return (\\n <div className=\\"blog\\">\\n ...\\n </div >\\n )\\n}\\n\\n\\nuseRouteMatch catches the param in exactly the way we want. And this is cleaner than using Route in the sense that you can observe which route, in which way, affects this component directly inside the code of this component.\\n\\nNow on matchedArticleTitle changed, our useEffect hook will update the state of activeBlogArticleTitle in our redux store. And we can change our content based on this unique article name (as selectedArticle is changed).\\n","title":"Lazy React Router","date":"2021-07-12T00:00:00.000Z","id":"blog0002","tag":"react","intro":"Inside a routed component, we introduce useRouteMatch on type annotation and the way to extract params."},{"content":"\\n##### Loop Through a Directory\\n\\nIn node.js, specifically in backend, we can use fs.readdir to return a list of files and directories of our target directory. Though fs is not available in frontend, we record a function that achieve similar objective:\\n\\njavascript\\nconst getModules = () => {\\n function importAll(r: any) {\\n let files: any[] = [];\\n r.keys().map((item: any, index: any) => {\\n files.push(r(item));\\n });\\n return files;\\n }\\n\\n //@ts-ignore\\n const files: { default: string }[] = importAll(\\n require.context(\\"./articles\\", true, /\\\\.md$/)\\n );\\n return files;\\n};\\n\\n\\nForgive me to have //@ts-ignore here as I don\'t want the hassle of annotating types to the utility function that I wouldn\'t maintain.\\n\\nThe most important part is the function: require.context, which takes (directoryName:string, useSubdirectories: boolean, regExp: RegExp) as its positioinal argument.\\n\\nThis blog page is an example of using this util function. Whenever I add an md file, the function can loop through my target directory, add a new file path to the array I am going to map, and generate a new post automatically.\\n","title":"On Looping all Files in Frontend","date":"2021-07-13T00:00:00.000Z","id":"blog0003","tag":"react","intro":"In backend we can loop through the files inside a directory using fs.readdir, we introduce a function that can achive the same thing in frontend using webpack\'s require.context function."},{"content":"\\n#### Basic Commands\\n\\n- Create an virtual environment:\\n bash\\n conda create --name ENV_NAME python=3.8\\n \\n- Remove an virtual environment\\n\\n bash\\n conda env remove -n ENV_NAME\\n \\n\\n- Register ENV to jupyter by\\n bash\\n python -m ipykernel install --user --name=ENV_NAME\\n \\n- Remove ENV in jupyter by\\n bash\\n jupyter kernelspec uninstall ENV_NAME\\n \\n- List all existing virtual environments:\\n bash\\n conda env list\\n \\n- List all libraries inside your activated virtual environment:\\n\\n bash\\n conda list\\n \\n\\n- Create a requirements.txt\\n\\n bash\\n conda list -e > requirements.txt # for conda\\n pip freeze > requirements.txt # for pip\\n \\n\\n- conda/pip install the packages in requirements.txt\\n bash\\n conda install --file requirements.txt\\n pip install -r requirements.txt\\n \\n\\n#### Common Packages\\n\\nbash\\nconda install -c conda-forge jupyterlab\\nconda install -c conda-forge tensorflow\\nconda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\\n\\n","title":"Useful Conda Commands","date":"2021-07-18T00:00:00.000Z","id":"blog0005","tag":"python","intro":"Record the common commands that is needed to create a new virtual environment with conda."},{"content":"\\n##### Folder Structure and Scripts\\n\\n<center>\\n <img width=\\"220\\" src=\\"/assets/tech/01.png\\"/>\\n</center>\\n<br/>\\n\\nLet me take the action in this blog as an example.\\n\\njavascript\\n// app.action.ts\\nexport const UPDATE_BLOG = \\"UPDATE_BLOG\\";\\n\\nexport type UpdateBlog = ReturnType<typeof updateBlog>;\\nexport const updateBlog = (update: Partial<TAppState[\\"blog\\"]>) => {\\n return {\\n type: UPDATE_BLOG,\\n payload: update,\\n };\\n};\\n\\n\\nWe can do our api-related logic in the .saga below! Later whenever we want to modify our api, saga is the only place we need to look at:\\n\\njavascript\\n// app.saga.ts\\nimport { takeEvery, fork } from \'redux-saga/effects\';\\nimport * as appActions from \\"../actions/app.actions\\";\\n\\nfunction* watchUpdateBlog() {\\n yield takeEvery(appActions.UPDATE_BLOG, updateBlog);\\n}\\n\\nfunction* updateBlog(action: appActions.UpdateBlog) {\\n console.log(\\"I am saga\\");\\n yield put(your action, if any);\\n}\\n\\nexport default [\\n fork(watchUpdateBlog)\\n]\\n\\n\\njavascript\\n// root.saga.ts\\nimport { all } from \\"redux-saga/effects\\";\\nimport appSaga from \\"./app.saga\\";\\n\\nfunction* rootSaga() {\\n yield all([...appSaga]);\\n}\\n\\nexport default rootSaga;\\n\\n\\njavascript\\n// app.reducer.ts\\nimport * as appActions from \\"../actions/app.actions\\";\\n\\nexport default (state: TAppState = initialState, action: AnyAction): TAppState => {\\n switch (action.type) {\\n case appActions.UPDATE_BLOG: {\\n const update = (action as appActions.UpdateBlog).payload;\\n return {\\n ...state, blog: {\\n ...state.blog, ...update\\n }\\n }\\n default:\\n return state;\\n }\\n}\\n\\n\\njavascript\\n// root.reducer.ts\\nimport { combineReducers } from \\"redux\\";\\nimport appReudcer from \\"./app.reducer\\";\\n\\nconst rootReducer =\\n combineReducers <\\n any >\\n {\\n app: appReudcer,\\n };\\n\\nexport default rootReducer;\\n\\n\\nFinally:\\n\\njavascript\\n// store.ts\\nimport { createStore, applyMiddleware } from \\"redux\\";\\nimport { createLogger } from \\"redux-logger\\";\\nimport createSagaMiddleware from \\"redux-saga\\";\\nimport rootReducer from \\"../reducers/root.reducer\\";\\nimport rootSaga from \\"../sagas/root.saga\\";\\n\\nconst logger = createLogger({ collapsed: true });\\nconst sagaMiddleware = createSagaMiddleware();\\n\\nconst store = createStore(rootReducer, applyMiddleware(logger, sagaMiddleware));\\n\\nsagaMiddleware.run(rootSaga);\\n\\nexport default store;\\n\\n\\nNow\\n\\njavascript\\nimport { Provider } from \\"react-redux\\";\\nReactDOM.render(\\n <Provider store={store}>\\n <App />\\n </Provider>,\\n document.getElementById(\\"root\\")\\n);\\n\\n\\nand happy saga!\\n\\n<center>\\n <img src=\\"/assets/tech/02.png\\"/>\\n</center>\\n\\n##### Intercept the Async Actions and Error Handling\\n\\njavascript\\nimport { put, call } from \\"redux-saga/effects\\";\\n\\nexport const safe = (showLoading: boolean, saga: any, ...args: any) =>\\n function* (action: any) {\\n try {\\n if (showLoading) {\\n // yield put(loadingActions.setLoading(true));\\n }\\n console.log(\\"intercepted\\");\\n yield call(saga, ...args, action);\\n } catch (error) {\\n switch (error.request.status) {\\n case 400:\\n console.log(${error.response.status}: Bad Request);\\n break;\\n case 401:\\n break;\\n case 404:\\n console.log(${error.response.status}: Not Found);\\n break;\\n case 405:\\n case 500:\\n console.log(${error.response.status}: Internal Server Error);\\n break;\\n default:\\n console.log(${error.response.status}: Service Unavailable);\\n break;\\n }\\n const response = error.response.data;\\n // yield put(showAlert([response.message], \'error\'));\\n } finally {\\n if (showLoading) {\\n // yield delay(500);\\n // yield put(loadingActions.setLoading(false));\\n }\\n }\\n };\\n\\n\\nIn app.saga.ts now we can replace our controller updateBlog by\\n\\njavascript\\n// app.saga.ts\\nimport { safe } from \\"./middlewares/safe\\";\\n\\nfunction* watchUpdateBlog() {\\n // if true, do whatever you want, like displaying loading popover\\n yield takeEvery(appActions.UPDATE_BLOG, safe(true, updateBlog);\\n}\\n\\n\\nResult:\\n\\n<center>\\n<img src=\\"/assets/tech/03.png\\"/>\\n</center>\\n","title":"On Redux-Saga","date":"2021-07-19T00:00:00.000Z","id":"blog0007","tag":"react","intro":"Record a functional Redux-Saga setup."},{"content":"\\n#### Algorithms\\n\\n##### The Merge Sort\\n\\npython\\ndef merge_sort(arr):\\n if len(arr) == 1:\\n return arr\\n\\n mid_index = int(len(arr)/2)\\n\\n L = arr[0:mid_index]\\n R = arr[mid_index:]\\n\\n L = merge_sort(L)\\n R = merge_sort(R)\\n\\n\\n- The algorithm is locked and looped here until a return is resolved.\\n- When merge_sort(len-1 array)\'s return two sorted arrays to the previous call, L and R then merge into a len-2 array arr, and get returned.\\n- Then the locked merge_sort(len-2 array)s\' call continue, L and R then merge into a sorted len-4 array arr.\\n- Similarly, the locked merge_sort(len-4 array)s\' call return two sorted arrays separately, they then merge into a len-8 sorted array.\\n\\npython\\n head_L, head_R, i = 0, 0, 0\\n\\n while head_L < len(L) and head_R < len(R):\\n if L[head_L] < R[head_R]:\\n arr[i] = L[head_L]\\n head_L += 1\\n else:\\n arr[i] = R[head_R]\\n head_R += 1\\n i += 1\\n\\n while head_L < len(L):\\n arr[i] = L[head_L]\\n head_L += 1\\n i += 1\\n\\n while head_R < len(R):\\n arr[i] = R[head_R]\\n head_R += 1\\n i += 1\\n\\n print(f\\"sorting L:{L} and R:{R}, result:{arr}\\")\\n return arr\\n\\n\\nWe try to sort the following and print log:\\n\\npython\\nto_be_sorted = [2,10,5,100,66,24,27,30]\\nmerge_sort(to_be_sorted)\\n\\n\\nwhich yields\\n\\ntext\\nsorting L:[2] and R:[10], result:[2, 10]\\nsorting L:[5] and R:[100], result:[5, 100]\\nsorting L:[2, 10] and R:[5, 100], result:[2, 5, 10, 100]\\nsorting L:[66] and R:[24], result:[24, 66]\\nsorting L:[27] and R:[30], result:[27, 30]\\nsorting L:[24, 66] and R:[27, 30], result:[24, 27, 30, 66]\\nsorting L:[2, 5, 10, 100] and R:[24, 27, 30, 66], result:[2, 5, 10, 24, 27, 30, 66, 100]\\n\\n\\n##### The Time Complexity\\n\\n<center>\\n <img width=\\"380\\" src=\\"/assets/tech/merge-sort.png\\" />\\n</center>\\n<br/>\\n\\n<center></center>\\n\\nThe algorithm in plain text:\\n\\n- We are bisecting our length-$n$ long array by half until the resulting array has length $1$, which takes $\\\\log_2 n$ steps to complete. For each dividing steps, it takes exactly $n$ operations for cloning elements to the evenly divided pairs of arrays (L and R)\'s. At the end of bisections, we get at most $O(n\\\\log n)$ operations.\\n\\n- Starting from length-1 array, we merge pairs of adjacent arrays in the 3 while loops above, these triples of while loops in total take at most $O(n)$ operations. But it just takes $\\\\log n$ stages of steps to merge from length-1 array to length-$n$ array.\\n\\nTherefore by summing up, the time complexity of merge sort is $O(n\\\\log n)$.\\n","title":"Algorithm: Merge Sort and its Time Complexity","date":"2021-07-25T00:00:00.000Z","id":"blog0009","tag":"algorithm","intro":"Study the implementation of sorting algorithms"},{"content":"\\n#### Exercises\\n\\n##### Exercise 1 ($\\\\bf{h}$-index)\\n\\n> **Problem.** If a scholar has at least $h$ of their papers cited $h$ times, then the $h$-index of a scholar is defined to be $h$. Find $h$.\\n\\npython\\n# Algorithm:\\ndef hIndex(citations):\\n result = 0\\n length = len(citations)\\n n_cites = [0] * (length+1)\\n\\n for c in citations:\\n c_ = min(c, length)\\n n_cites[c_] += 1\\n\\n total = 0\\n for i in range(length, -1, -1):\\n total += n_cites[i]\\n if total == i:\\n result = total\\n break\\n\\nreturn result\\n\\n\\n**Complexities.**\\n\\n- **Time.** The loop for c in citations takes $O(n)$ operations and the loop for i in range(length, -1, -1) also takes $O(n)$ operations, the algorithm in total has time complexity $O(n)$.\\n- **Space.** The space complexity is $O(n)$ as well since we have constructed a new array n_cites.\\n\\n---\\n\\n##### Exercise 2 (Trie)\\n\\nThis exercise presents us a well-known sequential data structure called **Trie**. It is beneficial for us to see it at least once to get motivated for tackling the next similar problems.\\n\\n> **Problem.** Given a list of words, for each word find the shortest unique prefix. You can assume a word will not be a substring of another word (i.e., play and playing won\'t be in the same words list).\\n\\nExample:\\n\\n- Input: [\'james\', \'john\', \'jack\', \'techlead\']\\n- Ouput: [\'jam\', \'jo\',\'jac\', \'t\']\\n\\nWe tackle it by defining our node by:\\n\\npython\\n# each children is identified/pointed by a character\\nclass Node:\\n def __init__(self, char):\\n self.count = 0\\n self.char = char\\n self.children = {}\\n\\n\\npython\\nclass Trie:\\n def __init__(self):\\n self.root = Node(\\"\\")\\n\\n def insert(self, word):\\n curr_node = self.root\\n for c in word:\\n if c not in curr_node.children:\\n curr_node.children[c] = Node(c)\\n curr_node = curr_node.children[c]\\n curr_node.count += 1\\n\\n def unique_prefix(self, word):\\n curr_node = self.root\\n prefix = \\"\\"\\n\\n for c in word:\\n if curr_node.count == 1:\\n return prefix\\n else:\\n curr_node = curr_node.children[c]\\n prefix += curr_node.char\\n return prefix\\n\\n\\nFinally we define\\n\\npython\\ndef shortest_unique_prefix(words):\\n trie = Trie()\\n\\n for word in words:\\n trie.insert(word)\\n\\n unique_prefix = []\\n for word in words:\\n unique_prefix.append(trie.unique_prefix(word))\\n\\n return unique_prefix\\n\\n\\nand\\n\\npython\\nwords = [\'james\', \'john\', \'jack\', \'techlead\']\\nshortest_unique_prefix(words)\\n\\n\\ngives [\'jam\', \'jo\', \'jac\', \'t\'].\\n\\n**Complexities.**\\nDenote $\\\\ell$ the maximum length among all $n$ words, i.e., $\\\\ell=$ max([len(w) for w in words]).\\n\\n- **Time.** For each of $n$ words we spend $O(1)$ operations to go from one node to another, it also takes $O(1)$ operation to check if the current node has count == 1, so in total our time complexity is $O(n\\\\ell)$.\\n- **Space.** In the worst case we insert $n$ distinct words that starts from different characters, therefore the space complexity is $O(n\\\\ell)$.\\n","title":"Exercises on Algorithms","date":"2021-07-26T00:00:00.000Z","id":"blog0010","tag":"algorithm","intro":"A note to solved problems and calculate the related time complexities."},{"content":"\\n#### Special Types\\n\\n##### Infer T from T[]\\n\\ntypescript\\ntype ArrayElement<ArrayType extends readonly unknown[]> =\\n ArrayType extends readonly (infer ElementType)[] ? ElementType : never;\\n\\n\\n##### Combine T1[\\"fields\\"] = T2[] and S1[\\"fields\\"] = S2[] to get T with T[\\"fields\\"] = (T2 & S2)[]\\n\\nThis is a record of real use case in my project:\\n\\nBoth frontend and backend receive csvConfig with type CSVConfig. For code sharing purpose, this type is not repeatedly defined in both front- and back-end separately, rather it is defined in a custom npm-package installed from our local npm-registry.\\n\\nHowever, as business logic grows, there are **_additional_** properties in the ArrayType of CSVConfig[\\"fields\\"] (i.e., the S in S[]) that are redundant to the backend and make sense to the frontend only.\\n\\nWe can of course add additional property in the type definition inside our npm-package project, but why don\'t we just augment our ArrayType of CSVConfig[\\"fields\\"] if these additional properties have nothing to do with the backend? This gets rid of the hessels of npm link and npm unlink --no-save to our npm-package locally (by the way, npm unlink without --no-save can be disastrous).\\n\\nSuppose that the type CSVConfig[\\"fields\\"] is T[]. We want to augment T by intersecting T with the ArrayType (again, that means the S in S[]) of\\n\\ntypescript\\n// not a valid syntax\\nTDataProcessorConfig[\'fields\'] = { defaultValue?: string, processors?: IDataProcessor[] }[].\\n\\n\\nHeuristically we want our augmented fields to be like { [k: keyof T]: T[k], defaultValue?: string, processors?: IDataProcessor[] }[]. Now we can augment our CSVConfig by\\n\\ntypescript\\ntype TCSVConfigFields = ArrayElement<CSVConfig[\\"fields\\"]>;\\ntype TDataProcessorField = ArrayElement<TDataProcessorConfig[\\"fields\\"]>;\\n\\ntype TAugmentedCSVConfig = Omit<CSVConfig, \\"fields\\"> & {\\n fields: (TCSVConfigField & TDataProcessorField)[];\\n};\\n\\n\\nBy experiment Omit is necessary, otherwise for any variable that inherits type TAugmentedCSVConfig, its array element of fields property is still accessible to keys in TCSVConfigField only.\\n\\nIt seems that property\'s type is not overridable by intersection at the moment.\\n","title":"Typescript Type Tricks","date":"2021-07-14T00:00:00.000Z","id":"blog0011","tag":"typescript","intro":"Useful custom type and last resort to get correct type that we may encounter in typescript."},{"content":"\\n##### Procedures\\n\\nGo to https://disqus.com/, click get started, and go along until you get your link to embed disqus\'s javascript into your site, which is something like\\n\\njavascript\\nhttps://short-name.disqus.com/embed.js\\n\\n\\nWe can either embed the comment plug-in manually, or directly install a nice wrapper from <a href=\\"https://www.npmjs.com/package/disqus-react\\">here</a>.\\n\\nIn either case, we need to reload the script in order to get a new thread connecting to disqus whenever we change our article, therefore we need the following:\\n\\ntypescript\\nconst prevId = useRef<string>(\\"\\");\\n\\nuseEffect(() => {\\n const sameAsPrevId = prevId.current === activeArticle?.id;\\n\\n if (sameAsPrevId) {\\n return;\\n }\\n\\n (Window as any).DISQUS?.reset({\\n reload: true,\\n config: function () {\\n this.page.identifier =\\n process.env[\\"REACT_APP_WEB_HOST\\"] + activeArticle.id;\\n this.page.url = process.env[\\"REACT_APP_WEB_HOST\\"] + activeArticle.id;\\n },\\n })(function () {\\n var d = document;\\n var s = d.createElement(\\"script\\");\\n s.src = \\"https://c-c-lee-blog.disqus.com/embed.js\\";\\n s.async = true;\\n s.setAttribute(\\"data-timestamp\\", new Date() + \\"\\");\\n (d.head || d.body).appendChild(s);\\n })();\\n\\n prevId.current = activeArticle.id;\\n}, [activeArticle]);\\n\\n\\nPart of the code is taken directly from the instruction of disqus when you get start.\\n\\n##### Potential Traps\\n\\n_By experiment_, in (Window as any).DISQUS?.reset we need to change both\\n\\n- this.page.identifier and\\n- this.page.url.\\n These parameters are consistent with what we put into the props of DiscussionEmbed:\\n\\ntypescript\\nimport { DiscussionEmbed } from \\"disqus-react\\";\\n\\n<DiscussionEmbed\\n shortname={\\"c-c-lee-blog\\"}\\n config={{\\n url: process.env[\\"REACT_APP_WEB_HOST\\"] + activeArticle.id,\\n identifier: process.env[\\"REACT_APP_WEB_HOST\\"] + activeArticle.id,\\n title: activeArticle.title,\\n language: \\"en_us\\",\\n }}\\n/>;\\n\\n\\nJust need to be careful about the number of times the component rerender, which may be a trap that courses troubles. If it happens that the component rendered twice, problem would occur when the \\"reload\\" action is first dispatched with the old identifier.\\n\\nHappy blogging.\\n\\n##### References\\n\\n- https://stackoverflow.com/questions/8944287/disqus-loading-the-same-comments-for-dynamic-pages\\n","title":"Disqus Comment Plug-in in React","date":"2021-08-12T00:00:00.000Z","id":"blog0015","tag":"react","intro":"An introduction to disqus that plug a small comment box into our website."},{"content":"\\n#### Solutions\\n\\n##### What do we have?\\n\\nThere are two solutions after googling for a while if one insists on using react:\\n\\n- Do **_server side rendering_** using framework like next.js.\\n- **_Pre-render_** our react app into several pieces of rendered static htmls.\\n I would like to talk about the second approach as it just requires a web host be able to serve html files for reader from outside (github is a good choice in this regard combined with <a href=\\"https://www.npmjs.com/package/gh-pages\\">gh-pages</a>) .\\n\\n##### Use Case and Tradeoff\\n\\nThe drawback is glaringly obvious, pre-rendering is an awful choice for highly dynamic pages like a forum. But our page wouldn\'t change frequently (as a personal website), our use case is perfectly fine.\\n\\nBut even our page does change frequently, we can **_selectively_** choose not to pre-render this particular page and let react handle the content in its own way by error handling tricks, but at last such content cannot be easily scrapped.\\n\\n#### Pre-rendering Procedures\\n\\n##### Step 1: Webpack Config via react-app-rewired and customize-cra\\n\\nWe will make use of two npm packages, <a href=\\"https://www.npmjs.com/package/react-app-rewired\\">react-app-rewired </a> and <a href=\\"https://www.npmjs.com/package/customize-cra\\">customize-cra</a>:\\n\\nbash\\nyarn add react-app-rewired customize-cra @types/customize-cra\\n\\n\\n- react-app-rewired helps override our webpack-config **_without ejection_** and;\\n- customize-cra providers us with helper functions to override webpack config via react-app-rewired.\\n\\nNext we need <a href=\\"https://www.npmjs.com/package/prerender-spa-plugin\\">prerender-spa-plugin</a> to decompose our react app into pre-rendered htmls:\\n\\nbash\\nyarn add prerender-spa-plugin\\n\\n\\nCreate config-overrides.js in the root directory, where:\\n\\njavascript\\nconst { override, addWebpackPlugin } = require(\\"customize-cra\\");\\nconst path = require(\\"path\\");\\nconst PrerenderSPAPlugin = require(\\"prerender-spa-plugin\\");\\nconst Renderer = PrerenderSPAPlugin.PuppeteerRenderer;\\n\\nconst preRenderPlugin = new PrerenderSPAPlugin({\\n staticDir: path.join(__dirname, \\"build\\"),\\n indexPath: path.join(__dirname, \\"build\\", \\"index.html\\"),\\n routes: [\\n \\"/\\",\\n \\"/about\\",\\n \\"/skills\\",\\n \\"/experience\\",\\n \\"/portfolio\\",\\n \\"/blog\\",\\n \\"/lang-study\\",\\n \\"/artworks\\",\\n \\"/math-material\\",\\n \\"/contact\\",\\n ],\\n renderer: new Renderer({\\n timeout: 0,\\n maxConcurrentRoutes: 1,\\n renderAfterTime: 5000,\\n headless: false,\\n }),\\n});\\n\\nmodule.exports = override(addWebpackPlugin(preRenderPlugin));\\n\\n\\nHere we use renderer to preview rendered pages for debug purpose. If you are sure your routes for decompoisition are correct, you may skip it by setting renderAfterTime to 0. Removing renderer courses error to me.\\n\\nIn package.json we replace\\n\\njson\\n\\"scripts\\": {\\n ...\\n \\"build\\": \\"react-scripts build\\",\\n },\\n\\n\\nby\\n\\njson\\n\\"scripts\\": {\\n ...\\n \\"build\\": \\"react-app-rewired build\\",\\n },\\n\\n\\n##### Step 2: Decompose React App by Routes\\n\\nIn package.json we add/replace the value of homepage:\\n\\njson\\n{\\n ...,\\n \\"homepage\\": \\"https://machingclee.github.io\\",\\n}\\n\\n\\nthis will provide process.env.PUBLIC_URL a value \\"https://machingclee.github.io\\" when process.env.NODE_ENV === \\"production\\", and otherwise an empty string so that it does nothing in development mode.\\n\\nNext in our routing:\\n\\ntypescript\\nimport { BrowserRouter } from \\"react-router-dom\\";\\n\\nReactDOM.render(\\n <BrowserRouter basename={process.env.PUBLIC_URL + \\"/\\"}>\\n <App />\\n </BrowserRouter>,\\n document.getElementById(\\"root\\")\\n);\\n\\n\\nKeep in mind that hash routing **_will fail_** in pre-rendering process.\\n\\nWhen we yarn build (which runs react-app-rewired build), we get the following structure in our build directory:\\n\\n<center>\\n <a href=\\"/assets/tech/004.png\\"> <img width=\\"460\\" src=\\"/assets/tech/004.png\\"/> </a>\\n</center>\\n\\n##### Step 3: Selective Pre-rendering by Means of 404 Error Handling\\n\\nNotice that /blog and /lang-study have no sub-routings, it will cause 404 error once we navigate to, say, /blog/On-Redux-Saga since /blog/On-Redux-Saga.html does not exist. When 404 error occurs, github would first look for 404.html, we can manipulate this mechanism into rendering our dynamic contents by javascript. Just make an identical copy of index.html in our ./build and name it 404.html:\\n\\njson\\n\\"scripts\\": {\\n ...,\\n \\"build\\": \\"react-app-rewired build && cp build/index.html build/404.html\\n}\\n\\n\\nAnd we are done!\\n\\n##### Step 4: Generate Additional Subroutes (e.g., blog articles)\\n\\nIn my case all my subroutes are based on content in my md file, I can get all routes dynamically by file paths:\\n\\njavascript\\nimport fs from \\"fs\\";\\n\\nconst getAllFiles = (dir: string) => {\\n const mdFiles: string[] = [];\\n\\n const getFiles = (dir: string) => {\\n const paths = fs.readdirSync(dir);\\n paths.forEach((p) => {\\n const newPath = path.join(${dir}/${p});\\n const pathStat = fs.statSync(newPath);\\n if (pathStat.isDirectory()) {\\n getFiles(newPath);\\n } else {\\n if (newPath.endsWith(\\".md\\")) {\\n mdFiles.push(newPath);\\n }\\n }\\n });\\n };\\n\\n getFiles(dir);\\n\\n return mdFiles;\\n};\\n\\n\\nYou can generate your routes in your own case, and concat your additional routes inside config-overrides.\\n\\nIn case you cannot use import statement, at the same level of compilerOptions try to look at tsconfig.json and try to add:\\n\\njson\\n\\"ts-node\\": {\\n \\"compilerOptions\\": {\\n \\"module\\": \\"CommonJS\\"\\n }\\n},\\n\\n\\nMy final addtional routes file is like this, which gets imported from config-overrides for concatenation:\\n\\njson\\n{\\n \\"routes\\": [\\n \\"/blog/Lazy-React-Router\\",\\n \\"/blog/On-Looping-all-Files-in-Frontend\\",\\n \\"/blog/Useful-Conda-Commands\\",\\n \\"/blog/Web-Scrapping-with-Selenium-and-Beautifulsoup-on-Chrome\\",\\n \\"/blog/On-Redux-Saga\\",\\n \\"/blog/Algorithm-Merge-Sort-and-its-Time-Complexity\\",\\n \\"/blog/Exercises-on-Algorithms\\",\\n \\"/blog/Typescript-Type-Tricks\\",\\n \\"/blog/Disqus-Comment-Plug-in-in-React\\",\\n \\"/blog/Make-your-React-App-Scrapable-by-Google-Search-Engine\\",\\n \\"/lang-study/Asmongold-Reacts-to-Preach-Quitting-WoW\\",\\n \\"/lang-study/Asmongold-Reacts-to-Is-FFXIV-Winning-the-MMO-War-By-Zepla\\",\\n \\"/lang-study/Is-WoW-2-Blizzard-s-only-option-in-2021-Asmongold-Reacts-to-Bellular\\",\\n \\"/lang-study/Asmongold-reacts-to-fan-made-memes-Reddit-Recap-33-FFXIV-Special-\\",\\n \\"/lang-study/Asmongold-Reacts-to-WillE-Quitting-WoW-Full-Time-Content-Creation\\",\\n \\"/lang-study/The-Best-MMO-Asmongold-Summit1g-Talk-FFXIV-vs-WoW\\",\\n \\"/lang-study/Asmongold-reacts-to-fan-made-memes-Reddit-Recap-34-FFXIV-Special-\\",\\n \\"/lang-study/Asmongold-Reacts-to-Stoopzz-Quitting-WoW-after-15-Years\\",\\n \\"/lang-study/Asmongold-Reacts-to-FFXIV-s-10-Most-Prestigious-Things-to-FLEX-with-By-Zepla\\",\\n \\"/lang-study/Asmongold-on-Blizzard-Firing-Diablo-4-Game-Director-More\\",\\n \\"/lang-study/Asmongold-Reacts-to-Diablo-2-Resurrected-NEW-CINEMATICS\\",\\n \\"/lang-study/Asmongold-Reacts-to-Most-Popular-Games-2004-2020-\\",\\n \\"/lang-study/Asmongold-Reacts-to-The-Awful-Side-of-FFXIV-\\"\\n ]\\n}\\n\\n\\n#### Results: Evidence Proving Pre-rendering Ocurred\\n\\n##### Show Contents by Pre-rendered Html\\n\\nNavigation to https://machingclee.github.io/skills:\\n\\n <center>\\n <a href=\\"/assets/tech/005.png\\"><img width=\\"460\\" src=\\"/assets/tech/005.png\\"/></a>\\n </center>\\n <br/>\\n which is exactly <a href=\\"https://github.com/machingclee/machingclee.github.io/blob/master/skills/index.html\\"><strong><i>this file</i></strong></a>. Moreover, it becomes a standard react app when user starts to navigate anywhere in the app (no html will be downloaded any more). It can be thought of as having 10 entry points rather than 1 in the past.<br/><br/>\\n\\n##### Show Contents by Javascript Rendering\\n\\nNavigation to https://localhost:3000/skills:\\n\\n <center>\\n <a href=\\"/assets/tech/006.png\\"><img width=\\"460\\" src=\\"/assets/tech/006.png\\"/></a>\\n </center>\\n <br/>\\n\\nwhich is our index.html in build.\\n","title":"Make your React App Scrapable by Google Search Engine","date":"2021-08-14T00:00:00.000Z","id":"blog0016","tags":"react","intro":"SPAs (Single Page App) are known to be unfriendly to search engines and we try to deal with this problem for react app."},{"content":"\\n#### The Entire GAN and DCGAN Script\\n\\n<details>\\n<summary> Simple GAN </summary>\\n\\npython\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.datasets as datasets\\nimport torchvision.transforms as transforms\\nfrom torch.utils.data import DataLoader\\nfrom torch.utils.tensorboard import SummaryWriter\\n\\n\\nclass Discriminator (nn.Module):\\n def __init__(self, img_dim):\\n super().__init__()\\n self.disc = nn.Sequential(\\n nn.Linear(img_dim, 128),\\n nn.LeakyReLU(0.1),\\n nn.Linear(128, 1),\\n nn.Sigmoid()\\n )\\n\\n def forward(self, x):\\n return self.disc(x)\\n\\n\\nclass Generator(nn.Module):\\n def __init__(self, z_dim, img_dim):\\n super().__init__()\\n self.gen = nn.Sequential(\\n nn.Linear(z_dim, 256),\\n nn.LeakyReLU(0.1),\\n nn.Linear(256, img_dim),\\n nn.Tanh()\\n )\\n\\n def forward(self, x):\\n return self.gen(x)\\n\\n\\ndevice = \\"cuda\\" if torch.cuda.is_available() else \\"cpu\\"\\nlr = 3e-4\\nz_dim = 64\\nimg_dim = 28*28*1\\nbatch_size = 32\\nnum_epochs = 50\\n\\n\\n\\ndisc = Discriminator(img_dim).to(device)\\ngen = Generator(z_dim, img_dim).to(device)\\nfixed_noise = torch.randn(batch_size, z_dim).to(device)\\ntransform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Normalize(\\n (0.5,),\\n (0.5,)\\n )\\n])\\ndataset = datasets.MNIST(\\n root=\\"./dataset/\\",\\n transform=transform,\\n download=True\\n)\\nloader= DataLoader(dataset, batch_size, shuffle=True)\\n\\nopt_disc = optim.Adam(disc.parameters(), lr=lr)\\nopt_gen= optim.Adam(gen.parameters(), lr=lr)\\n\\ncriterion = nn.BCELoss()\\nwriter_fake = SummaryWriter(f\\"./runs/GAN_MNIST/fake\\")\\nwriter_real = SummaryWriter(f\\"./runs/GAN_MNIST/real\\")\\nstep = 0\\n\\n\\nfor epoch in range(num_epochs):\\n for batch_idx, (real, _) in enumerate(loader):\\n real = real.view(-1, 784).to(device)\\n batch_size = real.shape[0]\\n\\n noise = torch.randn(batch_size, z_dim).to(device)\\n fake = gen(noise)\\n\\n disc_real = disc(real).view(-1)\\n lossD_real = criterion(disc_real, torch.ones_like(disc_real))\\n\\n # don\'t want opt_disc.step() update fake, so create a detached version fake at this point\\n # also the gradient that is used to update disc has nothing to do with gen\\n # as the graph of fake involve gen, we have to detach fake to avoid affecting gen itself\\n\\n disc_fake = disc(fake.detach()).view(-1)\\n lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\\n\\n lossD = (lossD_real + lossD_fake)/2\\n\\n disc.zero_grad()\\n lossD.backward()\\n opt_disc.step()\\n\\n output = disc(fake).view(-1)\\n lossG = criterion(output, torch.ones_like(output))\\n gen.zero_grad()\\n lossG.backward()\\n opt_gen.step()\\n\\n if batch_idx == 0:\\n print(\\n f\\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\\\\n Loss D: {lossD:.4f}, loss G: {lossG:.4f}\\"\\n )\\n\\n with torch.no_grad():\\n fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\\n data = real.reshape(-1, 1, 28, 28)\\n img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\\n img_grid_real = torchvision.utils.make_grid(data, normalize=True)\\n\\n writer_fake.add_image(\\n \\"Mnist Fake Images\\", img_grid_fake, global_step=step\\n )\\n writer_real.add_image(\\n \\"Mnist Real Images\\", img_grid_real, global_step=step\\n )\\n step += 1\\n\\n\\n</details>\\n\\n<details>\\n<summary>DCGAN</summary>\\n\\npython\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nimport torchvision\\nimport torchvision.datasets as datasets\\nimport torchvision.transforms as transforms\\n\\nfrom torch.utils.data import DataLoader\\nfrom torch.utils.tensorboard import SummaryWriter\\n\\n\\nclass Discriminator(nn.Module):\\n def __init__(self, channels_img, features_d):\\n super(Discriminator, self).__init__()\\n #Input: N x img_channels x 64 x 64\\n self.disc = nn.Sequential(\\n nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\\n # 32 x 32\\n nn.LeakyReLU(0.2),\\n self._block(features_d, features_d*2, 4, 2, 1), # 16x16\\n self._block(features_d*2, features_d*4, 4, 2, 1), # 8x8\\n self._block(features_d*4, features_d*8, 4, 2, 1), # 4x4\\n nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), # 1x1\\n nn.Sigmoid()\\n )\\n\\n\\n def _block(self, in_channels, out_channels, kernel_size, stride, padding):\\n return nn.Sequential(\\n nn.Conv2d(\\n in_channels,\\n out_channels,\\n kernel_size,\\n stride,\\n padding,\\n bias=False\\n ),\\n nn.BatchNorm2d(out_channels),\\n nn.LeakyReLU(0.2)\\n )\\n\\n def forward(self, x):\\n return self.disc(x)\\n\\n\\nclass Generator(nn.Module):\\n def __init__(self, z_dim, channels_img, features_g):\\n super(Generator, self).__init__()\\n # Input: N x z_dim x 1 x 1\\n self.gen = nn.Sequential(\\n self._block(z_dim, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4\\n self._block(features_g*16, features_g * 8, 4, 2, 1), # 8 x 8\\n self._block(features_g*8, features_g * 4, 4, 2, 1), # 16 x 16\\n self._block(features_g*4, features_g * 2, 4, 2, 1), # 32 x 32\\n nn.ConvTranspose2d(\\n features_g*2,\\n channels_img,\\n 4,2,1\\n ), # 64 x 64\\n nn.Tanh()\\n )\\n\\n def _block(self, in_channels, out_channels, kernel_size, stride, padding):\\n return nn.Sequential(\\n nn.ConvTranspose2d(\\n in_channels,\\n out_channels,\\n kernel_size,\\n stride,\\n padding,\\n bias=False\\n ),\\n nn.BatchNorm2d(out_channels),\\n nn.ReLU()\\n )\\n\\n def forward(self, x):\\n return self.gen(x)\\n\\n\\ndef initialize_weights(model):\\n for m in model.modules():\\n if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\\n nn.init.normal_(m.weight.data, 0.0, 0.02)\\n\\n\\ndevice = torch.device(\\"cuda\\") if torch.cuda.is_available() else \\"cpu\\"\\nLEARNING_RATE=2e-4\\nBATCH_SIZE = 128\\nIMAGE_SIZE = 64\\nCHANNELS_IMG = 1\\nZ_DIM = 100\\nNUM_EPOCHS = 5\\nFEATURES_DISC = 64\\nFEATURES_GEN = 64\\n\\n\\ntransform = transforms.Compose(\\n [\\n transforms.Resize(IMAGE_SIZE),\\n transforms.ToTensor(),\\n transforms.Normalize(\\n [0.5 for _ in range(CHANNELS_IMG)],\\n [0.5 for _ in range(CHANNELS_IMG)]\\n )\\n ]\\n)\\n\\ndataset=datasets.MNIST(\\n root=\\"./dataset/\\",\\n train=True,\\n transform=transform,\\n download=True\\n )\\n\\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\\n\\n\\ngen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\\ndisc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\\n\\ninitialize_weights(gen)\\ninitialize_weights(disc)\\n\\nopt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5,0.999))\\nopt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5,0.999))\\ncriterion = nn.BCELoss()\\n\\nfixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\\nwriter_real = SummaryWriter(f\\"./logs/real\\")\\nwriter_fake = SummaryWriter(f\\"./logs/fake\\")\\n\\nstep = 0\\n\\n\\ngen.train()\\ndisc.train()\\n\\n\\nfor epoch in range(NUM_EPOCHS):\\n for batch_idx, (real, _) in enumerate(loader):\\n real = real.to(device)\\n noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(device)\\n fake = gen(noise)\\n\\n disc_real = disc(real).reshape(-1)\\n disc_fake = disc(fake.detach()).reshape(-1)\\n\\n loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\\n loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\\n\\n loss_disc = (loss_disc_fake + loss_disc_real)/2\\n\\n disc.zero_grad()\\n loss_disc.backward()\\n opt_disc.step()\\n\\n output = disc(fake).reshape(-1)\\n loss_gen = criterion(output, torch.ones_like(output))\\n\\n gen.zero_grad()\\n loss_gen.backward()\\n opt_gen.step()\\n\\n\\n\\n if batch_idx % 100 == 0:\\n print(\\n f\\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\\\\n Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\\"\\n )\\n with torch.no_grad():\\n fake = gen(fixed_noise)\\n\\n img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\\n img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\\n\\n writer_fake.add_image(\\n \\"Mnist Fake Images\\", img_grid_fake, global_step=step\\n )\\n writer_real.add_image(\\n \\"Mnist Real Images\\", img_grid_real, global_step=step\\n )\\n step += 1\\n\\n\\n\\n\\n</details>\\n\\n<br/>\\n\\n#### What Happens in the Train Loop\\n\\n##### fake.detach()\\n\\nIn the train loop of our simple GAN:\\n\\npython\\ndisc = Discriminator(img_dim).to(device)\\ngen = Generator(z_dim, img_dim).to(device)\\n\\nopt_disc = optim.Adam(disc.parameters(), lr=lr)\\nopt_gen = optim.Adam(gen.parameters(), lr=lr)\\n\\ncriterion = nn.BCELoss()\\n\\nfor epoch in range(num_epoch):\\n for batch_index, (real, _) in enumerate(loader):\\n real = real.reivew(-1, 784).to(device)\\n batch_size = real.shape[0]\\n\\n noise = torch.randn(batch_size, z_dim).to(device)\\n fake = gen(noise)\\n\\n disc_real = disc(real).view(-1)\\n lossD_real = criterion(disc_real, torch.ones_like(disc_real))\\n\\n disc_fake = disc(fake.detach()).view(-1)\\n lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\\n\\n lossD = (lossD_real + lossD_fake)/2\\n\\n\\n- Note that we have used fake.detach() here. After detaching our parameter/tensor fake will not have .grad and requires_grad=False. Also note that fake.detach() is a new variable, it will not mutate fake.\\n\\n- The main reason for detach() is that when opt_disc.step() get executed, it will update every parameter in the graph for which requires_grad=True. As fake is going to be **_reused_**, plus the gradient that is used to update disc has nothing to do with gen, we therefore create a detached version of fake at this point to avoid affecting gen itself.\\n\\n##### disc.zero_grad(), lossD.backward(), opt_disc.step()\\n\\nAnd we want to discuss the following 3 lines:\\n\\npython\\n disc.zero_grad()\\n lossD.backward()\\n opt_disc.step()\\n\\n\\n- disc.zero_grad(): It sets the .grad of all disc.parameters() to 0.\\n\\n- lossD.backward(): when backward() is executed, every tensor (variable/parameter) that is involved in the calculation of lossD will be assigned a computed derivative. Since lossD is at the top of the computation graph, lossD.grad will be None.\\n\\n What\'s more, lossD_real.grad and lossD_fake.grad are the derivatives\\n\\n $$\\n d\\\\,\\\\text{lossD_real}=\\\\frac{\\\\partial \\\\,\\\\text{lossD}}{\\\\partial\\\\,{\\\\text{lossD_real}}}\\\\quad \\\\text{and}\\n \\\\quad d \\\\,\\\\text{lossD_fake}=\\\\frac{\\\\partial\\\\, \\\\text{lossD} }{\\\\partial \\\\,\\\\text{lossD_fake}}\\n $$\\n\\n respectively. Similarly for a parameter/tensor, param_1, involved in lossD_real will have an assigned value param_1.grad which is $d\\\\,\\\\text{param_1}=\\\\partial \\\\, \\\\text{lossD}/\\\\partial \\\\, \\\\text{param_1}$ and so on and so forth.\\n\\n- opt_disc.step(): When this line is executed, the parameters\' .grad attribute will be used to update the parameter in disc.\\n\\n Recall that since opt_disc = optim.Adam(disc.parameters(), lr=lr), opt_disc possses the references to those parameters.\\n\\n#### Stackoverflow\'s Comments on module.zero_grad(), loss.backward() and optimizer.step()\\n\\n- Post 1 is a numerical explanation of the what\'re happening in .backward() and .step().\\n <details>\\n <summary>details</summary>\\n <br/>\\n\\n Some answers explained well, but I\'d like to give a specific example to explain the mechanism.\\n\\n Suppose we have a function $z = 3 x^2 + y^3$.\\n The updating gradient formula of $z$ w.r.t $x$ and $y$ is:\\n\\n $$\\n \\\\begin{aligned}\\n x&:=x-\\\\alpha \\\\frac{\\\\partial z}{\\\\partial x} = x-\\\\alpha \\\\cdot 6x\\\\\\\\\\n y&:=y-\\\\alpha \\\\frac{\\\\partial z}{\\\\partial y} = y-\\\\alpha \\\\cdot 3y^2\\n \\\\end{aligned}\\n $$\\n\\n Initial values are $x=1$ and $y=2$.\\n\\n python\\n x = torch.tensor([1.0], requires_grad=True)\\n y = torch.tensor([2.0], requires_grad=True)\\n z = 3*x**2+y**3\\n\\n print(\\"x.grad: \\", x.grad)\\n print(\\"y.grad: \\", y.grad)\\n print(\\"z.grad: \\", z.grad)\\n\\n # print result should be:\\n x.grad: None\\n y.grad: None\\n z.grad: None\\n \\n\\n Then calculating the gradient of $x$ and $y$ in current value ($x=1$, $y=2$)\\n\\n $$\\n \\\\begin{aligned}\\n \\\\frac{\\\\partial z}{\\\\partial x}&=6x=6\\\\\\\\\\n \\\\frac{\\\\partial z}{\\\\partial y}&= 3y^2 = 12\\n \\\\end{aligned}\\n $$\\n\\n python\\n # calculate the gradient\\n z.backward()\\n\\n print(\\"x.grad: \\", x.grad)\\n print(\\"y.grad: \\", y.grad)\\n print(\\"z.grad: \\", z.grad)\\n\\n # print result should be:\\n x.grad: tensor([6.])\\n y.grad: tensor([12.])\\n z.grad: None\\n \\n\\n Finally, using SGD optimizer to update the value of x and y according the formula:\\n\\n $$\\n \\\\begin{aligned}\\n x& := x-\\\\alpha \\\\frac{\\\\partial z}{\\\\partial x} = x-0.1\\\\times 6x = 1- 0.1\\\\times 6 = 0.4\\\\\\\\\\n y& := y-\\\\alpha \\\\frac{\\\\partial z}{\\\\partial y} = y - 0.1\\\\times 3y^2 = 2-0.1 \\\\times 12 = 0.8\\n \\\\end{aligned}\\n $$\\n\\n python\\n # create an optimizer, pass x,y as the paramaters to be update, setting the learning rate lr=0.1\\n optimizer = optim.SGD([x, y], lr=0.1)\\n\\n # executing an update step\\n optimizer.step()\\n\\n # print the updated values of x and y\\n print(\\"x:\\", x)\\n print(\\"y:\\", y)\\n\\n # print result should be:\\n x: tensor([0.4000], requires_grad=True)\\n y: tensor([0.8000], requires_grad=True)\\n \\n\\n </details>\\n <br/>\\n\\n- Post 2 discusses how to avoid updating parameter by using .detach().\\n <details>\\n <summary>details</summary>\\n <br/>\\n\\n Let\'s say we defined a model: model, and loss function: criterion and we have the following sequence of steps:\\n\\n python\\n pred = model(input)\\n loss = criterion(pred, true_labels)\\n loss.backward()\\n \\n\\n pred will have an grad_fn attribute, that references a function that created it, and ties it back to the model. Therefore, loss.backward() will have information about the model it is working with.\\n\\n Try removing grad_fn attribute, for example with:\\n\\n python\\n pred = pred.clone().detach()\\n \\n\\n Then the model gradients will be None and consequently weights will not get updated.\\n\\n And the optimizer is tied to the model because we pass model.parameters() when we create the optimizer.\\n </details>\\n <br/>\\n\\n#### Tensorboard\\n\\n##### Command to Look at logs Directory\\n\\nbash\\ntensorboard --logdir ./logs\\n\\n\\n##### Where do logs Come From?\\n\\nWe have imported SummaryWriter:\\n\\npython\\nfrom torch.utils.tensorboard import SummaryWriter\\n\\n\\nWe have also defined separate writers:\\n\\npython\\nwriter_real = SummaryWriter(f\\"./logs/real\\")\\nwriter_fake = SummaryWriter(f\\"./logs/fake\\")\\n\\n\\nWe have run add_image method when batch_idx % 100 == 0:\\n\\npython\\nfor batch_idx, (real, _) in enumerate(loader):\\n ...\\n if batch_idx % 100 == 0:\\n fake = gen(fixed_noise)\\n with torch.no_grad():\\n img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\\n img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\\n\\n writer_fake.add_image(\\n \\"Mnist Fake Images\\", img_grid_fake, global_step=step\\n )\\n writer_real.add_image(\\n \\"Mnist Real Images\\", img_grid_real, global_step=step\\n )\\n\\n step += 1\\n\\n\\nResult:\\n\\n<center>\\n<a href=\\"/assets/tech/007.png\\">\\n<img width=\\"420\\" src=\\"/assets/tech/007.png\\"/>\\n</a>\\n</center>\\n<br/>\\n\\n#### Using 1.65 GB Celebrities Dataset of 202,599 Images\\n\\nWe can download the dataset from kaggle:\\nhttps://www.kaggle.com/dataset/504743cb487a5aed565ce14238c6343b7d650ffd28c071f03f2fd9b25819e6c9\\n\\nNow replace our MNIST dataset by:\\n\\npython\\n# dataset=datasets.MNIST(root=\\"./dataset/\\",train=True,transform=transform,download=True)\\ndataset = datasets.ImageFolder(root=\\"dataset/celeb_dataset/\\", transform=transform)\\nloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\\n\\n\\nand set CHANNELS_IMG = 3.\\n\\n#### References\\n\\n- https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step\\n\\n- https://ithelp.ithome.com.tw/articles/10216440\\n\\n- https://www.youtube.com/watch?v=OljTVUVzPpM\\n\\n- https://www.youtube.com/watch?v=IZtv9s_Wx9I\\n","title":"Understand Pytorch via GAN","date":"2021-08-20T00:00:00.000Z","id":"blog0017","tags":"pytorch","intro":"Understand Pytorch by implementing a simple version of GAN and also a DCGAN."},{"content":"\\n#### tf.data.Dataset\\n\\nStoring a set of images require a lot of memory, but saving a list of strings does not. In tensorflow we can base our pipeline on saving image paths.\\n\\n##### tf.data.Dataset.list_files\\n\\nWe take the horse2zebra dataset as an example.\\n\\npython\\ndataset = tf.data.Dataset.list_files(\\"horse2zebra/*/**\\")\\n\\n\\nWe can use take to get a specific amount of images from dataset generator:\\n\\nbash\\nfor d in dataset.take(1):\\n print(d)\\n# output: tf.Tensor(b\'horse2zebra\\\\\\\\testB\\\\\\\\n02391049_1020.jpg\', shape=(), dtype=string)\\n\\n\\n##### Data Processing with tf.data.Dataset: .map(), .filter(), .cache(), .shuffle(), .batch()\\n\\nMost of the time our dataset is structured as dataset/class1/a.jpg, dataset/class2/b.jpg, ..., so it is much more helpful to transform the list of images into img, label format.\\n\\nWe will construct (generated) batches of tensors (this constitutes our dataset) in the following procedures\\n\\n$$\\n\\\\begin{aligned}\\n&\\\\quad \\\\,\\\\,\\\\,\\\\, \\\\text{image_paths}\\\\\\\\\\n&\\\\to \\\\text{image_path, label tensor}\\\\\\\\\\n&\\\\to \\\\text{image array tensor, label tensor}\\n\\\\end{aligned}\\n$$\\n\\nThese will be done with the help of .map(), which will also be used to handle data processing. For this we define:\\n\\npython\\ndef get_label(file_path):\\n return tf.strings.split(file_path, os.path.sep)[-2]\\n\\ndef path_to_imgLabel(file_path):\\n label = get_label(file_path)\\n img = tf.io.read_file(file_path)\\n img = tf.image.decode_jpeg(img, channels=3)\\n return img, label\\n\\ndef normalize_img(img):\\n img = tf.cast(img, dtype=tf.float32)\\n # Map values in the range [-1, 1]\\n return (img / 127.5) - 1.0\\n\\ndef preprocess_train_image(img):\\n img = tf.image.random_flip_left_right(img)\\n img = tf.image.resize(img, [*orig_img_size])\\n img = tf.image.random_crop(img, size=[*input_img_size])\\n img = normalize_img(img)\\n return img\\n\\n\\nNow we can chain our processing as follows:\\n\\npython\\ndataset = tf.data.Dataset.list_files(\\"horse2zebra/*/**\\").map(path_to_imgLabel)\\n# at this point our dataset generates (img, label)\'s\\n\\ntrain_horses = dataset \\\\\\n.filter(lambda _, label : label == \\"trainA\\") \\\\\\n.map(lambda img, label: img) \\\\ # this line is specific to this file, usually we keep the label\\n.map(preprocess_train_image, num_parallel_calls=autotune) \\\\\\n.cache() \\\\\\n.shuffle(buffer_size) \\\\\\n.batch(batch_size)\\n\\n\\nNow we can run\\n\\npython\\nfor d in train_zebras.take(1):\\n print(d)\\n\\n\\nto check if the batch of data suits our training purpose.\\n\\n#### Graph Plotting by matplotlib.pyplot\\n\\nWe mainly use ax (array of axes)\\n\\npython\\nimport matplotlib.pyplot as plt\\n\\n_, ax = plt.subplots(4, 2, figsize=(10, 15))\\nfor i, samples in enumerate(zip(train_horses.take(4), train_zebras.take(4))):\\n horse = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\\n zebra = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\\n ax[i, 0].imshow(horse)\\n ax[i, 1].imshow(zebra)\\n\\nplt.show()\\n\\n\\nResult: <br/>\\n\\n<img width=\\"420\\" src=\\"/assets/tech/008.png\\"/>\\n<br/><br/>\\n\\n#### ReflectionPadding2D\\n\\nThis is a built-in layer in pyTorch, but in tensorflow we need to built it manually:\\n\\npython\\nclass ReflectionPadding2D(layers.Layer):\\n def __init__(self, padding=(1, 1), **kwargs):\\n self.padding = tuple(padding)\\n super(ReflectionPadding2D, self).__init__(**kwargs)\\n\\n def call(self, input_tensor, mask=None):\\n padding_width, padding_height = self.padding\\n # no padding for batch_size and channel axis\\n padding_tensor = [\\n [0, 0],\\n [padding_height, padding_height],\\n [padding_width, padding_width],\\n [0, 0],\\n ]\\n return tf.pad(input_tensor, padding_tensor, mode=\\"REFLECT\\")\\n\\n\\n#### Residual Blocks\\n\\nThere are many version of residue blocks. In cycleGAN their blocks keep the number of filters and also the spatial dimension. In some other cases the middle Conv2D layer shrinks the filter depth and finally restored it for the addition operation in skip connection.\\n\\npython\\ndef residual_block(\\n x,\\n activation,\\n kernel_initializer=kernel_init,\\n kernel_size=(3, 3),\\n strides=(1, 1),\\n padding=\\"valid\\",\\n gamma_initializer=gamma_init,\\n use_bias=False,\\n):\\n dim = x.shape[-1]\\n input_tensor = x\\n\\n x = ReflectionPadding2D()(input_tensor)\\n x = layers.Conv2D(\\n dim,\\n kernel_size,\\n strides=strides,\\n kernel_initializer=kernel_initializer,\\n padding=padding,\\n use_bias=use_bias,\\n )(x)\\n x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\\n x = activation(x)\\n\\n x = ReflectionPadding2D()(x)\\n x = layers.Conv2D(\\n dim,\\n kernel_size,\\n strides=strides,\\n kernel_initializer=kernel_initializer,\\n padding=padding,\\n use_bias=use_bias,\\n )(x)\\n x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\\n x = layers.add([input_tensor, x])\\n return x\\n\\n\\nThough the kenrnel_size is (3,3), but there will be a ReflectionPadding2D and therefore the finally the width\\n\\n$$\\n[\\\\text{width} -3 + 1] + 2 = \\\\text{width}\\n$$\\n\\nremains unchanged ($+2$ comes from padding), and so does the height.\\n\\n#### Model.compile() and Model.train_step()\\n\\nIn GANs most of the last training step must be implemented manually. We record the one in cycleGAN for reference. The technique will apply to all other custom training.\\n\\nNote the analogy with PyTorch\'s .backward() and .step().\\n\\npython\\nclass CycleGan(keras.Model):\\n def __init__(\\n self,\\n generator_G,\\n generator_F,\\n discriminator_X,\\n discriminator_Y,\\n lambda_cycle=10.0,\\n lambda_identity=0.5,\\n ):\\n super(CycleGan, self).__init__()\\n self.gen_G = generator_G\\n self.gen_F = generator_F\\n self.disc_X = discriminator_X\\n self.disc_Y = discriminator_Y\\n self.lambda_cycle = lambda_cycle\\n self.lambda_identity = lambda_identity\\n\\n def compile(\\n self,\\n gen_G_optimizer,\\n gen_F_optimizer,\\n disc_X_optimizer,\\n disc_Y_optimizer,\\n gen_loss_fn,\\n disc_loss_fn,\\n ):\\n super(CycleGan, self).compile()\\n self.gen_G_optimizer = gen_G_optimizer\\n self.gen_F_optimizer = gen_F_optimizer\\n self.disc_X_optimizer = disc_X_optimizer\\n self.disc_Y_optimizer = disc_Y_optimizer\\n self.bv = gen_loss_fn\\n self.discriminator_loss_fn = disc_loss_fn\\n self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\\n self.identity_loss_fn = keras.losses.MeanAbsoluteError()\\n\\n def train_step(self, batch_data):\\n # x is Horse and y is zebra\\n real_x, real_y = batch_data\\n\\n # For CycleGAN, we need to calculate different\\n # kinds of losses for the generators and discriminators.\\n # We will perform the following steps here:\\n #\\n # 1. Pass real images through the generators and get the generated images\\n # 2. Pass the generated images back to the generators to check if we\\n # we can predict the original image from the generated image.\\n # 3. Do an identity mapping of the real images using the generators.\\n # 4. Pass the generated images in 1) to the corresponding discriminators.\\n # 5. Calculate the generators total loss (adverserial + cycle + identity)\\n # 6. Calculate the discriminators loss\\n # 7. Update the weights of the generators\\n # 8. Update the weights of the discriminators\\n # 9. Return the losses in a dictionary\\n\\n with tf.GradientTape(persistent=True) as tape:\\n # Horse to fake zebra\\n fake_y = self.gen_G(real_x, training=True)\\n # Zebra to fake horse -> y2x\\n fake_x = self.gen_F(real_y, training=True)\\n\\n # Cycle (Horse to fake zebra to fake horse): x -> y -> x\\n cycled_x = self.gen_F(fake_y, training=True)\\n # Cycle (Zebra to fake horse to fake zebra) y -> x -> y\\n cycled_y = self.gen_G(fake_x, training=True)\\n\\n # Identity mapping\\n # expect/hope that G|_{zebras} = id and F|_{horses} = idm\\n # i.e., almost no change\\n same_x = self.gen_F(real_x, training=True)\\n same_y = self.gen_G(real_y, training=True)\\n\\n # Discriminator output\\n disc_real_x = self.disc_X(real_x, training=True)\\n disc_fake_x = self.disc_X(fake_x, training=True)\\n\\n disc_real_y = self.disc_Y(real_y, training=True)\\n disc_fake_y = self.disc_Y(fake_y, training=True)\\n\\n # Generator adverserial loss\\n gen_G_loss = self.generator_loss_fn(disc_fake_y)\\n gen_F_loss = self.generator_loss_fn(disc_fake_x)\\n\\n # Generator cycle loss\\n cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\\n cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\\n\\n # Generator identity loss\\n id_loss_G = (\\n self.identity_loss_fn(real_y, same_y)\\n * self.lambda_cycle\\n * self.lambda_identity\\n )\\n id_loss_F = (\\n self.identity_loss_fn(real_x, same_x)\\n * self.lambda_cycle\\n * self.lambda_identity\\n )\\n\\n # Total generator loss\\n total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\\n total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\\n\\n # Discriminator loss\\n disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\\n disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\\n\\n # Get the gradients for the generators\\n # loss.backward() as in pyTorch\\n grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\\n grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\\n\\n # Get the gradients for the discriminators\\n disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\\n disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\\n\\n # Update the weights of the generators\\n # optimizer.step() as in pyTorch\\n self.gen_G_optimizer.apply_gradients(\\n zip(grads_G, self.gen_G.trainable_variables)\\n )\\n self.gen_F_optimizer.apply_gradients(\\n zip(grads_F, self.gen_F.trainable_variables)\\n )\\n\\n # Update the weights of the discriminators\\n self.disc_X_optimizer.apply_gradients(\\n zip(disc_X_grads, self.disc_X.trainable_variables)\\n )\\n self.disc_Y_optimizer.apply_gradients(\\n zip(disc_Y_grads, self.disc_Y.trainable_variables)\\n )\\n\\n # Conclusion: the only difference to pytorch is that we need to\\n # wrap the calculation of loss to get the calculation graph\\n # i.e., wrap the stuff whose weight needs to be updated.\\n # which usually starts from the beginning of getting batch_data\\n\\n return {\\n \\"G_loss\\": total_loss_G,\\n \\"F_loss\\": total_loss_F,\\n \\"D_X_loss\\": disc_X_loss,\\n \\"D_Y_loss\\": disc_Y_loss,\\n }\\n\\n\\n#### Monitor\\n\\npython\\nclass GANMonitor(keras.callbacks.Callback):\\n \\"\\"\\"A callback to generate and save images after each epoch\\"\\"\\"\\n\\n def __init__(self, num_img=4):\\n self.num_img = num_img\\n\\n def on_epoch_end(self, epoch, logs=None):\\n _, ax = plt.subplots(4, 2, figsize=(12, 12))\\n for i, img in enumerate(test_horses.take(self.num_img)):\\n prediction = self.model.gen_G(img)[0].numpy()\\n prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\\n img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\\n\\n ax[i, 0].imshow(img)\\n ax[i, 1].imshow(prediction)\\n ax[i, 0].set_title(\\"Input image\\")\\n ax[i, 1].set_title(\\"Translated image\\")\\n ax[i, 0].axis(\\"off\\")\\n ax[i, 1].axis(\\"off\\")\\n\\n prediction = keras.preprocessing.image.array_to_img(prediction)\\n prediction.save(\\n \\"generated_img_{i}_{epoch}.png\\".format(i=i, epoch=epoch + 1)\\n )\\n plt.show()\\n plt.close()\\n\\n\\n#### Start the Training Process\\n\\nOur gen_G and gen_F only need optimizer, we calculate the loss on our own. We also pass tensorflow built-in loss functions for convenience (which will turns out to be one of the summands in our total loss).\\n\\npython\\n# Loss function for evaluating adversarial loss\\nadv_loss_fn = keras.losses.MeanSquaredError()\\n\\n# Define the loss function for the generators\\ndef generator_loss_fn(fake):\\n fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\\n return fake_loss\\n\\n# Define the loss function for the discriminators\\ndef discriminator_loss_fn(real, fake):\\n real_loss = adv_loss_fn(tf.ones_like(real), real)\\n fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\\n return (real_loss + fake_loss) * 0.5\\n\\n# Create cycle gan model\\ncycle_gan_model = CycleGan(\\n generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\\n)\\n\\n# Compile the model\\ncycle_gan_model.compile(\\n gen_G_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\\n gen_F_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\\n disc_X_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\\n disc_Y_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\\n gen_loss_fn=generator_loss_fn,\\n disc_loss_fn=discriminator_loss_fn,\\n)\\n\\n# Callbacks\\nplotter = GANMonitor()\\ncheckpoint_filepath = \\"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}\\"\\nmodel_checkpoint_callback = keras.callbacks.ModelCheckpoint(\\n filepath=checkpoint_filepath\\n)\\n\\n# Here we will train the model for just one epoch as each epoch takes around\\n# 7 minutes on a single P100 backed machine.\\ncycle_gan_model.fit(\\n tf.data.Dataset.zip((train_horses, train_zebras)),\\n epochs=1,\\n callbacks=[plotter, model_checkpoint_callback],\\n)\\n\\n\\n#### References\\n\\n- https://keras.io/examples/generative/cyclegan/\\n","title":"Dataset Pipeline and Custom Training in Tensorflow","date":"2021-08-26T00:00:00.000Z","id":"blog0018","tags":"tensorflow","intro":"Revist Tensorflow with a complete dataset pipeline using tf.data.Dataset and custom training via cycle-GAN."},{"content":"\\n#### Google Drive Mounting\\n\\npython\\nfrom google.colab import drive\\ndrive.mount(\'/content/drive\')\\n\\n\\nNow we can work on colab as in we are working in local environment.\\n\\n#### Unzip Files\\n\\nNow you can cd into your working directory. For me:\\n\\npython\\n! cd \'/content/drive/My Drive/Colab Notebooks/cycleGAN-tensorflow\'\\n\\n\\nInside cycleGAN-tensorflow I have a compressed file called horse2zebra.zip. We can now unzip it by\\n\\npython\\n!unzip horse2zebra.zip\\n\\n\\nwhich will create a new folder called horse2zebra and decompress your files into it.\\n","title":"Colab Setting","date":"2021-08-27T00:00:00.000Z","id":"blog0019","tags":"coding, deep-learning, python","intro":"Basic command needed to mount a google drive and also to unzip compressed large dataset."},{"content":"\\nThis is simply horrible, just go to https://react-icons.github.io/react-icons and install it.\\n","title":"Comprehensive List of SVG Icon Available in React","date":"2021-08-27T00:00:00.000Z","id":"blog0020","tags":"react","intro":"Record a useful package that contains a comprehensive list of svg icons.","toc":false},{"content":"\\n#### Callbacks\\n\\nBoth callbacks below accept a startAt and step as keyword arguments. I will adjust when I start to adjust the training process (step=2 when I adjust the learning the first time, startAt will be the epoch that I want to base on).\\n\\n##### EpochCheckpoint\\n\\nThis is to save the model on every end of epoch (modulo some constant, say 5).\\n\\npython\\nfrom tensorflow.keras.callbacks import Callback\\n\\nclass EpochCheckpoint(Callback):\\n def __init__(self, output_dir, step=1 , every=1, startAt=0, model_title=\\"prediction-model\\"):\\n super(Callback, self).__init__()\\n self.step = step\\n self.output_dir = output_dir\\n self.every = every\\n self.intEpoch = startAt\\n self.model_title = model_title\\n\\n def on_epoch_end(self, epoch, logs={}):\\n if (self.intEpoch + 1) % self.every == 0:\\n p = os.path.sep.join([self.output_dir, self.model_title + \\"epoch-{}-{}.hdf5\\".format(self.step, self.intEpoch + 1)])\\n self.model.save(p, overwrite=True)\\n\\n self.intEpoch += 1\\n\\n\\n##### TrainingMonitorCallback\\n\\ntensorboard would be more helpful, but one cannot use tensorboard if the training process is held in colab. For me I would generalize the approach by using this monitor callback.\\n\\nTwo functionalities for this callback:\\n\\n- In this callback we will save a figure for each epoch to track how acc, val_acc, loss and val_loss differs when epoch increases.\\n- A complete log of this values will be stored in a json file stored in jsonPath.\\n\\npython\\nfrom tensorflow.keras.callbacks import BaseLogger\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport json\\nimport os\\nimport tensorflow.keras.backend as K\\n\\nclass TrainingMonitorCallback(BaseLogger):\\n def __init__(self, fig_dir, step=1, jsonPath=None, startAt=0):\\n super(TrainingMonitorCallback, self).__init__()\\n self.fig_dir = fig_dir\\n self.jsonPath = jsonPath\\n self.startAt = startAt\\n self.step=step\\n\\n def on_train_begin(self, logs={}):\\n self.H = {}\\n if self.jsonPath is not None:\\n if os.path.exists(self.jsonPath):\\n self.H = json.loads(open(self.jsonPath).read())\\n\\n if self.startAt > 0:\\n for k in self.H.keys():\\n self.H[k] = self.H[k][:self.startAt]\\n\\n def on_epoch_end(self, epoch, logs={}):\\n print(\\"[INFO] learning rate: {}\\".format(K.get_value(self.model.optimizer.lr)))\\n for (k, v) in logs.items():\\n l = self.H.get(k, [])\\n l.append(float(v))\\n self.H[k] = l\\n\\n if self.jsonPath is not None:\\n f = open(self.jsonPath, \\"w\\")\\n f.write(json.dumps(self.H, indent=4))\\n f.close()\\n\\n if len(self.H[\\"loss\\"]) > 0:\\n epoch = len(self.H[\\"loss\\"])\\n N = np.arange(0, len(self.H[\\"loss\\"]))\\n plt.style.use(\\"ggplot\\")\\n plt.figure()\\n _, ax1 = plt.subplots()\\n l1, = ax1.plot(N, self.H[\\"loss\\"], label=\\"train_loss\\", color=\\"olive\\")\\n l2, = ax1.plot(N, self.H[\\"val_loss\\"], label=\\"val_loss\\", color=\\"red\\")\\n ax2 = ax1.twinx()\\n l3, = ax2.plot(N, self.H[\\"accuracy\\"], label=\\"train_acc\\", color=\\"royalblue\\")\\n l4, = ax2.plot(N, self.H[\\"val_accuracy\\"], label=\\"val_acc\\", color=\\"midnightblue\\")\\n plt.title(\\"Training Loss and Accuracy [Epoch {}]\\".format(len(self.H[\\"loss\\"])))\\n plt.xlabel(\\"Epoch #\\")\\n plt.ylabel(\\"Loss/Accuracy\\")\\n\\n plt.legend([l1,l2,l3,l4], [\\"train_loss\\", \\"val_loss\\", \\"train_acc\\", \\"val_acc\\"], loc=\'lower left\')\\n\\n # save the figure\\n plt.savefig(f\\"{self.fig_dir}/epoch-{self.step}-{epoch}.png\\")\\n plt.close()\\n\\n\\n##### Usage\\n\\nWe define\\n\\npython\\ncallbacks = [\\n EpochCheckpoint(output_dir=\\"./checkpoints\\"),\\n TrainingMonitorCallback(\\n fig_dir=\\"./checkpoints-figure\\",\\n jsonPath=\\"./checkpoints-json/loss.json\\"\\n )\\n]\\n\\n\\nand plug it in the model.fit\'s keyward argument, callbacks.\\n\\nFor example:\\n\\npython\\nmodel.fit(\\n train_dataset,\\n steps_per_epoch=len(train_dataset),\\n epochs=50,\\n validation_data=val_dataset,\\n validation_steps=len(val_dataset),\\n callbacks=callbacks\\n)\\n\\n\\nResult on every end of epoches:\\n\\n<img width=\\"420\\" src=\\"/assets/tech/009.png\\"/>\\n<br /><br />\\n\\n#### Restart Training Based on Past Epoch\\n\\nWhen monitoring our training process, at some point the loss and val_loss will diverge or both do not decrease. Then we can interrupt the process by control + c.\\n\\nWe load our model/model_weight using a specific path (that is stored by using our checkpoint callback). We also adjust the learning rate when we find our training becomes stagnant.\\n\\nBoth mentioned callbacks accept step and startAt as their kwargs in the constructor. For example, my prev_model_path below means (manual adjustment) step=3 and epoch=35 accumulatively (this is the total number of epoches in the training, never reset to 0 for new step).\\n\\nSo when our next epoch is completed, the new saved model will be 4-36.hdf5.\\n\\npython\\nfrom tensorflow.keras.models import load_model\\n\\nprev_model_path = \\"./checkpoints/prediction-modelepoch-3-35.hdf5\\"\\nstart_at_epoch = 35\\nnew_lr = 1e-4\\nstep = 4\\n\\nmodel = load_model(prev_model_path)\\nprint(\\"[INFO] step: {}, start at epoch: {}\\".format(step, start_at_epoch))\\nprint(\\"[INFO] old learning rate: {}\\".format(K.get_value(model.optimizer.lr)))\\nK.set_value(model.optimizer.lr, new_lr)\\nprint(\\"[INFO] new learning rate: {}\\".format(K.get_value(model.optimizer.lr)))\\n\\ncallbacks = [\\n EpochCheckpoint(output_dir=\\"./checkpoints\\", step=step, startAt=start_at_epoch),\\n TrainingMonitorCallback(\\n fig_dir=\\"./checkpoints-figure\\",\\n jsonPath=\\"./checkpoints-json/loss.json\\",\\n startAt=start_at_epoch,\\n step=step\\n )\\n]\\n\\nmodel.fit(\\n train_dataset,\\n steps_per_epoch=len(train_dataset),\\n epochs=50,\\n validation_data=val_dataset,\\n validation_steps=len(val_dataset),\\n callbacks=callbacks\\n)\\n\\n","title":"Tensorflow Callbacks and Restart Training Process Based on Past Epoches","date":"2021-08-28T00:00:00.000Z","id":"blog0021","tags":"tensorflow","intro":"Introduce useful callbacks that I use in monitoring training process. Also introduce how to retrain the model from a specific epoach."},{"content":"\\n#### Preprocessing\\n\\nAs a usual practice every classifier starts with pre-processing the dataset for normalization so that\\n\\n- the model needs not to learn the distribution of specific feature among the dataset and;\\n- focus on just learning the features.\\n\\nTo make use of VGG-16 we need to carry out exactly the same data pre-processing:\\n\\npython\\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\\n\\n\\nwe also import the following as a routine:\\n\\npython\\nimport tensorflow as tf\\n\\nfrom tensorflow.keras import optimizers as optim\\nfrom tensorflow.keras import losses\\nfrom tensorflow.keras import metrics\\nfrom tensorflow.keras.applications import VGG16\\nfrom tensorflow.keras import Input\\nfrom tensorflow.keras import layers\\nfrom tensorflow.keras.models import Model\\n\\n\\n#### Dataset Pipeline with VGG\'s preprocess_input\\n\\nIn the sequel our label will be an nonnegative integer.\\n\\nAs we have no incentive to construct one-hot vectors as labels, we will be using sparse-categorical-entropy loss.\\n\\npython\\ndef get_label(file_path):\\n return tf.strings.split(file_path, os.path.sep)[-2]\\n\\ndef path_to_imgLabel(file_path):\\n label = tf.cast(tf.strings.to_number(get_label(file_path)), dtype=tf.int32)\\n img = tf.io.read_file(file_path)\\n img = tf.image.decode_jpeg(img, channels=3)\\n return img, label\\n\\ndef preprocess_train_image(img, label):\\n img = tf.image.resize(img, (224, 224))\\n img = preprocess_input(img)\\n return img, label\\n\\ntrain_dataset = tf.data.Dataset.list_files(\\"./trainingset/*/**\\") \\\\\\n.map(path_to_imgLabel) \\\\\\n.map(preprocess_train_image, num_parallel_calls=autotune) \\\\\\n.cache() \\\\\\n.shuffle(buffer_size) \\\\\\n.batch(batch_size)\\n\\nval_dataset = tf.data.Dataset.list_files(\\"./validationset/*/**\\") \\\\\\n.map(path_to_imgLabel) \\\\\\n.map(preprocess_train_image, num_parallel_calls=autotune) \\\\\\n.cache() \\\\\\n.batch(batch_size)\\n\\n\\n#### Network Surgery\\n\\n##### Objective\\n\\nSuppose that I need to classify a dataset into 544 labels, then I want to make use of the features extracted by VGG-16, flatten and classify these features by our fully connected layers.\\n\\n##### Implementation\\n\\nWe start off by constructing VGG16 model without head:\\n\\npython\\nvgg_feature_model = VGG16(\\n weights=\\"imagenet\\",\\n include_top=False,\\n input_tensor=Input((224,224,3))\\n )\\n\\n\\nAs inspected from vgg_feature_model.summary() the VGG model accepts inputs of shape (None, 224, 224, 3). This is why we define vgg_feature_model this way.\\n\\nvgg_feature_model in fact accepts a input tensor of flexible shapes in its input_tensor arguement. Sometimes we may also want\\n\\npython\\nvgg_feature_model = VGG16(..., input_tensor=Input((256,256,3)))\\n\\n\\nbecause (256, 256) is a common image size in datasets, as long as the image size is not too far from (224, 224) we are fine.\\n\\nNext we define our feed-forward network for classification:\\n\\npython\\ndef forward_to_head(feature_model):\\n feature = feature_model.output\\n feature = layers.Flatten()(feature)\\n\\n head = layers.Dense(1024)(feature)\\n head = layers.Dropout(0.5)(head)\\n\\n head = layers.Dense(544)(head)\\n head = layers.Softmax()(head)\\n\\n return head\\n\\noutput = forward_to_head(vgg_feature_model)\\nmodel = Model(vgg_feature_model.input, output)\\n\\n\\n##### Start the Training\\n\\nSince our feed-forward network has no trained weights on any image data, there is a huge imbalance between the performance of VGG-16 and that of our network. For better result, we first freeze the training parameters:\\n\\npython\\nfor layer in vgg_feature_model.layers:\\n layer.trainable = False\\n\\n\\nand warm-up our dense network:\\n\\npython\\nmodel.compile(\\n optimizer=optim.Adam(learning_rate=1e-2, global_clipnorm=1),\\n loss=\\"sparse_categorical_crossentropy\\",\\n metrics=[\\"accuracy\\"]\\n)\\n\\nmodel.fit(\\n train_dataset,\\n steps_per_epoch=len(train_dataset),\\n epochs=50,\\n validation_data=val_dataset,\\n validation_steps=len(val_dataset),\\n callbacks=callbacks\\n)\\n\\n\\nWe would not expect very good result at this point, undesired phenomenon would arise such as increasing losses or stagnant accuracies.\\n\\nAfter a few epoches (the \\"few\\" is also a hyper-parameter for us to figure out). We can stop it by control + c and:\\n\\npython\\nfor layer in vgg_feature_model.layers:\\n layer.trainable = True\\n\\nmodel.fit(\\n train_dataset,\\n steps_per_epoch=len(train_dataset),\\n epochs=50,\\n validation_data=val_dataset,\\n validation_steps=len(val_dataset),\\n callbacks=callbacks\\n)\\n\\n","title":"Transfer Learning Based on VGG-16","date":"2021-08-28T00:00:00.000Z","id":"blog0022","tags":"deep-learning, tensorflow","intro":"In classification tasks there are already state-of-the-art models trained from a myriad of images. We try to make a network surgery on one of them (VGG-16 this time) to quickly classifiy our custom dataset with good result."},{"content":"\\n#### Define Our Mapping Functions to Tensorflow Dataset\\n\\n##### Image path to (img, label) format\\n\\nWe start from pre-processing functions that handle the validation dataset and also arbitrary images with which we will feed into our model:\\n\\npython\\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\\n\\nautotune = tf.data.experimental.AUTOTUNE\\n\\ndef get_label(file_path):\\n return tf.strings.split(file_path, os.path.sep)[-2]\\n\\ndef path_to_imgLabel(file_path):\\n label = tf.cast(tf.strings.to_number(get_label(file_path)), dtype=tf.int32)\\n img = tf.io.read_file(file_path)\\n img = tf.image.decode_jpeg(img, channels=3)\\n return img, label\\n\\ndef preprocess_train_image(img, label):\\n img = tf.image.resize(img, (vgg_img_size, vgg_img_size))\\n img = preprocess_input(img)\\n return img, label\\n\\n\\n##### Image Augmentations I - The Basic Pipeline\\n\\nSince tf.imaga can just provide us a limited amounts of augmentations to our input data, like tf.image.random_flip_left_right, for a richer resouce of augmentation we try to import albumentations library:\\n\\npython\\nfrom albumentations import (\\n Compose,\\n RandomBrightnessContrast,\\n ImageCompression,\\n HueSaturationValue,\\n HorizontalFlip,\\n Rotate\\n)\\n\\n\\nand define our transforms that will be applied to images when we take a batch of images from generator:\\n\\npython\\ntransforms = Compose([\\n Rotate(limit=40),\\n RandomBrightnessContrast(p=0.5),\\n ImageCompression(quality_lower=85, quality_upper=100, p=0.5),\\n HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\\n HorizontalFlip()\\n])\\n\\n\\nWe define the data augmentation function (the last line, image_augmentation) that will take img, label as arguments:\\n\\npython\\nfrom functools import partial\\n\\ndef aug_fn(image, img_size):\\n data = {\\"image\\": image}\\n aug_data = transforms(**data)\\n aug_img = aug_data[\\"image\\"]\\n aug_img = tf.image.resize(aug_img, size=[img_size, img_size])\\n return aug_img\\n\\ndef image_augmentation(image, label, img_size):\\n aug_img = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)\\n img_shape=(img_size, img_size, 3)\\n aug_img.set_shape(img_shape)\\n return aug_img, label\\n\\nimage_augmentation = partial(image_augmentation, img_size=vgg_img_size)\\n\\n\\n##### Image Augmentations II - Same Augmentations to a pair of Images\\n\\nIn image segmentations our image augmentations have to be carried out equally to both the images and masks.\\n\\nSuppose our image training dataset, ./cars, has a associated mask dataset, ./mask, we pair them as follows:\\n\\npython\\ndataset = tf.data.Dataset.list_files(\\"./cars/*\\")\\n\\ndef path_to_imgLabel(file_path):\\n mask_filepath = tf.strings.regex_replace(file_path, \\"cars\\", \\"masks\\" )\\n mask_filepath = tf.strings.regex_replace(mask_filepath, \\".jpg\\", \\"_mask.gif\\" )\\n\\n img = tf.io.read_file(file_path)\\n img = tf.image.decode_jpeg(img)\\n\\n img = tf.image.resize(img, (img_size, img_size))\\n\\n mask = tf.io.read_file(mask_filepath)\\n mask = tf.image.decode_gif(mask)\\n mask = tf.image.resize(mask, (img_size, img_size))\\n mask = tf.reshape(mask, (img_size, img_size, 3))\\n mask = tf.reduce_mean(mask, axis=-1, keepdims=True)\\n\\n return img, mask\\n\\ntrain_data = dataset.map(path_to_imgLabel)\\n\\n\\nNext we apply color-invariant augmentations to the pair (img, mask)\'s and color-related augmentation to img\'s, for that we define two transformations:\\n\\npython\\ntransforms_general = Compose(\\n [\\n Rotate(limit=40),\\n HorizontalFlip(),\\n ],\\n additional_targets={\'mask0\': \'image\'}\\n)\\n\\ntransform_color = Compose(\\n [\\n RandomBrightnessContrast(p=0.5),\\n ImageCompression(quality_lower=85, quality_upper=100, p=0.5),\\n HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\\n ]\\n)\\n\\n\\nthey kwarg additional_targets={\'mask0\': \'image\'} means that mask0 will share the same set of transformation with image, and after the transformation we can retrieve the transformed mask by the key mask0.\\n\\n**_Note the tricky part here_**. It might be tempting to define additional_targets={\'mask\': \'image\'}, but unfortunately there are 4 reserved keywords that cannot be used as a key in the dictionary for additional_targets.\\n\\nFrom <a href=\\"https://albumentations.ai/docs/examples/example_multi_target/\\">documentation</a> these are image, mask, bboxes and keypoints.\\n\\nNow we define the next pair of functions which bring the transformations above into play:\\n\\npython\\ndef aug_fn(image, mask):\\n transformed = transforms_general(image=image, mask0=mask)\\n image = transformed[\'image\']\\n mask = transformed[\'mask0\']\\n\\n aug_data = transform_color(image=image)\\n image = aug_data[\'image\']\\n return image, mask\\n\\ndef image_augmentation(image, mask):\\n transformed = tf.numpy_function(func=aug_fn, inp=[image, mask], Tout=[tf.float32, tf.float32])\\n aug_img = transformed[0]\\n aug_mask = transformed[1]\\n aug_img.set_shape((img_size, img_size, 3))\\n aug_mask.set_shape((img_size, img_size, 1))\\n\\n return aug_img, aug_mask\\n\\n\\nBeware of the argument Tout above, it has to be a list that specifies the data type of the output. Otherwise OperatorNotAllowedInGraphError would occur since the function has no idea what is the data type that tf.numpy_function will return.\\n\\n#### Finish the Pipeline\\n\\nThe strategy of our complete dataset pipeline is to:\\n\\n1. take path to img, label;\\n2. then map the images by functions taking img, label to img, label multiple times, which depends on the number of mapping we need.\\n\\n For example, in the chain of .map(image_augmentation).map(preprocess_train_image) below we are free to remove the augmentation part and leave preprocess_train_image alone (which is what it is initially when there is no augmentation implemented).\\n\\npython\\ntrain_dataset = tf.data.Dataset.list_files(\\"./train_dataset/*/**\\") \\\\\\n.map(path_to_imgLabel) \\\\\\n.map(image_augmentation, num_parallel_calls=autotune).prefetch(autotune) \\\\\\n.map(preprocess_train_image, num_parallel_calls=autotune) \\\\\\n.cache() \\\\\\n.shuffle(buffer_size) \\\\\\n.batch(batch_size)\\n\\nval_dataset = tf.data.Dataset.list_files(\\"./val_dataset/*/**\\") \\\\\\n.map(path_to_imgLabel) \\\\\\n.map(preprocess_train_image, num_parallel_calls=autotune) \\\\\\n.cache() \\\\\\n.batch(batch_size)\\n\\n\\n#### Read the Result of Image Augmentation\\n\\nAs usual we import:\\n\\npython\\nimport matplotlib.pyplot as plt\\n\\n\\nand define:\\n\\npython\\ndef view_image(ds):\\n image, label = next(iter(ds)) # extract 1 batch from the dataset\\n image = image.numpy().astype(\'uint8\')\\n label = label.numpy().astype(\'uint8\')\\n\\n fig = plt.figure(figsize=(22, 22))\\n for i in range(20):\\n ax = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\\n ax.imshow(image[i])\\n ax.set_title(f\\"Label: {label[i]}\\")\\n\\n\\nFinally we run:\\n\\npython\\nview_image(train_dataset)\\n\\n\\nResult: <br/>\\n<br/>\\n\\n<center>\\n<a href=\\"/assets/tech/010.png\\">\\n<img width=\\"420\\" src=\\"/assets/tech/010.png\\"/>\\n</a>\\n</center>\\n<br/>\\n\\n#### References\\n\\n- https://www.youtube.com/watch?v=rAdLwKJBvPM\\n- https://albumentations.ai/docs/examples/example_multi_target/\\n","title":"Image Augmentation with Custom Dataset Pipeline","date":"2021-08-29T00:00:00.000Z","id":"blog0023","tags":"tensorflow","intro":"We introduce <a href=\\"https://albumentations.ai/docs/getting_started/installation/\\">albumentations</a> for image augmentation that helps generalize our model to unknown data."},{"content":"\\n#### Introduction I: Scrap Elements by Tagname and ClassName\\n\\nWe start off by importing three libraries:\\n\\npython\\nfrom selenium import webdriver\\nfrom bs4 import BeautifulSoup\\nimport time\\n\\n\\nWe will browse our webpage by webdriver.Chrome. We will then use BeautifulSoup to extract data based on HTML file structure such as tag name, such as class, etc.\\n\\nWe use open rice as a data source as an example:\\n\\npython\\nurl = \\"https://www.openrice.com/zh/hongkong/restaurants?what=%E5%8F%B0%E9%A2%A8\\"\\n\\n\\nWe sleep for 10 seconds to make sure all datas have been loaded before getting page_source:\\n\\npython\\nbrowser = webdriver.Chrome(\\"C:/Users/user/Repos/Python/2021-07-20-TrySelenium/chromedriver.exe\\")\\nbrowser.get(url)\\ntime.sleep(10)\\nhtml = browser.page_source\\n\\n\\nHere html is a string of HTML source. We can now parse it using BeautifulSoup as follows\\n\\npython\\nsoup = BeautifulSoup(html, \\"html.parser\\")\\n\\n\\nWe now scrap all div elements that bear the classname content-cell-wrapper:\\n\\npython\\nprint(len(soup.find_all(\\"div\\", {\\"class\\": \\"content-cell-wrapper\\"})))\\n\\n\\nAnd we get 8. If we further study the HTML structure, we can distill the data by using regular expression.\\n\\n#### Introduction II: Automatic Images Scrapping Through Google\\n\\n##### Discussion and Pattern\\n\\nWe note that searching name in google images is the same as having GET-request to the https://www.google.com/search?q=name&tbm=isch&hl=en&sa=X, therefore we can combine selenium and BeautifulSoup to scrap all img elements.\\n\\nWe also note that each img grasps the src attribute to detemine the image source. However, not every src is of the form http://....jpg, .png, .gif, etc which provides a direct GET-request to the image. Indeed the frontend may create an url(which is to be specified in src) using URL.createObjectURL(). This url is not a direct link to the image anymore, instead this will become a binary data represented by base-64 encoded string.\\n\\nFor example, an image may require access tokens in cookie/local storage, for which the frontend developer cannot put an image link to src as it will create a get request **_without_** header.\\n\\n##### Code Implementation\\n\\nWe divide the steps by functions from top to bottom. The method scrap accepes labels, which is used to\\n\\n- create GET-requests for image scrapping and;\\n- create a folder ./download_dir/{label} which stores all the scrapped images.\\n\\npython\\n# download ChromeDriver first according to your chrome version\\n# we need the path for .exe file.\\nfrom selenium import webdriver\\nfrom bs4 import BeautifulSoup\\nimport time\\nimport requests\\nimport re\\nimport base64\\nimport os\\nimport sys\\n\\nclass GoogleScrapper:\\n def __init__(\\n self,\\n query_url=\\"https://www.google.com/search?q={}&tbm=isch&hl=en&sa=X\\",\\n download_dir=\\"./scrapped_birds\\",\\n chromedriver_path=\\"C:/Users/jameslcc/Desktop/chromedriver.exe\\"\\n ):\\n self.query_url = query_url\\n self.download_dir = download_dir\\n self.chromedriver_path = chromedriver_path\\n\\n def scrap_img_els(self, label, wait_before_scrapping=0):\\n url = self.query_url.format(label)\\n browser = webdriver.Chrome(self.chromedriver_path)\\n browser.get(url)\\n time.sleep(wait_before_scrapping)\\n html = browser.page_source\\n browser.close()\\n\\n soup = BeautifulSoup(html, \\"html.parser\\")\\n\\n return soup.find_all(\\"img\\")\\n\\n def download_imgs(self, img_els, label):\\n class_dir = f\\"{self.download_dir}/{label}\\"\\n\\n if not os.path.exists(class_dir):\\n os.mkdir(class_dir)\\n\\n for i, img_el in enumerate(img_els):\\n img_src = img_el.get(\'src\', \'\')\\n try:\\n # if src refers to external link\\n if img_src.startswith(\\"http\\"):\\n r = requests.get(img_src)\\n if r.status_code == 200:\\n contype_type = r.headers.get(\'content-type\')\\n # image/png, image/jpeg, etc\\n if contype_type.startswith(\\"image/\\"):\\n file_ext = re.sub(\\"image/\\", \\"\\",contype_type)\\n file_path = f\\"{class_dir}/{label}-{i}.\\" + file_ext\\n with open(file_path, \'wb\') as f:\\n f.write(r.content)\\n print(f\\"{file_path} has been saved\\")\\n\\n # if src refers to internal link\\n if img_src.startswith(\\"data:\\"):\\n # example: data:image/jpeg;base64,/9j/4AAQSkZJRgABAQ ...\\n head, data = img_src.split(\',\', 1)\\n file_ext = head.split(\';\')[0].split(\'/\')[1]\\n plain_data = base64.b64decode(data)\\n\\n with open(f\\"{class_dir}/{label}-{i}.\\" + file_ext, \'wb\') as f:\\n f.write(plain_data)\\n print(f\\"{class_dir}/{label}-{i}.\\" + file_ext+ \\" has been saved\\")\\n\\n except:\\n error_msg = sys.exc_info()[1]\\n # raise Exception(error_msg)\\n print(error_msg)\\n\\n def scrap(self, labels, wait_before_scrapping = 0):\\n # catter for the case when labels is simply a string, we want a list of labels\\n if isinstance(labels, str):\\n labels = [labels]\\n\\n for label in labels:\\n img_els = self.scrap_img_els(label, wait_before_scrapping)\\n self.download_imgs(img_els, label)\\n\\nscrapper = GoogleScrapper()\\n\\n","title":"Scrapping Images with Selenium and Beautifulsoup on Chrome","date":"2021-08-30T00:00:00.000Z","id":"blog0024","tag":"python","intro":"Record a flow of data-scrapping."},{"content":"\\nThis is an exercise from udemy course in which I need to create a function apply_all which accepts parameters int* arr_1, size_t size_1, int* arr_2, size_t size_2 such that arr_1 of size size_1 and arr_2 of size size_2 are multiplied to generate an array of size $\\\\text{size_1}\\\\times \\\\text{size_2}$.\\n\\nFor example, array_1=[1, 2, 3] and array_2=[10, 20] are multiplied to produce [10, 20, 30, 20, 40, 60].\\n\\nWe first include the following standard library:\\n\\ncpp\\n#include <iostream>\\nusing namespace std;\\n\\n\\nNext our apply_all will produce a pointer pointing to an array allocated in **_heap memory_**:\\n\\ncpp\\nint* apply_all(int* arr_1, size_t size_1, int* arr_2, size_t size_2) {\\n\\tsize_t total_size{ size_1 * size_2 };\\n\\t// when using new, we are allocating new memory in heap to store the integer array.\\n\\tint* new_arr = new int[total_size];\\n\\n\\tint index{ 0 };\\n\\tfor (size_t j{ 0 }; j < size_2; j++) {\\n\\t\\tfor (size_t i{ 0 }; i < size_1; i++) {\\n\\t\\t\\t*(new_arr + index) = arr_2[j] * arr_1[i];\\n\\t\\t\\tindex++;\\n\\t\\t}\\n\\t}\\n\\treturn new_arr;\\n}\\n\\n\\n**Warning to myself.** We cannot create a local variable inside apply_all and simply return the address of that variable. Since local variable are saved in **_stack memory_**, memory allocated in the function will be poped out/deallocated once the result is returned. The resulting address will point to garbage data.\\n\\nNext we define a simple function to print arrays:\\n\\ncpp\\nvoid print(int* const result, int const size) {\\n\\tcout << \\"Result: \\";\\n\\tcout << \\"[ \\";\\n\\tfor (int i{ 0 }; i < size; i++) {\\n\\t\\tcout << *(result + i) << \\" \\";\\n\\t}\\n\\tcout << \\"]\\" << endl;\\n}\\n\\n\\nFinally we combine the above result:\\n\\ncpp\\nint main() {\\n\\tint array_1[5]{ 1,2,3,4,5 };\\n\\tint array_2[3]{ 10,20,30 };\\n\\tint* result = apply_all(array_1, 5, array_2, 3);\\n\\n\\tcout << \\"Array1: \\";\\n\\tprint(array_1, 5);\\n\\tcout << \\"Array2: \\";\\n\\tprint(array_2, 3);\\n\\tprint(result, 15);\\n\\n\\t// raw pointer, deallocate memory:\\n\\tdelete[] result;\\n}\\n\\n\\nthe output in console is:\\n\\ntext\\nArray1: Result: [ 1 2 3 4 5 ]\\nArray2: Result: [ 10 20 30 ]\\nResult: [ 10 20 30 40 50 20 40 60 80 100 30 60 90 120 150 ]\\n\\n\\n**Remark.** The variable name of an raw array in C++ in fact stores an address to the first element of that array. Therefore the variable name of an array is interchangeable with pointers in many use case.\\n","title":"C++ Beginner Notes 01 - Stack and Heap","date":"2021-09-08T00:00:00.000Z","id":"blog0025","tag":"C++","intro":"We write simple functions to understand cpp syntax, memory in stack and memory in heaps by using raw pointer.","toc":false},{"content":"\\n#### Shallow Copy and Trouble\\n\\nWe first define a class of objects for which we will do a shallow copy:\\n\\ncpp\\nclass Shallow {\\nprivate:\\n\\tint* data;\\npublic:\\n\\tvoid set_data(int d) {\\n\\t\\t*data = d;\\n\\t};\\n\\tint get_data() {\\n\\t\\treturn *data;\\n\\t};\\n\\tShallow(int d);\\n\\tShallow(const Shallow& source);\\n\\t~Shallow();\\n};\\n\\nShallow::Shallow(int d) {\\n\\tdata = new int;\\n\\t*data = d;\\n\\tcout << \\"1-arg constructor called\\" << endl;\\n}\\nShallow::Shallow(const Shallow& source) :\\n\\tdata{ source.data }\\n{\\n\\tcout << \\"copy constructor called\\" << endl;\\n};\\nShallow::~Shallow() {\\n\\tdelete data;\\n\\tcout << \\"destructor called to free memory\\";\\n};\\n\\n\\nWe will be doing a shallow copy using copy constructor above. The destructor defined above is legit, but then a problem will be raised when any instance of Shallow is out of scope of some function, say:\\n\\ncpp\\nvoid display_object(Shallow s) {\\n\\t// error still occurs even the following line is commented out:\\n\\tcout << s.get_data() << endl;\\n}\\n\\n\\nThe reason is that our copy constructor just copy the address of the object\'s data just passed in. But when a destructor of a shallow copy is called, the original object will have data pointing to an invalid data, compiler will then throw an error.\\n\\nLet\'s simulate this problem below:\\n\\ncpp\\nint main() {\\n\\tShallow obj_1{ 100 };\\n\\tShallow obj_2{ obj_1 };\\n\\t// since pointing to the same data (originally 100) in heap,\\n\\t// both obj_1.data and obj_2.data become 1000:\\n\\tobj_2.set_data(1000);\\n\\t// when obj_2 is out of scope, destructor is called:\\n\\tdisplay_object(obj_2);\\n\\t// obj_1.data points to invalid data, error occurs and we fail to return status code 0:\\n\\treturn 0;\\n}\\n\\n\\n<center>\\n<img src=\\"/assets/tech/012.png\\"/>\\n</center>\\n<br/>\\n\\n#### Deep Copying Instead\\n\\nSince pointing to the same allocated memory is a trouble. All data of any new instance of a class through copy constructor should point to the new allocated memory instead. This is easily adjustable by modifying the copy constructor as below:\\n\\ncpp\\nShallow::Shallow(const Shallow& source) :\\n\\tShallow{ *source.data }\\n{\\n\\tcout << \\"copy constructor called\\" << endl;\\n};\\n\\n\\nIn other words, we delegate our copy constructor to our 1-argument constructor which is designed originally to allocate new memory in heap.\\n\\nNow with exactly the same code, our main can run faultlessly.\\n\\n#### Further Discussion with l-value\\n\\nAs another observation:\\n\\ncpp\\nint main() {\\n\\tint x{ 100 };\\n\\t// or int y = x;\\n\\tint y{ x };\\n\\ty = 1000;\\n\\tcout << x << \' \' << y << endl;\\n\\treturn 0;\\n}\\n\\n\\nWe can observe that the output is\\n\\ntext\\n100 1000\\n\\n\\nTherefore the copy constructor of int is indeed a deep copy (always allocate a new memory).\\n\\nBut then how to make a new variable y that really makes a reference to x? We will need a concept called l-value reference, which is done by calling\\n\\ncpp\\nint main() {\\n\\tint x{ 100 };\\n\\tint& y = x;\\n\\ty = 1000;\\n\\tcout << x << \' \' << y << endl;\\n\\treturn 0;\\n}\\n\\n\\nand this time the output is\\n\\ntext\\n1000 1000\\n\\n\\nThe l-value concept is usually introduced before we witness the definition of function prototypes like int func(type &variable). Yes the & operator here indeed indicates we want a l-value variable to be passed in.\\n\\n#### Move-Semantics\\n\\n##### Use Case\\n\\nThe opposite of l-value is r-value and this kind of reference is indicated by &&. The concept of r-value becomes very useful when it comes to move-semantics. Loosely speaking the most easily understandable use cases are:\\n\\n- When a data is to be constructed and passed into an l-value. For example, we may assign a large object as a member of a class, and this object is not going to be reused **_anywhere else_**.\\n- When we want to pass a large object into a function which is not going to be reused **_anywhere else_**.\\n\\nThe main problem to solve is\\n\\n> Creating an temp object and copying it for another object/function is inefficient.\\n\\nWe would like to implement a move contructor which can reduce the number of times calling the copy constructor. The move constructor usually accepts an r-value for initiallization.\\n\\nThere are two ways to construct r-value:\\n\\n- Return value of a function without assigning it to any l-value.\\n- Object initialization without variable name.\\n\\n##### Code Implementation of Move Constructor\\n\\nWe rename the Shallow class above to Data as we do not focus on shallow copy anymore. Now the whole implementation of the Data class is (we haved just added additionally one constrctor below and changed the logging in destructor):\\n\\ncpp\\n#include <iostream>\\nusing namespace std;\\n\\nclass Data {\\nprivate:\\n\\tint* data;\\npublic:\\n\\tint get_data() {\\n\\t\\treturn *data;\\n\\t};\\n\\tData(int d);\\n\\tData(const Data& source);\\n\\tData(Data&& source);\\n\\t~Data();\\n};\\n\\nData::Data(int d) {\\n\\tdata = new int;\\n\\t*data = d;\\n\\tcout << \\"1-arg constructor called\\" << endl;\\n}\\n\\nData::Data(const Data& source) :\\n\\tData{ *source.data }\\n{\\n\\tcout << \\"copy constructor called\\" << endl;\\n};\\n\\nData::Data(Data&& source) {\\n\\tdata = source.data;\\n\\tsource.data = nullptr;\\n\\tcout << \\"move constructor called\\" << endl;\\n}\\n\\nData::~Data() {\\n\\tif (data != nullptr) {\\n\\t\\tcout << \\"destructor called to free memory\\" << endl;\\n\\t}\\n\\tdelete data;\\n};\\n\\nvoid display_object(Data s) {\\n\\tcout << \\"Shallow.data: \\" << s.get_data() << endl;\\n}\\n\\nData create_data() {\\n\\tData new_shallow{ 100 };\\n\\treturn new_shallow;\\n}\\n\\nint main() {\\n\\t// This call the move constructor:\\n\\tdisplay_object(create_data());\\n\\n\\t// This does not:\\n\\t// display_object(Data{ 123 });\\n\\n\\t// This does:\\n\\t// display_object(std::move(Data{ 123 }));\\n}\\n\\n\\nWe can observe that the **_unreused_** (we assume this is the case) variable returned from create_data() does not trigger the copy constructor, for which the copy constructor is supposed to allocate new memory to store our temporary object.\\n\\nNow the function call will be way more efficient if the object to pass into the function is very large.\\n\\n#### References\\n\\n- https://www.youtube.com/watch?v=IOkgBrXCtfo\\n","title":"C++ Beginner Notes 02 - Shallow Copy, Deep Copy and Move Semantics","date":"2021-09-09T00:00:00.000Z","id":"blog0026","tag":"C++","wip":false,"intro":"We list some potential problem of using shallow copy and how to avoid them by deep copy. We can also improve computation efficiency when a variable is never reused but needed to be passed into a function/class attribute. We achieve this by using move constructor."},{"content":"\\n#### Basic Article in xeLaTeX\\n\\nlatex\\n\\\\documentclass[12pt,a4paper]{article}\\n\\\\usepackage{amsmath}\\n\\\\usepackage{amsfonts}\\n\\\\usepackage{amssymb}\\n\\\\usepackage[dvipsnames]{xcolor}\\n\\\\usepackage{graphicx}\\n\\\\usepackage[left=2.4cm,right=2.4cm,top=2cm]{geometry}\\n\\\\usepackage{hyperref}\\n\\\\usepackage{array}\\n\\\\usepackage{enumitem}\\n\\\\usepackage{graphbox}\\n\\\\usepackage{float}\\n\\\\usepackage[AutoFakeSlant=0.1, AutoFakeBold=true]{xeCJK}\\n\\\\setCJKmainfont{SimSun}\\n\\\\usepackage{multicol}\\n\\\\linespread{1.4}\\n\\\\thispagestyle{empty}\\n\\n\\\\begin{document}\\n\\n\\n\\\\end{document}\\n\\n\\n#### Math in Times with Adjusted BigOp Symbols\\n","title":"$\\\\LaTeX$ template","date":"2021-09-10T00:00:00.000Z","id":"blog0028","tag":"latex","intro":"Record some starting template that I made in the past.","toc":false},{"content":"\\n#### \u539f\u756b\u8655\u7406\\n\\n\u770b\u8655\u7406\u6d41\u7a0b\uff0c\u53ef\u4ee5\u9810\u5148\u628a\u7dda\u7a3f\u62c6\u958b\u518d\u5c40\u90e8\u5730\u628a\u4f5c\u54c1\u5b8c\u6210\u3002\u5982\u679c\u50cf\u6211\u4e00\u6a23\u6700\u7d42\u5f69\u5716\u7684\u4e0a\u8272\u5716\u5c64\u53ea\u6709\u4e00\u5c64\u7684\u8a71\uff0c\u4fbf\u8981\u518d\u7d30\u5fc3\u628a\u6700\u7d42\u6210\u54c1\u62c6\u958b\u3002\\n\\n\u56e0\u70ba\u4fee\u5716\u57fa\u672c\u4e0a\u5fc5\u9808\uff0c\u4f5c\u756b\u4e5f\u4e0d\u7528\u5c0d\u7dda\u7a3f\u592a\u57f7\u7740\u3002\u4f5c\u54c1\u6574\u9ad4\u9060\u6bd4\u6700\u7d42\u7684\u7dda\u689d\u7cbe\u7dfb\u6027\u91cd\u8981\u3002\u6700\u5f8c\u6211\u5011\u9084\u5f97\u7528\u539a\u5857\u7684\u65b9\u5f0f\u628a\u5c0d live2D \u4f86\u628a\u4e0d\u5b8c\u6574\u7684\u90e8\u5206\u88dc\u5b8c\uff0c\u90a3\u6642\u5019\u518d\u628a\u7dda\u78e8\u7cbe\u7d30\u4e00\u9ede\u4e5f\u4e0d\u665a\u3002\\n\\n<p>\u4f8b\u5982\u9019\u5f35\u89d2\u8272\u539f\u672c\u9577\u9019\u6a23\uff1a</p>\\n<img width=\\"600\\" src=\\"/assets/tech/019.png\\"/>\\n<p/>\\n\\n<center></center>\\n\\n\u5728\u5b8c\u6210 live2D \u5f8c\u9577\u9019\u6a23\uff1a\\n\\n<img width=\\"600\\" src=\\"/assets/tech/018.png\\"/>\\n<p/>\\n\\n<center></center>\\n\\n\u56e0\u70ba\u7b2c\u4e00\u6b21\u6703\u8e29\u4e0d\u5c11\u5751\uff0c\u800c\u4e14\u5728\u88fd\u4f5c\u904e\u7a0b\u4e0d\u53ef\u80fd\u907f\u514d\u56de\u982d\u53bb\u4fee\u539f\u672c\u7684 psd \u6a94\u6848\uff0c\u56e0\u6b64\u5fc5\u9808\u61c2\u5f97\u5982\u4f55\u5728 live2D \u5c0e\u5165\u66f4\u6539\u904e\u7684 psd \u6a94\u6848\uff1a\\n\\n#### \u66f4\u6539\u904e\u7684 psd \u6a94\u6848\u5c0e\u5165\\n\\n\u4f8b\u5982\u6211\u7684 .cmo3 \u6a94 (live2D \u5132\u5b58\u6a94\u6848\u7684\u683c\u5f0f) \u5f9e\u4e00\u958b\u59cb\u5c31\u4f7f\u7528\u4e86\u540d\u70ba 14.psd \u7684\u6a94\u6848\u3002\u7576\u6211\u5011\u4fee\u6539 14.psd \u5f8c\uff0c\u53ef\u4ee5\u628a psd \u6a94\u76f4\u63a5\u62c9\u5230 live2D \u7684\u5de5\u4f5c\u8996\u7a97\u9032\u884c\u66f4\u65b0\u3002\\n\\n<img width=\\"600\\" src=\\"/assets/tech/020.png\\"/>\\n<p/><p/>\\n<img src=\\"/assets/tech/021.png\\"/>\\n<p/>\\n<img src=\\"/assets/tech/022.png\\"/>\\n<br/><br/>\\n\\n#### Live2D \u5c40\u90e8\u7684\u6ce8\u610f\u4e8b\u9805\\n\\n\u4e94\u5b98\u7684\u8655\u7406\u624b\u6cd5\uff0cAngle X Angle Y Angle Z \u7684\u89d2\u5ea6\u8655\u7406\uff0c\u7b49\u7b49\u7b49\u7b49\uff0c\u90fd\u975e\u5e38\u63a8\u85a6\u770b\u300c\u53c3\u8003\u300d\u90e8\u5206\u7684 **Live2D \u8d85\u5165\u9580\u8b1b\u5ea7\u7cfb\u5217**\u3002\\n\\n\u4e94\u5b98\u4e2d\u6709\u4e9b\u90e8\u5206\u4e0d\u5bb9\u6613\u6709\u5751\u53ef\u4ee5\u8e29\uff0c\u50cf\u982d\u9aee\uff0c\u8033\u554a\uff0c\u9f3b\u4e4b\u985e\u7684\u90fd\u662f\u975e\u5e38\u5b89\u5168\u7684\u90e8\u5206\u3002\u4ee5\u4e0b\u7d00\u9304\u6211\u89ba\u5f97\u6709\u5fc5\u8981\u8a18\u4f4f\u7684\u5730\u65b9\uff1a\\n\\n##### \u982d\u9aee\\n\\n\u9664\u975e\u8ddf\u6211\u7684\u4f8b\u5b50\u4e00\u6a23\u89d2\u8272\u5e36\u6709\u5e3d\u5b50\uff0c\u4e0d\u7136\u6b63\u5e38\u60c5\u6cc1\u4e0b\u982d\u9aee\u9700\u5206\u70ba\\n\\n- \u8907\u6578 (\u6216\u4e00) \u500b\u524d\u9aee\\n- \u8033\u6735\u524d\u7684\u5169\u689d\u5074\u9aee\\n- \u653e\u5728\u6700\u5f8c\u9762\u7684**\u5f8c\u9aee**\\n\\n\u982d\u90e8\u5206\u6700\u7406\u60f3\u662f\u5b8c\u6574\u7684\u79bf\u982d\u982d\u578b\u3002\u4f46\u4e0d\u4e00\u5b9a\u662f\u79bf\u982d\uff0c\u96d6\u7136\u5f88\u591a\u6642\u770b\u4e0d\u5230\u4f46\u53ef\u4ee5\u628a**\u9aee\u6839\u7d66\u756b\u51fa\u4f86**\u3002\\n\\n##### \u773c\\n\\n\u65b0\u624b\u7684\u7b2c\u4e00\u500b\u5751\u3002\u901a\u5e38 CG \u5c11\u5973\u90fd\u5e36\u6709\\n\\n- \u776b\u6bdb\uff1b\\n- \u5b64\u5ea6\u5411\u4e0a\u7684\u4e0a\u534a\u90e8\uff1b\\n- \u5b64\u5ea6\u5411\u4e0b\u7684\u4e0b\u534a\u90e8\uff1b\\n- \u773c\u73e0 (\u8acb\u4fdd\u7559\u5b8c\u6574\u7684\u6a62\u5713\u578b)\u3002\\n\\n\u9019\u4e9b\u5728 live2D \u88fd\u4f5c\u904e\u7a0b\u90fd\u5fc5\u9808\u88ab\u5206\u62c6\u51fa\u4f86\u3002\\n\\n<img src=\\"/assets/tech/023.png\\"/>\\n\\n##### \u53e3\\n\\n\u53e3\u5206\u70ba\u4e0a\u5507\u8ddf\u4e0b\u5507\uff0c\u770b\u98a8\u683c\u53ef\u4f7f\u7528\u5169\u6392\u7259\u9f52\uff1a\\n\\n<img src=\\"/assets/tech/024.png\\"/>\\n<p/>\\n<center></center>\\n\\n\u6211\u5011\u5229\u7528\u4e0a\u5507\u8ddf\u4e0b\u5507\u7684\u5f62\u72c0\u4f86\u505a\u51fa\u4e0d\u540c\u7684\u60c5\u611f\u3002\\n\\n\u53e6\u5916\u56e0\u70ba\u53e3\u90e8\u8b8a\u578b\u5c24\u70ba\u8907\u96dc\uff0c\u5916\u52a0\u4e0a\u6211\u5011\u6709\u9700\u8981\u6642\u523b\u66f4\u6539 mesh \u7684\u5f62\u72c0\u4f86\u906e\u6a94\u6709\u6a5f\u6703\u9732\u51fa\u7684\u53e3\u8154\u5167\u90e8\u3002\u56e0\u6b64\u63a8\u85a6\u9019\u7a2e mesh \u7684\u5206\u4f48\u65b9\u6cd5\uff1a\\n\\n<img width=\\"600\\" src=\\"/assets/tech/025.png\\"/>\\n<img width=\\"600\\" src=\\"/assets/tech/026.png\\"/>\\n\\n<p/><p/>\\n<center></center>\\n\\n\u597d\u8655\uff1a\\n\\n> \u5728 live2D \u88cf mesh \u4ee5\u5916\u7684\u5340\u57df\u6703\u88ab\u81ea\u52d5\u522a\u9664\u4e26\u4e0d\u986f\u793a\u5728\u756b\u9762\u4e0a\u3002\u7576\u6211\u5011\u903c\u4e0d\u5f97\u5df2\u9700\u8981 \u589e\u52a0 / \u522a\u6e1b \u4f5c\u70ba\u906e\u64cb\u7684\u8272\u584a\u5340\u57df\u7684\u6642\u5019\uff0c\u6211\u5011\u53ef\u4ee5\u8abf\u6574\u6700\u5916\u5468\u7684\u7bc0\u9ede\u800c\u4e0d\u5f71\u97ff\u6700\u5167\u5c64\u7d93\u7531 deformer \u9020\u6210\u7684\u8b8a\u5316\u3002\\n\\n\x3c!-- #### \u907f\u514d\u4f7f\u7528\u81ea\u5b9a\u7fa9\u53c3\u6578\uff0c\u7406\u89e3\u9810\u8a2d\u53c3\u6578\u4f7f\u7528\u65b9\u5f0f\\n\\n\\n\\n#### Deformers\\n##### Grid-based Surface\\n##### Rotation --\x3e\\n\\n#### \u6210\u54c1\\n\\n<center>\\n <video controls width=\\"500\\">\\n <source src=\\"/assets/videos/001.mp4\\" type=\\"video/mp4\\">\\n Sorry, your browser doesn\'t support embedded videos.\\n </video>\\n</center>\\n<p/><p/>\\n\\n#### \u53c3\u8003/\u6559\u5b78\\n\\n- Live2D \u8d85\u5165\u9580\u8b1b\u5ea7\u7cfb\u5217\uff1a\\n https://www.youtube.com/watch?v=rl3XFoLf3XA&list=PL_B-UPbBHi7TY2K-Wah40rlX3FvD7Expp\\n\\n- <a href=\\"https://www.youtube.com/watch?v=LXV4Q4e1RbU\\">\\n \u7528Live2D Cubism & FaceRig\u6210\u70baVTuber\u6216\u9059\u8ddd\u4e0a\u5b78\u5427!\\n </a>\\n\\n \u4e00\u4e9b facerig \u8ddf obs \u7684\u57fa\u672c\u89c0\u5ff5\uff0c\u91cd\u9ede\u662f\u6b64\u5f71\u7247 description \u4e2d\u7684\u9023\u7d50\uff1a\\n\\n- <a href=\\"https://www.cg-method.com/live2d-facerig-parameters/\\"> live2D \u88cf\u54ea\u4e9b\u9810\u8a2d\u53c3\u6578\u53ef\u76f4\u63a5\u5728 facerig \u4e2d\u4f7f\u7528\\n </a>\\n\\n \u5982\u679c\u4e0d\u5e78\u4f7f\u7528\u4e86\u81ea\u5b9a\u7fa9\u53c3\u6578\u9020\u51fa\u5f88\u6f02\u4eae\u7684\u6548\u679c\uff0c\u90a3\u5c0d\u4e0d\u8d77\u4e86\u4f60\u8981\u5b78\u7fd2\u5982\u4f55\u628a\u81ea\u5b9a\u7fa9\u53c3\u6578\u53d6\u4ee3 facerig \u9ed8\u8a8d\u7684\u53c3\u6578 (\u6709\u597d\u5e7e\u500b\u5730\u65b9\u6211\u56e0\u6b64\u8981\u780d\u6389\u91cd\u505a)\uff1a\\n\\n- <a href=\\"https://www.cg-method.com/facerig-custom-parameter/\\"> \u30ab\u30b9\u30bf\u30e0\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\uff06Live2D \u306e\u30a2\u30cb\u30e1\u30fc\u30b7\u30e7\u30f3\uff08\u30e2\u30fc\u30b7\u30e7\u30f3\uff09\u306e\u8a2d\u5b9a\u65b9\u6cd5\u307e\u3068\u3081\\n </a>\\n\\n \u9019\u4e9b\u9ed8\u8a8d\u53c3\u6578\u4e0d\u4f46\u9650\u5236\u4e86\u53ef\u4f7f\u7528\u7684\u53c3\u6578\u540d\u7a31\uff0c\u9084\u9650\u5236\u4e86\u4ed6\u5011\u53ef\u4f7f\u7528\u7684**_\u503c\u57df_**\u3002\u4f8b\u5982\u81ea\u5b9a\u7fa9\u53c3\u6578\u9001\u7d66\u4f60 -30 \u5230 30 \u9019\u500b\u5340\u57df\uff0c\u7d93\u904e\u52aa\u529b\u641c\u5c0b\u73fe\u5728\u9084\u6c92\u6709\u65b9\u6cd5\u628a\u5b83 scale \u6210 0 \u5230 1 \u9593\u7684\u53c3\u6578\uff0c\u5fc5\u9808\u91cd\u505a\uff01\u6240\u4ee5\u9084\u662f\u4e56\u4e56\u7528\u9810\u8a2d\u53c3\u6578\u5427\uff01\\n","title":"Live2D \u5fc3\u5f97","date":"2021-09-18T00:00:00.000Z","id":"blog0029","tag":"live2D","intro":"\u7d00\u9304\u4e00\u4e9b\u5751\u8ddf\u9084\u6709\u8a18\u61b6\u7684\u6b65\u9a5f"},{"content":"\\n#### Chrome Options and Driver\\n\\nA ChromOptions object is instantiated as follows:\\n\\npython\\nfrom selenium import webdriver\\noptions = webdriver.ChromeOptions()\\n\\n\\nAvailable methods of ChromeOption object can be found in the <a href=\\"https://selenium-python.readthedocs.io/api.html?highlight=option#selenium.webdriver.chrome.options.Options.add_experimental_option\\">documentation of Selenium </a>. Common usages:\\n\\n- for .add_argument, for a list of arguments we refer to <a href=\\"https://chromium.googlesource.com/chromium/src/+/refs/heads/main/chrome/common/chrome_switches.cc\\">here</a>.\\n\\n For example, if we want to maximize the window on chrome launched, then write option.add_argument(\\"--start-maximized\\").\\n\\n- for preference, we use .add_experimental_option, for a list of preferences we refer to <a href=\\"https://chromium.googlesource.com/chromium/src/+/refs/heads/main/chrome/common/pref_names.cc\\">here</a>. I am not sure whether .set_preference would override the default preference.\\n\\nIn our case we want to specify the default download directory (which cannot be changed after the Driver object is instantisated):\\n\\npython\\noptions = webdriver.ChromeOptions()\\nprefs = {\\"download.default_directory\\" : f\\"{os.getcwd()}/download\\"}\\noptions.add_experimental_option(\\"prefs\\", prefs)\\n\\n\\nNow we build our driver using our options.\\n\\npython\\nbrowser = webdriver.Chrome(executable_path=DRIVER_PATH, chrome_options=options)\\n\\n\\n#### Implicit Wait\\n\\nFrom the <a href=\\"https://selenium-python.readthedocs.io/waits.html\\">documentation of Selenium</a> implicit-wait is:\\n\\n> An implicit wait tells WebDriver to poll the DOM for a certain amount of time when trying to find any element (or elements) not immediately available. The default setting is 0 (zero). Once set, the implicit wait is set for the life of the WebDriver object.\\n\\nWe will discuss **_explicit-wait_** later in the discussion of downloading files.\\n\\nIn my situation I choose to wait for 10 seconds as dom elements may take time to render:\\n\\npython\\nbrowser.implicitly_wait(10)\\n\\n\\n#### Select Element and Click\\n\\nIn case an automation task can be done where every desired dom element can be selected by id attribute, we define the following util function:\\n\\npython\\ndef get_el_by_id(id):\\n el = browser.find_element_by_id(id)\\n assert el is not None, f\\"Element of id: {id} cannot be found\\"\\n return el\\n\\n\\nA list of methods to find dom element(s) can be found <a href=\\"https://selenium-python-zh.readthedocs.io/en/latest/locating-elements.html\\">here</a>.\\n\\n##### Buttons\\n\\nSelecting a button and click is as simple as:\\n\\npython\\nsubmit_btn = get_el_by_id(\\"butSubmit\\")\\nsubmit_btn.click()\\n\\n\\n##### Dropdown List and Selection\\n\\nWe can select an element in dropdown list by using their value attribute:\\n\\npython\\nfrom selenium.webdriver.support.select import Select\\n\\n# dropdown_id: the id of the dropdown dom element\\n# target_value: the value attribute of our target\\nselection_list = Select(get_el_by_id(dropdown_id))\\nselection_list.select_by_value(target_value)\\n\\n\\n#### Download Files and Explicit Wait\\n\\nAs usual we select an element that would trigger download action and click it:\\n\\npython\\nconfirm_btn = get_el_by_id(\\"downloadBtn\\")\\nconfirm_btn.click()\\n\\n\\nNext we look at the download directory:\\n\\npython\\nimport time\\n\\ndef every_downloads_chrome(browser):\\n if not browser.current_url.startswith(\\"chrome://downloads\\"):\\n browser.get(\\"chrome://downloads/\\")\\n return browser.execute_script(\\"\\"\\"\\n var items = document.querySelector(\'downloads-manager\')\\n .shadowRoot.getElementById(\'downloadsList\').items;\\n\\n if (items.length == 0) {\\n return [\\"no_download\\"]\\n }\\n\\n if (items.every(e => e.state === \\"COMPLETE\\")) {\\n return items.map(e => e.fileUrl || e.file_url);\\n }\\n \\"\\"\\")\\n# waits for all the files to be completed and returns the paths\\n\\ntime.sleep(4)\\n# timeout for 2 minutes\\npaths = WebDriverWait(browser, 120, 1).until(every_downloads_chrome)\\nassert len(paths) >= 1 and paths[-1] != \\"no_download\\", \\"No file is being downloaded\\"\\n# get the name of lastly downloaded file:\\nlatest_download_filename = paths[-1].split(\\"/\\")[-1]\\n\\n# print the downloaded file\\nfor filepath in paths:\\n print(f\\"Files donwloaded: {filepath}\\")\\n\\n\\nPoints to note:\\n\\n- Since website may have a delay after clicking the download button, we wait for 4 seconds to make sure there is a file to be downloaded.\\n\\n- We also handle the case that the server has an internal error that ruins our download process. In this case, we return [\\"no_download\\"] when items.length == 0.\\n\\n Note that the return value [] wouldn\'t stop the waiting process by experiment.\\n\\nDocumentation for WebDriverWait: https://selenium-python.readthedocs.io/api.html?highlight=WebDriverWait.\\n\\n#### References\\n\\n- <a href=\\"https://stackoverflow.com/questions/38335671/where-can-i-find-a-list-of-all-available-chromeoption-arguments\\">Stackoverflow - List of possible options for chrome driver</a>\\n\\n- <a href=\\"https://stackoverflow.com/questions/48263317/selenium-python-waiting-for-a-download-process-to-complete-using-chrome-web/48267887\\">Stackoverflow - Explit-wait for downloading files</a>\\n","title":"Automation Task for Chrome","date":"2021-10-02T00:00:00.000Z","id":"blog030","tag":"python, selenium","intro":"Simple click and download (and wait for its completion!) tasks that are achieved in python."},{"content":"\\nWhen performing unit tests we usually import modules in project-directory/src. For this to work, we have to make sure sys.path contains our project-directory when running pytest, so that\\n\\npython\\nfrom src.utils.module_name import SomeClass\\n\\n\\nbecomes possible. It is as simple as executing (instead of pytest -vv test/...)\\n\\ntext\\npython -m pytest -vv test/...\\n\\n\\nand make sure we have cded into project directory when running this script.\\n","title":"How to Include Project Directory in sys.path when Running pytest","date":"2021-10-20T00:00:00.000Z","id":"blog032","tag":"python","intro":"As titled.","toc":false},{"content":"\\n#### Modifier Keys\\n\\n- ^ Control Key\\n- # Windows Key\\n- + Shift\\n- ! Alt\\n\\n#### My Usages\\n\\nautohotkey\\nKeyWait, Shift\\nKeyWait, LWin\\nKeyWait, RWin\\n\\n#NoEnv\\n#SingleInstance\\n#MaxHotkeysPerInterval 3000\\n\\n;Process, Priority, , H\\nSendMode Input\\n\\n; Show scroll velocity as a tooltip while scrolling. 1 or 0.\\ntooltips := 0\\n\\n; The length of a scrolling session.\\n; Keep scrolling within this time to accumulate boost.\\n; Default: 500. Recommended between 400 and 1000.\\ntimeout := 500\\n\\n; If you scroll a long distance in one session, apply additional boost factor.\\n; The higher the value, the longer it takes to activate, and the slower it accumulates.\\n; Set to zero to disable completely. Default: 30.\\nboost := 30\\n\\n; Spamming applications with hundreds of individual scroll events can slow them down.\\n; This sets the maximum number of scrolls sent per click, i.e. max velocity. Default: 60.\\nlimit := 60\\n\\n; Runtime variables. Do not modify.\\ndistance := 0\\nvmax := 1\\n\\n; Key bindings\\nWheelUp:: Goto Scroll\\nWheelDown:: Goto Scroll\\n#WheelUp:: Suspend\\n#WheelDown:: Goto Quit\\n\\nScroll:\\n t := A_TimeSincePriorHotkey\\n if (A_PriorHotkey = A_ThisHotkey && t < timeout)\\n {\\n ; Remember how many times we\'ve scrolled in the current direction\\n distance++\\n\\n ; Calculate acceleration factor using a 1/x curve\\n v := (t < 80 && t > 1) ? (150.0 / t) - 1 : 1\\n\\n ; Apply boost\\n if (boost > 1 && distance > boost)\\n {\\n ; Hold onto the highest speed we\'ve achieved during this boost\\n if (v > vmax)\\n vmax := v\\n else\\n v := vmax\\n\\n v *= distance / boost\\n }\\n\\n ; Validate\\n v := (v > 1) ? ((v > limit) ? limit : Floor(v)) : 1\\n\\n if (v > 1 && tooltips)\\n QuickToolTip(\\"\xd7\\"v, timeout)\\n\\n MouseClick, %A_ThisHotkey%, , , v\\n }\\n else\\n {\\n ; Combo broken, so reset session variables\\n distance := 0\\n vmax := 1\\n\\n MouseClick %A_ThisHotkey%\\n }\\nreturn\\n\\nQuit:\\n QuickToolTip(\\"Exiting Accelerated Scrolling...\\", 1000)\\n Sleep 1000\\nExitApp\\n\\nQuickToolTip(text, delay)\\n{\\n ToolTip, %text%\\n SetTimer ToolTipOff, %delay%\\nreturn\\n\\nToolTipOff:\\n SetTimer ToolTipOff, Off\\n ToolTip\\nreturn\\n}\\n\\n^space::#space\\n+WheelDown::WheelLeft\\n+WheelUp::WheelRight\\n\\n\\n#If !(WinActive(\\"ahk_exe ffxiv_dx11.exe\\"))\\n!1::\\nRun %comspec% /k code \\"C:\\\\Users\\\\machingclee\\\\Repos\\\\wonderbricks\\\\2024-02-03-wb-billie-mobile\\",, Hide\\nreturn\\n!2::\\nRun %comspec% /k code \\"C:\\\\Users\\\\machingclee\\\\Repos\\\\wonderbricks\\\\2024-02-03-wb-billie-web\\",, Hide\\nreturn\\n!3::\\nRun %comspec% /k code \\"C:\\\\Repos\\\\some\\\\backend\\",, Hide\\nreturn\\n!4::\\nRun %comspec% /k code \\"C:\\\\Users\\\\machingclee\\\\Repos\\\\wonderbricks\\\\2024-02-03-wb-backend-node\\",, Hide\\nreturn\\n!0::\\nRun %comspec% /k code \\"C:\\\\Users\\\\machingclee\\\\Repos\\\\2024-02-18-blogs\\\\machingclee.github.io.source\\\\app\\",, Hide\\nreturn\\n#If\\n\\n\\n\\n\\n\\n#Up::\\n Send {LWin down}{Tab down}\\n KeyWait, Up\\n Send {LWin up}{Tab up}\\nreturn\\n\\n^#A:: Winset, Alwaysontop, , A\\n\\n#5::\\n Send {ctrl down}{shift down}{alt down}{5}\\n KeyWait, 5\\n\\n Send {ctrl up}{shift up}{alt up}\\nReturn\\n\\n#4::\\n Send {ctrl down}{shift down}{alt down}{4}\\n KeyWait, 4\\n\\n Send {ctrl up}{shift up}{alt up}\\nReturn\\n\\n#3::\\n Send {ctrl down}{shift down}{alt down}{3}\\n KeyWait, 3\\n\\n Send {ctrl up}{shift up}{alt up}\\nReturn\\n\\n#2::\\n Send {ctrl down}{shift down}{alt down}{2}\\n KeyWait, 2\\n\\n Send {ctrl up}{shift up}{alt up}\\nReturn\\n\\n#1::\\n Send {ctrl down}{shift down}{alt down}{1}\\n KeyWait, 1\\n\\n Send {ctrl up}{shift up}{alt up}\\nReturn\\n\\n#0::\\n Send {ctrl down}{shift down}{alt down}{0}\\n KeyWait, 0\\n\\n Send {ctrl up}{shift up}{alt up}\\nReturn\\n\\n#If !(WinActive(\\"ahk_exe ffxiv_dx11.exe\\") or WinActive(\\"ahk_exe Photoshop.exe\\") or WinActive(\\"ahk_exe chrome.exe\\") or WinActive(\\"ahk_exe Code.exe\\") or WinActive(\\"TwitchUI.exe\\"))\\n^w::\\nSend !{Space}n\\nReturn\\n#If\\n\\n\\n\\n\\n#IfWinActive ahk_exe Unity.exe\\n !MButton:: !LButton\\n Return\\n#ifWinActive\\n\\n#IfWinActive ahk_exe Substance Painter.exe\\n #!LButton::\\n Send {Ctrl down}{RButton down}\\n KeyWait, LButton\\n Send {Ctrl up}{RButton up}\\n Return\\n\\n MButton::\\n Send {Ctrl down}{Alt down}{Lbutton down}\\n KeyWait, MButton\\n Send {Ctrl up}{Alt up}{Lbutton up}\\n Return\\n\\n !MButton::\\n Send {Alt down}{Lbutton down}\\n KeyWait, MButton\\n Send {Alt up}{Lbutton up}\\n Return\\n\\n WheelUp::Send {WheelDown}\\n WheelDown::Send {WheelUp}\\n Return\\n#ifWinActive\\n\\n#IfWinActive ahk_exe sai2.exe\\n #!LButton::^!LButton\\n Esc::Send +{Home}\\n\\n ^LButton::\\n Send {Shift down}{Ctrl down}{LButton down}\\n KeyWait, LButton\\n\\n Send {Shift up}{Ctrl up}{LButton up}\\n Return\\n\\n MButton::\\n Send {Space down}{LButton down}\\n KeyWait, MButton\\n\\n Send {Space up}{LButton up}\\n Return\\n#ifWinActive\\n\\n#IfWinActive ahk_exe ZBrush.exe\\n MButton::\\n Loop{\\n GetKeyState, state, Alt\\n If state = D\\n Break\\n\\n else\\n\\n Send {Alt down}{Rbutton down}\\n KeyWait, MButton\\n\\n Send {RButton Up}\\n Sleep, 1\\n Send {alt up}\\n Return\\n }\\n Return\\n\\n !MButton::\\n Send {Rbutton down}\\n KeyWait, MButton\\n Send {RButton Up}\\n Return\\n#ifWinActive\\n\\n#IfWinActive ahk_exe maya.exe\\n Mbutton::!MButton\\n !Mbutton:: !LButton\\n Return\\n#IfWinActive\\n\\n#IfWinActive ahk_exe Photoshop.exe\\n MButton::\\n Send {Space Down}\\n Send {LButton Down}\\n KeyWait, MButton\\n Send {LButton Up}\\n Send {Space Up}\\n Return\\n#IfWinActive\\n\\n#IfWinActive ahk_exe Photoshop.exe\\n #!LButton::#!RButton\\n !^RButton::!^LButton\\n#IfWinActive\\n\\n#IfWinActive ahk_exe Photoshop.exe\\n $^z::\\n while(GetKeyState(\\"CTRL\\", \\"p\\") && GetKeyState(\\"z\\", \\"p\\"))\\n {\\n If (A_TimeSinceThisHotkey < 250)\\n {\\n Sendinput ^z\\n sleep, 250\\n }\\n else\\n {\\n SendInput ^z\\n Sleep, 100\\n }\\n }\\n Return\\n#IfWinActive\\n\\n#IfWinActive ahk_exe Photoshop.exe\\n $^y::\\n While (GetKeyState(\\"CTRL\\", \\"p\\") && GetKeyState(\\"y\\", \\"p\\"))\\n {\\n If (A_TimeSinceThisHotkey < 250)\\n {\\n Sendinput ^y \\n sleep, 250 \\n } \\n else\\n { \\n SendInput ^y\\n Sleep, 100\\n }\\n }\\n Return\\n#ifWinActive\\n\\n#IfWinActive ahk_exe InDesign.exe\\n MButton::\\n Send {Space down}{LButton down}\\n KeyWait, MButton\\n\\n Send {Space up}{LButton up}\\n Return \\n#ifWinActive \\n\\n#IfWinActive ahk_exe ffxiv_dx11.exe \\n NumpadEnd::l \\n\\n\\n NumpadSub::\\n Send {CtrlDown}{F1}\\n KeyWait, NumpadSub\\n Send {CtrlUp}\\n Return\\n\\n NumpadEnter::\\n Send {CtrlDown}{F2}\\n KeyWait, NumpadEnter\\n Send {CtrlUp}\\n Return\\n\\n NumpadMult::\\n Send {CtrlDown}{F3}\\n KeyWait, NumpadMult\\n Send {CtrlUp}\\n Return\\n\\n\\n NumpadAdd::\\n Send {CtrlDown}{F4}\\n KeyWait, NumpadAdd\\n Send {CtrlUp}\\n Return\\n\\n\\n\\n NumLock::Return\\n\\n\\n Numpad0::\\n Send {CtrlDown}{F7}\\n KeyWait, Numpad0\\n Send {CtrlUp}\\n Return\\n\\n NumpadDiv::\\n Send {CtrlDown}{F6}\\n KeyWait, NumpadDiv\\n Send {CtrlUp}\\n Return\\n\\n\\n NumpadDot::\\n Send {CtrlDown}{F5}\\n KeyWait, NumpadDot\\n Send {CtrlUp}\\n Return\\n\\n #G::\\n Send {AltDown}{F12}\\n KeyWait, G\\n Send {AltUp}\\n Return\\n\\n NumpadDel::\\n SendInput, {Shift Down}={Shift Up}\\n Return\\n#ifWinActive\\n\\n","title":"Auto Hotkey Record","date":"2021-10-17T00:00:00.000Z","id":"blog033","tag":"coding, autohotkey","intro":"A record of my latest autohotkey setup for different applications.","toc":false},{"content":"\\n#### Debuggable Test File\\n\\nWe have discussed how to include project directory in sys.path when running pytest. Sometimes it is much more convenient to debug a utility function rather than testing it.\\n\\nIn order to make statements like from src.utils.abc import Abc **_always_** possible, our sys.path has to include the directory path that contains the folder src.\\n\\nSuppose I want to record a concrete usage of a utility class in a test test/unit/test_abc.py, where:\\n\\npython\\n# test/unit/test_abc.py\\nfrom src.utils.abc import Abc\\n\\ndef test_abc():\\n abc = Abc()\\n abc.start()\\n\\nif __name__ == \\"__main__\\":\\n test_abc()\\n\\n\\nNow test_abc.py is not only a testable file, it is also debuggable because we can trigger debug action in vscode as if we are running the script directly (so that __name__ == \\"__main__\\").\\n\\nAn appropriate configuration for debugger has to be made in order to make src folder accessible.\\n\\n#### Debugger Configuration: .vscode/launch.json\\n\\njson\\n// .vscode/launch.json\\n{\\n \\"version\\": \\"0.2.0\\",\\n \\"configurations\\": [\\n {\\n \\"name\\": \\"Python: Current File\\",\\n \\"type\\": \\"python\\",\\n \\"request\\": \\"launch\\",\\n \\"program\\": \\"${file}\\",\\n \\"console\\": \\"integratedTerminal\\",\\n \\"env\\": { \\"PYTHONPATH\\": \\"${workspaceRoot}\\" }\\n }\\n ]\\n}\\n\\n\\nNow happy debugging (just press F5).\\n","title":"Python Debugger with Project Directory Included in sys.path","date":"2021-10-25T00:00:00.000Z","id":"blog034","tag":"python","intro":"Record a structure that makes a test_sth.py debuggable and make sure project directory path is included in sys.path when debug mode is enabled."},{"content":"\\n#### Folder Structure\\n\\nWe start from the structure, sqs_client folder contains a utility class SqsClient in __init__.py which I have written in the past.\\n\\n<center>\\n <img src=\\"/assets/tech/027.png\\"/>\\n</center>\\n<p/>\\n\\nOriginally all functions are written within a class defined in __init__.py. It causes mental fatigue when there is a bunch of functions but to only one of them we want to focus.\\n\\nIt also violates the **_separation of concerns_** principle for each single .py file (as a module) when all functionality are written in just a single file.\\n\\nYou may also notice I have an underscore in _listen.py. As in other languages this notation has something to do with privateness. However:\\n\\n#### Naming Convention --- Can we Make Class Methods Private?\\n\\nIn python there is **_no concept_** of private methods, the best you can do is to use **_name mangling_** as in:\\n\\npython\\nclass A:\\n def __some_method(self):\\n pass\\n\\n\\nBut users can still get access to it by a = A() and a._A__some_method().\\n\\nWorse still, functions named with double underscored as prefix cannot be imported by any other files. This makes our approach of decomposing a class impossible (as we shall see).\\n\\nA single underscore _ is enough to indicate a function should just be used internally (you can inspect it form other built-in modules in python like os). In IDEs, these methods will be sorted at the bottom in auto-completion.\\n\\n#### Implementation of Code Separation\\n\\nAs usual our __init__.py will be the entry point of our class and:\\n\\npython\\n# __init__.py\\nclass SqsClient:\\n def __init__(self,...):\\n self.sqs_client = ...\\n\\n # we remove the following function and import it from send_message.py\\n # def send_message(self, message=\\"n/a\\"):\\n # self.sqs_client.send_message(MessageBody=message)\\n\\n from ._listen import _listen\\n from .send_message import send_message\\n from .send_test_message import send_test_message\\n from .start_listening_test_message import start_listening_test_message\\n from .add_receive_message_handler import add_receive_message_handler\\n from .start_listening import start_listening\\n\\n\\nAnd we take send_message.py as an example:\\n\\npython\\n# send_message.py\\ndef send_message(self, message=\\"n/a\\"):\\n self.sqs_client.send_message(MessageBody=message)\\n\\n\\nAs we can see:\\n\\n- We can copy and paste the methods written in __init__.py directly to another file without any modification of code (this make any refactoring rather simple!).\\n- In separated file we can still get access to attributes assigned to self.\\n\\nWhen we init an instance by sqs_client = SqsClient(), we get access to all methods! We can run something like\\n\\npython\\nsqs_client.send_message(message=\\"123\\")\\nsqs_client.start_listening()\\n# ... etc\\n\\n\\nand our __init__.py now just plays the role as an entry point.\\n","title":"Decompose a Class into Separate Files","date":"2021-10-26T00:00:00.000Z","id":"blog035","tag":"python","intro":"When a class grows to 500 to 1000 lines of codes it becomes hard and tedious to maintain because there starts to be many class methods, some are short, some are huge. It is a good starting point to separate particularly long functions from the class with ***very very minimal effort***, but how?"},{"content":"\\n#### Datetime Object Construction\\n\\n##### From Constructor\\n\\npython\\nfrom datetime import datetime\\n\\ndatetime(year, month, day, hour)\\ndatetime(year, month, day, hour, minute)\\ndatetime(year, month, day, hour, minute, secons)\\n...\\n\\n\\nare all valid method overloading for datetime object construction.\\n\\n##### From String\\n\\n###### From Local Time String to Local Datetime\\n\\npython\\nfrom datetime import datetime\\n\\nhk_time = \\"2021-11-11 10:32:34.126377\\"\\nhk_time_datetime = datetime.strptime(hk_time, \'%Y-%m-%d %H:%M:%S.%f\')\\n\\n\\n###### From Local Time String to UTC Datetime\\n\\npython\\nfrom datetime import datetime, timezone\\n\\nlocal_time = \\"2021-11-11 10:32:34.126377\\"\\nlocal_time_datetime = datetime.strptime(local_time, \'%Y-%m-%d %H:%M:%S.%f\') # local datetime object\\nutc_time_datetime = local_time_datetime.astimezone(timezone.utc)\\n\\nprint(local_time_datetime.hour, utc_time_datetime.hour)\\n# output: 10, 2\\n\\n\\n###### From UTC Time String to Local Datetime\\n\\npython\\nfrom datetime import datetime, timezone\\nfrom dateutil import tz\\n\\nutc_time = \\"2021-11-11 10:32:34.126377\\"\\nutc_time = datetime.strptime(utc_time, \'%Y-%m-%d %H:%M:%S.%f\') # This is still a local datetime\\nutc_time_datetime = utc_time.replace(tzinfo=timezone.utc) # correct time-zone info\\n\\nlocal_zone = tz.tzlocal()\\nhk_time_datetime = utc_time_datetime.astimezone(local_zone)\\n\\n\\n##### From Timestamp\\n\\nNo matter which timezone we use, we still get the unique timestamp as below:\\n\\npython\\nlocal = local_time_datetime.timestamp\\nutc = utc_time_datetime.timestamp\\n\\nprint(local, utc)\\n# result: 1636597954.126377, 1636597954.126377\\n\\n\\nTherefore timestamp is also a good data normalization of time, and the following script will do:\\n\\npython\\nlocal_datetime_obj = datetime.fromtimestamp(timestamp)\\n\\n\\nAlso note that we have the following conversion as timestamp may be generated from frontend which use javascript:\\n\\npython\\njs_timestamp = int(py_timestamp * 1000)\\npy_timestamp = js_timestamp / 1000\\n\\n\\n#### Datetime Object Manipulation\\n\\n##### Timedelta as Summand\\n\\nSometimes we want to **_calculate_** the offset time of a given time.\\n\\nFor example in a GPS track recording project, a trip may start at datetime start_time and end at datetime end_time. We need GNSS base stations data that are within two hours before start_time and 1 hour after end_time, then we do the following operations:\\n\\npython\\nfrom datetime import timedelta\\n\\noffset_start_time = start_time - timedelta(hours=2)\\noffset_end_time = end_time + timedelta(hours=1)\\n\\n\\nthe resulting object is still a datetime object.\\n\\nNow we can download the base stations data from Geodetic Survey of Hong Kong (GSHK) by converting them to utc+0 format (we can use .astimezone(timezone.utc) to generate utc datetime) and getting the corresponding hours.\\n\\n##### Timedelta from Subtraction\\n\\nSince base stations data are not immediately available at the time we stop our trip. We need to check whether the current time (the time we download the differential data) is far enough from datetime object end_time, say 15 minutes.\\n\\npython\\nfrom datetime import datetime\\n\\ndifference = datetime.now() - end_time # This is a Timedelta object\\ndiff_in_min = difference.total_seconds() / 60 # difference.seconds is not always what we want\\n\\nif diff_in_min >= 15:\\n # download data\\n pass\\n\\n","title":"Datetime Object in Python","date":"2021-11-20T00:00:00.000Z","id":"blog037","tag":"python","intro":"Record a list of useful utility functions in handling date-time object in python."},{"content":"\\n#### Lunch.json\\n\\nThe following configuration can enable us to run whatever file in debug mode.\\n\\njson\\n{\\n \\"version\\": \\"0.2.0\\",\\n \\"configurations\\": [\\n {\\n \\"type\\": \\"node\\",\\n \\"request\\": \\"launch\\",\\n \\"name\\": \\"Debug File\\",\\n \\"program\\": \\"${file}\\",\\n \\"env\\": {\\n \\"NODE_ENV\\": \\"some-stage\\",\\n \\"BDSONS_DB_PWD\\": \\"some-password\\"\\n },\\n \\"cwd\\": \\"${workspaceFolder}\\"\\n }\\n ]\\n}\\n\\n\\nIt is worth noting that some test cases also require specific environment variable. But how do we actually run a test file using library like ava (which is very similar to pytest)?\\n\\nFor that we need an analog of if __name__ == \\"__main__\\": in python, which turns out to be\\n\\njs\\nif (require.main === module) {\\n // our test function\\n}\\n\\n\\nBy using these we can design a test file that can be both executed by test library and also by developer individually.\\n","title":"Debug Javascript For Individual File","date":"2021-12-01T00:00:00.000Z","id":"blog038","tag":"javascript","intro":"Sometimes it is convenient to directly execute a test file to understand how the project work. In the past we discussed how to make a runnable test file in python, in javascript we can use exactly the same approach for debugging."},{"content":"\\n#### The Boilerplate\\n\\nFor Next.js and Electron there is an official boilerplate for new comers to work on:\\n\\n<a href=\\"https://github.com/vercel/next.js/tree/canary/examples/with-electron-typescript?fbclid=IwAR2DFf6dHAIEpaTp16FMfq-cUwIfwBXhgLuzscU6wkB_NCF4Bz-fRtS21W4\\">Link for Next.js, Electron and Typescript</a>\\n\\nUnforturnately there is no documentation on how to set up a debugger for this kind of \\"side product\\".\\n\\n#### Debugger Configuation in Vscode\\n\\n##### The launch.json File\\n\\nHaving struggled for a while I finally come up with the following configuration:\\n\\njson\\n// <project-dir>/.vscode/launch.json\\n{\\n \\"version\\": \\"0.2.0\\",\\n \\"configurations\\": [\\n {\\n \\"type\\": \\"node\\",\\n \\"request\\": \\"launch\\",\\n \\"name\\": \\"Next: Node\\",\\n \\"runtimeExecutable\\": \\"npm\\",\\n \\"runtimeArgs\\": [\\"run\\", \\"dev\\"],\\n \\"port\\": 9229,\\n \\"console\\": \\"integratedTerminal\\"\\n }\\n ]\\n}\\n\\n\\n##### The next.config.js File\\n\\nNext in the root directory create a file named next.config.js. Inside it we add:\\n\\njs\\n// <project-dir>/next.config.js\\n\\nmodule.exports = {\\n webpack: (config) => {\\n config.output = config.output || {};\\n config.output.devtoolModuleFilenameTemplate = function (info) {\\n return \\"file:///\\" + encodeURI(info.absoluteResourcePath);\\n };\\n return config;\\n },\\n};\\n\\n\\nNow we can press F5 to start npm run dev in debug mode. It works in frontend as well but in vscode I mostly debug backend script since chrome dev tool is much more convenient for debugging frontend code.\\n","title":"Debug Nextjs with Typescript in VSCode","date":"2021-12-02T00:00:00.000Z","id":"blog039","tag":"javascript, react, nextjs","intro":"I am working on a desktop application by using Electron + Next.js in typescript. Since I cannot live without debugger, I have spent time searching debug config in the internet and finally come into a functioning configuration!"},{"content":"\\n#### Result for the Moment\\n\\nThe following is a demo of the application I have made so far by Next.js and Electron.js.\\n\\n<center>\\n <video controls width=\\"500\\">\\n <source src=\\"/assets/videos/002.mp4\\" type=\\"video/mp4\\">\\n Sorry, your browser doesn\'t support embedded videos.\\n </video>\\n</center>\\n\\n<p/>\\n\\nThis is essentially the remake of an app I made using WPF, with which to achive this stage of result it took me almost 2 month to learn and work it out. But with next.js and electron it just took me about a week, in which I made this app when I am off my work.\\n\\n<p/>\\n\\n#### Starting Template\\n\\nNext.js\'s creator, Vercel, has a starting template <a href=\\"https://github.com/vercel/next.js/tree/canary/examples/with-electron-typescript?fbclid=IwAR07uNHSsGiBtukkMq8aXdv6maFhmRahu6_pKbxbjD9W14ielfZt6EN_Aw0\\"> <b>here</b></a> to begin with.\\n\\n#### Redux in Electron\\n\\nThe usual routine of adding redux stores also applies to Electron application with Next (the Render part). To add debugger for redux, we need:\\n\\ntext\\nyarn add electron-devtools-installer\\n\\n\\nand add\\n\\njavascript\\nif (process.env.NODE_ENV === \\"development\\") {\\n app.whenReady().then(async () => {\\n const installer = await import(\\"electron-devtools-installer\\");\\n const REDUX_DEVTOOLS = installer.REDUX_DEVTOOLS;\\n const installExtension = installer.default;\\n\\n installExtension(REDUX_DEVTOOLS)\\n .then((name) => console.log(Added Extension: ${name}))\\n .catch((err) => console.log(\\"An error occurred: \\", err));\\n });\\n}\\n\\n\\nright before app.on(\'ready\', ...).\\n\\n#### Lessons I learn\\n\\n##### Should we use pages/api?\\n\\n**_No_**. The resulting production build is nothing more than a static page. Of course in development we can call api to communicate with operating system, but afterwards there is no way to deploy your application to anywhere else.\\n\\nInstead, please keep using frontend\'s event emitter --- the ipcRenderer and the backend event receiver, the ipcMain. In my application I have wrapped up these two components into two functions in order to just focus on the data flow:\\n\\njs\\n// ipc-renderer-util.ts\\n\\nconst emit = <S, T>(eventKey: string, data: S) => {\\n return (\\n new Promise() <\\n T >\\n ((resolve, _reject) => {\\n global.ipcRenderer.once(eventKey, (_event, data: T) => {\\n resolve(data);\\n });\\n global.ipcRenderer.send(eventKey, data);\\n })\\n );\\n};\\n\\nexport default {\\n emit,\\n};\\n\\n\\njs\\n// ipc-main.util.ts\\n\\nimport { ipcMain } from \\"electron\\";\\nimport { IpcMainEvent } from \\"electron/main\\";\\n\\nconst listen = <S, T>(eventKey: string, callback: (data: S) => Promise<T>) => {\\n console.log(\\"eventKey\\", eventKey);\\n ipcMain.on(eventKey, (event: IpcMainEvent, data: S) => {\\n callback(data).then((result) => {\\n event.sender.send(eventKey, result);\\n });\\n });\\n};\\n\\nexport default {\\n listen,\\n};\\n\\n\\nUsage in \\"frontend\\" (the web view):\\n\\njs\\nconst addNote = async (note: Note) => {\\n const res = await ipcRendererUtil.emit<Note, CustomApiResponse>(\\n \\"ADD_NOTE\\",\\n note\\n )\\n return res;\\n}\\n\\n\\nUsage in \\"backend\\" (the file running Electron):\\n\\njs\\nipcMainUtil.listen < Note,\\n CustomApiResponse >\\n (\\"ADD_NOTE\\",\\n async (note) => {\\n const result = (await db.notes.insert) < Note > note;\\n return { success: true, result };\\n });\\n\\n\\n##### Images in Next.js can just be accessed in ./public folder. How to get rid of this Restriction?\\n\\nAt some point we may want the location to store images to be more controllable. We can achieve this by the following config:\\n\\njs\\n// next.config.js\\n\\nconst withImages = require(\\"next-images\\");\\n\\nmodule.exports = withImages({\\n webpack: (config) => {\\n config.output = config.output || {};\\n config.output.devtoolModuleFilenameTemplate = function (info) {\\n return \\"file:///\\" + encodeURI(info.absoluteResourcePath);\\n };\\n return config;\\n },\\n images: {\\n disableStaticImages: true,\\n },\\n});\\n\\n\\n##### How to get standard directory path such as Pictures, Documents, etc?\\n\\nWe use ~/Documents as an example:\\n\\njs\\nimport { app } from \\"electron\\";\\n\\napp.on(\\"ready\\", async () => {\\n app.getPath(\\"documents\\"); // path for documents\\n});\\n\\n\\nA complete list of possible paths can be found in this <a href=\\"https://www.electronjs.org/docs/latest/api/app#appgetpathname\\">documentation</a>.\\n\\n##### How to convert imagepath into base64 encoded data?\\n\\nAs I am doing it so often, I have summarized it in the following function. I make use of the conversion between img and canvas element:\\n\\njs\\nconst getBase64DataUrlFromImagePath = (imgPath: string) => {\\n return (\\n new Promise() <\\n { base64DataURL: string, width: number, height: number } >\\n ((resolve, reject) => {\\n const img = new Image();\\n img.src = imgPath;\\n img.onload = () => {\\n const _canvas = document.createElement(\\"canvas\\");\\n _canvas.width = img.width;\\n _canvas.height = img.height;\\n const _ctx = _canvas.getContext(\\"2d\\");\\n if (_ctx) {\\n _ctx.drawImage(img, 0, 0, img.width, img.height);\\n const dataURL = _canvas.toDataURL();\\n resolve({\\n base64DataURL: dataURL,\\n width: img.width,\\n height: img.height,\\n });\\n }\\n };\\n })\\n );\\n};\\n\\nexport default getBase64DataUrlFromImagePath;\\n\\n\\nIf one wants a base64 encoed string instead of a dataUrl, the following conversion\\n\\njs\\nconst base64Data = base64DataURL.replace(/^data:image\\\\/png;base64,/, \\"\\");\\n\\n\\nwill do.\\n\\n##### How to effectively make a \'click outside\' handler in React?\\n\\nWe create the following hook:\\n\\njs\\nimport { MutableRefObject } from \\"hoist-non-react-statics/node_modules/@types/react\\";\\nimport { useEffect } from \\"react\\";\\n\\nexport default <T = any>({\\n onClickOutside,\\n targetRef,\\n}: {\\n onClickOutside: (e: MouseEvent) => void,\\n targetRef: MutableRefObject<T | null>,\\n}) => {\\n const handleClickOutside = (event: any) => {\\n // @ts-ignore\\n if (targetRef.current && !targetRef.current.contains(event.target)) {\\n onClickOutside(event);\\n }\\n };\\n\\n useEffect(() => {\\n document.addEventListener(\\"click\\", handleClickOutside, true);\\n return () => {\\n document.removeEventListener(\\"click\\", handleClickOutside, true);\\n };\\n });\\n};\\n\\n\\nIf we click outside the targetRef.current, our onClickOutside should take effect.\\n\\n##### How to make right click context menu?\\n\\nThe react-contextmenu package is very simple to use, highly suggested!\\n\\nhttps://www.npmjs.com/package/react-contextmenu\\n\\n##### How to do persistent data storage? Any other choices apart from sqlite?\\n\\nIn the past I am just used to sqlite. A deeper consideration has been made since this is my second time to make a desktop application rigorously.\\n\\nLocally what I really need is no more than a few collections of json data, and in this direction I come into nedb, which is a mongo based local storage, each collection is saved in a *.db file with exactly the same query as we learn from mongoose (we can even borrow types in mongoose library when using nedb).\\n\\nIn fact, each *.db file is nothing more than rows of json data, which can be read directly in any text editor, easy to read and debug!\\n\\nSince in the original nedb library the only way to get data is by callback, we can switch to another highly related library\\n\\nhttps://www.npmjs.com/package/nedb-promises\\n\\nwhich has promisified all the functions in nedb for us.\\n\\n##### How to crop an image and get the base64DataUrl using canvas?\\n\\nThis will be quite a long story to get correct coordinates. Given that I have x, y (the upper-left vertix) and width, height prepared from a convas, then our whole workflow:\\n\\njs\\nconst getCroppedImageAndExtractText = async (pageId: string) => {\\n if (!canvasRef.current) {\\n return;\\n }\\n const x = Math.floor(coorRef.current.X_0_scaled);\\n const y = Math.floor(coorRef.current.Y_0_scaled);\\n const width = Math.floor(coorRef.current.width_scaled);\\n const height = Math.floor(coorRef.current.height_scaled);\\n\\n console.log(x, y, width, height);\\n\\n const _canvas = document.createElement(\\"canvas\\");\\n _canvas.width = width;\\n _canvas.height = height;\\n const _ctx = _canvas.getContext(\\"2d\\");\\n if (!_ctx) {\\n return;\\n }\\n _ctx.drawImage(canvasRef.current, x, y, width, height, 0, 0, width, height);\\n const base64DataURL = _canvas.toDataURL();\\n};\\n\\n\\nHere coordinates stored in coorRef get updated by our mousemove eventHandler.\\n\\n##### How to use the vision-api from google to do text extraction?\\n\\nFrom the previous step we have base64DataURL, we can then convert it to base64Data, convert it to Buffer object and call vision-api library:\\n\\njs\\nimport vision from \\"@google-cloud/vision\\";\\n\\nexport async function textExtraction(base64Data: string) {\\n try {\\n const client = new vision.ImageAnnotatorClient(option);\\n const buffer = Buffer.from(base64Data, \\"base64\\");\\n const [textDetections] = await client.textDetection(buffer);\\n //@ts-ignore\\n const [annotation] = textDetections.textAnnotations;\\n return annotation.description;\\n } catch (err) {\\n console.log(err);\\n return null;\\n }\\n}\\n\\n\\n##### How to make event listener to enable \'widen a div by dragging the edge\'?\\n\\n**Stategy.**\\n\\n- Record the current width of the div in a ref variable and fix it.\\n- When mouse down, we start to record the change in width.\\n- On mouse move, we change the state of width by calculating the increment in width, and add it to ref, then setState.\\n- When mouse up, we update the current width of the div in ref.\\n\\nNote that in this process there will be intermediate event listeners that should be being deleted, otherwise they will get accumulated when we repeat the drag and move process.\\n\\njs\\nconst mousedownHandler = useCallback((e) => {\\n if (!mouseIsDownRef.current) {\\n window.addEventListener(\\"mousemove\\", mousemoveHandler)\\n originalVocabListClientRectLeft.current = vocabListRef.current?.offsetWidth || 0\\n mouseIsDownRef.current = true;\\n oldClientX.current = e.clientX;\\n listenerCacheRef.current.push({ eventKey: \\"mousemove\\", listener: mousemoveHandler });\\n dragbarWasHit.current = resizeBarRef.current?.contains(e.target) as boolean;\\n } else {\\n mouseIsDownRef.current = false;\\n }\\n }, [])\\n\\n const mousemoveHandler = useCallback((e: globalThis.MouseEvent) => {\\n // @ts-ignore\\n if (resizeBarRef.current && dragbarWasHit.current && mouseIsDownRef.current) {\\n const increment = oldClientX.current - e.clientX;\\n setVocablistWidth((originalVocabListClientRectLeft.current || 0) + increment);\\n }\\n }, [])\\n\\n const mouseupHandler = useCallback((e) => {\\n setDragBarColorOpacity(0);\\n mouseIsDownRef.current = false;\\n mouseIsMovingRef.current = false;\\n dragbarWasHit.current = false;\\n originalVocabListClientRectLeft.current = vocabListRef.current?.offsetWidth || 0\\n if (listenerCacheRef.current) {\\n const numOfListeners = listenerCacheRef.current.length;\\n for (let i = 0; i < numOfListeners; i++) {\\n const pop = listenerCacheRef.current.pop();\\n if (pop) {\\n const { eventKey, listener } = pop;\\n window.removeEventListener(eventKey, listener)\\n }\\n }\\n }\\n }, [])\\n\\n useEffect(() => {\\n window.addEventListener(\\"mousedown\\", mousedownHandler)\\n window.addEventListener(\\"mouseup\\", mouseupHandler)\\n }, [])\\n\\n\\n##### Usage of debounce in lodash\\n\\ndebounce is used when there will be several similar calls within a short interval, and we just want to trigger the last call.\\n\\nUsages: search field, or text field that gets updated automatically. The usual strategy is to make these fields an uncontrolled component, we just look at the onChange event, and add a debounce function (that triggers update) in this onChange function.\\n\\nThe event-targets (like buttons, textfields, etc) are itself internally stateful, we will use a stateless function to make reference to a stateful uncontrolled component, get updated information and trigger our debounced event handler.\\n\\nTherefore our event handler needs not to update for triggering new function, this makes debounce possible.\\n\\n**Example 1.**\\n\\njs\\n ...\\n const searchDounce = useCallback(debounce((e: ChangeEvent<HTMLInputElement>) => {\\n FuncStore\\n const value = searchRef.current?.value;\\n if (value === \\"\\" || value) {\\n dispatch(dictSlice.actions.setSearchText(value));\\n }\\n }, 800), [])\\n\\n const searchHandler = (e: ChangeEvent<HTMLInputElement>) => {\\n searchDounce(e);\\n }\\n ...\\n return (\\n ...\\n <input\\n placeholder={\\"Search\\"}\\n ref={searchRef}\\n onChange={searchHandler}\\n />\\n ...\\n )\\n\\n\\n**Example 2.** In this example we have api-call after the text editing in a textarea is complete. Extremely good use case of debounce:\\n\\njs\\n// logic that should not be triggered repeatedly is wrapped in a debounce:\\nconst saveTextDebounce = useCallback(\\n debounce(async () => {\\n if (selectedPageId) {\\n const textAreaValue = textAreaRef.current?.value || \\"\\";\\n const res = await apiUtil.updatePage(selectedPageId, {\\n $set: { imageText: textAreaValue.trim() },\\n });\\n const { success, message } = res;\\n dispatch(\\n uiSlice.actions.updateNotification({\\n open: true,\\n message: success\\n ? \\"message saved succesfully\\"\\n : JSON.stringify(message),\\n severity: success ? \\"success\\" : \\"error\\",\\n })\\n );\\n }\\n }, 800),\\n [selectedPageId]\\n);\\n\\nconst changeHandler = (e: ChangeEvent<HTMLTextAreaElement>) => {\\n if (selectedPage) {\\n const text = e.target.value;\\n dispatch(\\n dictSlice.actions.setSelectedPageImageText({ selectedPageId, text })\\n );\\n saveTextDebounce();\\n }\\n};\\n\\n\\n##### How to register a global shortcut?\\n\\nIn my case I register ctrl+shift+n as a shortcut to my new page button. Right below my mainWindow.load(url):\\n\\njs\\n// electorn-src/index.ts\\nmainWindow.loadURL(url);\\n\\nglobalShortcut.register(\\"CommandOrControl+Shift+N\\", () => {\\n mainWindow.webContents.send(\\"GLOBAL_NEW_PAGE\\");\\n});\\n\\n\\nIn our frontend:\\n\\njs\\nexport default function Pages() {\\n const { pathname } = useRouter();\\n useEffect(() => {\\n global.ipcRenderer.on(\\"GLOBAL_NEW_PAGE\\", () => {\\n const newPageBtnEl = document.querySelector(\\"button#new-page-button\\");\\n if (newPageBtnEl) {\\n (newPageBtnEl as HTMLButtonElement).click();\\n }\\n })\\n }, [])\\n ...\\n\\n","title":"Nextjs with Electron","date":"2021-12-06T00:00:00.000Z","id":"blog040","tag":"react, nextjs, google-cloud, electron","intro":"Since FF14 has a new patch of huge update, in order to take this oppurtunity to learn Japanese I decided to write a desktop application again."},{"content":"\\n#### Situation I Encountered\\n\\nAs I am used to image processing in python than in nodejs, therefore I wrote a script in jupyter lab and wish to run it in my Electron-Nextjs application.\\n\\nThe script is very simple, it is used to compress all the images inside a folder:\\n\\npy\\n# compress_image.py\\n\\nfor image_name in os.listdir(IMG_DIR):\\n if image_name.endswith(\\".jpg\\"):\\n print(\\"processing\\", image_name)\\n pic = Image.open(os.path.join(IMG_DIR, image_name)).convert(\'RGB\')\\n\\n temp_img_dir = os.path.abspath(os.path.join(IMG_DIR, \\"..\\", \\"_screenshots\\"))\\n if not os.path.exists(temp_img_dir):\\n os.makedirs(temp_img_dir)\\n\\n tar_image_path = os.path.abspath(os.path.join(temp_img_dir, image_name))\\n pic.save(tar_image_path, optimize=True, quality=60)\\n print(tar_image_path, \\"saved\\")\\n\\nshutil.rmtree(IMG_DIR)\\nos.rename(temp_img_dir, IMG_DIR)\\n\\n\\n#### Call this Python Script in Nodejs and Read any Log from the print\\n\\nA function that takes cmd: string as input and output exitCode: number is created:\\n\\njs\\n// exec-util.js\\n\\nimport { exec } from \\"child_process\\";\\n\\nconst execUtil = async (cmd: string) => {\\n return (\\n (await new Promise()) <\\n number >\\n ((resolve, _) => {\\n console.log(\\"spwanning process with command: \\", cmd);\\n const child = exec(cmd);\\n child.stdout?.setEncoding(\\"utf8\\");\\n child.stderr?.setEncoding(\\"utf8\\");\\n\\n child.stdout?.on(\\"data\\", function (data) {\\n console.log(\\"stdout: \\" + data);\\n });\\n child.stderr?.on(\\"data\\", function (data) {\\n console.log(\\"stderr: \\" + data);\\n });\\n\\n child.on(\\"close\\", function (code) {\\n console.log(\\"exit code: \\" + code);\\n if (code === null) {\\n resolve(1);\\n } else {\\n resolve(code);\\n }\\n });\\n })\\n );\\n};\\n\\n\\nNow in my nodejs program (in fact, in Electron) I call the following:\\n\\njs\\nconst cmd = conda activate <env> && python <some/location/compress_image.py>;\\nconst exitCode = await execUtil(cmd);\\n\\n\\nWhich gives me the output coming from my print command in my python script (apart from exit code: 0):\\n\\ntext\\nstdout: C:\\\\Users\\\\user\\\\OneDrive\\\\Documents\\\\SCDictionary\\\\web-assets\\\\_screenshots\\\\1638900870035.jpg saved\\nprocessing 1638900889189.jpg\\nC:\\\\Users\\\\user\\\\OneDrive\\\\Documents\\\\SCDictionary\\\\web-assets\\\\_screenshots\\\\1638900889189.jpg saved\\nprocessing 1638900915323.jpg\\nC:\\\\Users\\\\user\\\\OneDrive\\\\Documents\\\\SCDictionary\\\\web-assets\\\\_screenshots\\\\1638900915323.jpg saved\\nprocessing 1638900919493.jpg\\nC:\\\\Users\\\\user\\\\OneDrive\\\\Documents\\\\SCDictionary\\\\web-assets\\\\_screenshots\\\\1638900919493.jpg saved\\nprocessing 1638900925208.jpg\\n...\\nexit code: 0\\n\\n","title":"Call Python Script in Node.js","date":"2021-12-11T00:00:00.000Z","id":"blog041","tag":"javascript","intro":"Sometimes we would like to call python script inside our node js program. We discuss how to execute a command, how to read the log and error in node-js and how to wait until this process is done."},{"content":"\\n#### Why Compression\\n\\nIn Electron saving a jpeg image with decent quality can produce a file of size ranged from 500kb to 2mb, which is way too big for storing and display in web. Like my page \\"Japanese Study\\" each \\"note\\" has at least 50 images.\\n\\nTherefore before saving any images, data compression is necessary in backend and we will use an npm library called sharp, it can compress and save an image even it is a buffer, a usual format for data transimission in nodejs backend.\\n\\n#### Implementations\\n\\nWe create the following utility function:\\n\\njs\\nimport sharp from \\"sharp\\";\\n\\nconst compressImageAndSave = (imageBuffer: Buffer, imagePath: string) => {\\n return (\\n new Promise() <\\n { err: Error, info: sharp.OutputInfo } >\\n ((resolve, _) => {\\n sharp(imageBuffer)\\n .jpeg({ mozjpeg: true, quality: 80 })\\n .toFile(imagePath, (err, info) => {\\n resolve({ err, info });\\n });\\n })\\n );\\n};\\n\\nexport default compressImageAndSave;\\n\\n\\nsharp can actually take many formats of data to create an sharp object. Instead of Buffer, you may also want string, uint8-array, etc. You can also reduce quality of png, webp, etc.\\n\\nNow an image that takes 2mb (saved from raw buffer and base64-encoded image) can now be reduced to 100kb with an unvisible downgrade of quality.\\n","title":"Nodejs Image Compression in Backend with sharp","date":"2021-12-16T00:00:00.000Z","id":"blog042","tag":"javascript","intro":"We discuss how to compress images effectively in nodejs backend."},{"content":"\\n#### Problem of Looping a Generator Using for Loop\\n\\nGiven a generator gen we can loop through the element that it generates by writing\\n\\npython\\nfor g in gen:\\n # do something\\n pass\\n\\n\\nThis kind of interations is fine if we do simple scripting/small tasks. I want to point out the downside of using for loop:\\n\\n##### No Control on When to Stop\\n\\nHowever, looping in this way means we have no control on when to stop our for loop until the generator get exhausted (which usually results from breaking the while loop in that generator). Especially when we do such kind of iteration in another thread, we wish to stop it in our will.\\n\\n##### Why break is not always Working\\n\\nYou may think well we can add a conditional break inside the for loop, but what if gen itself is blocking once certain condition is met and never ends? In this case any logic to break the for loop will never run.\\n\\nFor example, gen can be a generator that read the tail part of a file, i.e., it yields the latest line of the file whenever there is a new line written to that file. If the implementation has no stopping mechanism (like tailer.follow in <a href=\\"https://pypi.org/project/tailer/\\">tailer</a>), how do we stop looping a generator manaully?\\n\\nChanging the source code is not the best way as very likely we have to share the code within our team.\\n\\n#### If Interupting a Loop of Generator is What you Need\\n\\nInstead we create a while loop to consume the output of the generator:\\n\\npython\\ngen = my_gen()\\nstop_event = Event()\\n\\nwhile True and not stop_event.is_set():\\n try:\\n line = next(gen)\\n # do something\\n except StopIteration:\\n print(\\"stopped generator\\")\\n break\\n\\n\\nBe careful we have to handle the StopInteration exception that is raised when the while loop in gen breaks.\\n\\nNow we can stop the iteration whenever we trigger stop_event.set().\\n\\n#### Concret Example of Generator\\n\\nConsider the following generator:\\n\\npython\\ndef my_gen():\\n count = 0\\n start = time.time()\\n while True:\\n now = time.time()\\n time_diff = now - start\\n if 2 <= time_diff and time_diff < 6 and count <= 4 :\\n count += 1\\n yield(\\"start\\")\\n if time_diff >= 6:\\n break\\n\\n\\nNow if we run\\n\\npython\\nprint(\\"start generator\\")\\nfor line in my_gen():\\n print(line)\\nprint(\\"stopped generator\\")\\n\\n\\nwe get\\n\\npython\\nstart generator # at 0th second\\nstart # at 2th second\\nstart # at 2th second\\nstart # at 2th second\\nstart # at 2th second\\nstart # at 2th second\\nstopped generator # at 6th second\\n\\n\\n- Basically the generator is blocking (as we yield nothing) from 0 to 2nd second;\\n- print 5 start\'s at the 2nd second, blocking until 6th second;\\n- and the code continue to print stopped generator when it gets out of the for loop.\\n","title":"Python Generator","date":"2022-01-04T00:00:00.000Z","id":"blog043","tag":"python","intro":"A simple remark on generator in python."},{"content":"\\n#### GAN\\n\\n##### Import\\n\\npython\\n%matplotlib inline\\nfrom numpy.random import randint\\nfrom tensorflow.keras.datasets import mnist\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.losses import BinaryCrossentropy\\nfrom tensorflow.keras.layers import Input, Conv2D, Dense, Reshape, Flatten\\nfrom tensorflow.keras.models import Model\\n\\nimport numpy as np\\nimport cv2\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt\\nimport os\\n\\n\\n##### For GPU Training\\n\\npython\\nos.environ[\'TF_XLA_FLAGS\'] = \'--tf_xla_enable_xla_devices\'\\n\\nconfig = tf.compat.v1.ConfigProto(\\n gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\\n)\\nconfig.gpu_options.allow_growth = True\\nsession = tf.compat.v1.Session(config=config)\\ntf.compat.v1.keras.backend.set_session(session)\\n\\n\\n##### Constants\\n\\npython\\nbatch_size = 128\\nimg_shape = (28, 28, 1)\\n\\n\\n##### Models\\n\\n###### Generator\\n\\npython\\ndef get_generator(img_shape=img_shape, z_dim=100):\\n input = Input(shape=(z_dim,))\\n x = Dense(128)(input)\\n x = LeakyReLU(alpha=0.01)(x)\\n x = Dense(28*28, activation=\\"tanh\\")(x)\\n x = Reshape(img_shape)(x)\\n model = Model(input, x)\\n return model\\n\\n\\n###### Discriminator\\n\\npython\\ndef build_discriminator(img_shape=(28,28,1)):\\n input = Input(shape=img_shape)\\n x = Flatten()(input)\\n x = Dense(128)(x)\\n x = LeakyReLU(alpha=0.01)(x)\\n x = Dense(1, activation=\\"sigmoid\\")(x)\\n model = Model(input, x)\\n return model\\n\\n\\n##### Dataset Generator Using tf.data.Dataset\\n\\npython\\ndef get_dataset_gen(batch_size = batch_size):\\n (x_train, _), (_, _) = mnist.load_data()\\n x_train = x_train/127.5 - 1\\n x_train = np.expand_dims(x_train, axis=-1)\\n dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(1000).batch(batch_size)\\n return iter(dataset)\\n\\n\\n**Remark.** Sometimes we want dataset to be images mapped by image filepaths. Apart from tf.data.Dataset.list_files(target/dir/*.jpg), we can also save the list of imagepaths in notebook and apply .from_tensor_slices() as above, we then use .map to get the images.\\n\\n##### Training\\n\\n###### On Gradient Tapes\\n\\nSince generator and discriminator are two separate models, they are not trained at the same time. Namely, the update on weights inside generator and update on that in discriminator are independent of each other.\\n\\nHere we use two tf.GradientTape\'s to record calculations, and do the differentation separately.\\n\\n###### On Model.fit\\n\\nWe can still train a GAN by tricky use of Model.fit (by setting discrimnator.trainable to False before the compilation of the model).\\n\\nIn my opinion using gradient tape is much more straight-foward once we get the idea.\\n\\n###### Start the Training Loop\\n\\npython\\ngen_opt = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\\ndisc_opt = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\\nfig = plt.figure()\\nfig.set_figheight(20)\\nfig.set_figwidth(20)\\n\\nfor epoch in range(10):\\n dataset_gen = get_dataset_gen()\\n\\n for batch_num, batch_of_num_imgs in enumerate(dataset_gen):\\n batch_of_num_imgs_tensor = tf.convert_to_tensor(batch_of_num_imgs)\\n noise = tf.random.normal((batch_size, 100), mean=0.0, stddev=1.0)\\n\\n with tf.GradientTape(persistent=True) as tape:\\n faked_imgs = gen(noise)\\n critic_on_reals = disc(batch_of_num_imgs_tensor)\\n critic_on_fakeds = disc(faked_imgs)\\n\\n disc_loss_on_real_imgs = bcentropy(tf.ones_like(critic_on_reals), critic_on_reals)\\n disc_loss_on_faked_imgs = bcentropy(tf.zeros_like(critic_on_fakeds), critic_on_fakeds)\\n\\n disc_loss = 0.5*disc_loss_on_real_imgs + 0.5*disc_loss_on_faked_imgs\\n gen_loss = bcentropy(tf.ones_like(critic_on_fakeds), critic_on_fakeds)\\n\\n grad_disc = tape.gradient(disc_loss, disc.trainable_variables)\\n grad_gen = tape.gradient(gen_loss, gen.trainable_variables)\\n\\n gen_opt.apply_gradients(zip(grad_gen, gen.trainable_variables))\\n disc_opt.apply_gradients(zip(grad_disc, disc.trainable_variables))\\n\\n print(f\\"epoch{epoch}, processing batch {batch_num}\\", end=\\"\\\\r\\")\\n if batch_num % 10 == 0:\\n noise = np.random.normal(0, 1.0, (batch_size, 100))\\n imgs = gen.predict(noise)\\n for i in range(0,4):\\n for j in range(0, 4):\\n index = 4*i + j\\n img = imgs[index]\\n plt.subplot(4, 4, 1 + 4*i + j)\\n plt.imshow(img, cmap=\\"gray\\")\\n plt.savefig(f\\"epoch-{str(epoch).zfill(3)}_batch-{str(batch_num).zfill(3)}\\", dpi=80, bbox_inches=\\"tight\\")\\n\\n\\n##### Results at 5-th Epoch and 380-th Batch, batch_size = 128\\n\\nEach epoch has 460 batches, that means we have run through $460 * 4 + 380 = 2220$ batches.\\n\\n<center>\\n<a href=\\"/assets/tech/039.png\\">\\n<img src=\\"/assets/tech/039.png\\" width=\\"550\\"/>\\n</a>\\n</center>\\n\\n#### DCGAN\\n\\n##### Import\\n\\npython\\n%matplotlib inline\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport tensorflow as tf\\n\\nfrom tensorflow.keras.datasets import mnist\\nfrom tensorflow.keras.layers import BatchNormalization, Dense, Flatten, Reshape\\nfrom tensorflow.keras.layers import LeakyReLU\\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.optimizers import Adam\\n\\n\\n##### Constants\\n\\npython\\nimg_rows = 28\\nimg_cols = 28\\nchannels = 1\\n\\nimg_shape = (img_rows, img_cols, channels)\\n\\nz_dim = 100\\n\\n\\n##### Models\\n\\n###### Generator\\n\\npython\\ndef build_generator():\\n model = tf.keras.Sequential()\\n model.add(Dense(7*7*256, use_bias=False, input_shape=(100,)))\\n model.add(BatchNormalization())\\n model.add(LeakyReLU())\\n\\n model.add(Reshape((7, 7, 256)))\\n assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\\n\\n model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding=\'same\', use_bias=False))\\n assert model.output_shape == (None, 7, 7, 128)\\n model.add(BatchNormalization())\\n model.add(LeakyReLU())\\n\\n model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\'same\', use_bias=False))\\n assert model.output_shape == (None, 14, 14, 64)\\n model.add(BatchNormalization())\\n model.add(LeakyReLU())\\n\\n model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding=\'same\', use_bias=False, activation=\'tanh\'))\\n assert model.output_shape == (None, 28, 28, 1)\\n\\n return model\\n\\n\\n###### Discriminator\\n\\npython\\ndef build_discriminator():\\n model = Sequential()\\n model.add(\\n Conv2D(32,\\n kernel_size=3,\\n strides=2,\\n input_shape=img_shape,\\n padding=\'same\')\\n )\\n model.add(LeakyReLU(alpha=0.01))\\n model.add(\\n Conv2D(64,\\n kernel_size=3,\\n strides=2,\\n input_shape=img_shape,\\n padding=\'same\'))\\n model.add(BatchNormalization())\\n model.add(LeakyReLU(alpha=0.01))\\n model.add(\\n Conv2D(128,\\n kernel_size=3,\\n strides=2,\\n input_shape=img_shape,\\n padding=\'same\'))\\n model.add(BatchNormalization())\\n model.add(LeakyReLU(alpha=0.01))\\n model.add(Flatten())\\n model.add(Dense(1, activation=\'sigmoid\'))\\n\\n return model\\n\\n\\n##### Auxilary Function: sample_images\\n\\npython\\ndef sample_images(generator, image_grid_rows=4, image_grid_columns=4):\\n z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim))\\n gen_imgs = generator.predict(z)\\n gen_imgs = 0.5*gen_imgs + 0.5\\n fig, axs = plt.subplots(image_grid_rows,\\n image_grid_columns,\\n figsize=(4, 4),\\n sharey=True,\\n sharex=True\\n )\\n count = 0\\n for i in range(image_grid_rows):\\n for j in range(image_grid_columns):\\n axs[i, j].imshow(gen_imgs[count, :, :, 0], cmap=\\"gray\\")\\n axs[i, j].axis(\\"off\\")\\n count += 1\\n\\n plt.show()\\n\\n\\n##### Define Custom Training using Gradient Tape and Start\\n\\n##### Start the Training\\n\\npython\\ngenerator = build_generator()\\ndiscriminator = build_discriminator()\\n\\nlosses = []\\naccuracies = []\\niteration_checkpoints = []\\n\\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\\n\\ndef train(iterations, batch_size, sample_interval):\\n (x_train, _), (_, _) = mnist.load_data()\\n x_train = x_train/127.5 - 1.0\\n \\"\\"\\"\\n np.shape(x_train) = (60000, 28, 28)\\n for conv2D we need the channel dimension at the last axis\\n \\"\\"\\"\\n x_train = np.expand_dims(x_train, axis=3)\\n real = np.ones((batch_size, 1))\\n fake = np.zeros((batch_size, 1))\\n gen_opt = Adam(1e-4)\\n disc_opt = Adam(1e-4)\\n\\n for i in range(iterations):\\n print(f\\"iteration: {i}\\", end = \\"\\\\r\\")\\n idxs = np.random.randint(0, x_train.shape[0], batch_size)\\n imgs = x_train[idxs]\\n \\"\\"\\"\\n We define tf.Variable(n) outside gradient tape,\\n in gradient tape we record the calculation done on our training input (in tensor).\\n When we use tape.gradient outside the tape,\\n we are doing differentiation evaluated at the variable n.\\n \\"\\"\\"\\n z = tf.random.normal((batch_size, z_dim), 0, 1)\\n\\n with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\\n gen_imgs = generator(z, training=True)\\n decisions_for_gen = discriminator(gen_imgs, training=True)\\n decisions_for_real = discriminator(imgs, training=True)\\n\\n gen_loss = cross_entropy(real, decisions_for_gen)\\n\\n disc_fake_loss = cross_entropy(fake, decisions_for_gen)\\n disc_real_loss = cross_entropy(real, decisions_for_real)\\n disc_loss = 0.5*(disc_fake_loss + disc_real_loss)\\n\\n grad_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\\n grad_of_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\\n\\n gen_opt.apply_gradients(zip(grad_of_gen, generator.trainable_variables))\\n disc_opt.apply_gradients(zip(grad_of_disc, discriminator.trainable_variables))\\n\\n if (i+1) % sample_interval == 0:\\n iteration_checkpoints.append(i+1)\\n print(\\"%d [D loss: %f] [G loss: %f]\\" % (i + 1, disc_loss, gen_loss))\\n sample_images(generator)\\n\\n\\nFinally we start the training\\n\\npython\\niterations = 20000\\nbatch_size = 128\\nsample_interval = 1000\\n\\ntrain(iterations, batch_size, sample_interval)\\n\\n\\n##### Results at the 3000-th Batch, batch_size = 128\\n\\nCompared to the result of basic GAN with 2220 batches, the result of DCGAN at 3000-th batch is much better!\\n\\n<center>\\n<a href=\\"/assets/tech/040.png\\">\\n<img src=\\"/assets/tech/040.png\\"/>\\n</a>\\n</center>\\n","title":"GAN and DCGAN in Tensorflow","date":"2022-02-15T00:00:00.000Z","id":"blog044","tag":"tensorflow, deep-learning","intro":"In the past I have learnt the most basic GAN and DCGAN using pytorch, but I am more familiar with tensorflow. I attempt to understand gradient tape using by using these two GAN again with mnist dataset."},{"content":"\\n#### Start from Docker Hub\\n\\nWe don\'t need to install anything from mongo website, we just need to pull the mongo image from docker hub.\\n\\nTo have authentication, a super/root user called admin must be created in the first place. For that, instead of just running the image, we pass the environment variables\\n\\n- MONGO_INITDB_ROOT_USERNAME and\\n- MONGO_INITDB_ROOT_USERNAME\\n in\\n\\ntext\\ndocker run\\n -d\\n --name bddms-mongo \\\\\\n -p 27017:27017 \\\\\\n -e MONGO_INITDB_ROOT_USERNAME=admin \\\\\\n -e MONGO_INITDB_ROOT_PASSWORD=123 \\\\\\n -v \\"/c/Users/user/OneDrive/Documents/db_backup/docker_bddms\\":/data/db mongo\\n\\n\\nHere my local directory /c/Users/user/OneDrive/Documents/db_backup/docker_bddms is mount as a volume to the container\'s directory /data/db.\\n\\n#### Create User in Mongo Shell\\n\\nWhen this image is run in the detached mode, run\\n\\ntext\\ndocker exec -it cb39731a97ff sh\\n\\n\\nto get into the cli of the container running our mongo instance with id cb39731a97ff.\\n\\nWe are ready to init the mongo shell and add user. For this, run\\n\\ntext\\nmongo -u admin -p 123\\n\\n\\nFor sheer purpose of fulfilling the hard-coded config in the project, I need:\\n\\n- A user called bdsons with pwd bdsonspass;\\n- authSource=admin for authentication, i.e., create a user bdsons **_inside_** the database admin;\\n- bdsons to have access right to the database bdsons.\\n\\nTherefore:\\n\\ntext\\nuse admin\\n\\ndb.createUser({ \\\\\\nuser: \\"bdsons\\", pwd: \\"bdsonspass\\", \\\\\\nroles:[{db: \\"admin\\", role:\\"readWrite\\"}, {db:\\"bdsons\\", role:\\"readWrite\\"}] \\\\\\n})\\n\\n\\nThen by running db.getUsers() we have\\n\\ntext\\n[\\n ...,\\n {\\n \\"_id\\" : \\"admin.bdsons\\",\\n \\"userId\\" : UUID(\\"815c8f70-a65f-452d-a877-864d2d69fa00\\"),\\n \\"user\\" : \\"bdsons\\",\\n \\"db\\" : \\"admin\\",\\n \\"roles\\" : [\\n {\\n \\"role\\" : \\"readWrite\\",\\n \\"db\\" : \\"bdsons\\"\\n },\\n {\\n \\"role\\" : \\"readWrite\\",\\n \\"db\\" : \\"admin\\"\\n }\\n ],\\n \\"mechanisms\\" : [\\n \\"SCRAM-SHA-1\\",\\n \\"SCRAM-SHA-256\\"\\n ]\\n }\\n]\\n\\n\\nauthSource in the connection string to mongodb will look at user.db for authentication (trap: not the \\"db\\" in the roles).\\n\\n#### Connect to the mongoDb with Auth\\n\\nNow I can connect to the database named bdsons by using the string:\\n\\ntext\\nmongodb://bdsons:bdsonspass@localhost:27017/bdsons?authSource=admin\\n\\n","title":"Set up Local Environment for Mongo using Docker","date":"2022-02-25T00:00:00.000Z","id":"blog045","tag":"coding, mongo","intro":"Since I have docker already installed, just record a few steps to get the local development ready using mongo db with authentication."},{"content":"\\n#### In Python\\n\\nThis python logger not only log the message in the console, it also pipes all the message inside a file which we can review as long as this file is saved in a volume.\\n\\npython\\nimport logging\\nimport datetime\\nimport os\\nfrom utils.get_config import get_config\\n\\ndef create_logger(log_dir,logging_level=logging.INFO):\\n if not os.path.exists(log_dir):\\n os.makedirs(log_dir)\\n\\n logger = logging.getLogger()\\n logger.setLevel(logging_level)\\n formatter = logging.Formatter(\\n \'[%(levelname)1.1s %(asctime)s %(module)s:%(lineno)d] %(message)s\',\\n datefmt=\'%Y%m%d %H:%M:%S\')\\n\\n sh = logging.StreamHandler()\\n sh.setLevel(logging_level)\\n sh.setFormatter(formatter)\\n\\n log_filename = datetime.datetime.now().strftime(\\"%Y-%m-%d_%H_%M_%S.log\\")\\n fh = logging.FileHandler(os.path.sep.join([log_dir, log_filename]))\\n fh.setLevel(logging_level)\\n fh.setFormatter(formatter)\\n\\n logger.addHandler(sh)\\n logger.addHandler(fh)\\n\\n return logger\\n\\nlogger_dir = get_config(\\"log_dir\\", \\"sbg-logs\\")\\nlogger = create_logger(logger_dir)\\n\\n\\n#### In javascript\\n\\nUnlike python, we will use npm package tracer to customize the logger as follows:\\n\\njavascript\\nimport tracer from \\"tracer\\";\\nimport fs from \\"fs\\";\\nimport path from \\"path\\";\\nconst date = new Date();\\nconst year = date.getFullYear();\\nconst month = date.getMonth();\\nconst day = date.getDate();\\nconst hour = date.getHours();\\nconst mins = date.getMinutes();\\nconst seconds = date.getSeconds();\\nconst dateString = ${year}-${month}-${day}_${hour}-${mins}-${seconds};\\nconst logFileDir = \\"./logs\\";\\nconst logFileLocation = path.join(logFileDir, ${dateString}.log);\\n\\nif (!fs.existsSync(logFileDir)) {\\n fs.mkdirSync(logFileDir);\\n}\\nif (!fs.existsSync(logFileLocation)) {\\n fs.open(logFileLocation, \\"w\\", () => {});\\n}\\n\\nconst logger = tracer.colorConsole({\\n transport: function (data) {\\n fs.createWriteStream(logFileLocation, {\\n flags: \\"a\\",\\n encoding: \\"utf-8\\",\\n mode: 0o666,\\n }).write(data.rawoutput + \\"\\\\n\\");\\n },\\n format: [\\n \\"[{{timestamp}}-<{{title}}>-{{file}}:{{line}}] {{message}}\\", //default format\\n {\\n error:\\n \\"[{{timestamp}}-<{{title}}>-{{file}}:{{line}}] {{message}} \\\\nCall Stack:\\\\n{{stack}}\\", // error format\\n },\\n ],\\n dateformat: \\"HH:MM:ss.L\\",\\n preprocess: function (data) {\\n data.title = data.title.toUpperCase();\\n },\\n});\\n\\nexport default logger;\\n\\n","title":"Customized Logger in Python and Javascript","date":"2022-03-07T00:00:00.000Z","id":"blog046","tag":"python, javascript","intro":"Logging is undoubtedly the most important part of an application. We study how to create a logger that tells us a message comes from which file and which line."},{"content":"\\n#### IBM Documentation\\n\\nhttps://www.ibm.com/docs/it/dsm?topic=logs-create-sqs-queue-configure-s3-objectcreated-notifications\\n","title":"How to Configure S3 to Send SQS Message","date":"2022-03-10T00:00:00.000Z","id":"blog047","tag":"aws","intro":"Resource that explains how to construct correct policy on sqs-queue to allow s3 to send message to that queue."},{"content":"\\n#### Create pages/\\\\_document.js\\n\\nSince we will not be going to edit this file, even our project is written in typescript, it is no harm to add this file and never touch it.\\n\\nNow\\n\\njs\\n// pages/_document.js\\nimport React from \\"react\\";\\nimport Document, { Html, Head, Main, NextScript } from \\"next/document\\";\\nimport { ServerStyleSheets } from \\"@material-ui/core/styles\\";\\n\\nclass MyDocument extends Document {\\n static async getInitialProps(ctx) {\\n const sheets = new ServerStyleSheets();\\n const originalRenderPage = ctx.renderPage;\\n\\n ctx.renderPage = () =>\\n originalRenderPage({\\n enhanceApp: (App) => (props) => sheets.collect(<App {...props} />),\\n });\\n\\n const initialProps = await Document.getInitialProps(ctx);\\n\\n return {\\n ...initialProps,\\n // Styles fragment is rendered after the app and page rendering finish.\\n styles: [\\n ...React.Children.toArray(initialProps.styles),\\n sheets.getStyleElement(),\\n ],\\n };\\n }\\n\\n render() {\\n return (\\n <Html lang=\\"en\\">\\n <Head />\\n <body>\\n <Main />\\n <NextScript />\\n </body>\\n </Html>\\n );\\n }\\n}\\n\\nexport default MyDocument;\\n\\n\\n#### Edit \\\\_app.tsx\\n\\nFinally\\n\\njs\\n// pages/_app.tsx\\nuseEffect(() => {\\n const jssStyles = document.querySelector(\\"#jss-server-side\\");\\n if (jssStyles) {\\n jssStyles.parentElement?.removeChild(jssStyles);\\n }\\n}, []);\\n\\n\\n#### Reference\\n\\n- <a href=\\"https://stackoverflow.com/questions/66089290/materialui-makestyles-undoes-custom-css-upon-refresh-in-nextjs\\">MaterialUI makeStyles undoes custom css upon refresh in NextJS</a>\\n","title":"Additional Configuration for makeStyles in Next js","date":"2022-03-13T00:00:00.000Z","id":"blog048","tag":"react, nextjs","intro":"Without additional setup, due to the natural of serverside rendering, some styles would become undefined and we study how to avoid them."},{"content":"\\n#### Summary from Research\\n\\nIn conclusion, when we are concerned with the performance of the model under different/specific IoU, i.e., when bounding box accuracy is our concern, then we use mAP.\\n\\nHowever, some model is well-trained for annotating target object already, but it fails to spot diversified target that cause missing detection, then IoU is not our concern, we can simply use **_F1- score_**.\\n\\n#### F1-Score\\n\\nDefine\\n\\n$$\\n\\\\texttt{Precision} := \\\\frac{TP}{TP+FP}\\\\quad \\\\text{and}\\\\quad \\\\texttt{Recall} := \\\\frac{TP}{TP+FN},\\n$$\\n\\nthen we define F1-score to be\\n\\n$$\\n\\\\texttt{F1-score} := \\\\frac{2}{\\\\displaystyle\\\\frac{1}{ \\\\texttt{Precision}} + \\\\frac{1}{\\\\texttt{Recall}}} = \\\\frac{2\\\\times \\\\texttt{Precision}\\\\times \\\\texttt{Recall}}{ \\\\texttt{Precision}+\\\\texttt{Recall}}.\\n$$\\n\\n<p></p>\\n\\nI refer to **[ZL]** for more detail on F1-score and how it behaves compared to arithmetic mean and geometric mean. In short summary, $\\\\texttt{F1-score}$ penalizes **_unbalanced_** $\\\\texttt{Precision}$ and $\\\\texttt{Recall}$.\\n\\n#### mAP\\n\\n##### Precision and Recall\\n\\nGiven that we have fixed a label $\\\\mathcal L$ to work with, say $\\\\mathcal L = \\\\texttt{damaged_display}$, we need the following table with\\n\\n$$\\n\\\\text{Recall} = \\\\frac{\\\\text{acc. }TP}{\\\\text{Total number of signboard of class $\\\\mathcal L$}}.\\n$$\\n\\nNote that the denominator is actually the total number of ground truths from **_all_** images, not just from a single image.\\n\\nBy sorting the results using objectiveness/confidence score, we get a table like:\\n\\n<center>\\n<a href=\\"/assets/tech/028.jpg\\" target=\\"_blank\\">\\n <img src=\\"/assets/tech/028.jpg\\" style=\\"width:90%\\"/>\\n</a>\\n</center>\\n<p/>\\n\\n##### Why Recall\\n\\nPrecision $\\\\displaystyle \\\\frac{TP}{TP+FP}$ (where $FP$ is the number of boxes identified as $\\\\mathcal L$ in correctly) is not enough as it doesn\'t count undetected signboard.\\n\\n##### Calculation of mAP from the Graph\\n\\nBy using the precision and recall columns we should be able to graph it as follows:\\n\\n<center>\\n<a href=\\"/assets/tech/029.png\\" target=\\"_blank\\">\\n <img src=\\"/assets/tech/029.png\\" />\\n</a>\\n</center>\\n<p/>\\n\\nNote that the area under the path now means exactly the average precision (that\'s why recall is a fraction, it helps normalize the number of calls).\\n\\nProgrammatically we don\'t calculate that area directly, we smooth the graph out for approximated average precision in the following way:\\n\\n<center>\\n<a href=\\"/assets/tech/029.png\\" target=\\"_blank\\">\\n <img src=\\"/assets/tech/030.png\\"/>\\n</a>\\n</center>\\n<p/>\\n\\nThe new dots is exactly the same as the value $p(r)=\\\\max_{r\'\\\\ge r}p(r\')$, where $r$ is the recall, our approximated precision is\\n\\n$$\\n\\\\frac{1}{\\\\text{number of rows in table}} \\\\times \\\\sum_{r\\\\ge 0} p(r)\\n$$\\n\\nwhich is, for example,\\n\\n$$\\n\\\\frac{1}{11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0) = 26.84\\\\%\\n$$\\n\\nin the figure above.\\n\\n#### References\\n\\n- **[ZL]** Zeya LT, <a href=\\"https://towardsdatascience.com/essential-things-you-need-to-know-about-f1-score-dbd973bf1a3\\">Essential Things You Need to Know About F1-Score</a>\\n","title":"Simple Introduction to mAP and F1-Score","date":"2022-03-21T00:00:00.000Z","id":"blog049","tag":"deep-learning","intro":"Explain why and how it can provides a performance index to a localization algorithm."},{"content":"\\n#### Sample Code for Shape Experimental Calculation\\n\\nSuppose we have the up-sampling part in part of the U-Net:\\n\\npython\\n u = UpSampling2D(size=2)(layer_input)\\n u = Conv2D(filters, kernel_size=4, strides=1, padding=\\"same\\", activation=\\"relu\\")(u)\\n\\n\\nWhat is the resulting shape of the output? For strides=1 and padding=\\"same\\" we can memorize the output shape are always unchanged.\\n\\nBut what if kernel_size = 3 and strides=2, padding=\\"valid\\"? There is no point to memorize the formula for output shape as we can always experiment it out as follows:\\n\\npython\\nx = tf.random.normal([1, 28,28,3])\\nx = Conv2D(32, kernel_size=3, strides=2, padding=\\"valid\\", activation=\\"relu\\")(x)\\nprint(tf.shape(x))\\n\\n# output: tf.Tensor([ 1 13 13 32], shape=(4,), dtype=int32)\\n\\n\\n#### Rigorous Proof to Formula of Shapes\\n\\nLet $s\\\\in \\\\mathbb N$, for $\\\\texttt{strides=}s$ and $\\\\texttt{padding=\\"same\\"}$ we can prove the following:\\n\\n> $\\\\displaystyle \\\\texttt{output_width} = \\\\left\\\\lfloor\\\\frac{\\\\text{input_width}-1}{s}\\\\right\\\\rfloor + 1 = \\\\left\\\\lceil\\\\frac{\\\\text{input_width}}{s}\\\\right\\\\rceil$\\n\\nThis is due to the following simple fact:\\n\\n> **Fact.** Let $w$ and $s$ be positive integers, there holds\\n>\\n> $$\\n> \\\\left\\\\lfloor\\\\frac{w-1}{s}\\\\right\\\\rfloor + 1 = \\\\left\\\\lceil\\\\frac{w}{s}\\\\right\\\\rceil.\\n> $$\\n\\n<proof>\\n\\n**Proof.** We do case by case study. If $w=ks$ for some positive $k\\\\in \\\\mathbb N$, then\\n\\n$$\\n\\\\text{LHS} = \\\\left\\\\lfloor k - \\\\frac{1}{s}\\\\right\\\\rfloor +1 = (k-1)+1=k = \\\\lceil k\\\\rceil = \\\\text{RHS}.\\n$$\\n\\nWhen $w=ks+j$, for some $k\\\\in \\\\mathbb N$ and $j\\\\in \\\\mathbb N\\\\cap (0,s)$, then\\n\\n$$\\n\\\\text{LHS} = \\\\left\\\\lfloor k+\\\\frac{j-1}{s}\\\\right\\\\rfloor + 1 = k+1 = \\\\left\\\\lceil k+\\\\frac{j}{s}\\\\right\\\\rceil = \\\\left\\\\lceil \\\\frac{ks+j}{s}\\\\right\\\\rceil = \\\\left\\\\lceil\\\\frac{w}{s}\\\\right\\\\rceil=\\\\text{RHS}.\\n$$\\n\\n</proof>\\n","title":"Resulting Shapes of Conv-net by Direct Experiment","date":"2022-03-23T00:00:00.000Z","id":"blog050","tag":"deep-learning","intro":"From time to time it is easy to forget the formula to calculate the output shape of Conv2D, MaxPooling2D, etc, layers. Record some sample code to test shapes easily."},{"content":"\\nInclude the *.d.ts file in tsconfig.json with the following content:\\n\\njavascript\\ndeclare module \\"react-images\\" {\\n export var Modal;\\n export var ModalGateway;\\n export default Carousel;\\n}\\n\\ndeclare module \'*.pdf\' {\\n const src: string;\\n export default src;\\n}\\n\\ndeclare module \'*.md\' {\\n const md: string;\\n export default md;\\n}\\n\\ndeclare module \\"json-to-pretty-yaml\\" {\\n export default { stringify(json: Object): string }\\n}\\n\\n","title":"Type Annotation Record in Typescript for 3rd-party Library and Usual Import","date":"2022-03-23T00:00:00.000Z","id":"blog051","tag":"typescript","toc":false,"intro":"Record some type annotation used for third party untyped library or those needed in importing files (like pdf)."},{"content":"\\n#### Description of an Unusual Excel File\\n\\nAssume that we are going to process the following \\"unstructured\\" (or structured in an unusual way) excel file:\\n\\n<a href=\\"/assets/tech/031.png\\">\\n <img src=\\"/assets/tech/031.png\\" width=\\"100%\\">\\n</a>\\n<p/>\\n\\nThe image displays an excel organized:\\n\\n- with some row being categories,\\n- with some row being items of categories right above, and regarded as the true header of the table,\\n- with some row meaning True and False (colored or not).\\n- with some column even being empty.\\n\\nIt cannot be regarded as a usual table, we have to carefully handle it case by case. That means we have to be very familiar with package that handle the excel files.\\n\\n#### Pandas\\n\\n##### Read Lines for Pandas\\n\\npython\\nimport pandas as pd\\n\\nexcel_file = \\"Program_Details.xlsx\\"\\ndf = pd.read_excel(excel_file, engine=\\"openpyxl\\", sheet_name=\\"Raw Data\\")\\n\\nsymp_row = df.iloc[2, 6:]\\nprint(symp_row)\\n\\n\\ntext\\nUnnamed: 6 Anxiety\\nUnnamed: 7 Bereavement\\nUnnamed: 8 Cool Emotions\\nUnnamed: 9 Debility \u865b\u5f31 \\\\n\u865b\u5f31\\nUnnamed: 10 Depression\\n ...\\nUnnamed: 220 US$\\nUnnamed: 221 HK$\\\\n(US$1=HK$8)\\nUnnamed: 222 Discount /\\\\nShipping /\\\\nSurplus\\nUnnamed: 223 price per drop\\nUnnamed: 224 NaN\\n\\n\\n- Regardless of the behaviour of how pandas handle empty rows, we can experiment on\\n python\\n df.iloc[i, 6:]\\n \\n to test which row index i we should start with.\\n- Empty cells have value nan when we cast to string.\\n- To iterate over the row, we use\\n python\\n # generator that gives (col_name, value):\\n df.iloc[i, 6:].items()\\n \\n- we can also use\\n\\n python\\n datas_df = df.iloc[0:, 1:4]\\n\\n for index, value in datas_df.iterrows():\\n col1, col2, col3 = value\\n \\n\\n to unpack the values.\\n\\n#### openpyxl\\n\\nHowever, there is no way for pandas to get the color of a cell, we resort to openpyxl which pandas base on in order to read .xlsx (another extension for excel file).\\n\\n##### Read Lines for openpyxl\\n\\npython\\nimport openpyxl\\nfrom openpyxl import load_workbook\\n\\nexcel_file = \\"Program_Details.xlsx\\"\\nwb = load_workbook(excel_file, data_only=True)\\nsheet = wb[\'Raw Data\']\\n\\n\\n- The cell of the i-th row and j-th column is sheet[i][j], the values is read by\\n python\\n cell = sheet[i][j]\\n cell.value\\n \\n- The background color of the cell can be obtained by\\n python\\n cell.fill.start_color.index\\n \\n\\n#### Conclusion\\n\\n- If we just want to read an excel file, then openpyxl is enough.\\n\\n- If we want to manipulate the data in an excel file, then we use pandas as the syntax\\n python\\n df.iloc[i:j, h:k]\\n \\n is very handy compared to openpyxl.\\n","title":"Two Methods to Read Excel Files in Python","date":"2022-03-24T00:00:00.000Z","id":"blog052","tag":"python","intro":"Record two methods to read excel files in Python"},{"content":"\\n#### Results\\n\\n<center>\\n<a href=\\"/assets/tech/032.png\\">\\n<img src=\\"/assets/tech/032.png\\" width=\\"300\\" style=\\"margin-right:20px\\"/>\\n</a>\\n<a href=\\"/assets/tech/033.png\\">\\n<img src=\\"/assets/tech/033.png\\" width=\\"300\\"/>\\n</a>\\n</center>\\n<p/>\\n\\n#### With Gradient Tape, We Don\'t Need Model.training = False\\n\\nI create this whole model following the video in reference 1) of Reference section, however, I never get the training work for any algorithm that involves train_on_batch (I am suspicious about that after certain version of tensorflow, the call Model.training = False before compiling models fails).\\n\\nNevertheless it is a very good exercise for me to write down the formula and implement the update of parameter by gradient tape because I have to know what\'s happening behind the scene clearly.\\n\\nHope you also enjoy writing down the explicit formula of the losses and updating the parameters to have the feeling of \\"training separately\\" (the reason for Model.training = False).\\n\\n#### Preparation\\n\\n##### For Cuda\\n\\nIn order to enable cuda for traininig using GPU some special script needs to be run at the beginning.\\n\\npython\\nos.environ[\'TF_XLA_FLAGS\'] = \'--tf_xla_enable_xla_devices\'\\n\\nconfig = tf.compat.v1.ConfigProto(\\n gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8\\n))\\nconfig.gpu_options.allow_growth = True\\nsession = tf.compat.v1.Session(config=config)\\ntf.compat.v1.keras.backend.set_session(session)\\n\\n\\n##### Preliminary Import\\n\\npython\\n%matplotlib inline\\n\\nfrom tensorflow.keras.models import Model\\nimport os\\nimport matplotlib.pyplot as plt\\nimport tensorflow as tf\\nimport numpy as np\\nfrom tensorflow_addons.layers import InstanceNormalization\\nfrom tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, LeakyReLU, Activation, Concatenate\\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\\nfrom tensorflow.keras.initializers import RandomNormal\\nfrom tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError\\nfrom tensorflow.keras.optimizers import Adam\\nfrom numpy.random import randint\\nfrom numpy import load\\nfrom numpy import zeros, ones, asarray\\nfrom sklearn.utils import resample\\nfrom random import random\\n\\ndataset_name = \\"horse2zebra\\"\\nn_sample = 1000\\n\\n\\n##### Source of Training Data\\n\\nhttps://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/\\n\\n#### Formulas and Explanation\\n\\nIn cycle-GAN we have 4 models:\\n\\n- $g_{AB}:A \\\\to B$, a generator from domain $A$ to domain $B$\\n- $g_{BA}:B\\\\to A$, a generator from domain $B$ to domain $A$\\n- $d_A:A\\\\to [0,1]$, a discriminator model to discriminate images in domain $A$\\n- $d_B:B\\\\to [0,1]$, a discriminator model to discriminate images in domain $B$\\n\\nHere by \\"domain\\" we mean a category/class of images. For each training loop we will be training each of the models separately.\\n\\nDenote $p_A$ the distribution of data in domain $A$, and $p_B$ that in $B$. In each loop mathematically we wish to minimize the loss:\\n\\n$$\\n\\\\begin{align*}\\n\\\\mathcal L_{g_{AB}} &= \\\\mathbb E_{a\\\\sim p_A, b\\\\sim p_B} \\\\bigg(\\n5\\\\cdot \\\\mathcal L_{\\\\mathrm{id}} +\\n10\\\\cdot \\\\mathcal L_{\\\\mathrm{cycle}} +\\n\\\\mathcal L_{g\\\\text{-}d}\\n\\\\bigg)(a, b) \\\\\\\\\\n\\\\mathcal L_{d_A}(a,b) &= \\\\mathbb E_{a\\\\sim p_A, b\\\\sim p_B} \\\\frac{1}{2}\\\\bigg(\\\\|d_A(a) - 1\\\\|_2^2 + \\\\|d_A(g_{BA}(b)) - 0\\\\|_2^2\\\\bigg)\\n\\\\end{align*}\\n$$\\n\\nHere\\n\\n$$\\n\\\\begin{align*}\\n\\\\mathcal L_{\\\\mathrm{id}}(a, b) &= \\\\|b - g_{AB}(b)\\\\|_1\\\\\\\\\\n\\\\mathcal L_{\\\\mathrm{cycle}}(a,b) &= \\\\|a - g_{BA}(g_{AB}(a))\\\\|_1\\\\\\\\\\n\\\\mathcal L_{g\\\\text{-}d}(a, b)&= \\\\|d_B(g_{AB}(a)) - 1\\\\|_2^2\\n\\\\end{align*}\\n$$\\n\\n$\\\\mathcal L_\\\\mathrm{id}$ is called **_identitiy loss_**, $\\\\mathcal L_{\\\\mathrm{cycle}}$ is called **_cycle loss_** and $\\\\mathcal L_{g\\\\text{-}d}$ is called **generator\'s discriminator loss**.\\n\\nUnlike the simplest GANs, the result of discriminator is not a single value in $[0,1]$ any more, instead in this implementation we use a matrix of shape (None, 16, 16, 1). The negative logarithmic penality is replaced by mean-square loss (without taking root).\\n\\n$\\\\mathcal L_{g_{BA}}$ and $\\\\mathcal L_{d_B}(a,b)$ are similarly defined, the code implementation of the $\\\\mathcal L_{g_{AB}}$ can be found in implementation section of this article:\\n\\npython\\n5 * mae(X_realA, same_inA) + 10 * mae(X_realA, cycle_A) + g_BtoA_disc_loss\\n\\n\\n#### Implementation\\n\\n##### Helper Functions\\n\\npython\\ndef load_img_from_path(image_path):\\n pixels = load_img(image_path, target_size=(256, 256))\\n pixels = img_to_array(pixels)\\n pixels = pixels/127.5 - 1\\n return pixels\\n\\ndef generate_real_samples(path, n_samples=n_sample, patch_shape=16):\\n image_paths = np.array(get_image_paths(path))\\n indexes = randint(0, len(image_paths), n_samples)\\n X = np.array([load_img_from_path(path) for path in image_paths[indexes]])\\n y = ones((n_samples, patch_shape, patch_shape, 1))\\n return X, y\\n\\ndef generate_fake_samples(g_model, dataset, patch_shape=16):\\n X = g_model.predict(dataset)\\n y = zeros((len(X), patch_shape, patch_shape, 1))\\n return X, y\\n\\ndef gen_dataset(n_sample=500):\\n train_A_paths = get_image_paths(f\\"{dataset_name}/trainA\\")\\n train_B_paths = get_image_paths(f\\"{dataset_name}/trainB\\")\\n\\n random_state = np.random.randint(0, 100)\\n print(\\"random_state\\", random_state)\\n shuffle_indexes = resample(range(len(train_A_paths)), replace=False, n_samples=500, random_state=random_state)\\n shuffled_trainA_img = (load_img_from_path(train_A_paths[index]) for index in shuffle_indexes)\\n shuffled_trainB_img = (load_img_from_path(train_B_paths[index]) for index in shuffle_indexes)\\n dataset = zip(shuffled_trainA_img, shuffled_trainB_img)\\n return dataset\\n\\ndef update_fake_img_pool(pool, images, max_size=50):\\n selected = []\\n\\n for image in images:\\n if len(pool) < max_size:\\n pool.append(image)\\n selected.append(image)\\n elif random() < 0.5:\\n selected.append(image)\\n else:\\n # take one from the pool and and update the pool\\n # the pool may contain new image that we never\\n index = randint(0, len(pool))\\n selected.append(pool[index])\\n pool[index] = image\\n\\n return asarray(selected)\\n\\ndef check_dataset():\\n dataset_ = gen_dataset()\\n for i in range(3):\\n img_A, img_B = next(dataset_)\\n img_A = (img_A + 1) * 127.5\\n img_B = (img_B + 1) * 127.5\\n plt.subplot(2, 3, 1+i)\\n plt.axis(\\"off\\")\\n plt.imshow(img_A.astype(\\"uint8\\"))\\n\\n plt.subplot(2, 3, 4+i)\\n plt.axis(\\"off\\")\\n plt.imshow(img_B.astype(\\"uint8\\"))\\n\\n\\n##### Generators\\n\\npython\\ndef resnet_block(n_filters, input_layer):\\n init = RandomNormal(stddev=0.02)\\n # as strides = 1, the shape is invariant\\n g = Conv2D(n_filters, (3, 3), padding=\\"same\\", kernel_initializer=init)(input_layer)\\n g = InstanceNormalization(axis=-1)(g)\\n g = Activation(\\"relu\\")(g)\\n g = Conv2D(n_filters, (3, 3), padding=\\"same\\", kernel_initializer=init)(g)\\n g = InstanceNormalization(axis=-1)(g)\\n g = Concatenate()([g, input_layer])\\n return g\\n\\ndef define_generator(image_shape, n_resnet=9):\\n init = RandomNormal(stddev=0.02)\\n x = Input(shape=image_shape)\\n g = Conv2D(64, (7, 7), padding=\\"same\\", kernel_initializer=init)(x)\\n g = InstanceNormalization(axis=-1)(g)\\n g = Activation(\\"relu\\")(g)\\n\\n g = Conv2D(128, (3, 3), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(g)\\n g = InstanceNormalization(axis=-1)(g)\\n g = Activation(\\"relu\\")(g)\\n\\n g = Conv2D(256, (3, 3), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(g)\\n g = InstanceNormalization(axis=-1)(g)\\n g = Activation(\\"relu\\")(g)\\n\\n for _ in range(n_resnet):\\n g = resnet_block(256, g)\\n\\n g = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(g)\\n g = InstanceNormalization(axis=-1)(g)\\n g = Activation(\\"relu\\")(g)\\n\\n g = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(g)\\n g = InstanceNormalization(axis=-1)(g)\\n g = Activation(\\"relu\\")(g)\\n\\n g = Conv2D(3, (7, 7), padding=\\"same\\", kernel_initializer=init)(g)\\n g = InstanceNormalization(axis=-1)(g)\\n y = Activation(\\"tanh\\")(g)\\n\\n model = Model(x, y)\\n\\n return model\\n\\n\\n##### Discriminators\\n\\npython\\ndef define_discriminator(image_shape):\\n init = RandomNormal(stddev=0.02)\\n x = Input(shape=image_shape)\\n d = Conv2D(64, (4, 4), strides=2, padding=\\"same\\", kernel_initializer=init)(x)\\n d = LeakyReLU(alpha=0.2)(d)\\n\\n d = Conv2D(128, (4, 4), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(d)\\n d = InstanceNormalization(axis=-1)(d)\\n d = LeakyReLU(alpha=0.2)(d)\\n\\n d = Conv2D(256, (4, 4), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(d)\\n d = InstanceNormalization(axis=-1)(d)\\n d = LeakyReLU(alpha=0.2)(d)\\n\\n d = Conv2D(512, (4, 4), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(d)\\n d = InstanceNormalization(axis=-1)(d)\\n d = LeakyReLU(alpha=0.2)(d)\\n\\n d = Conv2D(512, (4, 4), padding=\\"same\\", kernel_initializer=init)(d)\\n d = InstanceNormalization(axis=-1)(d)\\n d = LeakyReLU(alpha=0.2)(d)\\n\\n y = Conv2D(1, (4, 4), padding=\\"same\\", kernel_initializer=init)(d)\\n model = Model(x, y)\\n return model\\n\\n\\n##### Custom Training Loop with Gradient Tape\\n\\n###### Create Models\\n\\npython\\ndef update_fake_image_pool(pool, images, max_size=50):\\n selected = []\\n\\n for image in images:\\n if len(pool) < max_size:\\n pool.append(image)\\n selected.append(image)\\n elif random() < 0.5:\\n selected.append(image)\\n else:\\n # take one from the pool and and update the pool\\n # the pool may contain new image that we never\\n index = randint(0, len(pool))\\n selected.append(pool[index])\\n pool[index] = image\\n\\n return asarray(selected)\\n\\ng_model_AtoB = define_generator(image_shape)\\ng_model_BtoA = define_generator(image_shape)\\nd_model_A = define_discriminator(image_shape)\\nd_model_B = define_discriminator(image_shape)\\n\\nn_epochs, n_batch, = 100, 1\\nn_patch = d_model_A.output_shape[1]\\n\\nbatch_per_epoch = int(n_sample/n_epochs)\\nn_steps = batch_per_epoch * n_epochs\\n\\n\\ndef show_result(step):\\n dataset_ = gen_dataset()\\n fig = plt.figure()\\n fig.set_figheight(10)\\n fig.set_figwidth(10)\\n\\n for i in range(3):\\n img_A, _ = next(dataset_)\\n fake_B = g_model_AtoB.predict(np.array([img_A]))\\n cycle_A = g_model_BtoA.predict(fake_B)\\n img_A = (img_A + 1) * 127.5\\n fake_B = (fake_B[0] + 1) * 127.5\\n cycle_A = (cycle_A[0] + 1) * 127.5\\n\\n plt.subplot(3, 3, 1+i)\\n plt.axis(\\"off\\")\\n plt.imshow(img_A.astype(\\"uint8\\"))\\n\\n plt.subplot(3, 3, 4+i)\\n plt.axis(\\"off\\")\\n plt.imshow(fake_B.astype(\\"uint8\\"))\\n\\n plt.subplot(3, 3, 7+i)\\n plt.axis(\\"off\\")\\n plt.imshow(cycle_A.astype(\\"uint8\\"))\\n plt.savefig(f\'result_{str(step).zfill(2)}.png\')\\n\\n plt.show()\\n\\n\\n###### Start Training Loop\\n\\nNote that in gradient tape we pass an option persistent=True because we need to calculate the gradient several times using the same tape (record of calculations).\\n\\npython\\nmse = MeanSquaredError()\\nmae = MeanAbsoluteError()\\n\\ng_AB_opt = Adam(learning_rate=0.0002, beta_1 = 0.5)\\ng_BA_opt = Adam(learning_rate=0.0002, beta_1=0.5)\\n\\nd_A_opt = Adam(learning_rate = 0.0002, beta_1 = 0.5)\\nd_B_opt = Adam(learning_rate=0.0002, beta_1=0.5)\\n\\npoolA = list()\\npoolB = list()\\n\\nfor i in range(2000):\\n shfted_index = i + 4880\\n print(f\\"step {shfted_index}\\", end=\\"\\\\r\\")\\n if shfted_index % 10 == 0:\\n show_result(shfted_index)\\n\\n X_realA, y_realA = generate_real_samples(f\\"./{dataset_name}/trainA\\", n_batch, n_patch)\\n X_realB, y_realB = generate_real_samples(f\\"./{dataset_name}/trainB\\", n_batch, n_patch)\\n X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\\n X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\\n\\n X_fakeA = update_fake_img_pool(poolA, X_fakeA)\\n X_fakeB = update_fake_img_pool(poolB, X_fakeB)\\n\\n\\n X_realA = tf.convert_to_tensor(X_realA)\\n y_realA = tf.convert_to_tensor(y_realA)\\n X_realB = tf.convert_to_tensor(X_realB)\\n y_realB = tf.convert_to_tensor(y_realB)\\n X_fakeA = tf.convert_to_tensor(X_fakeA)\\n y_fakeA = tf.convert_to_tensor(y_fakeA)\\n X_fakeB = tf.convert_to_tensor(X_fakeB)\\n y_fakeB = tf.convert_to_tensor(y_fakeB)\\n\\n with tf.GradientTape(persistent=True) as tape:\\n fake_B = g_model_AtoB(X_realA, training=True)\\n cycle_A = g_model_BtoA(fake_B, training=True)\\n\\n fake_A = g_model_BtoA(X_realB, training=True)\\n cycle_B = g_model_AtoB(fake_A, training =True)\\n\\n same_inB = g_model_AtoB(X_realB)\\n same_inA = g_model_BtoA(X_realA)\\n\\n disc_real_A = d_model_A(X_realA)\\n disc_fake_A = d_model_A(fake_A)\\n\\n disc_real_B = d_model_B(X_realB)\\n disc_fake_B = d_model_B(fake_B)\\n\\n g_AtoB_disc_loss = mse(y_realB, disc_fake_B)\\n g_BtoA_disc_loss = mse(y_realA, disc_fake_A)\\n\\n total_g_AtoB_loss = 5 * mae(X_realB, same_inB) + 10 * mae(X_realB, cycle_B) + g_AtoB_disc_loss\\n total_g_BtoA_loss = 5 * mae(X_realA, same_inA) + 10 * mae(X_realA, cycle_A) + g_BtoA_disc_loss\\n\\n # train discriminator and generator separately:\\n disc_B_loss = 0.5 * (mse(y_realB, disc_real_B) + mse(y_fakeB, disc_fake_B))\\n disc_A_loss = 0.5 * (mse(y_realA, disc_real_A) + mse(y_fakeA, disc_fake_A))\\n\\n # derivatives\\n grad_d_A = tape.gradient(disc_A_loss, d_model_A.trainable_variables)\\n grad_d_B = tape.gradient(disc_B_loss, d_model_B.trainable_variables)\\n\\n grad_g_AtoB = tape.gradient(total_g_AtoB_loss, g_model_AtoB.trainable_variables)\\n grad_g_BtoA = tape.gradient(total_g_BtoA_loss, g_model_BtoA.trainable_variables)\\n\\n # back-propagate\\n d_A_opt.apply_gradients(zip(grad_d_A, d_model_A.trainable_variables))\\n d_B_opt.apply_gradients(zip(grad_d_B, d_model_B.trainable_variables))\\n\\n g_AB_opt.apply_gradients(zip(grad_g_AtoB, g_model_AtoB.trainable_variables))\\n g_BA_opt.apply_gradients(zip(grad_g_BtoA, g_model_BtoA.trainable_variables))\\n\\n\\n#### Reference\\n\\n- Unpaired image to image translation\u200b using cycleGAN in keras, <br/>\\n https://www.youtube.com/watch?v=2MSGnkir9ew\\n\\n- Tensoflow\'s guide to CycleGAN, <br/>\\n https://www.tensorflow.org/tutorials/generative/cyclegan\\n\\n- Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, <br/>\\n https://arxiv.org/abs/1703.10593\\n","title":"Cycle-GAN in Tensorflow","date":"2022-03-25T00:00:00.000Z","id":"blog053","tag":"tensorflow, deep-learning","intro":"Implement custom training loop in tensorflow for Cycle-GAN without the use of *trick* that sets Model.training = False."},{"content":"\\n#### Connection\\n\\n- **In mongoose.**\\n\\n javascript\\n // getConnection.ts\\n import mongoose from \\"mongoose\\"\\n\\n let connCache: typeof mongoose;\\n\\n export default getConnection = () => {\\n if (!connCache) {\\n const dbOptions = {\\n user: config.get(\'db.user\'),\\n pass: config.get(\'db.password\'),\\n maxPoolSize: config.get(\'db.poolSize\'),\\n ssl: config.get(\'db.ssl\'),\\n replicaSet: config.get(\'db.replicaSet\'),\\n authSource: config.get(\'db.authSource\'),\\n useUnifiedTopology: true,\\n useNewUrlParser: true,\\n };\\n connCache = await mongoose.connect(config.get(\'db.uri\'), dbOptions);\\n }\\n return connCache\\n }\\n \\n\\n Example of corresponding mongo db config:\\n\\n json\\n {\\n \\"uri\\": \\"mongodb+srv://some.net/dbName?retryWrites=true&w=majority\\"\\n \\"user\\": \\"user\\"\\n \\"password\\": \\"password\\"\\n \\"authSource\\": \\"admin\\"\\n \\"ssl\\": true\\n }\\n \\n\\n With mongoose being a singleton, after monngoose.connect() we can freely import our Model objects and do our CRUD operations using these objects.\\n\\n **Use of the Connection Object.** Sometimes it is more convenient to get the model by just using the modelName , i.e., conn.model(modelName), and this is the reason we export the method to get conn.\\n\\n- **In mongoengine.** After pip install mongoengine in our virtual environment, we can start connecting to our db as follows:\\n\\n python\\n from mongoengine import connect\\n from config2.config import config\\n\\n # if our os.environ[\\"ENV\\"] == abc, make sure config/abc.json (or yaml) exists\\n # with the field \\"db\\": { \\"name\\": ..., }\\n db_config = config.get(\\"db\\");\\n\\n connect(\\n alias=db_config[\\"name\\"],\\n host=db_config[\\"uri\\"],\\n username=db_config[\\"user\\"],\\n password=db_config[\\"pwd\\"],\\n authentication_source=db_config[\'auth_source\']\\n )\\n \\n\\n#### Schema Examples and DataTypes\\n\\n- **In mongoose.** The datatypes used in a schema are simply.\\n\\n - String\\n - Number\\n - Boolean\\n - [subSchema]\\n - [String]\\n - [{id: String, name: String}]\\n\\n Recall that String, Number and Boolean are default classes built-in in javascript.\\n\\n Note that a **_sub-schema_** (i.e., a schema of the sub-document/embedded-document) will have _id generated by default. If we just want an ordinary object, make sure we have _id: false as below:\\n\\n js\\n const subSchema = new Schema<T>({...}, {_id: false, timestamps: true})\\n \\n\\n or otherwise just use an **_inline-schema_** as simple as {id: String, name:String}.\\n\\n Finally we raise an example of a schema:\\n\\n js\\n export const OilModelName = \\"Oil\\";\\n\\n const schema =\\n new Schema() <\\n TOil >\\n ({\\n id: String,\\n name: {\\n english: String,\\n latin: String,\\n chinese: String,\\n },\\n symptomToTreat: [String],\\n type: String,\\n isDeleted: Boolean,\\n },\\n {\\n timestamps: true,\\n });\\n\\n export const OilModel = model(OilModelName, schema);\\n \\n\\n- **In mongoengine.** The class attributes inside a subclass of Document actually define the schema for that document.\\n\\n Generally a schema in mongoengine looks like:\\n\\n python\\n from mongoengine import Document\\n from mongoengine.fields import FloatField, DateTimeField\\n from datetime import datetime\\n\\n class VibrationSensorMetrics(Document):\\n meta = {\\n \\"collection\\": \\"vibrationsensormetrics\\"\\n }\\n rmsVelocity = FloatField()\\n createdAt = DateTimeField(default=datetime.utcnow())\\n \\n\\n Due to the value in meta, a new document will be saved in a collection named automatically as **_vibrationsensormetrics_** (lowered cases + \\"s\\")\\n\\n We have the following fields that define the type of each \\"key\\" in a document:\\n\\n - StringField\\n - DictField\\n - BooleanField\\n - IntField\\n - ListField\\n - DateTimeField\\n - ObjectIdField\\n - FloatField\\n - EmbeddedDocument\\n - EmbeddedDocumentField\\n - DictField\\n Among the fields above, DictField is used only when we know it is an object but don\'t know the data structure. For known structure, we use EmbeddedDocumentField and we explain it in the next section.\\n\\n#### Complicated Example with Nested Objects in mongoengine\\n\\nWe continue to complexify the schema of VibrationSensorMetrics above, we will use all of the datatypes defined in the previous section.\\n\\npython\\nfrom mongoengine import Document\\nfrom mongoengine.fields import (\\n StringField, BooleanField,\\n IntField, ListField, DateTimeField,\\n ObjectIdField, FloatField, EmbeddedDocument,\\n EmbeddedDocumentField\\n)\\nfrom datetime import datetime\\n\\n\\nclass FreqVelocity(EmbeddedDocument):\\n frequency = FloatField()\\n velocity = FloatField()\\n\\n\\nclass SpecificFreqVelocities(EmbeddedDocument):\\n ft = EmbeddedDocumentField(FreqVelocity)\\n rpm = EmbeddedDocumentField(FreqVelocity)\\n bsf = EmbeddedDocumentField(FreqVelocity)\\n bpfo = EmbeddedDocumentField(FreqVelocity)\\n bpfi = EmbeddedDocumentField(FreqVelocity)\\n\\n\\nclass VibrationSensorMetrics(Document):\\n meta = {\\n \'collection\': \'vibrationsensormetrics\',\\n \'indexes\': [\\n (\'sensorId\', \'measurableId\', \'workspaceId\'),\\n (\'sensorId\', \'measurableId\')\\n ]\\n }\\n\\n def save(self, *args, **kwargs):\\n if not self.createdAt:\\n self.createdAt = datetime.utcnow()\\n self.updatedAt = datetime.utcnow()\\n return super(VibrationSensorMetrics, self).save(*args, **kwargs)\\n\\n _id = ObjectIdField()\\n sensorId = ObjectIdField()\\n measurableId = ObjectIdField()\\n measurableComponentId = ObjectIdField()\\n workspaceId = ObjectIdField()\\n direction = StringField()\\n s3ObjectKey = StringField()\\n\\n freq = ListField(FloatField())\\n acceleration = ListField(FloatField())\\n velocity = ListField(FloatField())\\n stage = IntField()\\n rmsVelocity = FloatField()\\n specificFreqVelocity = EmbeddedDocumentField(SpecificFreqVelocities)\\n\\n startTime = DateTimeField()\\n endTime = DateTimeField()\\n createdAt = DateTimeField()\\n updatedAt = DateTimeField()\\n\\n archived = BooleanField()\\n\\n\\nThe EmbeddedDocumentField is used to embed a document (without ObjectId) into that key.\\n\\nAn example of the above schema:\\n\\njson\\n{\\n \\"_id\\": {\\n \\"$oid\\": \\"6243d2cfb6c5eb79cbded8dc\\"\\n },\\n \\"sensorId\\": {\\n \\"$oid\\": \\"623a8e98607fd5d3f2061412\\"\\n },\\n \\"measurableId\\": {\\n \\"$oid\\": \\"623bcb04dc39ae10e39c7857\\"\\n },\\n \\"measurableComponentId\\": {\\n \\"$oid\\": \\"623bcb04dc39ae10e39c7858\\"\\n },\\n \\"workspaceId\\": {\\n \\"$oid\\": \\"62219400a6533f774a0a8500\\"\\n },\\n \\"direction\\": \\"radial\\",\\n \\"s3ObjectKey\\": \\"ICC_Vibration/Erbessd_Phantom/189280107/20220304/0504_Ch1.txt\\",\\n \\"freq\\": [],\\n \\"acceleration\\": [],\\n \\"velocity\\": [],\\n \\"stage\\": 0,\\n \\"rmsVelocity\\": 0.7,\\n \\"specificFreqVelocity\\": {\\n \\"ft\\": {\\n \\"frequency\\": 6.346813346174303,\\n \\"velocity\\": 0.6951668919677223\\n },\\n \\"rpm\\": {\\n \\"frequency\\": 14.166666666666666,\\n \\"velocity\\": 0.3500187046119141\\n },\\n \\"bsf\\": {\\n \\"frequency\\": 56.86395840490943,\\n \\"velocity\\": 0.05195638432785299\\n },\\n \\"bpfo\\": {\\n \\"frequency\\": 139.62989361583465,\\n \\"velocity\\": 0.01651277478295155\\n },\\n \\"bpfi\\": {\\n \\"frequency\\": 172.036773050832,\\n \\"velocity\\": 0.007293205675754226\\n }\\n },\\n \\"startTime\\": {\\n \\"$date\\": \\"2022-03-03T21:04:00.000Z\\"\\n },\\n \\"endTime\\": {\\n \\"$date\\": \\"2022-03-03T21:04:05.000Z\\"\\n },\\n \\"timestamp\\": {\\n \\"$date\\": \\"2022-03-30T03:47:25.852Z\\"\\n }\\n}\\n\\n\\n#### Operations\\n\\nBasically in mongoose we rely on the Model instance and in mongoengine we rely on the Document instance.\\n\\n##### Init Models/Documents, Create and Save Documents, Collection Names\\n\\n- **In mongoose.** We create/init a Model object by either\\n\\n js\\n import { Schema, model } from \\"mongoose\\";\\n\\n type TCustom = { id: String, name: String, isDeleted: Boolean };\\n const schema =\\n new Schema() <\\n T >\\n ({ id: String, name: String, isDeleted: Boolean }, { timestamps: true });\\n\\n export const SymCatModel = model(SymCatModelName, schema);\\n \\n\\n or\\n\\n js\\n // create connection\\n const conn = await mongoose.connect(config.get(\\"db.uri\\"), dbOptions);\\n // init model\\n conn.model(\\"TheModel\\", theModelSchma);\\n // get the model\\n model = conn.model(\\"TheModel\\");\\n \\n\\n and use the async methods:\\n\\n - Model.create(object: TCustom) or\\n - Model.insertMany(objects: TCustom[])\\n to create documents (don\'t need to call .save() method).\\n\\n- **In mongoengine.** We create/init a Document instance, the instance is a callable function that can create a document:\\n\\n python\\n vsm_doc = VibrationSensorMetrics()\\n new_doc = vsm_doc(\\n rmsVelocity = 0.1\\n )\\n new_doc.save()\\n \\n\\n A new document will be saved in a collection named vibrationsensormetrics.\\n\\n##### Query for Documents\\n\\n- **In mongoose.** We use:\\n\\n - Model.find(filterQuery)\\n\\n- **In mongoengine.** We can either pass a **_keyword argument_** or a **_raw dictionary_** as a filterQuery to a Document instance:\\n - Document.objects.get(_id=some_id)\\n - Document.objects(_id=some_id).order_by(\\"-timestamp\\").first()\\n - Document.objects(__raw__={ \\"_id\\": some_id })\\n For example, we can assign vsm_doc = VibrationSensorMetrics() with vsm_doc in place of Document in the above methods.\\n\\n##### Update Documents\\n\\n- **In mongoose.**\\n\\n js\\n // or updateMany\\n const result = await OilModel.updateOne(\\n { id: oilUpdate.id },\\n { $set: oilUpdate },\\n { upsert: false }\\n );\\n \\n\\n- **In mongoengine.**\\n python\\n vsm_doc = VibrationSensorMetrics()\\n vsm_doc.objects(_id=some_id).update(__raw__={\\n \\"$set\\": {\\n f\'measurableComponent.{cpn_index}.velocity.value\': highest_velocity,\\n f\'measurableComponent.{cpn_index}.velocity.latest\': time\\n },\\n \\"unset\\": {\\n \\"measurableComponents\\": \\"\\"\\n }\\n },\\n upsert=False\\n )\\n \\n\\n##### Delete Documents\\n\\nFor the sake of completeness we also mention how to delete documents. Although in practice we prefer assigning a boolean to **_soft-delete_** a document instead.\\n\\n- **In mongoose**.\\n js\\n Model.deleteMany(filterQuery);\\n \\n- **In mongoengine**.\\n js\\n Document.objects((__raw__ = filterQuery)).delete();\\n \\n","title":"Mongoose in Nodejs and Mongoengine in Python","date":"2022-03-29T00:00:00.000Z","id":"blog054","tag":"python, nodejs, mongo","intro":"Record everything we should know about mongo with mongoose in nodejs and mongoengine in python."},{"content":"\\n#### Make Button able to be Ctrl-Clicked and Middle-Clicked\\n\\nIf we simply make a button with history.push() registered as a click event, then this button cannot be ctrl-clicked and middle-clicked. Or even if we implement window.open(href, \\"_blank\\") when only these two operations are performed, we cannot manage to keep the browser to stay in the current tab.\\n\\nThe only possible way is to use anchor, so let\'s record the following function that keep ordinary anchor behaviour and use history.push for simple left-click!\\n\\nThe key boolean we need is:\\n\\njs\\nconst middleClickedOrCtrlPressed = event.button === 1 || event.ctrlKey;\\n\\n\\nThis indicates either\\n\\n- the middle-button is clicked or\\n- the control key is pressed,\\n with that:\\n\\njs\\n const navToArticle = (articleId: string) => (event?: MouseEvent<HTMLElement>) => {\\n if (event) {\\n if (event.button === 1 || (event.ctrlKey && event.button === 0)) {\\n // middle-click or ctrl+leftclick\\n window.open(urlByArticleId(articleId), \\"_blank\\");\\n }\\n else {\\n history.push(urlByArticleId(articleId))\\n }\\n }\\n }\\n return (\\n ...\\n <a\\n href={urlByArticleId(articleId)}\\n onClick={navToArticle(articleId)}\\n >\\n <Button>\\n ...\\n </Button>\\n </a>\\n )\\n\\n\\nSometimes we use <a/> to wrap <button/> element that may have animation effect,\\nwe may need to setTimeout for history.push (with 50ms, say) for smoother experience.\\n\\n#### Scroll to Target Element\\n\\nRecently I have implemented a floating TOC (as shown in the picture or LHS of this article), this will pop up when the browser scroll down and go across some threshold depending on the bottom of the top TOC:\\n\\n<center>\\n<a href=\\"/assets/tech/034.png\\">\\n<img src=\\"/assets/tech/034.png\\" width=\\"600px\\"/>\\n</a>\\n</center>\\n\\n<p/>\\n<center></center>\\n\\nBut when there are too many titles, for example: in <a href=\\"/blog/article/Nextjs-with-Electron\\">this article</a>, then it is natural to wish the floating TOC can highlight and **_scroll to that title_** automatically.\\n\\nFor highlighting title, behind the scene we calculate at which position we have scrolled to and assign the corresponding active anchor in the floating TOC an active className. We now focus on how to get the **_relative position_** of our _active anchor_ from the top of its parent --- the floating TOC.\\n\\nSuppose as in the picture we want to highlight the _active anchor_: **_How to convert imagepath into base64 encoded data_**, which is stored in the variable titleAnchor, then:\\n\\n- The **relative position** of titleAnchor to its parent is calculated by\\n js\\n const scrollDistance = (titleAnchor as HTMLAnchorElement).offsetTop;\\n \\n- We get the parent by the assigned id\\n js\\n const floatingToc = document.querySelector(\\"#floating-toc\\");\\n \\n- Then we scroll to the specfic position by\\n js\\n floatingToc?.scrollTo({\\n left: 0,\\n top: scrollDistance,\\n behavior: \\"smooth\\",\\n });\\n \\n","title":"Ctrl- and Middle-Clickable Button; Method to Scroll to Target Element","date":"2022-03-30T00:00:00.000Z","id":"blog055","tag":"react","intro":"Record how to make button left- and middle-clickable; Record how to scroll to desired HTML element vertically."},{"content":"\\n#### About this Article\\n\\nThis article is an experiment inspired by this tutorial: <a href=\\"https://towardsdatascience.com/colorizing-black-white-images-with-u-net-and-conditional-gan-a-tutorial-81b2df111cd8\\">Colorizing black & white images with U-Net and conditional GAN \u2014 A Tutorial</a>.\\n\\nSince I am not used to pytorch which the tutorial bases on, the following things will be rewritten in tensorflow:\\n\\n- The models\\n- The data processing pipeline\\n- The generator of dataset\\n- The train loop (including the update of training weights)\\n- The visualization of our results\\n\\nEnjoy!\\n\\n#### Results\\n\\nSince the training datasets are just animated characters. The only common characteristic are the color of skins, therefore the model is not able to paint clothes, hair in a colorful way (as it has no idea how to learn).\\n\\nOriginal image:\\n\\n<center>\\n<img src=\\"/assets/tech/035.jpg\\"/> \\n<p/>\\n</center>\\n\\nTransferred to gray scale and let the GAN color it:\\n\\n<center>\\n<img src=\\"/assets/tech/036.png\\" width=\\"600\\"/>\\n<p/>\\n</center>\\n\\n#### Preliminary Import\\n\\n##### Usual Packages\\n\\npython\\nfrom numpy.random import randint\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError\\nfrom tensorflow.keras.initializers import HeNormal\\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\\nfrom tensorflow.keras.layers import (\\n Input, Conv2D, Conv2DTranspose, LeakyReLU,\\n Activation, Concatenate, BatchNormalization, ZeroPadding2D\\n)\\nimport numpy as np\\nimport cv2\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt\\nfrom glob import glob\\nimport os\\nfrom tensorflow.keras.models import Model\\nfrom tqdm.notebook import tqdm\\nfrom skimage.color import rgb2lab, lab2rgb\\nimport os\\n\\n%matplotlib inline\\n\\n\\n##### For Using GPU\\n\\npython\\nos.environ[\'TF_XLA_FLAGS\'] = \'--tf_xla_enable_xla_devices\'\\n\\nconfig = tf.compat.v1.ConfigProto(\\n gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\\n)\\nconfig.gpu_options.allow_growth = True\\nsession = tf.compat.v1.Session(config=config)\\ntf.compat.v1.keras.backend.set_session(session)\\n\\n\\n#### RGB Color Space and Lab Color Space (WIP)\\n\\nTo be added.\\n\\n#### Dataset Generator\\n\\nWe will again make use of the tf.data.Dataset.list_files as a starting point to create a tf.data.Dataset object.\\n\\nThe Dataset object is very handy because of the following useful methods:\\n\\n- .map()\\n- .filter()\\n- .shuffle(buffer_size)\\n- .batch(batch_size)\\n- .cache()\\n\\n###### Preprocessing Functions\\n\\npython\\nSIZE = 256\\n\\ndef path_to_img(file_path):\\n img = tf.io.read_file(file_path)\\n img = tf.image.decode_jpeg(img, channels=3)\\n img = tf.image.resize(img, (SIZE, SIZE))\\n return img\\n\\ndef rgb_normalize_to_0_1(img):\\n img = tf.cast(img, dtype=tf.float32)\\n return img/255\\n\\ndef rgb_denormalize_from_0_1(img):\\n return (img + 1)*127.5\\n\\ndef lab_normalize_to_minus1_to_1(img):\\n L = img[:, :, 0] / 50. - 1\\n ab = img[:, :, [1, 2]]/110.\\n return L[..., np.newaxis], ab\\n\\ndef lab_denormalize_from_minus1_to_1(img):\\n L = (img[:, :, [0]] + 1) * 50\\n ab = img[:, :, [1, 2]] * 110\\n return np.concatenate([L, ab], axis=-1)\\n\\ndef process_img(img):\\n # tf.numpy_function(func=lambda x: print(x), inp=[img], Tout=tf.float32)\\n img = img/255.\\n img = tf.image.random_flip_left_right(img)\\n\\n img_lab = tf.numpy_function(func=lambda x: rgb2lab(x).astype(\\"float32\\"),\\n inp=[img],\\n Tout=tf.float32)\\n\\n L, ab = tf.numpy_function(func=lab_normalize_to_minus1_to_1,\\n inp=[img_lab],\\n Tout=[tf.float32, tf.float32])\\n\\n return L, ab\\n\\n\\n###### Chaining Preprocessing Functions\\n\\nTherefore we just need to take care of how to preprocess data from individual file path. Dataset\'s api will handle the rest.\\n\\nAmong the above, we use .map to chain our data processing pipeline:\\n\\npython\\ndef get_data_generator():\\n buffer_size = 100\\n batch_size = 16\\n dataset = tf.data.Dataset.list_files(f\\"{dataset_name}/*.jpg\\")\\\\\\n .map(path_to_img)\\\\\\n .map(process_img)\\\\\\n .shuffle(buffer_size)\\\\\\n .batch(batch_size)\\n\\n return (data for data in iter(dataset))\\n\\n\\n#### Implementation\\n\\n##### Generator by UNet Structure\\n\\npython\\ndef conv_block(n_filters, input, kernel_initialization=None):\\n if kernel_initialization:\\n y = Conv2D(n_filters, (3, 3), strides=(2, 2), padding=\\"same\\", use_bias=False, kernel_initializer=kernel_initialization)(input)\\n else:\\n y = Conv2D(n_filters, (3, 3), strides=(2, 2), padding=\\"same\\", use_bias=False)(input)\\n y = BatchNormalization()(y)\\n y = LeakyReLU(0.2)(y)\\n return y\\n\\ndef upconv_block(n_filters, input, skip_connection):\\n u = Conv2DTranspose(n_filters, (3, 3), strides=(2, 2), padding=\\"same\\")(input)\\n u = Concatenate(axis=-1)([u, skip_connection])\\n u = Conv2D(n_filters, (3, 3), strides=1, padding=\\"same\\", activation=\\"relu\\")(u)\\n u = Conv2D(n_filters, (3, 3), strides=1, padding=\\"same\\", activation=\\"relu\\")(u)\\n return u\\n\\ndef get_generator():\\n init = HeNormal()\\n x = Input(shape=(SIZE, SIZE, 1))\\n d1 = conv_block(64, x, kernel_initialization=init)\\n d2 = conv_block(128, d1)\\n d3 = conv_block(256, d2)\\n\\n d4 = conv_block(512, d3)\\n\\n u3 = upconv_block(256, d4, d3)\\n u2 = upconv_block(128, u3, d2)\\n u1 = upconv_block(64, u2, d1)\\n\\n final = upconv_block(2, u1, x)\\n final = Activation(\\"tanh\\")(final)\\n\\n return Model(x, final)\\n\\n\\n##### PatchGAN Discriminator by Repeated Conv Blocks\\n\\npython\\ndef add_padding(padding=(1,1)):\\n return ZeroPadding2D(padding=padding)\\n\\ndef get_discriminator():\\n input = Input(shape=(256,256,2))\\n x = add_padding()(input)\\n x = Conv2D(64, (4, 4), strides=2, padding=\\"same\\", use_bias=False)(x)\\n x = LeakyReLU(0.2)(x)\\n\\n x = add_padding()(x)\\n x = Conv2D(128, (4, 4), strides=2, padding=\\"same\\", use_bias=False)(x)\\n x = BatchNormalization()(x)\\n x = LeakyReLU(0.2)(x)\\n\\n x = add_padding()(x)\\n x = Conv2D(256, (4, 4), strides=2, padding=\\"same\\", use_bias=False)(x)\\n x = BatchNormalization()(x)\\n x = LeakyReLU(0.2)(x)\\n\\n x = add_padding()(x)\\n x = Conv2D(512, (4, 4), strides=1, padding=\\"same\\", use_bias=False)(x)\\n x = BatchNormalization()(x)\\n x = LeakyReLU(0.2)(x)\\n\\n\\n x = add_padding()(x)\\n x = Conv2D(1, (4, 4), strides=1, padding=\\"same\\")(x)\\n return Model(input, x)\\n\\n\\n#### Training\\n\\n##### Functions to Visualize Intermediate Performance\\n\\npython\\ndef get_gray_image_from_path(img_path):\\n im_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\\n im_gray_3ch = np.concatenate([im_gray[...,np.newaxis] for _ in range(3)], axis=-1)\\n return im_gray_3ch\\n\\ndef visualize_result(epoch=0, step=0):\\n random_index = np.random.randint(0, len(imgs_paths))\\n img_path = imgs_paths[random_index]\\n im_gray_3ch = get_gray_image_from_path(img_path)\\n filename = os.path.basename(img_path)\\n\\n original_size = im_gray_3ch.shape[0:2][::-1]\\n img_ = cv2.resize(im_gray_3ch, dsize=(SIZE, SIZE), interpolation=cv2.INTER_CUBIC)\\n img_ = img_/255.\\n img_lab = rgb2lab(img_).astype(\\"float32\\")\\n L, _ = lab_normalize_to_minus1_to_1(img_lab)\\n faked_coloring = gen.predict(np.array([L]))[0]\\n colored_img_in_lab_in_minus1_to_1 = np.concatenate([L, faked_coloring], axis=-1)\\n colored_img_in_lab = lab_denormalize_from_minus1_to_1(colored_img_in_lab_in_minus1_to_1)\\n faked_colored_image = (lab2rgb(colored_img_in_lab) * 255).astype(\\"uint8\\")\\n faked_colored_image = cv2.resize(faked_colored_image, dsize=original_size, interpolation=cv2.INTER_CUBIC)\\n\\n plt.figure(figsize=(18, 30))\\n plt.subplot(1, 2, 1)\\n plt.axis(\\"off\\")\\n plt.imshow(im_gray_3ch.astype(\\"uint8\\"))\\n\\n plt.subplot(1, 2, 2)\\n plt.axis(\\"off\\")\\n plt.imshow(faked_colored_image)\\n\\n taget_folder = \\"./epoch_{}\\".format(str(epoch).zfill(2))\\n if not os.path.exists(taget_folder):\\n os.makedirs(taget_folder)\\n\\n plt.savefig(\\"./epoch_{}/result_{}_from_{}.png\\".format(str(epoch).zfill(2), str(step).zfill(3), filename), dpi=80, bbox_inches=\\"tight\\")\\n\\n\\n##### Start Training\\n\\n###### Training Without Noise\\n\\nNow we implement our custom training loop for 10 epochs. We use get_data_generator to get new dataset for each epoch.\\n\\nWe start our model/data initilization and training loop in separate code block:\\n\\npython\\ngen_opt = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\\ndisc_opt = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\\n\\ngen_L1_loss_lambda = 100\\n\\nmse = MeanSquaredError()\\nmae = MeanAbsoluteError()\\n\\n\\nand start:\\n\\npython\\nfor epoch in range(0, 10):\\n epoch = epoch + 1\\n batch = 0\\n data_generator = get_data_generator()\\n while True:\\n try:\\n batch += 1\\n print(f\\"{batch}-th batch\\", end=\\"\\\\r\\")\\n Ls, abs = next(data_generator)\\n real_images = tf.concat([Ls, abs], axis=-1)\\n\\n with tf.GradientTape(persistent=True) as tape:\\n faked_coloring = gen(Ls)\\n true_coloring = abs\\n\\n critic_on_faked_colorings = disc(faked_coloring)\\n critic_on_true_coloring = disc(true_coloring)\\n\\n\\n gen_loss = mse(tf.ones_like(critic_on_faked_colorings), critic_on_faked_colorings)\\\\\\n + 100 * mae(abs, faked_coloring)\\n\\n disc_loss = 0.5 * mse(tf.zeros_like(critic_on_faked_colorings), critic_on_faked_colorings)\\\\\\n + 0.5 * mse(tf.ones_like(critic_on_true_coloring), critic_on_true_coloring)\\n\\n\\n grad_gen = tape.gradient(gen_loss, gen.trainable_variables)\\n grad_disc = tape.gradient(disc_loss, disc.trainable_variables)\\n\\n gen_opt.apply_gradients(zip(grad_gen, gen.trainable_variables))\\n disc_opt.apply_gradients(zip(grad_disc, disc.trainable_variables))\\n\\n if batch % 10 == 0:\\n visualize_result(epoch, int(batch/10))\\n\\n except StopIteration:\\n print(f\\"Epoch {epoch} Ended\\")\\n break\\n except Exception as err:\\n print(err)\\n break\\n\\n\\n###### Training With Noise (WIP)\\n\\n###### Training With Pretrained VGG-16 as a Backbone (WIP)\\n\\n#### References\\n\\n- <a href=\\"https://towardsdatascience.com/colorizing-black-white-images-with-u-net-and-conditional-gan-a-tutorial-81b2df111cd8\\">\\n Colorizing black & white images with U-Net and conditional GAN \u2014 A Tutorial\\n </a>\\n- <a href=\\"https://arxiv.org/abs/1603.08511\\">\\n Richard Zhang, Phillip Isola, Alexei A. Efros, Colorful Image Colorization\\n </a>\\n- <a href=\\"https://arxiv.org/abs/1611.07004\\">\\n Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros, Image-to-Image Translation with Conditional Adversarial Networks\\n </a>\\n","title":"Color-GAN with Auto-Coloring on Animated Gray Images","date":"2022-04-01T00:00:00.000Z","id":"blog056","tag":"tensorflow, deep-learning","intro":"We study one of the GANs that can perform auto coloring to gray-images."},{"content":"\\n#### pathlib\\n\\n- pathlib.Path object can be fed into open().\\n- Path.touch(exist_ok=True) creates a new file if file_path does not exist, otherwise do nothing.\\n- Example:\\n\\n python\\n from pathlib import Path\\n\\n file_path = \'batches.json\'\\n json_filepath = Path(file_path)\\n json_filepath.touch(exist_ok=True)\\n\\n with open(json_filepath, \\"w+\\") as json_io:\\n pass\\n \\n\\n#### json\\n\\n- We use json.load(IODevice) to load json data into a dictionary.\\n- We use json.loads(String) to convert json string into a python object.\\n- We use json.dumps(Object) to convert python object into a string.\\n- Examples:\\n python\\n with open(json_file, \\"w+\\") as json_io:\\n data = json.load(f)\\n \\n python\\n json_file = Path(\'batches.json\')\\n json_file.touch(exist_ok=True)\\n with open(json_file, \\"w+\\") as json_io:\\n json_io.write(json.dumps(trip_batches, indent=4))\\n \\n","title":"pathlib and json in Python","date":"2022-04-07T00:00:00.000Z","id":"blog058","tag":"python","intro":"Record simple usages of pathlib and json in python"},{"content":"\\njson\\n{\\n \\"version\\": \\"0.2.0\\",\\n \\"configurations\\": [\\n {\\n \\"type\\": \\"node-terminal\\",\\n \\"name\\": \\"Debug Current TS File (ts-node)\\",\\n \\"request\\": \\"launch\\",\\n \\"command\\": \\"ts-node ${relativeFileDirname}/${fileBasename}\\",\\n \\"cwd\\": \\"${workspaceRoot}\\"\\n }\\n ]\\n}\\n\\n","title":"Typescript Debugger Config","date":"2022-04-11T00:00:00.000Z","id":"blog059","tag":"typescript","intro":"Set up debugger for node project in typescript with minimal config.","toc":false},{"content":"\\n#### SSH Into a Computer\\n\\nGiven that I have following following config in ~/.ssh/config\\n\\ntext\\nHost gp\\n HostName 12.34.567.89\\n User cclee\\n Port 1314\\n\\n\\nThen I can ssh into this computer by\\n\\ntext\\nssh gp\\n\\n\\nand inputting the password.\\n\\n#### SSH into an EC2 Instance\\n\\nWith EC2 we additionally need a private key. In both Windows (with git-bash) and unix-based system, we can use that key in .pem format.\\n\\ntext\\nHost uat\\n HostName ec2-some-number.ap-northeast-1.compute.amazonaws.com\\n User ubuntu\\n Port 22\\n PreferredAuthentications publickey\\n IdentityFile C:\\\\keys\\\\wbkeypair-tokyo-james.pem\\n\\nHost prd\\n HostName ec2-some-other-number.ap-northeast-1.compute.amazonaws.com\\n User ubuntu\\n Port 22\\n PreferredAuthentications publickey\\n IdentityFile C:\\\\keys\\\\wbkeypair-tokyo-james.pem\\n\\n\\n#### Download a File in SSH Client\\n\\n##### With ~/.ssh/config\\n\\nSuppose that we have identified the filepath to download, we can\\n\\ntext\\nscp gp:/the/file/path /local/file/path\\n\\n\\nand we will be done by typing the password in console.\\n\\n##### Without ~/.ssh/config\\n\\ntext\\nscp -P port-number user@domain:/destination source-file/directory\\n\\n","title":"SSH Config and Download File in SSH Client","date":"2022-04-12T00:00:00.000Z","id":"blog060","tag":"coding","intro":"Record script to download files using SSH client."},{"content":"\\n#### Lessons from the Implementation\\n\\nWe record the implementation of the ThreadPool in the next section. For this section, we record several encountered errors and methods to get around them.\\n\\n##### Box, Rc\\\\<T\\\\> | Weak\\\\<T\\\\>, RefCell\\\\<T\\\\>, Mutex\\\\<T\\\\>, Arc\\\\<t\\\\>\\n\\n- Box<dyn T>\\n - **_Single thread only_**\\n - Help save data on heap instead of memory\\n - Checked in compile time\\n- Rc<T> | Weak<T>\\n - **_Reference Count Type_**\\n - Checked in compile time\\n - Used in **_single thread only_**\\n - Created by let a = Rc:new(data)\\n - Cloned by Rc::clone(&a)\\n - a above is immutable\\n - Used when multiple reference is needed\\n - **_Mainly_** used as immutable reference\\n - Combined with RefCell<T>, we can implement **_interior mutability_**\\n - Rc<T>.downgrade() and Rc<T>.upgrade() can switch a reference to weak and strong respectively\\n - (cont\'d) Used when there is a recursive relation that has parent-children relationship\\n - weak_reference_count wouldn\'t affect the release of resource as long as the strong_reference_count of a variable goes to 0\\n- RefCell<T>\\n\\n - **_Reference Cell Type_**\\n - Only checked in runtime\\n - Used in **_single thread only_**\\n - Can be used in mutable and immutable reference\\n - Let let a = Rc::new(RefCell(4)), then a is immutable\\n - But a.borrow_mut() can create a mutable reference\\n - (cont\'d) After destructured, *a.borrow_mut() += 10 becomes valid\\n - **Example (Interior Mutability).**\\n\\n rust\\n // a should have been immutable,\\n let a = Rc::new(RefCell::new(123));\\n // but with RefCell::new, the data become mutable by using .borrow_mut()\\n let b = Rc::new(&a);\\n *b.borrow_mut() += 1000;\\n print!(\\"{:?}, {:?}\\", a, b);\\n\\n // print:\\n // RefCell { value: 1123 }, RefCell { value: 1123 }\\n \\n\\n - Some comments in the internet suggest avoiding interior mutability when possible\\n - An immutable ereference can be created by a.borrow()\\n\\n- Mutex<T>\\n - Used in **_multi-threaded scenario_**\\n - Used when internal state of the object of type T can change in different thread, this is to avoid data race\\n - Need Mutex<T>.lock.unwrap() to get the lock and gain right to access and mutate the value\\n- Arc<T>\\n - Used in **_multi-threaded scenario_**\\n - **_Thread-safe_** and **_multi-threaded version_** of Rc<T>, can be referenced by many threads\\n - A stands for **atomic**\\n - **Example.** Suppose that a single receiver: Arc<Mutex<mpsc::Receiver<Message>>> is passed to 10 threads:\\n rust\\n let message: Message = receiver.lock().unwrap().recv().unwrap();\\n \\n We will be using module mpsc (multi-producer, single-consumer) to get Sender and Receiver\\n\\n##### Access Struct\'s Field but Encounter: does not implement the Copy trait Error\\n\\nWe consider the following exmaple:\\n\\nrust\\nstruct Foo {\\n bar: Bar,\\n}\\n\\nimpl Foo {\\n fn foo(&self) {\\n self.bar.bar();\\n }\\n}\\n\\nstruct Bar {}\\n\\nimpl Bar {\\n fn bar(self) {}\\n}\\n\\nfn main() {\\n let foo = Foo { bar: Bar {} };\\n foo.foo();\\n}\\n\\n\\nBy cargo check we get\\n\\ntext\\n93 | let bar = self.bar;\\n | ^^^^^^^^\\n | |\\n | move occurs because self.bar has type Bar, which does not implement the Copy trait\\n | help: consider borrowing here: &self.bar\\n\\n\\n###### Reason of the Problem\\n\\nThe problem is that fn Bar.bar takes the ownership of the Bar instance (the self.bar). If self.bar.bar() were executable, then self.bar will be release **_accidentally_** after the execution of that function is finished, which is disastrous and should be forbiddened.\\n\\n<center></center>\\n\\n###### Solution: Universal Trick by using Option\\\\<T\\\\>\\n\\nIn other words, self.bar must be taken away **_intentionally_** in order to be fed into .bar method. The universal trick is to make self.bar be of type Option<T>, then\\n\\nrust\\n(self.bar as Option<T>).take()\\n\\n\\ncan take away **_both_** the ownership and the value of self.bar and set self.bar to None:\\n\\nrust\\n...\\nstruct Foo {\\n bar: Option<Bar>,\\n}\\n\\nimpl Foo {\\n fn foo(&mut self) {\\n if let Some(bar) = self.bar.take() {\\n bar.bar();\\n }\\n }\\n}\\n...\\n\\n\\nNote that because self.bar is mutated (as self.bar becomes Option::None), we need foo(&mut self) instead of foo(&self).\\n\\n<center></center>\\n\\n**Possible Scenario.**\\nThe problem discusses above occur occasionally. For example, if our struct contains a field named thread: thread::JoinHandle<()>, and if we want to execute thread.join(), then because the signature of .join is:\\n\\nrust\\npub fn join(self) -> Result<T>\\n\\n\\nwe must take out the thread intentionally by chaning the type of the field thread from type thread::JoinHandle<()> to Option<thread::JoinHandle<()>>.\\n","title":"Summarize Rust Beginning Tutorial by a Simplified Multithreading Web Server","date":"2022-04-15T00:00:00.000Z","id":"blog061","tag":"rust","intro":"Completed the Rust official tutorial from its <a href=\\"https://doc.rust-lang.org/book/title-page.html\\">online book</a> and concluded the 20 chapters by a multithreading simple web server. Try to record the subtitle detail for future reference."},{"content":"\\n#### Introduction: Why Care DCT?\\n\\nWe have been using **_Discrete Cosine Transform_** (DCT) without possibly awaring of it: DCT is actually the JPEG compression standard!\\n\\nIn audio analysis, DCT is used in computing MFCC coefficients to extract feature vector (for more on MFCCs, see **[KC]** in reference section), which makes deep learning on sound possible by\\n\\n- Treating the sequence of MFCC coefficients as a time sequence of data (at each \\"window\\" of suitable \\"hop length\\", we get a float vector of fixed size) or;\\n\\n- Stacking the MFCC coefficients and treat it as an image.\\n\\nIn the former case the use of sequence model such as LSTM or Transformer becomes possible. The later case makes it possible to use convolution layer such as Conv1D in Keras.\\n\\n#### Basic Knowledge to Work With\\n\\n##### Definitions\\n\\nIn what follows for $i,j=0,1,2,\\\\dots, N-1$ we will denote\\n\\n$$\\nu_j^i = \\\\alpha_j\\\\cos \\\\frac{(2i+1)j\\\\pi}{2N}\\\\quad\\\\text{and} \\\\quad \\\\alpha_j =\\\\begin{cases}\\n\\\\displaystyle\\\\frac{1}{\\\\sqrt{N}}& \\\\text{if } j = 0,\\\\\\\\\\n\\\\displaystyle\\\\sqrt{\\\\frac{2}{N}}& \\\\text{if } j \\\\neq 0.\\n\\\\end{cases}\\n$$\\n\\nFor each fixed $j$ we will consider $u_j(x):= u_j^x =\\\\alpha_j\\\\cos \\\\frac{(2x+1)j\\\\pi}{2N} $ as a function on a discrete domain. The vectors\\n\\n$$\\n\\\\left\\\\{u_j^{:} := \\\\left[\\\\left.u_j^i= \\\\alpha_j\\\\cos \\\\frac{(2i+1)j\\\\pi}{2N}\\\\quad \\\\text{ for }0\\\\leq i<N\\\\right]\\\\in \\\\mathbb R^N \\\\right| j=0,1,\\\\dots,N-1\\\\right\\\\}\\n$$\\n\\nform an orthonormal basis in $\\\\mathbb R^N$, by stacking all them together column by column\\n\\n$$\\nU := \\\\begin{bmatrix}\\n| &|& &|\\\\\\\\\\nu_0^{:}& u_1^{:}&\\\\cdots &u_{N-1}^{:} \\\\\\\\\\n|&|& &|\\n\\\\end{bmatrix},\\n$$\\n\\nthen $U$ is orthonormal: $U^TU = I$.\\n\\nThe orthogonality of $U$ and the construction of the basis $u_j^:$ will be explained in _Mathematics behind DCT_ section. For now let\'s enjoy the coding and see the results.\\n\\n##### Computations\\n\\nWe make explicit calculations in order to make readers comfortable with the definition above.\\n\\nFirst, given a gray-scale image $f:[0, N-1]\\\\times[0, N-1]\\\\to \\\\mathbb R$ there are always unique coefficients $F:[0, N-1]\\\\times[0, N-1]\\\\to \\\\mathbb R$ such that\\n\\n$$\\nf(p,q) =\\\\sum_{i=0}^{N-1}\\\\sum_{j=0}^{N-1}\\\\alpha_i \\\\alpha_j F(i, j) \\\\cos \\\\bigg(\\\\frac{(2p+1)i\\\\pi}{2N}\\\\bigg)\\\\cos\\\\bigg( \\\\frac{(2q+1)j\\\\pi}{2N}\\\\bigg).\\\\tag*{$(*)$}\\n$$\\n\\nIn fact by using $U$ defined in the previous section. Denote also $f, F$ as the matrix with entry $f(p,q), F(p,q)$ at $p$-th row and $q$-th column, if we define $F = U^T f U$, then\\n\\n$$\\nf=UFU^T.\\n$$\\n\\nYou can expand the RHS ($UFU^T$) to convince yourself that eventually you get the same expression as in $(*)$. On the other hand, by using this $U$ we have the reverse:\\n\\n$$\\nF(x,y) = \\\\sum_{h=0}^{N-1}\\\\sum_{k=0}^{N-1} f(h,k) \\\\alpha_x\\\\alpha_y \\\\cos \\\\bigg( \\\\frac{(2h+1)x\\\\pi}{2N}\\\\bigg)\\\\cos \\\\bigg(\\\\frac{(2k+1)y\\\\pi}{2N}\\\\bigg),\\n$$\\n\\nas this is not a summation of $\\\\cos$\'s of fixed frequencies, that\'s why we don\'t define $f=U^TfU$.\\n\\nOur coding will strictly follow the notation and calculation in this section.\\n\\n#### Implementation of JPEG Compression in Python\\n\\nFor readers who are more familiar with matlab, you may also follow the youtube video listed in **[ET]**. Most of the content is a translation from matlab to python based on my understanding.\\n\\nNotation and definition may be different from the video.\\n\\n##### Basic Import\\n\\npython\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\\n%matplotlib inline\\n\\n\\n##### Constants\\n\\npython\\nN = 8\\nIMAGE_PATH = \\"512_512_image.jpg\\"\\n\\n\\n##### Basis of Cosine Transform\\n\\npython\\ndef range2D(n):\\n indexes = []\\n for i in range(0, n):\\n for j in range(0, n):\\n indexes.append((i, j))\\n return indexes\\n\\ndef alpha(p):\\n return 1/np.sqrt(N) if p == 0 else np.sqrt(2/N)\\n\\ndef cos_basis(i,j):\\n def cal_result(a):\\n x=a[0]\\n y=a[1]\\n\\n return alpha(i) * alpha(j) * np.cos((2*x+1) * i * np.pi/(2*N)) * np.cos((2*y+1) * j * np.pi/(2*N))\\n return cal_result\\n\\n\\n##### Plot of the 2D Cosine Basis Functions\\n\\npython\\nfig = plt.figure()\\nfig.set_figheight(12)\\nfig.set_figwidth(12)\\nxy_range = range2D(N)\\nxy_plane_NxN = (np.array(xy_range).reshape(N,N,2))\\n\\nfor i in range(0, N):\\n for j in range(0, N):\\n xy_result = np.apply_along_axis(cos_basis(i,j), -1, xy_plane_NxN)\\n plt.subplot(N, N, 1 + N*i + j)\\n plt.imshow(xy_result, cmap=\\"gray\\")\\n\\n\\n<center>\\n<a href=\\"/assets/tech/037.png\\" target=\\"_blank\\">\\n <img src=\\"/assets/tech/037.png\\"/ width=\\"500\\">\\n</a>\\n</center>\\n\\n##### Compute Coefficients for Cosine Transform\\n\\nLet\'s prepare the following $8\\\\times 8$ matrix that convert the original pixels into compacted energy distribution:\\n\\npython\\nU = np.zeros((N,N))\\n\\nfor i in range(0, N):\\n for j in range(0, N):\\n U[i, j] = alpha(j) * np.cos((2*i+1)*j*np.pi/(2*N))\\n\\nU_t = U.transpose()\\n\\n\\nYou may check that U is indeed orthonormal by:\\n\\npython\\nnp.matmul(U, U_t)\\n\\n# Result:\\n# array([[ 1.00000000e+00, -1.01506949e-16, -3.78598224e-17,\\n# -1.03614850e-17, -1.33831237e-16, 7.88118312e-17,\\n# 2.00856868e-16, -1.70562493e-16],\\n# [-1.01506949e-16, 1.00000000e+00, 1.70298111e-17,\\n# -1.13980777e-17, 1.37840552e-16, -2.88419699e-16,\\n# -1.24636773e-16, 2.51800326e-17],\\n# [-3.78598224e-17, 1.70298111e-17, 1.00000000e+00,\\n# -1.61419161e-16, -2.81922100e-17, -1.25319880e-16,\\n# -1.83252893e-16, -2.79432802e-17],\\n# [-1.03614850e-17, -1.13980777e-17, -1.61419161e-16,\\n# 1.00000000e+00, -1.92659768e-17, 2.75108042e-16,\\n# 2.02487213e-16, 2.22263103e-18],\\n# [-1.33831237e-16, 1.37840552e-16, -2.81922100e-17,\\n# -1.92659768e-17, 1.00000000e+00, -8.54966705e-17,\\n# -2.36183557e-17, -3.71508058e-16],\\n# [ 7.88118312e-17, -2.88419699e-16, -1.25319880e-16,\\n# 2.75108042e-16, -8.54966705e-17, 1.00000000e+00,\\n# 1.21575873e-16, 8.41357861e-17],\\n# [ 2.00856868e-16, -1.24636773e-16, -1.83252893e-16,\\n# 2.02487213e-16, -2.36183557e-17, 1.21575873e-16,\\n# 1.00000000e+00, 3.92417644e-16],\\n# [-1.70562493e-16, 2.51800326e-17, -2.79432802e-17,\\n# 2.22263103e-18, -3.71508058e-16, 8.41357861e-17,\\n# 3.92417644e-16, 1.00000000e+00]])\\n\\n\\n##### Mask or Truncate the Energy Distribution\\n\\nHere we use a naive approach, we screen out the fourier coefficents by simply retaining those from the upper-left corner. For that, we create a mask:\\n\\npython\\ndef create_mask(closure):\\n mask = np.zeros((N, N))\\n for i in range(N):\\n for j in range(N):\\n if closure(i,j):\\n mask[i, j] = 1\\n return mask\\n\\n\\nFor example, by calling mask = create_mask(lambda i,j: 0<=i+j<=2) our mask will be like:\\n\\ntext\\n[[1. 1. 1. 0. 0. 0. 0. 0.]\\n [1. 1. 0. 0. 0. 0. 0. 0.]\\n [1. 0. 0. 0. 0. 0. 0. 0.]\\n [0. 0. 0. 0. 0. 0. 0. 0.]\\n [0. 0. 0. 0. 0. 0. 0. 0.]\\n [0. 0. 0. 0. 0. 0. 0. 0.]\\n [0. 0. 0. 0. 0. 0. 0. 0.]\\n [0. 0. 0. 0. 0. 0. 0. 0.]]\\n\\n\\n##### Result: Apply Cosine Transform\\n\\npython\\noriginal = load_img(IMAGE_PATH, target_size=(512, 512), color_mode=\\"grayscale\\")\\noriginal = img_to_array(original).astype(\\"uint8\\")\\noriginal = np.squeeze(original)\\noriginal.shape\\n\\nmask = create_mask(lambda i,j: 0<=i+j<=2)\\nprint(mask)\\nsteps = int(original.shape[0]/N) # we choose 512x512 image, therefore steps = 64\\nprint(steps)\\ncompressed = np.zeros_like(original)\\n\\nfor x in range(steps):\\n for y in range(steps):\\n sub_pixels = original[N * y: N*(y+1), N * x: N*(x+1)]\\n sub_pixels = sub_pixels - 127.5\\n fourier_coefficients = np.matmul(np.matmul(U_t, sub_pixels), U)\\n fourier_coefficients = fourier_coefficients * mask\\n reverted_pixels = np.matmul(np.matmul(U, fourier_coefficients), U_t)\\n reverted_pixels = reverted_pixels + 127.5\\n compressed[N * y: N*(y+1), N * x: N*(x+1)] = reverted_pixels\\n\\nfig.set_figheight(20)\\nfig.set_figwidth(20)\\nplt.subplot(1, 2, 1)\\nplt.imshow(original, cmap=\\"gray\\")\\nplt.subplot(1, 2, 2)\\nplt.imshow(compressed, cmap=\\"gray\\")\\nplt.savefig(\\"DCT_result\\", dpi=200, bbox_inches=\\"tight\\")\\n\\n\\n<center>\\n<a href=\\"/assets/tech/038.png\\" target=\\"_blank\\">\\n <img src=\\"/assets/tech/038.png\\"/ width=\\"500\\">\\n</a>\\n</center>\\n\\n#### Mathematics behind DCT\\n\\n**Notation.** Every vector will be considered as a **column vector** whenever the computation does not make sense when they are represented as a row.\\n\\n##### From DFT\\n\\nLet $N\\\\ge 1 $ be an integer and denote $w = e^{2\\\\pi i/N}$, define $\\nv_k:= (1,w^k, w^{2k},\\\\dots, w^{(N-1)k}),\\n$ then the matrix\\n\\n$$\\nV:=\\\\begin{bmatrix}\\n|&|& & |\\\\\\\\\\nv_0&v_1&\\\\cdots & v_{N-1}\\\\\\\\\\n|&|& & |\\n\\\\end{bmatrix}\\n$$\\n\\nis orthogonal simply because their inner product\\n\\n$$\\n(v_k,v_\\\\ell) = \\\\sum_{j=0}^{N-1}(w^k)^j (\\\\overline{w})^j = \\\\frac{(w^k \\\\overline{w}^\\\\ell )^{N} - 1}{\\\\underbrace{w^k\\\\overline{w}^\\\\ell-1}_{\\\\neq 0 \\\\text{ if $k\\\\neq \\\\ell$}}}\\n$$\\n\\nis $0$ when $k\\\\neq \\\\ell$, where $w^N=1$. Therefore the basis $\\\\mathcal V:=\\\\{v_0,v_1,\\\\dots,v_{N-1}\\\\}$ forms an orthogonal basis in $\\\\mathbb C^{N}$.\\n\\nBut how to find a similar basis in $\\\\mathbb R^n$? It is not as simple as taking the real part of the linear combination $v = V[v]_{\\\\mathcal V} = \\\\sum a_i v_i$ since the coordinate $[v]_{\\\\mathcal V}\\\\in \\\\mathbb C^N$ even $v\\\\in \\\\mathbb R^N$.\\n\\n##### DCT-2\\n\\n###### Strategy of Construction\\n\\nConsider the symmetric second difference matrix:\\n\\n$$\\nA = \\\\begin{bmatrix}\\n\\\\otimes=1& \\\\otimes\'=-1 &0&0&&\\\\cdots&&0\\\\\\\\\\n-1&2&-1 &0&&\\\\cdots&&0\\\\\\\\\\n0& -1&2&-1 &&\\\\cdots&&0\\\\\\\\\\n& &&&&&&\\\\\\\\\\n\\\\vdots& &&&\\\\ddots&&&\\\\vdots \\\\\\\\\\n& &&&&&&\\\\\\\\\\n&&&&&-1&2&-1\\\\\\\\\\n0&0&0&0&\\\\cdots&0&\\\\boxtimes=-1&\\\\boxtimes\'=1\\n\\\\end{bmatrix}\\n$$\\n\\nwhich arises when a second derivate is approximated by the central second difference:\\n\\n$$\\n-f\'\'(x) = \\\\frac{1}{h^2}\\\\bigg(-f(x-h)+2f(x)-f(x+h)\\\\bigg) + o(h)\\n$$\\n\\nas $h\\\\to 0$.\\n\\nLet $u_{k,:}\\\\in \\\\mathbb R^N$ denote the values of a function evaluated on the discretized domain $[x_{0}, x_{1},\\\\dots,x_{N-1}]$ of $[0, \\\\pi-\\\\frac{1}{N}]$, i.e., $u_{k,:} = f_k(x_{:})$ for some $f_k:\\\\mathbb [0, \\\\pi-\\\\frac{1}{N}] \\\\to \\\\mathbb R$, where : means stacking values by running through all indexes at that position.\\n\\nOur cosine transform is based on decomposing a function into a linear combination of cosine functions (of different frequencies) **_in discrete case_**. To find them, we will choose $u_{k,:}$ such that $Au_{k,:}=\\\\lambda_k u_{k,:}$. By the fact that:\\n\\n> **Fact.** Eigenvectors associated to different eigenvalues are linearly independent.\\n\\n<center></center>\\n\\nwe then obtain a set of eigenvectors (of different eigenvalues) solving the approximated problem\\n\\n$$\\n-\\\\frac{1}{N^2}f_k\'\'(x_{:})\\\\approx Au_k = \\\\lambda_k f_k(x_{:}).\\n$$\\n\\nWhich function would solve the ODE $-f\'\'=N^2\\\\lambda f$? Which is the trigonometric function!\\n\\n###### Why $(\\\\otimes, \\\\otimes\') = (1,-1)$ and $(\\\\boxtimes, \\\\boxtimes\')=(-1,1)$?\\n\\nThese two assignments are determined by imposing boundary conditions. More specific, we extend the domain from $[0, \\\\pi-\\\\frac{1}{N}]$ to $[-\\\\frac{1}{N}, \\\\pi]$, imagine we are solving $\\\\tilde u_k \\\\in \\\\mathbb R^{N+2}$ (append one point at the beginning and the tail of $x_{:}\\\\in \\\\mathbb R^N$ respectively).\\n\\nWe have not imposed any boundary condition to our solution $f_k$ yet. We will require our function be symmetric at $-\\\\frac{1}{2N}$ (or at $j=-\\\\frac{1}{2}$ if $j=-1,0,1,\\\\dots,N$, $x_j = \\\\frac{j\\\\pi}{N}$), this implies $f_k(x_{-1}) = f_k(x_0)$.\\n\\nAt another boundary due to the shift above, we try to require $f_k\'(x_{N-\\\\frac{1}{2}})=0$, this implies $f_k(x_{N-1}) = f_k(x_N)$.\\n\\n$$\\n\\\\left\\\\{\\n\\\\begin{align*}\\nu_{k,-1}&=f_k(x_{-1})=f_k(x_{0}) = u_{k,0}\\\\\\\\\\nu_{k,N-1}&=f_k(x_{N-1})= f_k(x_{N}) = u_{k,N}\\n\\\\end{align*}\\n\\\\right.\\n$$\\n\\nand plug this condition into the second difference formula to get:\\n\\n$$\\n\\\\left\\\\{\\n\\\\begin{align*}\\n-u_{k,-1}&+2u_{k,0} -u_{k,1} = u_{k,0} -u_{k,1} \\\\\\\\\\n-u_{k,N-2}&+ 2 u_{k,N-1} -u_{k,N} = -u_{k,N-2}+ u_{k,N-1}.\\n\\\\end{align*}\\n\\\\right.\\n$$\\n\\nThese become the necessary condition and thus have determined our $(\\\\otimes, \\\\otimes\') =(1,-1)$ and $(\\\\boxtimes, \\\\boxtimes\')=(-1,1)$, and therefore ensure our solution is a cosine function.\\n\\nDifferent values of $\\\\otimes$\'s and $\\\\boxtimes$\'s will correspond to different boundary condition imposed on $f_k$ for other combinations, see **[GS]** for a complete classification.\\n\\nThe solution corresponding to $(\\\\otimes, \\\\otimes\') = (1,-1)$ and $(\\\\boxtimes, \\\\boxtimes\')=(-1,1)$ is usally called the basis of **_DCT-2_** (or simply **_DCT_**).\\n\\n##### Derivation of the Basis of Discrete Cosine Transform\\n\\nWe have spent many effort to determine the matrix $A$ to work with, let\'s start with computing the basis directly for our only candidate:\\n\\nLet $\\\\ell(x) = ax + b$ for some $a,b\\\\in \\\\mathbb R$, denote\\n\\n$$\\nu_{k,0:N} = (1, w^{\\\\ell(1)k},w^{\\\\ell(2)k},\\\\dots w^{\\\\ell(N-1)k}),\\n$$\\n\\nnote that we have $\\\\sum_{k=0}^{N-1} u_{k,:} = \\\\sum_{i=0}^{N-1} (\\\\sum_{k=0}^{N-1} w^{\\\\ell(i)k}) e_i = \\\\frac{1-w^{N\\\\ell(i)}}{1-w^{\\\\ell(i)}} = 0$ whenever $\\\\ell(i)\\\\neq 0$ for integer $i$. As we will see this is indeed the case later for $\\\\ell (x) =\\\\frac{1}{2}x+ \\\\frac{1}{4}$.\\n\\nSince $\\\\ell(j\\\\pm 1) = \\\\ell(j) \\\\pm a$, then for $j=1,2,\\\\dots,N-2$, coordinate-wise:\\n\\n$$\\n\\\\begin{align*}\\n[Au_{k,:}]_{j} & = -w^{\\\\ell(j-1)k} + 2w^{\\\\ell(j)k} - w^{\\\\ell(j+1)k} \\\\\\\\\\n&= \\\\big(2-(w^{ak} + w^{-ak}) \\\\big)w^{\\\\ell(j)k}\\\\\\\\\\n&= \\\\bigg(2-2\\\\cos \\\\frac{2ak\\\\pi }{N}\\\\bigg)u_{k,j}.\\n\\\\end{align*}\\n$$\\n\\nTherefore $Au_{k,1:N-1} = (2-2\\\\cos \\\\frac{2ak\\\\pi i}{N}) u_{k,1:N-1}$ for whatever linear $\\\\ell$ we choose (here $x_{h:k} = [x_i \\\\quad \\\\text{ for }i\\\\text{ s.t. } h\\\\leq i <k]$).\\n\\nBy taking real part on both sides, as $A$ is real, we have\\n\\n$$\\nA\\\\left[ \\\\sum_{1\\\\leq j< N-1} \\\\cos \\\\bigg(\\\\frac{2\\\\pi }{N}\\\\ell(j)k\\\\bigg)e_j\\\\right]\\n= \\\\bigg(2-2\\\\cos \\\\frac{2ak\\\\pi }{N}\\\\bigg)\\\\sum_{1\\\\leq j< N-1} \\\\cos \\\\bigg(\\\\frac{2\\\\pi }{N}\\\\ell(j)k\\\\bigg)e_j,\\n$$\\n\\nhere $e_j$ denotes the standard basis in $\\\\mathbb R^N$. It remains to require $u_j := \\\\cos \\\\frac{2k\\\\pi}{N}\\\\ell(j)$ satisfies $u_{-1}=u_0$ and $u_{N-1}=u_N$, from which we can determine $a,b$ and thus $\\\\ell$.\\n\\nThe equation $u_1=u_0$ implies $\\\\cos \\\\big(\\\\frac{2k\\\\pi }{N}(-a+b)\\\\big) = \\\\cos \\\\big(\\\\frac{2k\\\\pi}{N}(b)\\\\big)$, since $\\\\cos$ is even, it is sufficient to require\\n\\n$$\\na-b = b \\\\iff a = 2b.\\n$$\\n\\nThe equation $u_{N-1}=u_N$ implies\\n\\n$$\\n\\\\cos \\\\bigg( 4kb\\\\pi -\\\\frac{2kb\\\\pi}{N}\\\\bigg) = \\\\cos \\\\bigg(4kb\\\\pi +\\\\frac{2kb\\\\pi}{N}\\\\bigg),\\n$$\\n\\nfor this to hold, it is sufficient to require $4b = 1$ (as $\\\\cos$ is always symmetric about $x = k\\\\pi$ for every $k\\\\in \\\\mathbb Z$), altogether we have $a =\\\\frac{1}{2}$, and thus for $k=0,1,2,3,\\\\dots,N-1$,\\n\\n$$\\n\\\\frac{2\\\\pi}{N}\\\\ell(j)k = \\\\frac{2\\\\pi}{N}\\\\bigg(\\\\frac{1}{2}j + \\\\frac{1}{4}\\\\bigg) k = \\\\frac{(2j+1)k\\\\pi}{2N}.\\n$$\\n\\n<p></p>\\n\\nFinally, we have\\n\\n$$\\nz_k := \\\\sum_{0\\\\leq j< N} \\\\alpha_j\\\\cos \\\\bigg( \\\\frac{(2j+1)k\\\\pi}{2N}\\\\bigg)e_j,\\\\quad Az_k = \\\\bigg(2-2\\\\cos \\\\frac{k\\\\pi}{N}\\\\bigg) z_k\\n$$\\n\\nfor suitably chosen $\\\\alpha_j$\'s, $j=0,1,2,\\\\dots,N-1$.\\n\\n#### Reference\\n\\n- **[KC]** Kartik Chaudhary, <a href=\\"https://towardsdatascience.com/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520\\"><i>Understanding Audio data, Fourier Transform, FFT and Spectrogram features for a Speech Recognition System</i></a>,\\n\\n- **[GS]** Gilbert Strang, <a href=\\"https://www.unioviedo.es/compnum/transversal_eng/DCT5.pdf\\"><i>The Discrete Cosine Transform</i></a>\\n\\n- **[ET]** Exploring Technologies, <a href=\\"https://www.youtube.com/watch?v=mUKPy3r0TTI\\"> <i>Discrete Cosine Transform (DCT) of Images and Image Compression</i></a>\\n","title":"Discrete Cosine Transform and JPEG Compression Implementation in Python","date":"2022-04-16T00:00:00.000Z","id":"blog062","tag":"math, python","intro":"Discrete Cosine Transform (DCT) is not only used in image processing, it is also used in signal processing of sound such as computing MFCC coefficients as a feature vector."},{"content":"\\njs\\nimport React, { ReactNode, useEffect, useState } from \\"react\\";\\nimport classnames from \\"classnames\\";\\nimport { makeStyles } from \\"@material-ui/core\\";\\n\\nconst useStyles = makeStyles((\\n {\\n \\"@keyframes fadein\\": {\\n \\"0%\\": {\\n opacity: 0,\\n },\\n \\"100%\\": {\\n opacity: 1\\n }\\n },\\n customFadein: {\\n \\"&.fade-in\\": {\\n animation: \\"$fadein .3s ease-in-out\\"\\n }\\n }\\n },\\n { name:\\"custom-fade-in\\" }\\n)\\n\\nor in latest tss framework:\\njs\\nimport React, { ReactNode, useEffect, useState } from \\"react\\";\\nimport classnames from \\"classnames\\";\\nimport { tss } from \\"tss-react/mui\\";\\nimport { keyframes } from \\"tss-react\\";\\n\\nconst useStyles = tss.create(() => ((\\n {\\n customFadein: {\\n \\"&.fade-in\\": {\\n height: \\"100%\\",\\n animation: ${keyframes\\n 0% {\\n opacity: 0;\\n },\\n 100% {\\n opacity: 1;\\n }\\n } 0.3s ease-in-out\\n }\\n }\\n })\\n));\\n\\nthen: \\njs\\nexport default function FadeIn({ children, dependencies = [] }:\\n {\\n children: ReactNode\\n dependencies?: any[]\\n }) {\\n const [fadeIn, setFadeIn] = useState(false);\\n const classes = useStyles();\\n\\n useEffect(() => {\\n setFadeIn(false);\\n setTimeout(() => { setFadeIn(true); }, 1);\\n }, [...dependencies]);\\n\\n return (\\n <div style={{ opacity: fadeIn ? 1 : 0 }}>\\n <div\\n className={classnames(classes.customFadein, fadeIn ? \\"fade-in\\" : \\"\\")}\\n >\\n <>\\n {children}\\n </>\\n </div>\\n </div>\\n );\\n}\\n\\n\\nSometimes you may want the element to fade-in and out on state change, you can pass the arguments into FadeIn component as follows:\\n\\njs\\n<FadeIn dependencies={[pathname]}>\\n ...\\n</Fadein>\\n\\n","title":"Element that has Default Fade-in Transition on Mount","date":"2022-04-19T00:00:00.000Z","id":"blog063","tag":"react","intro":"Record an element that I made for default fade-in effect on component mount and state change.","toc":false},{"content":"\\n#### Two Structures\\n\\nFrom the paper <a href=\\"https://arxiv.org/pdf/1512.03385.pdf\\"><i>Deep Residual Learning for Image Recognition</i></a> we can find two possible structure:\\n\\n<center>\\n<a href=\\"/assets/tech/041.png\\">\\n<img src=\\"/assets/tech/041.png\\" width=\\"600\\"/>\\n</a>\\n</center>\\n<p/>\\n<center></center>\\n\\nThe left is called a **_basic block structure_**, and the right is called a **_bottleneck structure_**.\\n\\n##### Basic Block Structure\\n\\nReference: <a href>https://keras.io/examples/generative/cyclegan/</a>\\n\\npython\\ndef residual_block(\\n x,\\n activation,\\n kernel_initializer=kernel_init,\\n kernel_size=(3, 3),\\n strides=(1, 1),\\n padding=\\"valid\\",\\n gamma_initializer=gamma_init,\\n use_bias=False,\\n):\\n dim = x.shape[-1]\\n input_tensor = x\\n\\n x = ReflectionPadding2D()(input_tensor)\\n x = layers.Conv2D(\\n dim,\\n kernel_size,\\n strides=strides,\\n kernel_initializer=kernel_initializer,\\n padding=padding,\\n use_bias=use_bias,\\n )(x)\\n x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\\n x = activation(x)\\n\\n x = ReflectionPadding2D()(x)\\n x = layers.Conv2D(\\n dim,\\n kernel_size,\\n strides=strides,\\n kernel_initializer=kernel_initializer,\\n padding=padding,\\n use_bias=use_bias,\\n )(x)\\n x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\\n x = layers.add([input_tensor, x])\\n return x\\n\\n\\n##### Bottleneck Structure\\n\\nFor deeper network, we use:\\n\\npython\\ndef residual_block(\\n x,\\n filter_depth,\\n strides=(1, 1),\\n reduce_dim=False,\\n reg=0.0001,\\n bn_eps=2e-5,\\n bn_mom=0.9\\n):\\n shortcut = x\\n # we batch-normalize along the the channel axis, which is -1:\\n bn = BatchNormalization(axis=-1, epsilon=bn_eps, momentum=bn_mom)(x)\\n act = Activation(\\"relu\\")(bn)\\n x = Conv2D(\\n int(filter_depth * 0.25),\\n (1, 1),\\n strides=strides,\\n use_bias=False,\\n kernel_regularizer=l2(reg)\\n )(act)\\n\\n x = BatchNormalization(axis=-1, epsilon=bn_eps, momentum=bn_mom)(x)\\n x = Activation(\\"relu\\")(x)\\n x = Conv2D(\\n int(filter_depth * 0.25),\\n (3, 3),\\n strides=strides,\\n padding=\\"same\\",\\n use_bias=False,\\n kernel_regularizer=l2(reg)\\n )(x)\\n\\n x = BatchNormalization(axis=-1, epsilon=bn_eps, momentum=bn_mom)(x)\\n x = Activation(\\"relu\\")(x)\\n x = Conv2D(\\n filter_depth,\\n (1, 1),\\n use_bias=False,\\n kernel_regularizer=l2(reg)\\n )(x)\\n\\n if reduce_dim:\\n shortcut = Conv2D(\\n filter_depth,\\n (1, 1),\\n strides=(2, 2),\\n use_bias=False,\\n kernel_regularizer=l2(reg)\\n )(act)\\n\\n x = add([x, shortcut])\\n\\n return x\\n\\n","title":"Different Kinds of Residue Blocks","date":"2022-04-20T00:00:00.000Z","id":"blog064","tag":"deep-learning","intro":"Record different kinds of residue block that I have seen."},{"content":"\\nTraditonal $\\\\LaTeX$ software compiles our .tex file into .div, .log, .out, aux, etc, files along with our target file .pdf, if we were to generate several separated reports it will be an headache seeing all these files in the same directory.\\n\\nWith pandoc + markdown, it results in only one single pdf file. Most importantly, everyone can change the md file and preview the change in the previewer without converting it into a pdf.\\n\\nTo enable this feature, install vscode-pandoc extension in vscode, install pandoc as in the instruction, then we can try to compile the following!\\n\\ntext\\n---\\ndocumentclass: article\\nfontsize: 11pt\\nheader-includes: |\\n \\\\usepackage{amsmath}\\n \\\\usepackage{amsfonts}\\n \\\\usepackage{amssymb}\\n \\\\usepackage{color}\\n \\\\usepackage{graphicx}\\n \\\\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}\\n \\\\usepackage{hyperref}\\n \\\\usepackage{array}\\n \\\\usepackage{enumitem}\\n \\\\usepackage{graphbox}\\n \\\\usepackage{float}\\n \\\\usepackage{multicol}\\n \\\\thispagestyle{empty}\\n \\\\usepackage[htt]{hyphenat}\\n \\\\usepackage{fancyvrb}\\n \\\\RecustomVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\\\\footnotesize}\\n---\\n\\n# I am\\n\\nJames! $\\\\int_a^b f(x)\\\\,dx$.\\n\\n\\nFinally by pressing F1 -> Pandoc Render -> pdf, we are done! A single .pdf file will be generated in the same directory.\\n\\nIn case we want a beamer presentation:\\n\\ntext\\n---\\ndocumentclass: beamer\\nfontsize: 12pt\\nheader-includes: |\\n \\\\usetheme{Boadilla}\\n \\\\usepackage{amsmath}\\n \\\\usepackage{amsfonts}\\n \\\\usepackage{listings}\\n \\\\usepackage{amssymb}\\n \\\\usepackage{color}\\n \\\\usepackage{graphicx}\\n \\\\usepackage{hyperref}\\n \\\\usepackage{array}\\n \\\\usepackage{enumitem}\\n \\\\usepackage{graphbox}\\n \\\\usepackage{float}\\n \\\\usepackage{multicol}\\n \\\\usepackage[htt]{hyphenat}\\n \\\\usepackage{pifont}\\n \\\\usepackage{fancyvrb}\\n \\\\RecustomVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\\\\footnotesize}\\n---\\n\\n\\nwith beamer tutorial: https://www.overleaf.com/learn/latex/Beamer\\n","title":"Pandoc","date":"2022-04-29T00:00:00.000Z","id":"blog067","tag":"latex","intro":"Write latex document using markdown in vscode.","toc":false},{"content":"\\n#### About this Article\\n\\nThe main objective of this article is to understand every function of YOLO-v3 implemented in tensorfow. We will understand the algorithm by examining the shape of every output.\\n\\nThe sections below are named by module_name::{ function_1, function_2, ... }, where module_name means we are studying the file module_name.py.\\n\\nThe link of **_source code_** are provided at the beginning of each of such sections.\\n\\n#### dataset::{ parse_annotation, bbox_iou }\\n\\n<a href=\\"https://github.com/machingclee/deep-learning-study/blob/main/2021-02-15-yolo-trials/2021-02-15-YOLOV3-head-detection/core/dataset.py\\"><i>Source Code</i></a>\\n\\npython\\ndef parse_annotation(self, annotation):\\n line = annotation.split()\\n image_path = line[0]\\n if not os.path.exists(image_path):\\n raise KeyError(\\"%s does not exist ... \\" % imagparse_annotation\\n\\n e_path)\\n image = cv2.imread(image_path)\\n bboxes = np.array([list(map(int, box.split(\',\'))) for box in line[1:]])\\n\\n if self.data_aug:\\n image, bboxes = self.random_horizontal_flip(np.copy(image), np.copy(bboxes))\\n image, bboxes = self.random_crop(np.copy(image), np.copy(bboxes))\\n image, bboxes = self.random_translate(np.copy(image), np.copy(bboxes))\\n\\n image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\\n image, bboxes = utils.image_preporcess(np.copy(image), [self.train_input_size, self.train_input_size], np.copy(bboxes))\\n return image, bboxes\\n line = annotation.split()\\n image_path = line[0]\\n if not os.path.exists(image_path):\\n raise KeyError(\\"%s does not exist ... \\" % image_path)\\n image = cv2.imread(image_path)\\n bboxes = np.array([list(map(int, box.split(\',\'))) for box in line[1:]])\\n\\n if self.data_aug:\\n image, bboxes = self.random_horizontal_flip(np.copy(image), np.copy(bboxes))\\n image, bboxes = self.random_crop(np.copy(image), np.copy(bboxes))\\n image, bboxes = self.random_translate(np.copy(image), np.copy(bboxes))\\n\\n image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\\n image, bboxes = utils.image_preporcess(np.copy(image), [self.train_input_size, self.train_input_size], np.copy(bboxes))\\n return image, bboxes\\n\\n\\nThis part is straightforward. $\\\\qed$\\n\\npython-1\\ndef bbox_iou(self, boxes1, boxes2):\\n\\n boxes1 = np.array(boxes1)\\n boxes2 = np.array(boxes2)\\n\\n boxes1_area = boxes1[..., 2] * boxes1[..., 3]\\n boxes2_area = boxes2[..., 2] * boxes2[..., 3]\\n\\n boxes1 = np.concatenate([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\\n boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\\n boxes2 = np.concatenate([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\\n boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\\n\\n\\nFrom the function we can deduce that $\\\\texttt{box}=(c_x,c_y,\\\\text{width},\\\\text{height})$, therefore\\n\\n$$\\nl\\n\\\\texttt{boxes[...,:2]} - \\\\texttt{boxes[...,2:]}*0.5 = \\\\texttt{[all upper-left corners]}\\n$$\\n\\nand\\n\\n$$\\n \\\\texttt{boxes1[..., :2]} + \\\\texttt{boxes1[..., 2:]} * 0.5=\\\\texttt{[all lower-right corners]},\\n$$\\n\\nwhile concatenating $(2,)$ np-array along the last axis simpliy means combing them into one $(4,)$ np-array. Numerically in training $\\\\texttt{boxes1}$ and $\\\\texttt{boxes2}$ are like:\\n\\nnone\\n[[0.59375 3.78125 0.5 0.625 ]]\\n\\n\\nand\\n\\nnone\\n[[ 0.5 3.5 3.625 2.8125 ]\\n [ 0.5 3.5 4.875 6.1875 ]\\n [ 0.5 3.5 11.65625 10.1875 ]]\\n\\n\\nrespectively,\\n\\npython-13\\n left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])\\n right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\\n\\n\\nThink of the above as entrywise comparisons that give an array of maximum, which yields the coordinates of intersection rectangle for each fixed $\\\\texttt{boxes1}$ to boxes in $\\\\texttt{boxes}$.\\n\\npython-15\\n inter_section = np.maximum(right_down - left_up, 0.0)\\n\\n\\nThe entries in $\\\\texttt{inter_section}$ are the $\\\\textit{width}$ and $\\\\textit{height}$ of the intersection, the (broadcasted) $\\\\texttt{np.maximum}$ is just a tricky way to handle empty intersection.\\n\\npython-16\\n inter_area = inter_section[..., 0] * inter_section[..., 1]\\n union_area = boxes1_area + boxes2_area - inter_area\\n\\n return inter_area / union_area\\n\\n\\n$\\\\qed$\\n\\npython-1\\ndef preprocess_true_boxes(self, bboxes):\\n\\n\\nHere $\\\\texttt{bboxes}$ are the boxes from annotation file in which each line takes the form:\\n\\n$$\\n\\\\texttt{some/directory/hash.jpg 79,537,107,574,0 297,547,318,575,0}\\n$$\\n\\npython-2\\n label = [np.zeros((self.train_output_sizes[i],\\n self.train_output_sizes[i],\\n self.anchor_per_scale,\\n 5 + self.num_classes)) for i in range(3)]\\n bboxes_xywh = [np.zeros((self.max_bbox_per_scale, 4)) for _ in range(3)]\\n bbox_count = np.zeros((3,))\\n\\n for bbox in bboxes:\\n bbox_coor = bbox[:4]\\n bbox_class_ind = bbox[4]\\n\\n onehot = np.zeros(self.num_classes, dtype=np.float)\\n onehot[bbox_class_ind] = 1.0\\n uniform_distribution = np.full(self.num_classes, 1.0 / self.num_classes)\\n deta = 0.01\\n smooth_onehot = onehot * (1 - deta) + deta * uniform_distribution\\n # bbox_xywh is ground truth\\n bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2]) * 0.5, bbox_coor[2:] - bbox_coor[:2]], axis=-1)\\n # bbox_xywh_scaled is scaled ground truth relative to stride (13, 26, 52, as a unit)\\n bbox_xywh_scaled = 1.0 * bbox_xywh[np.newaxis, :] / self.strides[:, np.newaxis]\\n\\n\\nNote that\\n\\n$$\\n\\\\texttt{bbox_xywh[np.newaxis, :]}\\n$$\\n\\nis of shape $(1, 4)$ and\\n\\n$$\\n\\\\texttt{1/self.strides[:, np.newaxis]}\\n$$\\n\\nis of shape $(3, 1)$, their multiplication will be conducted by \\"broadcasting\'\' in numpy, which yields a $(3, 4)$ dimensional numpy array. The product $\\\\texttt{bbox_xywh_scaled}$ consists of $(c_x,c_y,w,h)$ which use \\"stride\\" as a unit, so 1 means \\"1 grid\\" (recall there are $13\\\\times 13$, $26\\\\times 26$, $52\\\\times 52$ grids predictions from Darknet backbone).\\n\\npython-22\\n iou = []\\n exist_positive = False\\n for i in range(3):\\n anchors_xywh = np.zeros((self.anchor_per_scale, 4))\\n anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\\n anchors_xywh[:, 2:4] = self.anchors[i]\\n\\n\\n$\\\\texttt{anchors_xywh}$ essentially moves centers of $\\\\texttt{bbox_xywh_scaled}$ to the middle of the grid where the original center lies in, then the anchor boxes\' width and height are assigned, replacing the original width, height of $\\\\texttt{bbox_xywh_scaled}$.\\n\\npython-28\\n iou_scale = self.bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)\\n\\n\\nThe presence of $\\\\texttt{np.newaxis}$ is simply because multiplication between $(4,)$ and $(3,4)$ array does not make sense. The additional dimension expand $(4,)$ array into $(1,4)$ array, which is broadcasted and multiplied to $(3,4)$ array to give another $(3,4)$ array, and therefore, $\\\\texttt{iou_scale.shape} = (3, )$.\\n\\npython-29\\n iou.append(iou_scale)\\n iou_mask = iou_scale > 0.3 # a boolean list of length 3\\n\\n if np.any(iou_mask): # if one of them is True\\n xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32)\\n label[i][yind, xind, iou_mask, :] = 0\\n label[i][yind, xind, iou_mask, 0:4] = bbox_xywh\\n label[i][yind, xind, iou_mask, 4:5] = 1.0\\n label[i][yind, xind, iou_mask, 5:] = smooth_onehot\\n\\n\\n$\\\\texttt{label[i]}$ is initialized at the beginning which is of size\\n\\n$$\\n\\\\texttt{train_output_sizes}\\\\times \\\\texttt{train_output_sizes}\\\\times 3\\\\times 85\\n$$\\n\\nfor each $\\\\texttt{i}$, where $\\\\texttt{train_output_sizes} = 13,26$ or $52$.\\n\\npython-38\\n bbox_ind = int(bbox_count[i] % self.max_bbox_per_scale)\\n bboxes_xywh[i][bbox_ind, :4] = bbox_xywh\\n\\n\\n$\\\\texttt{bboxes_xywh}$ is initialized (with zeros) at the beginning, $\\\\texttt{bboxes_xywh.shape} = (3,150,4)$.\\n\\npython-40\\n bbox_count[i] += 1\\n\\n exist_positive = True\\n\\n if not exist_positive:\\n best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1) # flatten and take max\\n # best_detect belongs to which \\"i\\", namely, best \\"i\\"\\n best_detect = int(best_anchor_ind / self.anchor_per_scale)\\n # for this i, which index it is:\\n best_anchor = int(best_anchor_ind % self.anchor_per_scale)\\n # get the grid point in our 13x13, 26x26, 52x52 grid:\\n xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32)\\n\\n label[best_detect][yind, xind, best_anchor, :] = 0\\n label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh\\n label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\\n label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot\\n\\n bbox_ind = int(bbox_count[best_detect] % self.max_bbox_per_scale)\\n bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh\\n # assign bbox_xywh into the list instead of append,\\n # this is to make sure there are at most 150 boxes within all 3 scales.\\n\\n bbox_count[best_detect] += 1\\n label_sbbox, label_mbbox, label_lbbox = label\\n sbboxes, mbboxes, lbboxes = bboxes_xywh\\n return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes\\n\\n\\nIn short,\\n\\n$$\\n\\\\texttt{sbboxes, mbboxes, lbboxe}\\n$$\\n\\nare just ground truth bounding boxes (center, width and height), while\\n\\n$$\\n\\\\texttt{label_sbbox, label_mbbox, label_lbbox}\\n$$\\n\\nare ground truth bounding boxes with objectiveness and probabilities of **_each grid_**.\\n$\\\\qed$\\n\\n#### Model Structure of YOLOv3\\n\\nIn $\\\\texttt{darknet53}$ after a bunch of residue modules we get 3 branches $\\\\texttt{route_1}$, $\\\\texttt{route_2}$ and $\\\\texttt{cov}$, where route_1, route_2, conv = backbone.darknet53(input_layer) in YOLOv3 function. Moreover,\\n\\n$$\\n\\\\texttt{route_1.shape}= (-1,52,52,256)\\n$$\\n\\n$$\\n\\\\texttt{route_2.shape} = (-1,26,26, 512)\\n$$\\n\\n$$\\n\\\\texttt{conv.shape} =(-1,13,13,1024)\\n$$\\n\\n<center>\\n<a href=\\"/assets/tech/042-yolo_structure.png\\">\\n<img src=\\"/assets/tech/042-yolo_structure.png\\" width=\\"100%\\"/>\\n</a>\\n<p></p>\\n</center>\\n\\nEach branch then jumps into several stages of feature extractions, the whole process finally gives another 3 branches of undecoded/raw data of features, and they are endowed with the meaning of \\"grid-based detection\\" after reshaping into\\n\\n$$\\n(-1,\\\\texttt{output_size},\\\\texttt{output_size}, 3, 85)\\n$$\\n\\ndimensional array in decode function (see conv_output in the body).\\n\\n#### yolov3::{ YOLOv3, decode }\\n\\n<a href=\\"https://github.com/machingclee/deep-learning-study/blob/main/2021-02-15-yolo-trials/2021-02-15-YOLOV3-head-detection/core/yolov3.py\\"><i>Source Code</i></a>\\n\\npython-1\\ndef YOLOv3(input_layer):\\n route_1, route_2, conv = backbone.darknet53(input_layer)\\n\\n conv = common.convolutional(conv, (1, 1, 1024, 512))\\n conv = common.convolutional(conv, (3, 3, 512, 1024))\\n conv = common.convolutional(conv, (1, 1, 1024, 512))\\n conv = common.convolutional(conv, (3, 3, 512, 1024))\\n conv = common.convolutional(conv, (1, 1, 1024, 512))\\n\\n\\nAs padding=\\"same\\" is being used along the chain of conv nets, there is no spatial dimension change.\\n\\npython-9\\n conv_lobj_branch = common.convolutional(conv, (3, 3, 512, 1024))\\n conv_lbbox = common.convolutional(conv_lobj_branch, (1, 1, 1024, 3*(NUM_CLASS + 5)), activate=False, bn=False)\\n\\n conv = common.convolutional(conv, (1, 1, 512, 256))\\n conv = common.upsample(conv)\\n\\n conv = tf.concat([conv, route_2], axis=-1)\\n\\n conv = common.convolutional(conv, (1, 1, 768, 256))\\n conv = common.convolutional(conv, (3, 3, 256, 512))\\n conv = common.convolutional(conv, (1, 1, 512, 256))\\n conv = common.convolutional(conv, (3, 3, 256, 512))\\n conv = common.convolutional(conv, (1, 1, 512, 256))\\n\\n conv_mobj_branch = common.convolutional(conv, (3, 3, 256, 512))\\n conv_mbbox = common.convolutional(conv_mobj_branch, (1, 1, 512, 3*(NUM_CLASS + 5)), activate=False, bn=False)\\n\\n conv = common.convolutional(conv, (1, 1, 256, 128))\\n conv = common.upsample(conv)\\n\\n conv = tf.concat([conv, route_1], axis=-1)\\n\\n conv = common.convolutional(conv, (1, 1, 384, 128))\\n conv = common.convolutional(conv, (3, 3, 128, 256))\\n conv = common.convolutional(conv, (1, 1, 256, 128))\\n conv = common.convolutional(conv, (3, 3, 128, 256))\\n conv = common.convolutional(conv, (1, 1, 256, 128))\\n\\n conv_sobj_branch = common.convolutional(conv, (3, 3, 128, 256))\\n conv_sbbox = common.convolutional(conv_sobj_branch, (1, 1, 256, 3*(NUM_CLASS +5)), activate=False, bn=False)\\n\\n return [conv_sbbox, conv_mbbox, conv_lbbox]\\n\\n\\n$\\\\qed$\\n\\npython-1\\ndef decode(conv_output, i=0):\\n \\"\\"\\"\\n return tensor of shape [batch_size, output_size, output_size, anchor_per_scale, 5 + num_classes]\\n contains (x, y, w, h, score, probability)\\n \\"\\"\\"\\n\\n\\n$\\\\texttt{conv_output}$ is the output of $\\\\texttt{YOLOv3}$ (i.e., $\\\\texttt{conv_sbbox}$, $\\\\texttt{conv_mbbox}$ or $\\\\texttt{conv_lbbox}$).\\n\\npython-6\\n conv_shape = tf.shape(conv_output)\\n batch_size = conv_shape[0]\\n output_size = conv_shape[1]\\n\\n conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\\n\\n conv_raw_dxdy = conv_output[:, :, :, :, 0:2]\\n conv_raw_dwdh = conv_output[:, :, :, :, 2:4]\\n conv_raw_conf = conv_output[:, :, :, :, 4:5]\\n conv_raw_prob = conv_output[:, :, :, :, 5: ]\\n\\n y = tf.tile(tf.range(output_size, dtype=tf.int32)[:, tf.newaxis], [1, output_size])\\n x = tf.tile(tf.range(output_size, dtype=tf.int32)[tf.newaxis, :], [output_size, 1])\\n\\n\\nFor example, let\'s take $\\\\texttt{output_size}=13$, then\\n\\n$$\\n\\\\texttt{y = np.tile(np.arange(13)[:, np.newaxis], [1, 13])}\\n$$\\n\\nand\\n\\n$$\\n\\\\texttt{x = np.tile(np.arange(13)[np.newaxis, :], [13, 1])}\\n$$\\n\\nare respectively:\\n\\nnone\\n[[ 0 0 0 0 0 0 0 0 0 0 0 0 0] [[ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [ 1 1 1 1 1 1 1 1 1 1 1 1 1] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [ 2 2 2 2 2 2 2 2 2 2 2 2 2] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [ 3 3 3 3 3 3 3 3 3 3 3 3 3] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [ 4 4 4 4 4 4 4 4 4 4 4 4 4] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [ 5 5 5 5 5 5 5 5 5 5 5 5 5] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [ 6 6 6 6 6 6 6 6 6 6 6 6 6] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [ 7 7 7 7 7 7 7 7 7 7 7 7 7] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [ 8 8 8 8 8 8 8 8 8 8 8 8 8] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [ 9 9 9 9 9 9 9 9 9 9 9 9 9] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [10 10 10 10 10 10 10 10 10 10 10 10 10] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [11 11 11 11 11 11 11 11 11 11 11 11 11] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]\\n [12 12 12 12 12 12 12 12 12 12 12 12 12]] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]]\\n\\n\\nFor $\\\\texttt{x}$ and $\\\\texttt{y}$ we expand dimension again along the last axis (break every single element into a bracketed element) before concatenation:\\n\\npython-19\\n xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)\\n\\n\\nAt this point, $\\\\texttt{xy_grid}$ is $(13,13,2)$ dimensional.\\n\\npython-20\\n xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1])\\n xy_grid = tf.cast(xy_grid, tf.float32)\\n\\n\\nNow $\\\\texttt{xy_grid}$ is $(\\\\texttt{batch_size}, 13, 13, 3, 2)$ dimensional. Recall that\\n\\n<center>\\n<img src=\\"/assets/tech/042-decode_anchor.png\\" width=\\"360\\"/>\\n</center>\\n\\npython-22\\n pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]\\n pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]\\n pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)\\n\\n pred_conf = tf.sigmoid(conv_raw_conf)\\n pred_prob = tf.sigmoid(conv_raw_prob)\\n\\n return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)\\n\\n\\nBear in mind that decoded $\\\\texttt{x}$, $\\\\texttt{y}$ in $\\\\texttt{pred_xywh}$ denote the center of prediction rectangle, as is the output of the function $\\\\texttt{preprocess_true_boxes}$. $\\\\qed$\\n\\n#### yolov3::{ compute_loss }\\n\\n<a href=\\"https://github.com/machingclee/deep-learning-study/blob/main/2021-02-15-yolo-trials/2021-02-15-YOLOV3-head-detection/core/yolov3.py\\"><i>Source Code</i></a>\\n\\npython\\ndef bbox_giou(boxes1, boxes2):\\n\\n boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\\n boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\\n boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\\n boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\\n\\n boxes1 = tf.concat([tf.minimum(boxes1[..., :2], boxes1[..., 2:]),\\n tf.maximum(boxes1[..., :2], boxes1[..., 2:])], axis=-1)\\n boxes2 = tf.concat([tf.minimum(boxes2[..., :2], boxes2[..., 2:]),\\n tf.maximum(boxes2[..., :2], boxes2[..., 2:])], axis=-1)\\n\\n boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\\n boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\\n\\n left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])\\n right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])\\n\\n inter_section = tf.maximum(right_down - left_up, 0.0)\\n inter_area = inter_section[..., 0] * inter_section[..., 1]\\n union_area = boxes1_area + boxes2_area - inter_area\\n iou = inter_area / union_area\\n\\n enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2])\\n enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])\\n enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0)\\n enclose_area = enclose[..., 0] * enclose[..., 1]\\n giou = iou - 1.0 * (enclose_area - union_area) / enclose_area\\n\\n return giou\\n\\n\\npython-1\\ndef compute_loss(pred, conv, label, bboxes, i=0):\\n\\n conv_shape = tf.shape(conv)\\n batch_size = conv_shape[0]\\n output_size = conv_shape[1]\\n input_size = STRIDES[i] * output_size\\n conv = tf.reshape(conv, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))\\n\\n conv_raw_conf = conv[:, :, :, :, 4:5]\\n conv_raw_prob = conv[:, :, :, :, 5:]\\n\\n pred_xywh = pred[:, :, :, :, 0:4]\\n pred_conf = pred[:, :, :, :, 4:5]\\n\\n label_xywh = label[:, :, :, :, 0:4]\\n respond_bbox = label[:, :, :, :, 4:5] # objectiveness\\n label_prob = label[:, :, :, :, 5:]\\n\\n giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1)\\n input_size = tf.cast(input_size, tf.float32)\\n\\n bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)\\n giou_loss = respond_bbox * bbox_loss_scale * (1 - giou)\\n\\n\\nNote that for two sets $U,V\\\\in \\\\mathcal C$, where $\\\\mathcal C\\\\in 2^{\\\\mathbb R^2}$, the function $d(U,V) := 1-\\\\text{giou}(U,V)$ defines a metric, so $\\\\texttt{giou_loss}$ makes sense.\\n\\npython-24\\n iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :], bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :])\\n\\n\\n$\\\\texttt{bboxes}$ are batched inside Dataset(\\"train\\").__next__ before passing into $\\\\texttt{compute_loss}$ (in a while loop until image count reaches batch size). Therefore $\\\\texttt{bboxes.shape} = (16, 150, 4)$, where $150$ is the maximal number of anchors (most of them are zeros due to initialization), so we see 3 :\'s in $\\\\texttt{bboxes}$.\\n\\nFinally\\n\\n$$\\n\\\\texttt{pred_xywh.shape}=(16, 13,13,3,150,4)=\\\\texttt{bboxes.shape}\\n$$\\n\\nand\\n\\n$$\\n\\\\texttt{iou.shape} = (16,13,13,3,150)\\n$$\\n\\nwhere computation gets rid of the last dimension. $\\\\texttt{bboxes}$ is copied to every grid for computation because from the original paper of YOLOv3:\\n\\n> \\"the confidence prediction represents the IOU between the predicted box and any ground truth box\\"\\n\\npython-25\\n max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1)\\n respond_bgd = (1.0 - respond_bbox) * tf.cast( max_iou < IOU_LOSS_THRESH, tf.float32 )\\n\\n\\nIn the internet some people call $\\\\texttt{IOU_LOSS_THRESH}$ as $\\\\texttt{ignore_thresh}$. $\\\\texttt{respond_bgd}$ determines whether to penalize a prediction\\n\\n- that overlaps too few with ground truth anchors (i.e., detected wrong location) **_and_**\\n- that makes false positive error.\\n\\npython-27\\n conf_focal = tf.pow(respond_bbox - pred_conf, 2)\\n\\n\\nThe concept of focal loss with $\\\\gamma = 2$ was introduced in **[TY]**, which down-weights the loss contributed by well-classificed (high confidence) examples.\\n\\npython-28\\n conf_loss = conf_focal *\\n (\\n respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\\n +\\n respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)\\n )\\n\\n\\nWhere $\\\\texttt{tf.nn.sigmoid_cross_entropy_with_logits(labels=z, logits=x)}$ is\\n\\n$$\\n\\\\texttt{z} * -\\\\log(\\\\texttt{sigmoid}(\\\\texttt{x})) + (1 - \\\\texttt{z}) * -\\\\log(1 - \\\\texttt{sigmoid}(\\\\texttt{x})),\\n$$\\n\\ntherefore $\\\\texttt{x}$ has to be a raw prediction data (i.e., without signmoid).\\n\\npython-34\\n prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)\\n\\n giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1,2,3,4]))\\n conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1,2,3,4]))\\n prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1,2,3,4]))\\n\\n return giou_loss, conf_loss, prob_loss\\n\\n\\n$\\\\qed$\\n\\n#### Training\\n\\n<a href=\\"https://github.com/machingclee/deep-learning-study/blob/main/2021-02-15-yolo-trials/2021-02-15-YOLOV3-head-detection/train.py\\"><i>Source Code</i></a>\\n\\n###### Recap of Customized Training Loop with tf.GradientTape\\n\\nApart from predefined loss functions (such as $\\\\texttt{categorical_crossentropy}$ for classfication, $\\\\texttt{mse}$ for regression, etc), it is ocassional to come across non-standard loss functions from other repository with the use of $\\\\texttt{tf.GradientTape}$.\\n\\nSuch implementation usually involves 4 components:\\n\\n- **Component 1.** The model **architecture**\\n\\n- **Component 2.** The **loss function** used when computing the model loss\\n\\n- **Component 3.** The **optimizer** used to update the model weights\\n\\n- **Component 4.** The **step function** that encapsulates the forward and backward pass of the network\\n\\nNow the code below is self-explanatory:\\n\\n###### Start Training\\n\\npython\\ndef train_step(image_data, target, epoch):\\n\\n # image_data = batch of images\\n\\n with tf.GradientTape() as tape:\\n pred_result = model(image_data, training=True)\\n giou_loss = conf_loss = prob_loss = 0\\n\\n # optimizing process\\n for i in range(3):\\n conv, pred = pred_result[i*2], pred_result[i*2+1]\\n batch_label, batch_bboxes = target[i]\\n loss_items = compute_loss(pred, conv, batch_label, batch_bboxes, i)\\n giou_loss += loss_items[0]\\n conf_loss += loss_items[1]\\n prob_loss += loss_items[2]\\n\\n total_loss = giou_loss + conf_loss + prob_loss\\n\\n gradients = tape.gradient(total_loss, model.trainable_variables)\\n optimizer.apply_gradients(zip(gradients, model.trainable_variables))\\n\\n # update learning rate\\n global_steps.assign_add(1)\\n if global_steps < warmup_steps:\\n lr = global_steps / warmup_steps * cfg.TRAIN.LR_INIT\\n else:\\n lr = cfg.TRAIN.LR_END + 0.5 * (cfg.TRAIN.LR_INIT - cfg.TRAIN.LR_END) * (\\n (1 + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi))\\n )\\n optimizer.lr.assign(lr.numpy())\\n\\n # writing summary data\\n with writer.as_default():\\n tf.summary.scalar(\\"lr\\", optimizer.lr, step=global_steps)\\n tf.summary.scalar(\\"loss/total_loss\\", total_loss, step=global_steps)\\n tf.summary.scalar(\\"loss/giou_loss\\", giou_loss, step=global_steps)\\n tf.summary.scalar(\\"loss/conf_loss\\", conf_loss, step=global_steps)\\n tf.summary.scalar(\\"loss/prob_loss\\", prob_loss, step=global_steps)\\n writer.flush()\\n\\nfor epoch in range(cfg.TRAIN.EPOCHS):\\n for index, (image_data, target) in enumerate(trainset):\\n train_step(image_data, target, epoch)\\n\\n model.save_weights(\\"./checkpoints/yolov3-{}-{}.h5\\".format(cfg.WEIGHT_NAME_TO_SAVE, epoch))\\n\\n\\n#### Reference\\n\\n- YOLOv3 \u6e90\u7801\u89e3\u6790 1-5, https://blog.csdn.net/sxlsxl119/article/details/103028021\\n\\n- YOLOv3 \u7b97\u6cd5\u7684\u4e00\u70b9\u7406\u89e3, https://yunyang1994.gitee.io/2018/12/28/YOLOv3/\\n\\n- Joseph Redmon, Ali Farhadi, <a href=\\"https://arxiv.org/abs/1804.02767\\"> <i>YOLOv3: An Incremental Improvement </i></a>\\n\\n- **[TY]** Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He and Piotr Dollar, <a href=\\"https://arxiv.org/pdf/1708.02002.pdf?fbclid=IwAR38T65chV0UNPhBDbAExH021_afC0L6o9PEztpBPBAzqY3dBR8vOGy2qwg\\"><i>Focal Loss for Dense Object Detection</i></a>\\n\\n- Adrian Rosebrock, <a href=\\"https://www.pyimagesearch.com/2020/03/23/using-tensorflow-and-gradienttape-to-train-a-keras-model/\\"><i>Using TensorFlow and GradientTape to train a Keras model</i></a>\\n","title":"YOLOv3 Deep Dive","date":"2022-04-22T00:00:00.000Z","id":"blog065","tag":"deep-learning","intro":"I made this study note on 5th of March, 2021: <a href=\'https://github.com/machingclee/deep-learning-study/blob/main/2021-02-28-yolov3-algorithm-drilling/YOLOv3StudyNotes.pdf\'><i>2021-02-28-yolov3-algorithm-drilling</i></a> in pdf format. For a refresh of memory I reformat the content into an md file for this blog for easy reading and reference."},{"content":"\\n#### Results\\n\\nAfter the implementation and the results I was amazed by the horrible performance of WGAN-GP compared to that of WGAN.\\n\\nAlthough mnist is a toy dataset, but among all the GANs I have implemented none of them can converge that quickly even without tuning parameters for many times.\\n\\n##### By WGAN Using 4000 Batches\\n\\n<center>\\n<img src=\\"/assets/tech/043-01.png\\" width=\\"170\\"/>\\n<img src=\\"/assets/tech/043-02.png\\" width=\\"170\\"/>\\n<img src=\\"/assets/tech/043-03.png\\" width=\\"170\\"/>\\n<img src=\\"/assets/tech/043-04.png\\" width=\\"170\\"/>\\n<img src=\\"/assets/tech/043-05.png\\" width=\\"170\\"/>\\n<img src=\\"/assets/tech/043-06.png\\" width=\\"170\\"/>\\n<img src=\\"/assets/tech/043-07.png\\" width=\\"170\\"/>\\n<img src=\\"/assets/tech/043-09.png\\" width=\\"170\\"/>\\n</center>\\n\\n##### By WGAN-GP Using less than 1000 Batches\\n\\n<center>\\n<img src=\\"/assets/tech/046-01.png\\" width=\\"170\\"/>\\n<img src=\\"/assets/tech/046-02.png\\" width=\\"170\\"/>\\n<img src=\\"/assets/tech/046-03.png\\" width=\\"170\\"/>\\n</center>\\n\\n#### Wasserstein Metric\\n\\n##### Definitions, Basic Examples and some Mathematics\\n\\nGiven the real distribution $P_r$ of real data, we try to approximate it by a distribution $P_\\\\theta$, such that $g_\\\\theta(Z)\\\\sim P_\\\\theta $ for some network $g_\\\\theta$ with $Z$ being an unknown random variable (usually this is constructed from Gaussian distribution $\\\\mathcal N(0,1)$).\\n\\nFrom **[MA]** the **_Wasserstein metric_** behaves the best from experiment and theoretical observations:\\n\\n$$\\nW(P_r, P_\\\\theta) := \\\\inf_{\\\\gamma\\\\in \\\\Pi (P_r, P_\\\\theta) } \\\\mathbb E(\\\\|x-y\\\\|),\\n$$\\n\\nwhere for any probability measures $\\\\mu, \\\\nu $ on $X$, $\\\\Pi (\\\\mu, \\\\nu)$ denotes the set of all **_transportation plans_** form $\\\\mu$ to $\\\\nu$ such that $\\\\pi^1\\\\# \\\\gamma = P_r$ and $\\\\pi^2\\\\#\\\\gamma = P_\\\\theta$. Here $\\\\pi^i:X\\\\times X\\\\to X$\'s are canonical projections.\\n\\nBy an abuse of notation, for any probability measure $\\\\rho$ that is absolutely continuous w.r.t. Lebesuge measure $dx$ on $\\\\mathbb R^n$ we refer $\\\\rho$ also the density function $\\\\rho\\\\in L^1(\\\\mathbb R^n)$ and write $\\\\rho = \\\\rho\\\\,dx$ interchangeably, therefore $P_r=P_r\\\\,dx$ and $P_\\\\theta = P_\\\\theta\\\\,dx$ in our case, and thus the notation $W(P_r, P_\\\\theta)$ makes sense.\\n\\nThe following example is taken from **[MA]**, let\'s fill in the detail:\\n\\n<example>\\n\\n**Example.** Let $Z\\\\sim U[0,1]$ be a random variable sampled from uniform distribution on $[0,1]$, the sets $X_0 := \\\\{0\\\\} \\\\times [0,1]$ and $X_\\\\theta := \\\\{\\\\theta\\\\} \\\\times[0,1]$ form a disjoint subsets in $\\\\mathbb R^2$.\\n\\nConsider $X = X_0\\\\sqcup X_\\\\theta$, denote $P_0$ the distribution of $(0, Z)$ in $X$ and $P_\\\\theta$ that of $(\\\\theta, Z)$ in $X$, then $P_0$ and $P_\\\\theta$ obviously have no nonempty common support.\\n\\n<center>\\n<img src=\\"/assets/tech/045.png\\"/>\\n</center>\\n\\n<p><p/>\\n\\nWe consider the following metrics among these two distributions:\\n\\n- $W(P_0,P_\\\\theta) = |\\\\theta|$\\n\\n **Reason.** $W(P_0,P_\\\\theta)\\\\ge |\\\\theta|$ is obvious, for equality define the map $t:(0,x)\\\\mapsto (\\\\theta, x)$ on $X_0$ and $(\\\\theta,x)\\\\mapsto (\\\\theta,x)$ on $X_\\\\theta$, then $t:X\\\\to X$ satisfies $t\\\\# P_0 = P_\\\\theta$.\\n\\n From standard result in optimal transport, the map $(\\\\mathrm{id}, t):X\\\\to X\\\\times X$ induces a transportation plan $\\\\gamma := t\\\\# P_0\\\\in \\\\Pi(P_0, P_\\\\theta)$ which is a measure on $X\\\\times X$.\\n\\n Denote $d(x,y)=\\\\|x-y\\\\|$, we have\\n\\n $$\\n \\\\begin{aligned}\\n \\\\int_{X\\\\times X} d(x,y)\\\\, d\\\\gamma(x,y)\\n &= \\\\int_{X\\\\times X}\\\\,d(x,y) \\\\,d \\\\big((\\\\mathrm{id}, t)\\\\# P_0\\\\big)(x,y)\\\\\\\\\\n &= \\\\int_X d\\\\big((\\\\mathrm{id},t) (x)\\\\big)) \\\\,d P_0(x)\\\\\\\\\\n &= \\\\int_X d(x, t(x))\\\\,dP_0(x)\\\\\\\\\\n &= \\\\int_X |\\\\theta| \\\\,dP_0(x) \\\\\\\\\\n &= |\\\\theta|,\\n \\\\end{aligned}\\n $$\\n\\n therefore $W(P_0,P_\\\\theta)=|\\\\theta|$.\\n\\n- $JS(P_0,P_\\\\theta) = \\\\frac{1}{2}(KL(P_0\\\\|P_m) + KL(P_\\\\theta\\\\|P_m)) = \\\\log 2$, where $P_m = \\\\frac{P_0+P_\\\\theta}{2}$\\n\\n **Reason.** $KL(P_0\\\\|P_m)=\\\\int_{X} \\\\log (\\\\frac{P_0}{P_m}) P_0\\\\,dx$, since $X=X_0\\\\sqcup X_\\\\theta$ on which $P_0$ and $P_\\\\theta$ have no common support, therefore\\n\\n $$\\n KL(P_0\\\\|P_m) = \\\\int_{X_0} \\\\log \\\\frac{P_0}{\\\\frac{P_0 + 0}{2}}P_0\\\\,dx = \\\\log 2\\\\int_{X_0} P_0(x)\\\\,dx = \\\\log 2.\\n $$\\n\\n Similarly, $KL(P_\\\\theta\\\\|P_m) = \\\\log 2$, and the result follows.\\n\\n- $KL(P_0\\\\|P_\\\\theta) = KL(P_\\\\theta\\\\|P_0) = \\\\infty$\\n\\n **Reason.** Demoninator 0 on a set of positive measure.\\n\\n</example>\\n\\n##### The Usual form of Wasserstein Metric for Coding\\n\\nA remarkable result named **_Kantorovich-Rubinstein duality_** from optimal transport states that\\n\\n$$\\nW(P_r,P_\\\\theta) = \\\\sup_{\\\\|f\\\\|_L \\\\leq 1} (\\\\mathbb E_{x\\\\sim P_r}[f(x)] - \\\\mathbb E_{x\\\\sim P_\\\\theta}[f(x)])\\n$$\\n\\nSince $g_\\\\theta(Z) \\\\sim P_\\\\theta$, we can further write\\n\\n$$\\nW(P_r,P_\\\\theta) = \\\\sup_{\\\\|f\\\\|_L \\\\leq 1} (\\\\mathbb E_{x\\\\sim P_r}[f(x)] - \\\\mathbb E_{z\\\\sim p(z)}[f(g_\\\\theta(z))]),\\n$$\\n\\nwhere we assume that $Z\\\\sim p$, for some density $p\\\\in L^1(\\\\mathbb R^{\\\\texttt{z_dim}})$ with $\\\\texttt{z_dim}$ being the latent dimension which we need to search by experiment (rigorously the **_equality holds_** by a standard real analysis trick: prove it for first characteristic functions, second simple functions, finally use monotone convergence etc).\\n\\n###### WGAN Version\\n\\nIn coding we transform the last equality into an approximated form in $(*)$\\n\\n$$\\n\\\\begin{align*}\\n W(P_r,P_\\\\theta) &\\\\approx \\\\sup_{w} \\\\bigg(\\\\,\\\\,\\\\,\\\\mathbb E_{x\\\\sim P_r}[\\n f_w(\\\\!\\\\!\\\\!\\\\!\\\\underbrace{x}_{\\\\text{real sample}}\\\\!\\\\!\\\\!\\\\!)] -\\n \\\\mathbb E_{z\\\\sim p(z)}[f_w(\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\underbrace{g_\\\\theta(z)}_{\\\\text{generated sample}}\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!)\\n ]\\\\quad \\\\bigg)\\n \\\\tag*{$(*)$}\\\\\\\\\\n&=- \\\\inf_{w} \\\\bigg(\\\\mathbb E_{z\\\\sim p(z)}[f_w(g_\\\\theta(z))] - \\\\mathbb E_{x\\\\sim P_r}[f_w(x)]\\\\bigg). \\\\tag*{$(**)$}\\n\\\\end{align*}\\n$$\\n\\nWe will be replacing the set of 1-Lipschitz functions $f$ by a parametrized family $\\\\{f_w\\\\}$ and perform the following in train loop:\\n\\n<center>\\n<a href=\\"/assets/tech/044.png\\">\\n<img src=\\"/assets/tech/044.png\\" width=\\"600\\"/>\\n</a>\\n</center>\\n<p></p>\\n\\n- **Line 6 in Algorithm.**\\n\\n - We minimize the term\\n\\n $$\\n \\\\mathcal L_c:=\\\\mathbb E_{z\\\\sim p(z)}[f_w(g_\\\\theta(z))] - \\\\mathbb E_{x\\\\sim P_r}[f_w(x)]\\n $$\\n\\n in the RHS of $(**)$ w.r.t. $w = \\\\texttt{critic.trainable_variables}$ to get a result that is hopefully close to $W(P_r,P_\\\\theta)$. <br/>\\n\\n - We update critic by $\\\\partial \\\\mathcal L_c/\\\\partial w$.\\n\\n- **Line 11 in Algorithm.**\\n - We minimize\\n $$\\n \\\\mathcal L_g := -\\\\mathbb E_{z\\\\sim p(z)}[f_w(g_\\\\theta(z))]\\n $$\\n in the RHS of $(*)$ w.r.t. $\\\\theta=\\\\texttt{gen.trainable_variables}$ to get smaller $W(P_r,P_\\\\theta)$.\\n - We update generator by $\\\\partial \\\\mathcal L_g/\\\\partial \\\\theta$.\\n- Usually the candidates of $\\\\{f_w\\\\}$ are modified from our discriminator.\\n\\n- (cont\'d) As we will not use signmoid output any more, usually we call $f_w$ a **_critic_** which replaces the role played by the usual $\\\\log D$ trick. Then we can modify our discriminator to output any tensor of shape $(\\\\texttt{batch_size}, 1)$.\\n\\n###### WGAN-GP Version\\n\\nAs discussed in **[IG]** the weight clipping causes our critics to learn very simple functions, therefore we remove the constraint on the norm of $\\\\nabla_x f_w(x)$ for $x\\\\sim P_r$ and force the norms to be bounded by introducing a loss term in the following:\\n\\n$$\\n\\\\mathcal L = \\\\underbrace{ \\\\mathbb E_{z\\\\sim p(z)}[f_w(g_\\\\theta(z))] - \\\\mathbb E_{x\\\\sim P_r}[f_w(x)]}_{\\\\text{original critic loss}} + \\\\lambda\\\\times \\\\underbrace{\\\\mathbb E_{\\\\hat x \\\\sim P_{\\\\hat x}} \\\\big[(\\\\|\\\\nabla f_w(\\\\hat x)\\\\|_2 - 1)^2\\\\big]}_{\\\\text{gradient penality}}.\\n$$\\n\\nWhere $P_{\\\\hat x}$ denotes the uniform distribution of points along straight lines between any pair of points from $P_r$ and $P_\\\\theta$,\\n\\n- The first term is the old critic term that we need to minimize in order to approximate the Wasserstein distance.\\n- The second term will be mininized w.r.t. $w$ to control the growth of the gradients of $\\\\{f_w\\\\}$.\\n\\n<center>\\n<img src=\\"/assets/tech/047.png\\" width=\\"640\\"/>\\n</center>\\n\\n<p></p>\\n\\n- **Line 7 in Algorithm.** We minimize\\n $$\\n \\\\mathcal L_c := \\\\mathbb E_{x\\\\sim P_\\\\theta}[D_w(x)] - \\\\mathbb E_{x\\\\sim P_r}[D_w(x)] + \\\\lambda \\\\cdot \\\\mathbb E_{\\\\hat x \\\\sim P_{\\\\hat x}} \\\\big[(\\\\|\\\\nabla_x D_w(\\\\hat x)\\\\|_2 - 1)^2\\\\big]\\n $$\\n w.r.t. $w = \\\\texttt{critic.trainable_variables}$, where $D_w$ denotes critic, we will take $\\\\lambda = 10$.\\n- (cont\'d) We update the critic by $\\\\partial \\\\mathcal L_c/\\\\partial w$.\\n- **Line 12 in Algorithm.** Update of generator remains the same, we still minimize\\n $$\\n \\\\mathcal L_g:=-\\\\mathbb E_{z\\\\sim p(z)}[D_w(g_\\\\theta(z))]\\n $$\\n w.r.t. $\\\\theta = \\\\texttt{gen.trainable_variables}$.\\n- (cont\'d) We update the generator by $\\\\partial \\\\mathcal L_g/\\\\partial \\\\theta$.\\n- No BatchNormalization in critic, instead we use LayerNormalization.\\n- Remove weight-clipping.\\n- We use smaller learning rate.\\n- Anything else remain the same.\\n\\n#### Implementation of WGAN\\n\\n##### Constants\\n\\nWith exactly the same setup as in <a href=\\"/blog/article/GAN-and-DCGAN-in-Tensorflow\\"><i>DCGAN</i></a>, we edit the following constants:\\n\\npython\\nimg_rows = 28\\nimg_cols = 28\\nchannels = 1\\nweight_clip = 0.01\\nbatch_size = 64\\ncritic_iteration = 5\\nimg_shape = (img_rows, img_cols, channels)\\nlearning_rate = 1e-5\\nz_dim = 128\\n\\n\\n##### Critic\\n\\nWe have the same generator, but different discriminator, which we call critic as it no longer output a number in $[0,1]$:\\n\\npython\\ngen = build_generator()\\ncritic = build_critic()\\n\\ndef build_critic():\\n model = Sequential()\\n model.add(\\n Conv2D(32,\\n kernel_size=3,\\n strides=2,\\n input_shape=img_shape,\\n padding=\'same\')\\n )\\n model.add(LeakyReLU(alpha=0.01))\\n model.add(\\n Conv2D(64,\\n kernel_size=3,\\n strides=2,\\n input_shape=img_shape,\\n padding=\'same\'))\\n model.add(BatchNormalization())\\n model.add(LeakyReLU(alpha=0.01))\\n model.add(\\n Conv2D(128,\\n kernel_size=3,\\n strides=2,\\n input_shape=img_shape,\\n padding=\'same\'))\\n model.add(BatchNormalization())\\n model.add(LeakyReLU(alpha=0.01))\\n model.add(Flatten())\\n model.add(Dense(1))\\n\\n return model\\n\\n\\n##### Train Loop with Wasserstein Metric in Place of the Adversarial Loss\\n\\npython\\ndef train(iterations, batch_size, sample_interval):\\n (x_train, _), (_, _) = mnist.load_data()\\n x_train = x_train/127.5 - 1.0\\n # np.shape(x_train) = (60000, 28, 28), for conv2D we need the channel dimension at the last axis\\n x_train = np.expand_dims(x_train, axis=3)\\n\\n gen_opt = RMSprop(lr=learning_rate)\\n critic_opt = RMSprop(lr=learning_rate)\\n\\n for i in range(iterations):\\n print(f\\"iteration: {i+1}\\", end = \\"\\\\r\\")\\n\\n for j in range(critic_iteration):\\n z = tf.random.normal((batch_size, z_dim), 0, 1)\\n\\n update_gen = ((j+1) % (critic_iteration)) == 0\\n\\n with tf.GradientTape() as critic_tape, tf.GradientTape() as gen_tape:\\n idxs = np.random.randint(0, x_train.shape[0], batch_size)\\n imgs = x_train[idxs]\\n gen_imgs = gen(z, training=True)\\n\\n critic_fake = critic(gen_imgs)\\n critic_real = critic(imgs)\\n loss_critic = -(tf.math.reduce_mean(critic_real) - tf.math.reduce_mean(critic_fake))\\n\\n if update_gen:\\n loss_gen = - tf.math.reduce_mean(critic_fake)\\n\\n grad_of_critic = critic_tape.gradient(loss_critic, critic.trainable_variables)\\n critic_opt.apply_gradients(zip(grad_of_critic, critic.trainable_variables))\\n\\n weights = critic.get_weights()\\n weights = [tf.clip_by_value(w, -weight_clip, weight_clip) for w in weights]\\n critic.set_weights(weights)\\n\\n if update_gen:\\n grad_of_gen = gen_tape.gradient(loss_gen, gen.trainable_variables)\\n gen_opt.apply_gradients(zip(grad_of_gen, gen.trainable_variables))\\n\\n if (i+1) % sample_interval == 0:\\n iteration_checkpoints.append(i+1)\\n sample_images(gen)\\n\\n\\nFinally:\\n\\npython\\niterations = 20000\\nsample_interval = 100\\n\\ntrain(iterations, batch_size, sample_interval)\\n\\n\\n#### Implementation of WGAN-GP\\n\\nThere are just few modifications from WGAN, first for the constants:\\n\\n##### New Constants\\n\\npython\\nimg_rows = 28\\nimg_cols = 28\\nchannels = 1\\nbatch_size = 64\\ncritic_iteration = 5\\nimg_shape = (img_rows, img_cols, channels)\\nlearning_rate = 1e-4\\nz_dim = 128\\n\\n\\nWe just decrease learning_rate and remove critic_iteration.\\n\\n##### New Critic\\n\\nWe replace all BatchNormalization by LayerNormalization:\\n\\npython\\ndef build_critic():\\n model = Sequential()\\n model.add(\\n Conv2D(32,\\n kernel_size=3,\\n strides=2,\\n input_shape=img_shape,\\n padding=\'same\')\\n )\\n model.add(LeakyReLU(alpha=0.01))\\n model.add(\\n Conv2D(64,\\n kernel_size=3,\\n strides=2,\\n input_shape=img_shape,\\n padding=\'same\'))\\n model.add(LayerNormalization())\\n model.add(LeakyReLU(alpha=0.01))\\n model.add(\\n Conv2D(128,\\n kernel_size=3,\\n strides=2,\\n input_shape=img_shape,\\n padding=\'same\'))\\n model.add(LayerNormalization())\\n model.add(LeakyReLU(alpha=0.01))\\n model.add(Flatten())\\n model.add(Dense(1))\\n\\n return model\\n\\n\\n##### Gradient Penality\\n\\nNext we define function to compute gradient penality:\\n\\npython\\ndef gradient_penality(critic, real_sample, fake_sample):\\n epsilon = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\\n interpolated = epsilon * real_sample + (1 - epsilon) * fake_sample\\n\\n with tf.GradientTape() as gp_tape:\\n gp_tape.watch(interpolated)\\n critic_inter = critic(interpolated, training=True)\\n\\n grads = gp_tape.gradient(critic_inter, [interpolated])[0]\\n norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\\n penality = tf.reduce_mean((norm - 1.0) ** 2)\\n return penality\\n\\n\\n##### New Train Loop\\n\\nWe remove weight-clipping and modify the weight update of critic by adding gradient penality:\\n\\npython\\ndef train(iterations, batch_size, sample_interval):\\n (x_train, _), (_, _) = mnist.load_data()\\n x_train = x_train/127.5 - 1.0\\n # np.shape(x_train) = (60000, 28, 28), for conv2D we need the channel dimension at the last axis\\n x_train = np.expand_dims(x_train, axis=3)\\n\\n gen_opt = RMSprop(lr=learning_rate)\\n critic_opt = RMSprop(lr=learning_rate)\\n\\n for i in range(iterations):\\n print(f\\"iteration: {i+1}\\", end = \\"\\\\r\\")\\n\\n for j in range(critic_iteration):\\n z = tf.random.normal((batch_size, z_dim), 0, 1)\\n\\n update_gen = ((j+1) % (critic_iteration)) == 0\\n\\n with tf.GradientTape() as critic_tape, tf.GradientTape() as gen_tape:\\n idxs = np.random.randint(0, x_train.shape[0], batch_size)\\n imgs = x_train[idxs]\\n gen_imgs = gen(z, training=True)\\n\\n critic_fake = critic(gen_imgs, training=True)\\n critic_real = critic(imgs, training=True)\\n gp = gradient_penality(critic, imgs, gen_imgs)\\n\\n loss_critic = tf.math.reduce_mean(critic_fake) \\\\\\n - tf.math.reduce_mean(critic_real) + 10 * gp\\n\\n if update_gen:\\n loss_gen = - tf.math.reduce_mean(critic_fake)\\n\\n grad_of_critic = critic_tape.gradient(loss_critic, critic.trainable_variables)\\n critic_opt.apply_gradients(zip(grad_of_critic, critic.trainable_variables))\\n\\n if update_gen:\\n grad_of_gen = gen_tape.gradient(loss_gen, gen.trainable_variables)\\n gen_opt.apply_gradients(zip(grad_of_gen, gen.trainable_variables))\\n\\n if (i+1) % sample_interval == 0:\\n iteration_checkpoints.append(i+1)\\n # print(\\"%d [D loss: %f] [G loss: %f]\\" % (i + 1, loss_critic, loss_gen))\\n sample_images(gen)\\n\\n\\n#### Reference\\n\\n- **[MA]** Martin Arjovsky, Soumith Chintala, and Leon Bottou, <a href=\\"https://arxiv.org/pdf/1701.07875.pdf\\"><i>Wasserstein GAN</i></a>\\n\\n- **[IG]** Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville, <a href=\\"https://arxiv.org/pdf/1704.00028.pdf\\"><i>Improved Training of Wasserstein GANs</i></a>\\n","title":"WGAN and WGAN-GP","date":"2022-04-22T00:00:00.000Z","id":"blog066","tag":"deep-learning","intro":"We modify from DCGAN from <a href=\\"/blog/article/GAN-and-DCGAN-in-Tensorflow\\"><i>this post</i></a> to experiment on WGAN by weight-clipping, then we try to experiment WGAN-GP using gradient penality."},{"content":"\\n#### Why do we want First Principal Component?\\n\\nSuppose that the data $x_1,x_2,\\\\dots,x_n$ with $d$ features are represented as a row vector, denote the matrix\\n\\n$$\\n\\\\hat B := \\\\begin{bmatrix}\\n-\\\\!\\\\!\\\\!-x_1-\\\\!\\\\!\\\\!-\\\\\\\\\\n-\\\\!\\\\!\\\\!-x_2-\\\\!\\\\!\\\\!-\\\\\\\\\\n\\\\vdots\\\\\\\\\\n-\\\\!\\\\!\\\\!-x_n -\\\\!\\\\!\\\\!-\\n\\\\end{bmatrix}\\n- \\\\begin{bmatrix} 1\\\\\\\\ 1 \\\\\\\\ \\\\vdots \\\\\\\\ 1\\\\end{bmatrix}\\n \\\\underbrace{\\\\frac{1}{n}\\\\sum_{i=1}^n \\\\begin{bmatrix}\\n -\\\\!\\\\!\\\\!-x_i-\\\\!\\\\!\\\\!-\\n \\\\end{bmatrix}}_{=:\\\\mu}\\n =\\n\\\\begin{bmatrix}\\n-\\\\!\\\\!\\\\!-(x_1 - \\\\mu)-\\\\!\\\\!\\\\!-\\\\\\\\\\n-\\\\!\\\\!\\\\!-(x_2 - \\\\mu)-\\\\!\\\\!\\\\!-\\\\\\\\\\n\\\\vdots\\\\\\\\\\n-\\\\!\\\\!\\\\!-(x_n - \\\\mu)-\\\\!\\\\!\\\\!-\\n\\\\end{bmatrix}.\\n$$\\n\\nFor every unit vector $\\\\hat n\\\\in \\\\mathbb R^d$, it is easy to see that $\\\\hat B \\\\hat n$ represents all the magnitude of $x_i-\\\\mu$ projected onto $\\\\mathrm{span} (\\\\hat n)$.\\n\\nNext,\\n\\n$$\\n\\\\|\\\\hat B \\\\hat n\\\\|_2 = \\\\left\\\\|\\n \\\\begin{bmatrix}\\n \\\\langle x_1 - \\\\mu, \\\\hat n \\\\rangle \\\\\\\\\\n \\\\langle x_2 - \\\\mu, \\\\hat n \\\\rangle\\\\\\\\\\n \\\\vdots\\\\\\\\\\n \\\\langle x_n - \\\\mu, \\\\hat n \\\\rangle\\n \\\\end{bmatrix}\\n\\\\right \\\\|_2\\n=\\\\sqrt{\\\\sum_{i=1}^n \\\\langle x_i - \\\\mu, \\\\hat n \\\\rangle^2}\\n$$\\n\\nis the standard deviation of the set of values $\\\\{\\\\langle x_i-\\\\mu, \\\\hat n \\\\rangle\\\\}_{i=1}^n$ with mean $0$. Our target is now to find a direction $\\\\hat n $ so that the standard deviation is ***as huge as possible***, therefore along that direction our data $\\\\{x_1-\\\\mu,\\\\dots,x_n-\\\\mu\\\\}$ will be much more comparable and hopefully we can find clustering values for which the data can be grouped together.\\n\\nFinding such a direction is the same as finding $v\\\\in \\\\mathbb R^d$ with $\\\\|v\\\\|_2=1$ such that\\n\\n$$\\n\\\\sigma = \\\\max_{x\\\\in \\\\mathbb R\\\\setminus\\\\{0\\\\}} \\\\frac{\\\\|\\\\hat Bx\\\\|_2}{\\\\|x\\\\|_2} =\\\\| \\\\hat B v\\\\|_2.\\n$$\\n\\nWe call $v$ the **_first principal component_**.\\n\\n> **Conclusion.** **_First Principal Component_** is a unit vector in $v\\\\in \\\\mathbb R^d$ that maximizes the standard deviation of $\\\\{\\\\langle x_i-\\\\mu, v\\\\rangle\\\\}_{i=1}^n$, where $\\\\mu = \\\\frac{1}{n}\\\\sum_{i=1}^n x_i$.\\n\\n#### In Terms of Singular Value Decomposition\\n\\n$v$ is in fact the right singular vector corresponding to the largest singular value. The **_$k$-th principal component_** is accordingly the $k$-th right singular vector corresponding to the $k$-th largest singular value.\\n\\nNote that by constructon $v_{k+1} \\\\in (\\\\mathrm{span} \\\\{v_1,v_2,\\\\dots,v_k\\\\})^\\\\perp$.\\n","title":"Principal Component Analysis","date":"2022-04-30T00:00:00.000Z","id":"blog069","tag":"math","intro":"Before I forget, record my basic understanding on what is Principal Component Analysis and how it works."},{"content":"\\n**Convention.** $\\\\mathcal O$ denotes a zero matrix of appropriate size.\\n\\n> **Theorem (Singular Value Decomposition).**\\n>\\n> 1. Every matrix $A\\\\in \\\\mathbb C^{m\\\\times n}$ has a SVD:\\n>\\n> $$\\n> A=U\\\\Sigma V^*\\n> $$\\n>\\n> where $U\\\\in \\\\mathbb C^{m\\\\times m}, V\\\\in \\\\mathbb C^{n\\\\times n}$ are unitary and $\\\\Sigma\\\\in \\\\mathbb R^{m\\\\times n}$ is \\"diagonal\\".\\n>\\n> Furthermore, The singular values $\\\\sigma_j$\'s\\n>\\n> $$\\n> \\\\sigma_1\\\\ge\\\\sigma_2\\\\ge\\\\dots\\\\ge \\\\sigma_{\\\\min\\\\{m,n\\\\}}\\\\ge 0\\n> $$\\n>\\n> are **_uniquely determined_**.\\n>\\n> 2. If $A$ is **_square_** and $\\\\sigma_j$\'s are **_distinct_**, the left and right singular vectors $\\\\{u_j\\\\}$ and $\\\\{v_j\\\\}$ are **_unique up to a multiplicative constant with modulus 1_**.\\n\\n<center></center>\\n\\n**Remark.** The technique in the proof below remains valid when all $\\\\mathbb C$ is replaced by $\\\\mathbb R$.\\n\\n<proof>\\n\\n**Proof of Part (1).** The case that $m=1$ or $n=1$ is simple, let\'s assume $m,n\\\\ge 2$. Let $\\\\sigma_1=\\\\|A\\\\|_2$, then due to compactness of $\\\\{x\\\\in \\\\mathbb C^n:\\\\|x\\\\|_2=1\\\\}$ in $\\\\mathbb C^n$ and the continuity of the map $x\\\\mapsto \\\\|Ax\\\\|_2$, there must be $v_1\\\\in \\\\mathbb C^n$ with $\\\\|v_1\\\\|_2=1$ s.t. $\\\\|Av_1\\\\|_2=\\\\sigma_1$, so there is $u_1\\\\in \\\\mathbb C^m$, $\\\\|u_1\\\\|_2=1$, $Av_1=\\\\sigma_1 u_1$. Hence $\\\\|A\\\\|_2$ is our first singular value.\\n\\nExtend $u_1$ to an o.n. basis $\\\\{u_1,\\\\dots,u_m\\\\}$ of $\\\\mathbb C^m$ and $v_1$ to an o.n. basis $\\\\{v_1,\\\\dots,v_n\\\\}$ of $\\\\mathbb C^n$. Let $U_1$ be the matrix with columns $u_i$ and $V_1$ be that with columns $v_i$, then (see remark next page for the explanation of $B$)\\n\\n$$\\n\\\\begin{align}\\n U_1^* AV_1\\n &=[A]_{(v_1,\\\\dots,v_n)}^{(u_1,\\\\dots,u_m)} \\\\nonumber \\\\\\\\\\n &=\\n \\\\begin{array}{c c}\\n & \\\\begin{array} {@{} c c c c @{}}\\n Av_1 \\\\hspace{1.5cm}& Av_2 \\\\hspace{1.6cm} & \\\\cdots &\\\\hspace{0.6cm} Av_n \\\\hspace{0.6cm}\\n \\\\end{array} \\\\\\\\\\n \\\\begin{array}{c}\\n u_1 \\\\\\\\[0.6cm] u_2 \\\\\\\\ \\\\vdots \\\\\\\\ u_m\\n \\\\end{array}\\\\hspace{-1em} &\\n \\\\left(\\n \\\\begin{array}{@{} c | c c c @{}}\\n \\\\sigma_1 & & \\\\hspace{1.5cm} w^*\\\\text{(to be proved $\\\\mathcal O$)}& \\\\\\\\\\n \\\\hline\\n & & & \\\\\\\\\\n & & & & \\\\\\\\\\n \\\\mathcal O & & \\\\hspace{1.5cm} B:=[A]_{(v_2,\\\\dots,v_n)}^{\\\\normalsize(u_2,\\\\dots,u_m)} & \\\\\\\\\\n\\n \\\\hspace{1.5cm} & & &&\\n \\\\end{array}\\n \\\\right) \\\\\\\\\\n \\\\mbox{} % Blank line to match column names so as to align the = vertically\\n \\\\end{array} =:S.\\\\tag*{(1)} \\\\\\\\\\n \\\\\\\\[-12pt]\\n\\\\end{align}\\n$$\\n\\nNow\\n\\n$$\\n\\\\sigma_1^2=\\\\|A\\\\|_2^2\\\\ge \\\\|S\\\\|_2^2 \\\\ge \\\\left\\\\|S \\\\begin{bmatrix}\\\\sigma_1\\\\\\\\ w\\\\end{bmatrix}\\\\right\\\\|_2^2 \\\\ge \\\\sigma_1^2 + w^*w,\\n$$\\n\\nthis implies $w=\\\\mathcal O$. Note that we have $x\\\\perp v_1\\\\implies Ax \\\\perp Av_1$, and the only assumption to derive this result is $\\\\|Av_1\\\\|_2=\\\\|A\\\\|_2$, with $\\\\|v_1\\\\|_2=1$. We extract this as a technical corollary.\\n\\n> **Corollary.** Let $A\\\\in \\\\mathbb C^{m\\\\times n}$, $v\\\\in \\\\mathbb C^n$ with $\\\\|v\\\\|_2=1$. Then if $\\\\|Av\\\\|_2 = \\\\|A\\\\|_2$,\\n>\\n> $$\\n> w\\\\perp v \\\\implies Aw\\\\perp Av.\\n> $$\\n>\\n> The same is true when $\\\\mathbb C$ is replaced by $\\\\mathbb R$.\\n\\n<proof>\\n\\n**Proof.**\\nRepeat what we have done so far, i.e., replace $v$ by $v_1$ and $\\\\frac{Av_1}{\\\\|Av_1\\\\|_2}$ by $u_1$ in the argument preceding the corollary. Then once $w\\\\perp v$, one has $Aw\\\\perp Av_1=Av$.\\n\\n</proof>\\n\\nTo finish the proof let\'s induct on $k \\\\ge 4$, where $m+n=k$. Suppose any $m\\\\times n$ matrix with $m+n = 4,5,\\\\dots, k-1$ has SVD with uniquely determined singular values in descending order. Then for $m+n=k$, by induction hypothesis and according to equation (1), $B= U_2\\\\Sigma V_2^*$ with unique $\\\\Sigma$, and the existence of SVD follows from the formula:\\n\\n$$\\nU_1^* A V_1=\\n\\\\left[\\n \\\\begin{array}{c|c}\\n \\\\sigma_1 & \\\\mathcal O\\\\\\\\\\n \\\\hline\\n \\\\mathcal O& U_2\\\\Sigma V_2^*\\n \\\\end{array}\\n\\\\right]\\n=\\n\\\\left[\\n \\\\begin{array}{c|c}\\n 1 & \\\\mathcal O\\\\\\\\\\n \\\\hline\\n \\\\mathcal O& U_2\\n \\\\end{array}\\n\\\\right]\\n\\\\left[\\n \\\\begin{array}{c|c}\\n \\\\sigma_1 & \\\\mathcal O\\\\\\\\\\n \\\\hline\\n \\\\mathcal O& \\\\Sigma\\n \\\\end{array}\\n\\\\right]\\n\\\\left[\\n \\\\begin{array}{c|c}\\n 1 & \\\\mathcal O\\\\\\\\\\n \\\\hline\\n \\\\mathcal O& V_2^*\\n \\\\end{array}\\n\\\\right].\\n$$\\n\\nAlthough $\\\\Sigma$ is unique, it is dependent on $B$, while $B$ is dependent on the choice of basis. Fortunately under any changes of $(u_2,\\\\dots,u_m)$ and $(v_2,\\\\dots,v_n)$ to other o.n. bases, $U_2$ and $V_2$ will be replaced by other unitary matrices and $\\\\Sigma$ remains unchanged, hence singular values of $A$ are unique. The proof is almost completed by induction, except for the base case $m+n=4$, which is obvious by (1).\\n\\n</proof>\\n\\n<proof>\\n\\n**Proof of Part (2).**\\nLet\'s assume $A\\\\in \\\\mathbb C^{n\\\\times n}$ is square. It is clear that $\\\\sigma_1=\\\\|A\\\\|_2$ since $\\\\|A\\\\|_2$ is the largest possible singular value of $A$. We first prove that if the right singular vector of $\\\\sigma_1$ is not unique\\", then $\\\\sigma_1$ is not simple, i.e., $\\\\sigma_1$ is repeated in $\\\\Sigma$.\\n\\nLet $Av_1=\\\\sigma_1u_1$, $\\\\|v_1\\\\|_2=\\\\|u_1\\\\|_2=1$. Suppose there are other vectors $w,w\'\\\\in \\\\mathbb C^n$, with $\\\\|w\\\\|_2=\\\\|w\'\\\\|_2=1$ s.t. $Aw=\\\\sigma_1 w\'$. For the sake of contradiction, let\'s assume $w\\\\not\\\\in \\\\mathbb C v_1$, then the unit vector $v_2 := \\\\frac{w-\\\\langle v_1,w\\\\rangle v_1}{\\\\|w-\\\\langle v_1,w\\\\rangle v_1\\\\|_2}$ is orthogonal to $v_1$. Now $\\\\|Av_2\\\\|_2\\\\leq \\\\|A\\\\|_2 = \\\\sigma_1$, the inequality cannot be strict, otherwise since $w = c v_1 + sv_2$ with $|c|^2+|s|^2=1$, we have\\n\\n$$\\n\\\\sigma_1^2=\\\\|Aw\\\\|_2^2 = \\\\|c \\\\sigma_1 u_1 + s Av_2\\\\|_2^2= |c|^2|\\\\sigma_1|^2 + |s|^2 \\\\|Av_2\\\\|_2^2< \\\\sigma_1^2,\\n$$\\n\\nabsurd. We conclude $Av_2=\\\\sigma_1 u_2$, for some unit vector $u_2\\\\in (\\\\mathbb C u_1)^\\\\perp$. Now by the corollary one observes that\\n\\n$$\\nA|_{(\\\\mathrm{span}_\\\\mathbb C(v_1,v_2))^\\\\perp}:(\\\\mathrm{span}_\\\\mathbb C(v_1,v_2))^\\\\perp\\\\to (\\\\mathrm{span}_\\\\mathbb C(u_1,u_2))^\\\\perp,\\n$$\\n\\nand thus we can get a complete list of singular values with $\\\\sigma_1$ appears twice, a contradiction. Hence if $\\\\sigma_j$\'s are distinct, $w\\\\in \\\\mathbb C v_1$, i.e., $w$ and $v_1$ differ by a multiplicative constant with modulus 1. It follows that $u_1$ is unique up to a complex sign. Finally since $A|_{(\\\\mathbb C v_1)^\\\\perp} : (\\\\mathbb C v_1)^\\\\perp\\\\to (\\\\mathbb C u_1)^\\\\perp$, by choosing the bases of these two spaces, the uniqueness follows from induction on dimension of the square matrix.\\n\\n</proof>\\n","title":"Singular Value Decomposition","date":"2022-04-30T00:00:00.000Z","id":"blog070","tag":"math","toc":false,"intro":"We discuss the rigorous proof of Singular Value Decomposition."},{"content":"\\n#### Results\\n\\n<center>\\n<a href=\\"/assets/tech/051.png\\">\\n <img src=\\"/assets/tech/051.png\\" width=\\"650\\"/>\\n</a>\\n</center>\\n\\n<center>\\n<a href=\\"/assets/tech/052.png\\">\\n <img src=\\"/assets/tech/052.png\\" width=\\"650\\"/>\\n</a>\\n</center>\\n\\n#### Imports\\n\\npython\\nimport numpy as np\\nimport cv2\\nimport tensorflow as tf\\nimport matplotlib.pyplot as plt\\nimport os\\nimport numpy as np\\nimport random\\n\\nfrom numpy.random import randint\\nfrom tensorflow.keras.optimizers import Adam, RMSprop\\nfrom tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError, CategoricalCrossentropy\\nfrom tensorflow.keras.initializers import HeNormal\\n\\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\\nfrom tensorflow.keras.layers import (\\n Input, Conv2D, Conv2DTranspose, LeakyReLU, ReLU, Flatten, Dense,\\n Activation, Concatenate, BatchNormalization, ZeroPadding2D\\n)\\nfrom glob import glob\\nfrom tensorflow.keras import activations\\nfrom tensorflow.keras.models import Model\\nfrom tqdm.notebook import tqdm\\nfrom skimage.color import rgb2lab, lab2rgb\\nfrom tensorflow.keras import initializers\\n\\nfrom tensorflow_addons.layers import InstanceNormalization, SpectralNormalization\\n\\n%matplotlib inline\\n\\n\\n#### Cuda\\n\\npython\\nos.environ[\'TF_XLA_FLAGS\'] = \'--tf_xla_enable_xla_devices\'\\n\\nconfig = tf.compat.v1.ConfigProto(\\n gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\\n)\\nconfig.gpu_options.allow_growth = True\\nsession = tf.compat.v1.Session(config=config)\\ntf.compat.v1.keras.backend.set_session(session)\\n\\n\\n#### Constants\\n\\npython\\n# ----- model config -----\\ndataset_dir = \\"./dataset_with_deepcrack\\"\\nlabels = [\\"crack\\"]\\nweight_init = initializers.GlorotNormal()\\nweight_regularizer = None\\nimage_shape = (224, 224, 3)\\nnoise_dim = (224,224,3)\\nSIZE=image_shape[0]\\n\\nch = 64 # default filter depth for the conv blocks\\nn_dis = 6 # number of covolution in the block of discriminator\\nc_dim = len(labels) + 1 # number of domains for image translation, crack, water_seepage, normal\\nn_res = 6 # number of res_block in bottleneck of image-to-image module\\n\\n# ----- training config -----\\nbuffer_size = 1000\\nbatch_size=1\\n\\nlr = 1e-4\\nn_disc_iteration = 1 # critic iteration before update of generator\\n\\n\\n#### Load DCT Dictionary and Mask\\n\\npython\\nU = tf.convert_to_tensor(np.load(\\"./DCT_U.npy\\"))\\nU_T = tf.convert_to_tensor(np.load(\\"./DCT_U_T.npy\\"))\\nenergy_mask = tf.convert_to_tensor(np.load(\\"./DCT_mask.npy\\"))\\n\\nDCT_U_ = tf.cast(U, tf.float32)[tf.newaxis, ...,tf.newaxis]\\nDCT_U = tf.concat([DCT_U_] * 2352, axis=0)\\nDCT_U_T_ = tf.cast(U_T, dtype=tf.float32)[tf.newaxis,...,tf.newaxis]\\nDCT_U_T = tf.concat([DCT_U_T_] * 2352, axis=0)\\n\\nDCT_freq_domain_mask_ = tf.cast(energy_mask, dtype=tf.float32)[tf.newaxis,..., tf.newaxis]\\nDCT_freq_domain_mask = tf.concat([DCT_freq_domain_mask_] * 2352, axis=0)\\n\\n\\n#### Model Definitions\\n\\n##### conv\\n\\npython\\ndef conv(x_init, filters, kernel_size=4, strides=2, pad=0, pad_type=\'zero\', use_bias=True, sn=False):\\n x = ZeroPadding2D(padding=(pad, pad))(x_init)\\n if sn:\\n w = tf.compat.v1.get_variable(\\"kernel\\", shape=[kernel_size, kernel_size, x.get_shape()[-1], channels], initializer=weight_init,\\n regularizer=weight_regularizer)\\n x = Conv2D(input=x, filter=spectral_norm(w),\\n strides=[1, strides, strides, 1], padding=\'VALID\')\\n if use_bias:\\n bias = tf.get_variable(\\"bias\\", [filters], initializer=tf.constant_initializer(0.0))\\n x = tf.nn.bias_add(x, bias)\\n\\n else:\\n x = Conv2D(filters, kernel_size=(kernel_size,kernel_size), kernel_initializer=weight_init, kernel_regularizer=weight_regularizer, strides=(strides,strides), use_bias=use_bias)(x)\\n return x\\n\\n\\n##### deconv\\n\\npython\\ndef deconv(x, filters, kernel_size=4, strides=2, use_bias=True):\\n x = Conv2DTranspose(filters=filters,\\n kernel_size=kernel_size,\\n kernel_initializer=weight_init,\\n kernel_regularizer=weight_regularizer,\\n strides=strides,\\n padding=\'same\',\\n use_bias=use_bias)(x)\\n return x\\n\\n\\n##### discriminator\\n\\npython\\ndef discriminator():\\n channel = ch\\n input = Input(shape=(image_shape))\\n x = conv(input, channel, kernel_size=4, strides=2, pad=1, use_bias=True)\\n x = LeakyReLU(0.01)(x)\\n\\n for i in range(1, n_dis):\\n x = conv(x, channel * 2, kernel_size=4, strides=2, pad=1, use_bias=True)\\n x = LeakyReLU(0.01)(x)\\n\\n channel = channel * 2\\n\\n c_kernel = int(image_shape[0] / np.power(2, n_dis))\\n\\n logit = conv(x, 1, kernel_size=3, strides=1, pad=1, use_bias=False)\\n c = conv(x, c_dim, kernel_size=c_kernel, strides=1, use_bias=False)\\n\\n c = tf.reshape(c, shape=[-1, c_dim])\\n c = Dense(c_dim, activation=\\"softmax\\")(c)\\n\\n model = Model(input, [logit, c])\\n\\n return model\\n\\n\\npython\\n# Check shape:\\ndisc = discriminator()\\nx = tf.random.normal((1,)+image_shape, 0, 1)\\nprint(disc(x)[0].shape)\\nprint(disc(x)[1].shape)\\n\\n\\n##### res_block\\n\\npython\\ndef res_block(x_in, filters, use_bias=True, scope=\'resblock\'):\\n input = x_in\\n x = conv(input, filters, kernel_size=3, strides=1, pad=1, use_bias=use_bias)\\n x = InstanceNormalization()(x)\\n x = ReLU()(x)\\n x = conv(x, filters, kernel_size=3, strides=1, pad=1, use_bias=use_bias)\\n x = InstanceNormalization()(x)\\n return x + input\\n\\n\\n##### down_sample\\n\\npython\\ndef down_sample(x, scale_factor_h, scale_factor_w):\\n _, h, w, _ = x.get_shape().as_list()\\n new_size = [h // scale_factor_h, w // scale_factor_w]\\n return tf.image.resize(x, new_size, method=\'nearest\')\\n\\n\\n##### param_free_norm\\n\\npython\\ndef param_free_norm(x, epsilon=1e-5):\\n x_mean, x_var = tf.nn.moments(x, axes=[1, 2])\\n x_std = tf.sqrt(x_var + epsilon)\\n return (x - x_mean) / x_std\\n\\n\\n##### spade\\n\\npython\\ndef spade(segmap, x_init, filters=None, use_bias=True, sn=False) :\\n x = param_free_norm(x_init)\\n\\n _, x_h, x_w, n_filters = x_init.get_shape().as_list()\\n _, segmap_h, segmap_w, _ = segmap.get_shape().as_list()\\n\\n factor_h = segmap_h // x_h # 256 // 4 = 64\\n factor_w = segmap_w // x_w\\n\\n segmap_down = down_sample(segmap, factor_h, factor_w)\\n\\n segmap_down = conv(segmap_down, filters=128, kernel_size=5, strides=1, pad=2, use_bias=use_bias, sn=sn)\\n segmap_down = ReLU()(segmap_down)\\n\\n segmap_gamma = conv(segmap_down, filters=filters, kernel_size=5, strides=1, pad=2, use_bias=use_bias, sn=sn)\\n segmap_beta = conv(segmap_down, filters=filters, kernel_size=5, strides=1, pad=2, use_bias=use_bias, sn=sn)\\n\\n x = x * (1 + segmap_gamma) + segmap_beta\\n\\n return x\\n\\n\\n##### spade_resblock\\n\\npython\\ndef spade_resblock(segmap, x_init, filters, use_bias=True, sn=False):\\n channel_in = x_init.get_shape().as_list()[-1]\\n channel_middle = min(channel_in, filters)\\n\\n x = spade(segmap, x_init, channel_in, use_bias=use_bias, sn=False)\\n x = LeakyReLU(0.2)(x)\\n x = conv(x, filters=channel_middle, kernel_size=3, strides=1,\\n pad=1, use_bias=use_bias, sn=sn)\\n x = spade(segmap, x, filters=channel_middle, use_bias=use_bias,\\n sn=False)\\n x = LeakyReLU(0.2)(x)\\n x = conv(x, filters=filters, kernel_size=3, strides=1, pad=1,\\n use_bias=use_bias, sn=sn)\\n\\n if channel_in != filters:\\n x_init = spade(segmap, x_init, filters=channel_in, use_bias=use_bias, sn=False)\\n x_init = conv(x_init, filters=filters, kernel_size=1, strides=1,\\n use_bias=False, sn=sn)\\n return x + x_init\\n\\n\\n##### adative_noise_multiplier\\n\\npython\\ndef adative_noise_multiplier(z):\\n x = Conv2D(32, (5,5), 2, padding=\\"same\\")(z)\\n x = ReLU()(x)\\n x = Conv2D(64, (3,3), 2, padding=\\"same\\")(x)\\n x = ReLU()(x)\\n x = Flatten()(x)\\n x = Dense(128, activation=\\"relu\\")(x)\\n x = Dense(1, activation=\\"sigmoid\\")(x)\\n\\n return x\\n\\n\\n##### generator\\n\\npython\\ndef generator():\\n channel = ch\\n x_in = Input(shape=image_shape)\\n c_in = Input(shape=(image_shape[0], image_shape[1], c_dim))\\n\\n z = Input(shape=noise_dim)\\n\\n lambda_z = adative_noise_multiplier(z)\\n x = tf.concat([x_in, lambda_z * z], axis=-1)\\n # in the past, c_in is of shape (c_dim,), now c_in has also the spacial information, therefore no need to tf.tile(c_in, [1, x_in.shape[1], x_in.shape[2], 1])\\n x = conv(x, channel, kernel_size=7, strides=1, pad=3, use_bias=False)\\n x = InstanceNormalization()(x)\\n x = ReLU()(x)\\n\\n\\n # Down-Sampling\\n for i in range(3):\\n x = conv(x, channel*2, kernel_size=4, strides=2, pad=1, use_bias=False)\\n x = InstanceNormalization()(x)\\n x = ReLU()(x)\\n\\n channel = channel * 2\\n\\n # Bottleneck\\n for i in range(n_res):\\n x = res_block(x, channel, use_bias=False)\\n\\n # Up-Sampling\\n for i in range(3):\\n # need to feed the spatial control map here\\n x = deconv(x, channel//2, kernel_size=4, strides=2, use_bias=False)\\n # spade normalization instead\\n x = spade_resblock(c_in, x, channel//2)\\n x = ReLU()(x)\\n\\n channel = channel // 2\\n\\n\\n pre_mask = conv(x, filters=1, kernel_size=7, strides=1, pad=3, use_bias=False)\\n mask = Activation(\\"sigmoid\\")(pre_mask)\\n\\n defect_overlay = conv(x, filters=3, kernel_size=7, strides=1, pad=3, use_bias=False)\\n defect_overlay = tf.reshape(defect_overlay, (2352, 8, 8, 1))\\n defect_overlay = tf.einsum(\\"oik...,okj...->oij...\\", tf.einsum(\\"oikb,okjb->oijb\\", DCT_U_T, defect_overlay), DCT_U)\\n defect_overlay = tf.einsum(\\"oij...,oij...->oij...\\", DCT_freq_domain_mask, defect_overlay)\\n defect_overlay = tf.einsum(\\"oik...,okj...->oij...\\", tf.einsum(\\"oik...,okj...->oij...\\", DCT_U, defect_overlay), DCT_U_T)\\n defect_overlay = tf.reshape(defect_overlay, (224,224,3))\\n defect_overlay = Activation(\\"tanh\\")(defect_overlay)\\n model = Model([x_in, c_in, z], [defect_overlay, mask])\\n return model\\n\\n\\n#### Dataset Pipeline\\n\\n##### Helper Functions\\n\\n###### one_hot\\n\\npython\\ndef one_hot(index, length):\\n zeros = np.zeros((length, ))\\n zeros[index] = 1\\n return zeros\\n\\n\\n###### get_segment_and_spatialCatMap_and_defectOneHot_and_normalOnehot\\n\\npython\\nimport glob\\n\\ndef get_segment_and_spatialCatMap_and_defectOneHot_and_normalOnehot(segment_filepath, index):\\n # only EagerTensor can be decoded as follows:\\n index = int(bytes.decode(index))\\n segment_filepath = bytes.decode(segment_filepath)\\n segment = img_to_array(load_img(segment_filepath, target_size=(SIZE,SIZE), color_mode=\\"grayscale\\"))\\n segment = np.where(segment>0.5, 1, 0)\\n\\n spatial_cat_map = np.zeros((SIZE, SIZE, c_dim))\\n\\n # create spatial_categorical map from segmentation and index of the defect class\\n\\n for x in range(SIZE):\\n for y in range(SIZE):\\n if segment[x,y,0] > 0:\\n spatial_cat_map[x,y,index] = 1\\n\\n defect_onehot = one_hot(index, c_dim)\\n normal_onehot = one_hot(c_dim-1, c_dim) # onehot with last entry as 1, else 0\\n\\n return tf.convert_to_tensor(segment, dtype=tf.float32), \\\\\\n tf.convert_to_tensor(spatial_cat_map, dtype=tf.float32), \\\\\\n tf.convert_to_tensor(defect_onehot, dtype=tf.float32),\\\\\\n tf.convert_to_tensor(normal_onehot, dtype=tf.float32)\\n\\n\\n###### path_to_img\\n\\npython\\nimport random\\n\\ndef path_to_img(zipped_filepaths):\\n normal_filepath = zipped_filepaths[0]\\n defect_filepath = zipped_filepaths[1]\\n segment_filepath = zipped_filepaths[2]\\n defect_index = zipped_filepaths[3]\\n\\n normal = tf.io.read_file(normal_filepath)\\n normal = tf.image.decode_png(normal, channels=3)\\n normal = tf.image.resize(normal, (SIZE, SIZE))\\n normal = (normal/127.5) - 1\\n\\n\\n defect = tf.io.read_file(defect_filepath)\\n defect = tf.image.decode_png(defect, channels=3)\\n defect = tf.image.resize(defect, (SIZE, SIZE))\\n defect = (defect/127.5) - 1\\n\\n\\n segment, spatial_cat_map, defect_onehot, normal_onehot = tf.numpy_function(\\n func=get_segment_and_spatialCatMap_and_defectOneHot_and_normalOnehot,\\n inp=[segment_filepath, defect_index],\\n Tout = [tf.float32, tf.float32, tf.float32, tf.float32]\\n )\\n\\n return (normal, defect, segment, defect_index, spatial_cat_map, defect_onehot, normal_onehot)\\n\\n\\n##### Final Dataset Generator: get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen\\n\\npython\\ndef get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen(random_segmentations=False):\\n normals = []\\n defects = []\\n defect_segmentations = []\\n defect_indexes = []\\n\\n for label in labels:\\n index = labels.index(label)\\n normals += glob.glob(f\\"{dataset_dir}/{label}/normal/*.jpg\\")\\n defects += glob.glob(f\\"{dataset_dir}/{label}/defect/*.jpg\\")\\n defect_segmentations += glob.glob(f\\"{dataset_dir}/{label}/defect_segmentation/*.png\\")\\n\\n if random_segmentations:\\n random.shuffle(defect_segmentations)\\n\\n defect_indexes += [str(index)] * len(normals) # normals, defects, defect_segementations are all of the same length, just use one of them\\n\\n normal_defect_defectSeg = list(zip(normals, defects, defect_segmentations, defect_indexes))\\n dataset = tf.data.Dataset.from_tensor_slices(normal_defect_defectSeg)\\\\\\n .map(path_to_img)\\\\\\n .batch(batch_size)\\\\\\n .shuffle(buffer_size)\\\\\\n .cache()\\n\\n return iter(dataset)\\n\\n\\npython\\ndef view_dataset_sample():\\n normals, defects, segments, indexes, spatial_cat_maps, _, _ = next(get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen())\\n # spatial_cat_map_[:,:,index] = segments\\n for i in range(0, 1):\\n normal = normals[i].numpy().astype(\\"float32\\")\\n defect = defects[i].numpy().astype(\\"float32\\")\\n segment = segments[i].numpy().astype(\\"float32\\")\\n spatial_cat_map = spatial_cat_maps[i]\\n index = int(indexes[i])\\n label = labels[index]\\n\\n print(\\"label\\", label)\\n plt.figure(figsize=(6, 15))\\n\\n plt.subplot(1, 3, 1)\\n plt.imshow(((normal+1)*127.5).astype(\\"uint8\\"))\\n\\n plt.subplot(1, 3, 2)\\n plt.imshow(((defect+1)*127.5).astype(\\"uint8\\"))\\n\\n plt.subplot(1, 3, 3)\\n plt.imshow(spatial_cat_map[:,:,index])\\n\\nview_dataset_sample()\\n\\n\\n#### Train Loop\\n\\n##### Init Loss Util Functions\\n\\npython\\nmae = MeanAbsoluteError()\\ncce = CategoricalCrossentropy()\\n\\n\\npython\\ndef gradient_penality(critic, real_sample, fake_sample):\\n epsilon = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\\n interpolated = epsilon * real_sample + (1 - epsilon) * fake_sample\\n\\n with tf.GradientTape() as gp_tape:\\n gp_tape.watch(interpolated)\\n critic_inter = critic(interpolated, training=True)\\n\\n grads = gp_tape.gradient(critic_inter, [interpolated])[0]\\n norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\\n penality = tf.reduce_mean((norm - 1.0) ** 2)\\n return penality\\n\\n\\n##### Init Models\\n\\npython\\ngen = generator()\\ndisc = discriminator()\\n\\n\\n##### View Intermediate Performance\\n\\npython\\ndef view_generator_sample(image_name=None, show_in_notebook=True):\\n datagen = get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen()\\n dategen_randomSeg = get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen(random_segmentations=True)\\n\\n normals, defects, segments, indexes, spatial_cat_maps, _, _ = next(datagen)\\n normals_, defects_, segments_, indexes_, spatial_cat_maps_, _, _ = next(dategen_randomSeg)\\n # spatial_cat_map_[:,:,index] = segments\\n\\n if image_name is not None:\\n print(\\"\\")\\n print(\\"---------------------------------------------\\")\\n print(\\"---------\\")\\n print(image_name)\\n\\n plt.figure(figsize=(17, 10))\\n\\n for i in range(0, 1):\\n spatial_cat_map_randomSeg = spatial_cat_maps_[i]\\n\\n spatial_cat_map = spatial_cat_maps[i]\\n normal = normals[i].numpy().astype(\\"float32\\")\\n\\n defect = defects[i].numpy().astype(\\"float32\\")\\n\\n for row_count, (from_, spa_cat, description) in enumerate([\\n (defect, spatial_cat_map, \\"from_defect\\"),\\n (normal, spatial_cat_map, \\"from_normal\\"),\\n (normal, spatial_cat_map_randomSeg, \\"from_random_segment\\")\\n ]):\\n segment = segments[i].numpy().astype(\\"float32\\")\\n z = np.random.normal(0, 1, (1, )+noise_dim)\\n\\n top, m = gen.predict([from_[np.newaxis,...], spa_cat[np.newaxis,...], z])\\n top_layer = np.squeeze(top)\\n\\n\\n index = int(indexes[i])\\n label = labels[index]\\n\\n\\n\\n plt.subplot(3, 5, 5*row_count + 1)\\n plt.imshow(((from_+1)*127.5).astype(\\"uint8\\"))\\n\\n plt.subplot(3, 5, 5*row_count + 2)\\n plt.imshow(spa_cat[:,:,index])\\n\\n\\n plt.subplot(3, 5, 5*row_count + 3)\\n\\n plt.imshow(((top_layer+1)*127.5).astype(\\"uint8\\"))\\n\\n plt.subplot(3, 5, 5*row_count + 4)\\n print(\\"max mask value inside mask:\\", np.amax(m))\\n plt.imshow(np.squeeze(m))\\n\\n plt.subplot(3, 5, 5*row_count + 5)\\n gened_defects = from_ * (1-m) + top_layer * m\\n plt.imshow(((gened_defects[0]+1)*127.5).astype(\\"uint8\\"))\\n\\n\\n if image_name is not None:\\n plt.savefig(f\\"{image_name}\\", dpi=80, bbox_inches=\\"tight\\")\\n\\n if show_in_notebook:\\n plt.show()\\n\\nview_generator_sample()\\n\\n\\n##### Train Loop Definition\\n\\npython\\nepoches=8\\nsample_per_batch=15\\nshow_in_notebook=True # whether to show image in the notebook\\n\\ng_opt = Adam(lr=lr, beta_1=0, beta_2=0.9)\\nd_opt = Adam(lr=lr, beta_1=0, beta_2=0.9)\\n\\nfor epoch in range(epoches):\\n batch = 0\\n dataset_gen = get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen()\\n\\n if epoch >= 0 and epoch <= 2:\\n lambda_mask = 5.0\\n elif epoch >= 3 and epoch <= 5:\\n lambda_mask = 0.5\\n else:\\n lambda_mask = 0\\n\\n\\n for normals, defects, _, _, spatial_cat_maps, defect_onehots, normal_onehots in dataset_gen:\\n batch += 1\\n print(\\"epoch\\", epoch+1, \\"batch\\", batch, \\"lambda_mask\\", lambda_mask, end=\\"\\\\r\\")\\n\\n for iteration in range(n_disc_iteration):\\n update_gen = (iteration+1)%n_disc_iteration == 0\\n\\n for from_, to_, from_onehots, target_onehots in [(normals, defects, normal_onehots, defect_onehots), (defects, normals, defect_onehots, normal_onehots)]:\\n z_1 = tf.random.normal((1,)+noise_dim, 0, 1)\\n z_2 = tf.random.normal((1,)+noise_dim, 0, 1)\\n\\n with tf.GradientTape(persistent=True) as tape:\\n # naming convetion is from normal --\x3e defect. Roles of normal and defect can be interchanged\\n defect_overlays, n2d_masks = gen([from_, spatial_cat_maps, z_1], training=True)\\n gened_defects = from_ * (1-n2d_masks) + defect_overlays * n2d_masks\\n\\n restore_overlays, d2n_masks = gen([gened_defects, spatial_cat_maps, z_2], training=True)\\n restoration = gened_defects * (1-d2n_masks) + restore_overlays * d2n_masks\\n\\n d_logit_on_gened, d_cls_on_gened = disc(gened_defects, training=True)\\n d_logit_on_real, d_cls_on_real = disc(from_, training=True)\\n\\n cls_loss_on_gened = cce(target_onehots, d_cls_on_gened)\\n cls_loss_on_real = cce(from_onehots, d_cls_on_real)\\n\\n gp = gradient_penality(disc, from_, gened_defects)\\n\\n d_wgan_gp_loss = tf.math.reduce_mean(d_logit_on_gened) - tf.reduce_mean(d_logit_on_real) + 10 * gp\\n\\n\\n g_cycle_loss = mae(from_, restoration)\\n\\n g_mask_cycle_loss = mae(n2d_masks, d2n_masks)\\n\\n g_mask_vanishing_loss = -tf.math.log(tf.math.reduce_mean(mae(n2d_masks, 0.0) + mae(d2n_masks, 0.0)))\\n g_mask_spatial_constraint_loss = tf.math.reduce_mean(mae(n2d_masks, 0.0) + mae(d2n_masks, 0.0))\\n\\n g_wgan_gp_loss = - tf.math.reduce_mean(d_logit_on_gened)\\n\\n d_loss = tf.tensordot(\\n [1, 5.0 ],\\n [d_wgan_gp_loss, cls_loss_on_real],\\n axes = 1\\n )\\n\\n if update_gen:\\n g_loss = tf.tensordot(\\n [10.0, 5.0, lambda_mask, 5.0, 10.0, 1.0 ],\\n [g_cycle_loss, g_mask_cycle_loss, g_mask_vanishing_loss, g_mask_spatial_constraint_loss, cls_loss_on_gened, g_wgan_gp_loss],\\n axes = 1\\n )\\n\\n grad_of_d = tape.gradient(d_loss, disc.trainable_variables)\\n d_opt.apply_gradients(zip(grad_of_d, disc.trainable_variables))\\n\\n if update_gen:\\n grad_of_g = tape.gradient(g_loss, gen.trainable_variables)\\n g_opt.apply_gradients(zip(grad_of_g, gen.trainable_variables))\\n\\n\\n if (batch % sample_per_batch) == 0:\\n view_generator_sample(\\n image_name=\\"epoch-{}-batch-{}-224-DCT-ver\\".format(\\n str(epoch+1).zfill(3),\\n str(batch).zfill(5)\\n ),\\n show_in_notebook=show_in_notebook\\n )\\n\\n","title":"DefectGAN","date":"2022-05-09T00:00:00.000Z","id":"blog072","tag":"deep-learning, tensorflow","toc":true,"intro":"Some implementation on DefectGAN."},{"content":"\\n#### tensorflow-gpu, cudatoolkit and cuDNN\\n\\nFirst scroll down the to bottom of <a href=\\"https://www.tensorflow.org/install/source_windows?hl=zh-tw\\">this page</a> to get a list of compatible version of tensorflow-gpu, cuDNN and CUDA.\\n\\nFor me I have been using tensorflow-gpu 2.4.0, cuDNN 8.x and cuda 11.x. We can:\\n\\n- Install tensorflow-gpu 2.4.0 by pip install tensorflow-gpu==2.4.0.\\n- Install cudatoolkit 11.0 by conda install cudatoolkit==11.0.\\n- Download cuDNN 8.4.0 from <a href=\\"https://developer.nvidia.com/rdp/cudnn-download#a-collapse805-110\\">here</a>. Copy the files from\\n - downloaded\\\\cuDNN8.4.0\\\\{bin, include, lib}\\n into\\n - C:\\\\Users\\\\<name>\\\\anaconda3\\\\envs\\\\<env name>\\\\Library\\\\{bin, include, lib}\\n respectively.\\n\\n#### Possible error: zlibwapi.dll\\n\\nWhen running script with tensorflow, the program may raise the following error:\\n\\ntext\\nCould not locate zlibwapi.dll\\n\\n\\nTo solve it we following the instruction from sectoin **_3.1.3. Installing zlib_** from <a href=\\"https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-zlib-windows\\">here</a>.\\n\\n#### Indication of Running Tensorflow with GPU Successfully\\n\\nThe following messages should be logged in our console when we are running a script with tensorflow imported.\\n\\ntext\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\\nI tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll\\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\\npciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6\\ncoreClock: 1.725GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\\nI tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2\\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:\\npciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6\\ncoreClock: 1.725GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] 0\\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0: N\\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21821 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)\\nI tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll\\nI tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\\n\\nI tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0\\n\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll\\nI tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll\\nI tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\\n\\n","title":"Install tensorflow-gpu","date":"2022-05-15T00:00:00.000Z","id":"blog073","tag":"deep-learning","intro":"Record standard step to set up tensorflow-gpu, cuda and cudnn."},{"content":"\\n#### Iterables\\n\\n##### Types of Iterable Objects\\n\\nThere are several common objects that are iterable in shell scripts:\\n\\n- String separated by \\\\n. For example, consider the string iterable:\\n\\n python\\n iterable=$\'file1\\\\nfile2\'\\n\\n # or\\n\\n iterable=\\"\\n file1\\n file2\\n \\"\\n\\n for char in $iterable; do\\n echo $char\\n done\\n \\n\\n outputs\\n\\n none\\n file1\\n file2\\n \\n\\n- <a href=\\"http://mywiki.wooledge.org/glob\\">glob (click to see detail)</a> :\\n\\n python\\n for image in some/folder/*; do\\n echo \\"$image\\"\\n done\\n \\n\\n Due to character-encoding problem, it is never a good idea to loop the array generated by files=$(ls some/dir), as always we should use glob.\\n\\n- Array object like files=(\\"file1\\" \\"file2\\"). Common source of array can be built using glob, for example, arr=($some/folder/*.jpg).\\n\\n There is special syntax to loop array elements:\\n\\n python\\n for file in ${files[@]}; do\\n echo $file\\n done\\n \\n\\n##### Example of Iterables\\n\\nSome real example I created in my work:\\n\\npython\\nEXPER_IN=some/folder/INPUT\\nEXPER_OUT=some/folder/OUTPUT\\n# stock of images\\nSTOCK=some/folder/stock\\nRESULT=some/folder/result\\n\\nCONFIG_PATH=some/folder/config.json\\n\\nimages_arr=($STOCK/*.jpg)\\n\\n# can choose either one of the below:\\n# for image in $images_arr; do\\n# for image in $STOCK/*; do\\nfor image in $STOCK/*; do\\n image_name=$(basename -- $image)\\n image_name_with_jpg=$(echo $image_name | sed \\"s/.jpg//g\\")\\n cp $STOCK/$image_name $EXPER_IN\\n\\n python dsds_main.py debug-defect --config $CONFIG_PATH --tag dev\\n\\n target_dir=$RESULT/$image_name_with_jpg\\n mkdir $target_dir\\n mv $EXPER_OUT/* $target_dir\\n\\n rm $EXPER_IN/$image_name\\ndone\\n\\n\\n**Remarks.**\\n\\n- We use basename -- \\"a/b/c/d.jpg to get d.jpg.\\n- There is no mv -r command, we just have mv.\\n\\n#### If Statements and Booleans\\n\\n##### No Booleans in Bash\\n\\nFrom <a href=\\"https://devmanual.gentoo.org/tools-reference/bash/index.html#string-comparison-in-bash\\">this post</a>:\\n\\n- There are no Booleans in Bash\\n- Always compare against strings or numbers\\n Therefore asking how to construct _boolean_ in bash is the same thing as asking how to write _comparison_ in bash.\\n\\nWe usally wrap our comparison inside [[ comparison ]].\\n\\n##### List of All Comparisons\\n\\nCopied from <a href=\\"https://devmanual.gentoo.org/tools-reference/bash/index.html\\">here</a>.\\n\\n<center></center>\\n\\n> The [[ ]] form is generally safer than [ ] and should be used in all new code.\\n\\n| <div style=\\"width:500px\\">Operator</div> | Purpose |\\n| --------------------------------------- | -------------------------------------------------------------------------------------------------- | ------------------- |\\n| <img width=170/> | <img style=\\"width:calc(100%)\\"/> |\\n| == | String equality |\\n| != | String inequality |\\n| < | String lexiographic | comparison (before) |\\n| > | String lexiographic comparison (after) |\\n| =~ | String regular expression match (bash 3 only, not currently allowed in ebuilds) |\\n| -z \\"string\\" | String has zero length |\\n| -n \\"string\\" | String has non-zero length |\\n| -eq | Integer equality |\\n| -ne | Integer inequality |\\n| -lt | Integer less than |\\n| -le | Integer less than or equal to |\\n| -gt | Integer greater than |\\n| -ge | Integer greater than or equal to |\\n| -a file | Exists (use -e instead) |\\n| -b file | Exists and is a block special file |\\n| -c file | Exists and is a character special file |\\n| -d file | Exists and is a directory |\\n| -e file | Exists |\\n| -f file | Exists and is a regular file |\\n| -g file | Exists and is set-group-id |\\n| -h file | Exists and is a symbolic link |\\n| -k file | Exists and its sticky bit is set |\\n| -p file | Exists and is a named pipe (FIFO) |\\n| -r file | Exists and is readable |\\n| -s file | Exists and has a size greater than zero |\\n| -t fd | Descriptor fd is open and refers to a terminal |\\n| -u file | Exists and its set-user-id bit is set |\\n| -w file | Exists and is writable |\\n| -x file | Exists and is executable |\\n| -O file | Exists and is owned by the effective user id |\\n| -G file | Exists and is owned by the effective group id |\\n| -L file | Exists and is a symbolic link |\\n| -S file | Exists and is a socket |\\n| -N file | Exists and has been modified since it was last read |\\n| file1 -nt file2 | file1 is newer (according to modification date) than file2, or if file1 exists and file2 does not. |\\n| file1 -ot file2 | file1 is older than file2, or if file2 exists and file1 does not. |\\n| file1 -ef file2 | file1 and file2 refer to the same device and inode numbers. |\\n| first \\\\|\\\\| second | first or second (short circuit) |\\n| first && second | first and second (short circuit) |\\n| ! condition | not condition |\\n\\n##### Example of If Statement\\n\\npython\\nfor image in echo $STOCK/*; do\\n if [[ \\"$image\\" =~ \\\\.jpg || \\"$image\\" =~ \\\\.png ]]; then\\n echo $image\\n fi\\ndone\\n\\n\\n#### Point to Values in an Array\\n\\nTake images_arr=($STOCK/*.jpg) as in the example of iterables above, we can get specific element by:\\n\\n- ${images_arr[k]} the $(k+1)$-th element\\n- ${images_arr[-1]} the last element\\n- ${images_arr[@]:0:k} the first $k$ elements\\n\\n#### Read First few/Last few Lines or Skip Lines in std-out\\n\\nSuppose I am going read a file by cat main.py, then:\\n\\n- Read the first 3 lines:\\n python\\n cat main.py | head -3\\n \\n- Read the last 3 lines:\\n python\\n cat main.py | tail -3\\n \\n- Skip 3 lines from the top:\\n python\\n cat main.py | tail -n +3\\n \\n\\n#### Try-Catch in Bash\\n\\nWe mix iterable and try-catch in this example. The principles are\\n\\n- command_1 && command_2 $\\\\implies$ if command_1 succeeds, run command_2.\\n- command_1 || command_2 $\\\\implies$ if command_1 fails, run command_2.\\n\\npython\\ntrip01=some/other/folder/w0m8fnyb\\ntrip02=some/other/folder/i8y6v_yn\\ntrip03=some/other/folder/cght2jy4\\n\\nIMAGES=\\"\\n $trip02/P202203252_732_4395.jpg\\n $trip02/P202203252_743_4456.jpg\\n $trip03/P202203253_299_1789.jpg\\n\\"\\nTARGET_DIR=\\"some/folder/stock\\"\\nLOG=$(date +%Y-%m-%d--%Hh%Mm%Ss).log\\n# output 2022-05-20--10h24m52s.log:\\n\\nfor img in $IMAGES; do\\n {\\n {\\n # try\\n cp $img $TARGET_DIR\\n } && {\\n echo \\"copied from $img to $TARGET_DIR\\"\\n }\\n } || {\\n # catch, save the log etc\\n echo \\"fails to copy\\" > $LOG\\n }\\ndone\\n\\n\\n**Remarks.** The || will not surpress the exception handling of bash itself, it will keep reporting the root cause. The command after || helps us do extra work when problem occurs.\\n\\n#### Reference\\n\\n- <a href=\\"https://www.gnu.org/software/bash/manual/html_node/\\">Bash Reference Manual</a>\\n","title":"Shell Script Fundamentals","date":"2022-05-20T00:00:00.000Z","id":"blog075","tag":"coding","intro":"Record the basic knowledge we need to know in shell script."},{"content":"\\n#### Conv2DSamePadding: The good day in Tensorflow with padding=\\"same\\"\\n\\nThe output shape of\\n\\npython\\ntf.keras.layers.Conv2D(channels, kernel_size, s, padding=\\"same\\")\\n\\n\\nin tensorflow is more predictable because it always takes an input of shape (N, H, W, C) into an output of shape (N, H//s, W//s, channels). Unfortunately in pytorch padding=\\"same\\" will fail if $\\\\texttt{s} > 1$, but we usually need such an option to downsize the spatial dimenion by a constant multiple (usually that multiple is stride=2).\\n\\nTherefore we create an equivalent version in pytorch:\\n\\npython\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport math\\n\\nclass Conv2dSamePadding(nn.Conv2d):\\n \\"\\"\\"\\n This conv layer is used if we want to make the shape\\n of convolution more predictable.\\n \\"\\"\\"\\n\\n def __init__(self, *args, **kwargs):\\n super(Conv2dSamePadding, self).__init__(*args, **kwargs)\\n\\n def get_padding_for_same(self, kernel_size, stride, padding, input: torch.Tensor):\\n if isinstance(padding, int):\\n input = F.pad(input, (padding, padding, padding, padding))\\n if isinstance(kernel_size, int):\\n kernel_size = (kernel_size, kernel_size)\\n if isinstance(stride, int):\\n stride = (stride, stride)\\n _, _, H, W = input.shape\\n s_H = stride[0]\\n s_W = stride[1]\\n k_H = kernel_size[0]\\n k_W = kernel_size[1]\\n h2 = math.floor(H / s_H)\\n w2 = math.floor(W / s_W)\\n pad_W = (w2 - 1) * s_W + (k_W - 1) + 1 - W\\n pad_H = (h2 - 1) * s_H + (k_H - 1) + 1 - H\\n padding = (pad_W // 2, pad_W - pad_W // 2, pad_H // 2, pad_H - pad_H // 2)\\n return padding\\n\\n def forward(self, input):\\n padding = self.get_padding_for_same(self.kernel_size, self.stride, self.padding, input)\\n return self._conv_forward(F.pad(input, padding), self.weight, self.bias)\\n\\n\\nChecking:\\n\\npython\\nresult = Conv2dSamePadding(3, 6, 100, 3)(torch.randn(10, 3, 300, 300))\\nprint(result.shape)\\nresult = Conv2dSamePadding(3, 6, 100, 3)(torch.randn(10, 3, 299, 299))\\nprint(result.shape)\\nresult = Conv2dSamePadding(3, 6, 100, 3)(torch.randn(10, 3, 298, 298))\\nprint(result.shape)\\nresult = Conv2dSamePadding(3, 6, 100, 3)(torch.randn(10, 3, 297, 297))\\nprint(result.shape)\\n\\n\\nResult:\\n\\nnone\\ntorch.Size([10, 6, 100, 100])\\ntorch.Size([10, 6, 99, 99])\\ntorch.Size([10, 6, 99, 99])\\ntorch.Size([10, 6, 99, 99])\\n\\n\\n#### Spectral Normalization to Every Convolution Layer in a Module\\n\\npython\\ndef _add_sn(m):\\n if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\\n return spectral_norm(m)\\n else:\\n return m\\n\\ndef add_sn_(model: nn.Module):\\n model.apply(_add_sn)\\n\\n\\n#### Weights Initialization to Every Convolution Layer in a Module\\n\\npython\\ndef initialize_weights(model):\\n for m in model.modules():\\n if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\\n nn.init.xavier_normal_(m.weight.data)\\n\\n\\n#### Device Which Auto-Detect Cuda\\n\\npython\\nimport torch\\ndevice = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\")\\n\\n\\n#### Save and Load Model\\n\\npython\\ntorch.save(gen, \\"gen.pt\\")\\ntorch.load(\\"gen.pt\\")\\n\\n\\n#### Dataloader\\n\\nIn pytorch:\\n\\n- We use Dataset to construct the logic to generate input and ground truth in training. For this, we subclass from Dataset and implement:\\n - __getitem__(self, index) given an index i, give me a data dataset[i].\\n - __len__ give me total length of the dataset.\\n- We then use DataLoader class to wrap Dataset object to construct a dataset pipeline.\\n\\nNow we can test the outout of our dataset by dataset[0].\\n\\n##### Example of Dataset Object\\n\\npython\\nimport os\\nimport numpy as np\\nimport torch\\n\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchvision.io import read_image, image\\nfrom torchvision import transforms as T\\nfrom device import device\\nfrom config import ModelAndTrainingConfig as config\\n\\nclass DefectDataset(Dataset):\\n def __init__(self):\\n super(DefectDataset, self).__init__()\\n self.defect_filepath_arr = None\\n self.load_fect_filepath()\\n\\n def load_fect_filepath(self):\\n if self.defect_filepath_arr is None:\\n defect_data = []\\n dataset_dir = config.dataset_dir\\n\\n for defect in config.defect_labels:\\n normal_dir = f\\"{dataset_dir}/{defect}/normal\\"\\n defect_dir = f\\"{dataset_dir}/{defect}/defect\\"\\n mask_dir = f\\"{dataset_dir}/{defect}/defect_segmentation\\"\\n defect_index = config.labels.index(defect)\\n\\n for basename in os.listdir(normal_dir):\\n filename = basename.replace(\\".jpg\\", \\"\\")\\n normal_path = f\\"{normal_dir}/{filename}.jpg\\"\\n defect_path = f\\"{defect_dir}/{filename}.jpg\\"\\n defect_mask_path = f\\"{mask_dir}/{filename}.png\\"\\n defect_data.append([defect_index, normal_path, defect_path, defect_mask_path])\\n\\n self.defect_filepath_arr = defect_data\\n\\n def load_cls_index_and_imgs_from_index(self, index):\\n cls_index, normal_path, defect_path, defect_seg_path = self.defect_filepath_arr[index]\\n resize = T.Resize(config.image_shape[1:3])\\n np_normal = resize(read_image(normal_path)) / 127.5 - 1\\n np_defect = resize(read_image(defect_path)) / 127.5 - 1\\n np_defect_mask = resize(read_image(defect_seg_path, mode=image.ImageReadMode.GRAY)) / 255\\n np_defect_mask = torch.where(np_defect_mask > 0.5, 1, 0)\\n return cls_index, np_normal, np_defect, np_defect_mask\\n\\n def load_spatial_charactergorical_map_from_index(self, index):\\n cls_index, _, _, defect_mask = self.load_cls_index_and_imgs_from_index(index)\\n spartial_dim = tuple(config.image_shape[1:3])\\n spatial_cat_map = np.zeros((len(config.labels),) + spartial_dim)\\n spatial_cat_map[cls_index] = defect_mask[0]\\n return spatial_cat_map\\n\\n def get_num_of_batches(self):\\n return (len(self.defect_filepath_arr) // config.batch_size) + (0 if config.dataset_drop_last else 1)\\n\\n def __getitem__(self, index):\\n defect_cls_index, np_normal, np_defect, np_defect_seg = \\\\\\n self.load_cls_index_and_imgs_from_index(index)\\n\\n spatial_cat_map = self.load_spatial_charactergorical_map_from_index(index)\\n\\n return (\\n torch.as_tensor(defect_cls_index).type(torch.LongTensor).to(device),\\n torch.as_tensor(np_normal, dtype=torch.float, device=device),\\n torch.as_tensor(np_defect, dtype=torch.float, device=device),\\n torch.as_tensor(np_defect_seg, dtype=torch.float, device=device),\\n torch.as_tensor(spatial_cat_map, dtype=torch.float, device=device)\\n )\\n\\n def __len__(self):\\n return len(self.defect_filepath_arr)\\n\\n\\n##### Example of DataLoader Object\\n\\npython\\ndefect_dataset = DefectDataset()\\ndefectDataloader = DataLoader(dataset=defect_dataset,\\n batch_size=config.batch_size,\\n shuffle=True,\\n # prefetch_factor=config.batch_size,\\n drop_last=config.dataset_drop_last,\\n num_workers=config.dataset_num_workers)\\n\\n\\n##### On num_workers and prefetch_factor\\n\\nNote that from https://github.com/fastai/fastbook/issues/85\\n\\n> We always need to set **_num_workers=0_** when creating a DataLoader\\n> because Pytorch multiprocessing does not work on Windows.\\n\\n<center></center>\\n\\nMoreover, prefetch_factor can be positive only when num_workers is positive. Therefore in windows, both prefetch_factor and num_workers must be 0.\\n","title":"Pytorch Fundamentals","date":"2022-05-23T00:00:00.000Z","id":"blog076","tag":"deep-learning, pytorch","intro":"Record useful tools and commands in pytorch which I learn from translating DefectGAN model in tensorflow into pytorch."},{"content":"\\n#### What we can Archive:\\n\\n<center>\\n<img src=\\"/assets/tech/054.gif\\"/>\\n</center>\\n\\n#### Update Log Without new line in Console\\n\\nIn the image above, only the numerical values are changing, no new line will be further created. For this, we need to know <a href=\\"https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797\\"><b><i>ANSI Escape Sequences</i></b></a>.\\n\\nWe usually use the following:\\n|code|purpose|\\n|---|---|\\n|<img width=100/>|<img style=\\"width:calc(100%)\\"/>|\\n|ESC[#A | moves cursor ups|\\n| ESC[#B| moves cursor down |\\n| ESC[#C| moves cursor right |\\n| ESC[#D| moves cursor left |\\n| ESC[#E| moves cursor to beginning of next line, # lines down |\\n| ESC[#F| moves cursor to beginning of previous line, # lines up |\\n| ESC[0K | clears line|\\n\\n<p></p>\\n\\n<center></center>\\n\\nIn python the **escape code** is prefixed by Hexadecimal: \\\\x1B (in nodejs we use \\\\u001b).\\n\\n#### Procesure to Create Continuous Log\\n\\nSuppose that we want to print 8 lines of log continuosly, then:\\n\\n- **Step 1.** Printed 8 lines by print(\\"\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\").\\n- **Step 2.** Go up by 8 lines, for this, we need:\\n python\\n print(\\"\\\\x1B[8A\\")\\n \\n- **Step 3.** Now we can print our own 8 lines of log.\\n\\n We need to append ESC[0K at the end of each string to print to make sure no extra character survives.\\n\\n Why? We can see what happends when print(\\"1 2 3 4\\", end=\\"\\\\r\\") and print(\\"5 6\\", end=\\"\\\\r\\"), since the first line is longer, the result becomes 5 6 3 4 instead of 5 6.\\n\\n- **Step 4.** Upon completion of logging, we move cursor up by 8 rows again (step 2).\\n\\n#### Example of Continuous Logger\\n\\nWe summarize this workflow in print method below:\\n\\npython\\nfrom pydash.objects import get, set_\\n\\nclass ConsoleLog():\\n def __init__(self, lines_up_on_end=0):\\n self.CLR = \\"\\\\x1B[0K\\"\\n self.lines_up_on_batch_end = lines_up_on_end\\n self.record = {}\\n\\n def UP(self, lines):\\n return \\"\\\\x1B[\\" + str(lines + 1) + \\"A\\"\\n\\n def DOWN(self, lines):\\n return \\"\\\\x1B[\\" + str(lines) + \\"B\\"\\n\\n def on_print_end(self):\\n print(self.UP(self.lines_of_log))\\n print(self.UP(self.lines_up_on_batch_end))\\n\\n def print(self, key_values):\\n lines_of_log = len(key_values)\\n self.lines_of_log = lines_of_log\\n\\n\\n # for the first time,\\n # print self.lines_of_log number of lines to occupy the space\\n print(\\"\\".join([\\"\\\\n\\"] * (self.lines_of_log)))\\n print(self.UP(self.lines_of_log))\\n\\n for key, value in key_values:\\n if key == \\"\\" and value == \\"\\":\\n print()\\n else:\\n if key != \\"\\" and value != \\"\\":\\n prev_value = get(self.record, key, 0.)\\n curr_value = value\\n diff = curr_value - prev_value\\n sign = \\"+\\" if diff >= 0 else \\"\\"\\n print(\\"{0: <35} {1: <30}\\".format(key, value) + sign + \\"{:.5f}\\".format(diff) + self.CLR)\\n set_(self.record, key, value)\\n\\n self.on_print_end()\\n\\n def clear_log_on_epoch_end(self):\\n # usually before calling this line, print() has been run, therefore we are at the top of the log.\\n for _ in range(self.lines_of_log):\\n # clear lines\\n print(self.CLR)\\n # ready for next epoch\\n print(self.UP(self.lines_of_log))\\n\\n\\nNow we can call it at the end of the for loop:\\n\\npython\\nconsole_log = ConsoleLog(lines_up_on_end=1)\\n\\nfor ... in tqdm(data_gen,\\n total=n_batches,\\n desc=\\"Epoch {}\\".format(epoch),\\n bar_format=config.bar_format):\\n\\n console_log.print(\\n [\\n (\\"\\", \\"\\"),\\n (\\"d_loss\\", d_loss.item()),\\n (\\"- d_wgan_gp_loss\\", d_wgan_gp_loss.item()),\\n (\\"- d_cls_loss_on_real\\", cls_loss_on_real.item()),\\n (\\"\\", \\"\\"),\\n (\\"g_loss\\", g_loss.item()),\\n (\\"- g_spatial_cat_similarity_loss\\", g_spatial_cat_similarity_loss.item()),\\n (\\"- g_cycle_loss\\", g_cycle_loss.item()),\\n (\\"- g_mask_cycle_loss\\", g_mask_cycle_loss.item()),\\n (\\"- g_mask_vanishing_loss\\", g_mask_vanishing_loss.item()),\\n (\\"- g_mask_spatial_constraint_loss\\", g_mask_spatial_constraint_loss.item()),\\n (\\"- g_cls_loss_on_gened\\", g_cls_loss_on_gened.item()),\\n (\\"- g_wgan_gp_loss\\", g_wgan_gp_loss.item())\\n ])\\n# depends on whether we have next for loop:\\n# console_log.clear_log_on_epoch_end()\\n\\n\\nHere lines_up_on_end is provided because tqjm also needs to upate on its line, so we will need lines_up_on_end to be 1, normally it is 0.\\n","title":"Logging Without Printing New Lines","date":"2022-05-23T00:00:00.000Z","id":"blog077","tag":"python","intro":"Create a console log that freeze the position but keep updating the numerics in training."},{"content":"\\n#### Syntax\\n\\nIn training we can create a generator by data_gen = iter(dataloader). By using tqdm we can create a progress bar to visuallize the progress of the current Epoch:\\n\\npython\\nfor ... in tqdm(data_gen,\\n total=n_batches,\\n desc=\\"Epoch {}\\".format(epoch),\\n bar_format=config.bar_format):\\n ...\\n\\n\\n- total Since generator cannot provide the total legnth, we must provide total as kwarg.\\n- Note that total depends on drop_last in DataLoader (in pytorch).\\n- desc is easy to understand.\\n\\n#### bar_format\\n\\nThe bar_format we use can be formatted by:\\n\\ntext\\n\\"{desc}: {percentage:.1f}%|{bar:15}| {n}/{total_fmt} [{elapsed}, {rate_fmt}{postfix}]\\"\\n\\n\\nThis will show the information as shown in the picture:\\n\\n<center>\\n<a href=\\"/assets/tech/053.png\\">\\n<img width=\\"600\\" src=\\"/assets/tech/053.png\\"/>\\n</a>\\n</center>\\n<p></p>\\n","title":"tqdm: The Progress bar for Iterations","date":"2022-05-27T00:00:00.000Z","id":"blog079","tag":"python","intro":"Record the use of a progress bar when using for loop, and record how to customize the output."},{"content":"\\n#### Masks\\n\\nSuppose that\\n\\n- target_masks is of shape (N, width, height, 10, 2),\\n- where N denotes the number of batches,\\n- width, height denote the size of a feature,\\n- and for every given n, j, i, k, target_masks[n, j, i, k] is a one-hot vector.\\n Then we can create a mask by\\n\\npython\\npositive_sample = target_masks[..., 1] == 1\\n\\n\\nThis will create a mask of shape (N, width, height, 10) in which all values are boolean, but how do we know all the indexes n, j, i, k that the mask indicates as True?\\n\\nSometimes the mask itself is enough to filter out other related np.array\'s (by applying the mask like other_nparray[positive_sample]), but sometimes the spartial index j, i also provides us the information we need.\\n\\nFor that we will use the following function:\\n\\n#### get_indexes_from_mask(mask)\\n\\nThis function is as simple as:\\n\\npython\\ndef get_indexes_from_mask(mask):\\n return np.array(list(zip(*np.where(mask))))\\n\\n\\nFor example, suppose that each position j, i was assigned a box in boxes with shape (N, height, width, 4) and a score in scores with shape (N, height, width, 1), then we can create a mask by\\n\\npython\\nscore_mask = scores[..., 0] > 0.8\\n\\n\\nNow apart from getting the desired scores by\\n\\n- selected_scores=scores[score_mask] (this will be **_flattened_** and of shape (n\', 1)),\\n we can equivalently:\\n- python\\n selected_indexes = get_indexes_from_mask(score_mask)\\n selected_scores = np.array([scores[tuple(index)]\\n for index in selected_indexes])\\n \\n Note that\\n - numpy array does not accept int-np-array as an index (unlike bool-np-array),\\n - We need to convert int-np-array into int-tuple. The conversion is simply by tuple(np-array).\\n\\n#### tf.where, tf.gather (to be updated)\\n\\nEquivalently we can use tf.where and tf.gather.\\n","title":"Masks in Numpy and Retrive Corresponding Indexes","date":"2022-05-28T00:00:00.000Z","id":"blog080","tag":"python","intro":"Mask in numpy, tensorflow and pytorch is very useful to filter out desired values, but sometimes the index that the mask marks as True is also import, we record a function to retrive those information."},{"content":"\\n#### CV2\\n\\n- Read image and correct channels\\n python\\n img = cv2.imread(img_path)\\n img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\\n img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\\n \\n- Draw rectangle\\n python\\n cv2.rectangle(img, (x1, y1), (x2, y2), config.bbox_color, config.bbox_stroke)\\n \\n- Write image into disk:\\n python\\n cv2.imwrite(img_path, img)\\n \\n- Fill a polygon by coordinates:\\n python\\n # [[x1,y1], [x2,y2], ... ]\\n points = np.array( [[[10,10],[100,10],[100,100],[10,100]]], dtype=np.int32 )\\n img = np.zeros([240,320],dtype=np.uint8)\\n cv2.fillPoly(img, pts=points, color=(0, 255, 0))\\n \\n\\n#### Pillow\\n\\n- Read image\\n python\\n from PIL import Image\\n img = Image.open(img_path)\\n \\n- Resize image\\n python\\n img = img.resize((new_width, new_height), Image.BILINEAR)\\n \\n- Rotate image\\n python\\n img = img.rotate(angle, Image.BILINEAR) # in degree\\n \\n- Get image height and width:\\n python\\n h, w = img.height, img.width\\n \\n- Crop image:\\n python\\n box = (start_w, start_h, start_w + length, start_h + length)\\n region = img.crop(box)\\n \\n- Draw bounding box:\\n\\n python\\n from PIL import ImageDraw\\n\\n draw = ImageDraw.Draw(img) # img: PIL.Image.Image\\n for box in boxes:\\n xmin, ymin, xmax, ymax = box\\n draw.rectangle(((xmin, ymin), (xmax, ymax)), outline=(255,255,255,150), width=1)\\n \\n\\n#### shapely.geometry.Polygon\\n\\nTo check intersection of two polygonal regions, we can:\\n\\npython\\nfrom shapely.geometry import Polygon\\npoly1 = np.array([[x1, y1, x2, y2, x3, y3, x4, y4]])\\npoly2 = np.array([[x1_, y1_, x2_, y2_, x3_, y3_, x4_, y4_]])\\n\\np1 = Polygon(poly1).convex_hull\\np2 = Polygon(poly2).convex_hull\\n\\ninter_area = p1.intersection(p2).area\\niou = inter_area / (p1.area + p2.area - inter_area)\\n\\n","title":"CV2, Pillow and shapely for Polygons","date":"2022-05-28T00:00:00.000Z","id":"blog081","tag":"python","intro":"Record usual api for drawing apis in cv2, pillow and shapely.geometry."},{"content":"\\n#### Results\\n\\n<center>\\n<a href=\\"/assets/tech/054.jpg\\" target=\\"_blank\\">\\n<img width=\\"600\\" src=\\"/assets/tech/054.jpg\\"/>\\n</a>\\n</center>\\n\\n#### dataset.py\\n\\n##### Import\\n\\npython\\nfrom shapely.geometry import Polygon\\nfrom abc import ABC, abstractmethod\\nfrom torch.utils.data import Dataset\\nfrom PIL import Image\\nfrom torchvision.transforms import transforms\\nimport torch\\nimport os\\nimport numpy as np\\nimport math\\nimport cv2\\n\\n\\n##### CustomDataset\\n\\npython\\nclass CustomDataset(Dataset):\\n def __init__(self, img_path, gt_path, scale=0.25, length=512):\\n super(CustomDataset, self).__init__()\\n self.img_files = []\\n for img_file in sorted(os.listdir(img_path)):\\n if img_file.endswith(\\".jpg\\") or img_file.endswith(\\".png\\"):\\n self.img_files.append(os.path.join(img_path, img_file))\\n\\n self.gt_files = []\\n for gt_file in sorted(os.listdir(gt_path)):\\n if gt_file.endswith(\\".txt\\"):\\n self.gt_files.append(os.path.join(gt_path, gt_file))\\n\\n self.scale = scale\\n self.length = length\\n\\n def __getitem__(self, index):\\n with open(self.gt_files[index], \'r\', encoding=\\"utf-8\\") as f:\\n lines = f.readlines()\\n vertices, labels = extract_vertices(lines)\\n\\n img = Image.open(self.img_files[index])\\n img, vertices = adjust_height(img, vertices)\\n img, vertices = rotate_img(img, vertices)\\n img, vertices = crop_img(img, vertices, labels, self.length)\\n transform = transforms.Compose([transforms.ColorJitter(0.5, 0.5, 0.5, 0.25),\\n transforms.ToTensor(),\\n transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\\n\\n score_map, geo_map, ignored_map = get_score_geo(img, vertices, labels, self.scale, self.length)\\n return transform(img), score_map, geo_map, ignored_map\\n\\n def __len__(self):\\n return len(self.img_files)\\n\\n\\n##### extract_vertices\\n\\npython\\ndef extract_vertices(lines):\\n \'\'\'extract vertices info from txt lines\\n Input:\\n lines : list of string info\\n Output:\\n vertices: vertices of text regions <numpy.ndarray, (n,8)>\\n labels : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\\n \'\'\'\\n labels = []\\n vertices = []\\n for line in lines:\\n vertices.append(list(map(int, line.rstrip(\'\\\\n\').lstrip(\'\\\\ufeff\').split(\',\')[:8])))\\n label = 0 if \'###\' in line else 1\\n labels.append(label)\\n return np.array(vertices), np.array(labels)\\n\\n\\n##### adjust_height\\n\\npython\\ndef adjust_height(img, vertices, ratio=0.2):\\n \'\'\'adjust height of image to aug data\\n Input:\\n img : PIL Image\\n vertices : vertices of text regions <numpy.ndarray, (n,8)>\\n ratio : height changes in [0.8, 1.2]\\n Output:\\n img : adjusted PIL Image\\n new_vertices: adjusted vertices\\n \'\'\'\\n ratio_h = 1 + ratio * (np.random.rand() * 2 - 1)\\n old_h = img.height\\n new_h = int(np.around(old_h * ratio_h))\\n img = img.resize((img.width, new_h), Image.BILINEAR) # PIL api (caution, widthxheight)\\n\\n new_vertices = vertices.copy()\\n if vertices.size > 0:\\n new_vertices[:, [1, 3, 5, 7]] = vertices[:, [1, 3, 5, 7]] * (new_h / old_h)\\n return img, new_vertices\\n\\n\\n##### rotate_img\\n\\npython\\ndef rotate_img(img, vertices, angle_range=10):\\n \'\'\'rotate image [-10, 10] degree to aug data\\n Input:\\n img : PIL Image\\n vertices : vertices of text regions <numpy.ndarray, (n,8)>\\n angle_range : rotate range\\n Output:\\n img : rotated PIL Image\\n new_vertices: rotated vertices\\n \'\'\'\\n center_x = (img.width - 1) / 2\\n center_y = (img.height - 1) / 2\\n angle = angle_range * (np.random.rand() * 2 - 1) # from -10 to 10\\n img = img.rotate(angle, Image.BILINEAR) # PIL api\\n new_vertices = np.zeros(vertices.shape)\\n for i, vertice in enumerate(vertices):\\n new_vertices[i, :] = rotate_vertices(vertice, -angle / 180 * math.pi, np.array([[center_x], [center_y]]))\\n return img, new_vertices\\n\\n\\n##### rotate_vertices\\n\\npython\\ndef rotate_vertices(vertices, theta, anchor=None):\\n \'\'\'rotate vertices around anchor\\n Input:\\n vertices: vertices of text region <numpy.ndarray, (8,)>\\n theta : angle in radian measure\\n anchor : fixed position during rotation\\n Output:\\n rotated vertices <numpy.ndarray, (8,)>\\n \'\'\'\\n v = vertices.reshape((4, 2)).T\\n if anchor is None:\\n anchor = v[:, :1]\\n rotate_mat = get_rotate_mat(theta)\\n res = np.dot(rotate_mat, v - anchor)\\n return (res + anchor).T.reshape(-1)\\n\\n\\n##### get_rotate_mat\\n\\npython\\ndef get_rotate_mat(theta):\\n \'\'\'positive theta value means rotate clockwise\'\'\'\\n return np.array([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]])\\n\\n\\n##### crop_img\\n\\npython\\ndef crop_img(img, vertices, labels, length):\\n \'\'\'crop img patches to obtain batch and augment\\n Input:\\n img : PIL Image\\n vertices : vertices of text regions <numpy.ndarray, (n,8)>\\n labels : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\\n length : length of cropped image region\\n Output:\\n region : cropped image region\\n new_vertices: new vertices in cropped region\\n \'\'\'\\n h, w = img.height, img.width\\n # confirm the shortest side of image >= length\\n if h >= w and w < length:\\n img = img.resize((length, int(h * length / w)), Image.BILINEAR)\\n elif h < w and h < length:\\n img = img.resize((int(w * length / h), length), Image.BILINEAR)\\n ratio_w = img.width / w\\n ratio_h = img.height / h\\n assert(ratio_w >= 1 and ratio_h >= 1)\\n\\n new_vertices = np.zeros(vertices.shape)\\n if vertices.size > 0:\\n new_vertices[:, [0, 2, 4, 6]] = vertices[:, [0, 2, 4, 6]] * ratio_w\\n new_vertices[:, [1, 3, 5, 7]] = vertices[:, [1, 3, 5, 7]] * ratio_h\\n\\n # find random position\\n remain_h = img.height - length\\n remain_w = img.width - length\\n flag = True\\n cnt = 0\\n while flag and cnt < 1000:\\n cnt += 1\\n start_w = int(np.random.rand() * remain_w)\\n start_h = int(np.random.rand() * remain_h)\\n flag = is_cross_text([start_w, start_h], length, new_vertices[labels == 1, :])\\n box = (start_w, start_h, start_w + length, start_h + length)\\n region = img.crop(box)\\n if new_vertices.size == 0:\\n return region, new_vertices\\n\\n new_vertices[:, [0, 2, 4, 6]] -= start_w\\n new_vertices[:, [1, 3, 5, 7]] -= start_h\\n return region, new_vertices\\n\\n\\n##### is_cross_text\\n\\npython\\ndef is_cross_text(start_loc, length, vertices):\\n \'\'\'check if the crop image crosses text regions\\n Input:\\n start_loc: left-top position\\n length : length of crop image\\n vertices : vertices of text regions <numpy.ndarray, (n,8)>\\n Output:\\n True if crop image crosses text region\\n \'\'\'\\n if vertices.size == 0:\\n return False\\n start_w, start_h = start_loc\\n a = np.array([start_w, start_h, start_w + length, start_h,\\n start_w + length, start_h + length, start_w, start_h + length]).reshape((4, 2))\\n p1 = Polygon(a).convex_hull\\n epsilon = 1e-6\\n for vertice in vertices:\\n p2 = Polygon(vertice.reshape((4, 2))).convex_hull\\n inter = p1.intersection(p2).area\\n if 0.01 <= inter / (p2.area + epsilon) <= 0.99:\\n return True\\n return False\\n\\n\\n##### get_score_geo\\n\\npython\\ndef get_score_geo(img, vertices, labels, scale, length):\\n \'\'\'generate score gt and geometry gt\\n Input:\\n img : PIL Image\\n vertices: vertices of text regions <numpy.ndarray, (n,8)>\\n labels : 1->valid, 0->ignore, <numpy.ndarray, (n,)>\\n scale : feature map / image\\n length : image length\\n Output:\\n score gt, geo gt, ignored\\n \'\'\'\\n score_map = np.zeros((int(img.height * scale), int(img.width * scale), 1), np.float32)\\n geo_map = np.zeros((int(img.height * scale), int(img.width * scale), 5), np.float32)\\n ignored_map = np.zeros((int(img.height * scale), int(img.width * scale), 1), np.float32)\\n\\n index = np.arange(0, length, int(1 / scale))\\n index_x, index_y = np.meshgrid(index, index)\\n ignored_polys = []\\n polys = []\\n\\n for i, vertice in enumerate(vertices):\\n if labels[i] == 0:\\n ignored_polys.append(np.around(scale * vertice.reshape((4, 2))).astype(np.int32))\\n continue\\n\\n poly = np.around(scale * shrink_poly(vertice).reshape((4, 2))).astype(np.int32) # scaled & shrinked\\n polys.append(poly)\\n temp_mask = np.zeros(score_map.shape[:-1], np.float32)\\n cv2.fillPoly(temp_mask, [poly], 1)\\n\\n theta = find_min_rect_angle(vertice)\\n rotate_mat = get_rotate_mat(theta)\\n\\n rotated_vertices = rotate_vertices(vertice, theta)\\n x_min, x_max, y_min, y_max = get_boundary(rotated_vertices)\\n rotated_x, rotated_y = rotate_all_pixels(rotate_mat, vertice[0], vertice[1], length)\\n\\n # given p in Polygon(vertice), top_distance = p - Pr_{top_L}(p) = r(p)_y - r(Pr_{top_L}(p))_y = r(p)_y - ymin\\n # where r is the rotation anchored at the top-left corner\\n # and p in Polygon(vertice) only if r(p)_y - ymin >= 0\\n # d1 = distance from top to point (j, i)\\n\\n # the gt is top, bottom, left, right (\u4e0a, \u4e0b, \u5de6, \u53f3)\\n d1 = rotated_y - y_min\\n d1[d1 < 0] = 0\\n d2 = y_max - rotated_y\\n d2[d2 < 0] = 0\\n d3 = rotated_x - x_min\\n d3[d3 < 0] = 0\\n d4 = x_max - rotated_x\\n d4[d4 < 0] = 0\\n geo_map[:, :, 0] += d1[index_y, index_x] * temp_mask\\n geo_map[:, :, 1] += d2[index_y, index_x] * temp_mask\\n geo_map[:, :, 2] += d3[index_y, index_x] * temp_mask\\n geo_map[:, :, 3] += d4[index_y, index_x] * temp_mask\\n geo_map[:, :, 4] += theta * temp_mask\\n\\n cv2.fillPoly(ignored_map, ignored_polys, 1)\\n cv2.fillPoly(score_map, polys, 1)\\n return torch.Tensor(score_map).permute(2, 0, 1), torch.Tensor(geo_map).permute(2, 0, 1), torch.Tensor(ignored_map).permute(2, 0, 1)\\n\\n\\n##### shrink_poly\\n\\npython-1\\ndef shrink_poly(vertices, coef=0.3):\\n \'\'\'shrink the text region\\n Input:\\n vertices: vertices of text region <numpy.ndarray, (8,)>\\n coef : shrink ratio in paper\\n Output:\\n v : vertices of shrinked text region <numpy.ndarray, (8,)>\\n \'\'\'\\n x1, y1, x2, y2, x3, y3, x4, y4 = vertices\\n r1 = min(cal_distance(x1, y1, x2, y2), cal_distance(x1, y1, x4, y4))\\n r2 = min(cal_distance(x2, y2, x1, y1), cal_distance(x2, y2, x3, y3))\\n r3 = min(cal_distance(x3, y3, x2, y2), cal_distance(x3, y3, x4, y4))\\n r4 = min(cal_distance(x4, y4, x1, y1), cal_distance(x4, y4, x3, y3))\\n r = [r1, r2, r3, r4]\\n\\n # obtain offset to perform move_points() automatically\\n if cal_distance(x1, y1, x2, y2) + cal_distance(x3, y3, x4, y4) > \\\\\\n cal_distance(x2, y2, x3, y3) + cal_distance(x1, y1, x4, y4):\\n offset = 0 # two longer edges are (x1y1-x2y2) & (x3y3-x4y4)\\n else:\\n offset = 1 # two longer edges are (x2y2-x3y3) & (x4y4-x1y1)\\n\\n v = vertices.copy()\\n v = move_points(v, 0 + offset, 1 + offset, r, coef)\\n\\n\\n- The movement is always parellel to the edges.\\n- In each move_points, two adjacent vectice will be pushed towards each other.\\n- Each vertex will be adjusted twice in two directions in order to move towards center.\\n\\npython-25\\n v = move_points(v, 2 + offset, 3 + offset, r, coef)\\n v = move_points(v, 1 + offset, 2 + offset, r, coef)\\n v = move_points(v, 3 + offset, 4 + offset, r, coef)\\n return v\\n\\n\\n##### find_min_rect_angle\\n\\npython\\ndef find_min_rect_angle(vertices):\\n \'\'\'find the best angle to rotate poly and obtain min rectangle\\n Input:\\n vertices: vertices of text region <numpy.ndarray, (8,)>\\n Output:\\n the best angle <radian measure>\\n \'\'\'\\n angle_interval = 1\\n angle_list = list(range(-90, 90, angle_interval))\\n area_list = []\\n for theta in angle_list:\\n rotated = rotate_vertices(vertices, theta / 180 * math.pi)\\n x1, y1, x2, y2, x3, y3, x4, y4 = rotated\\n temp_area = (max(x1, x2, x3, x4) - min(x1, x2, x3, x4)) * \\\\\\n (max(y1, y2, y3, y4) - min(y1, y2, y3, y4))\\n area_list.append(temp_area)\\n\\n sorted_area_index = sorted(list(range(len(area_list))), key=lambda k: area_list[k])\\n min_error = float(\'inf\')\\n best_index = -1\\n rank_num = 10\\n # find the best angle with correct orientation\\n for index in sorted_area_index[:rank_num]:\\n rotated = rotate_vertices(vertices, angle_list[index] / 180 * math.pi)\\n temp_error = cal_error(rotated)\\n if temp_error < min_error:\\n min_error = temp_error\\n best_index = index\\n return angle_list[best_index] / 180 * math.pi\\n\\n\\n##### cal_distance\\n\\npython\\ndef cal_distance(x1, y1, x2, y2):\\n \'\'\'calculate the Euclidean distance\'\'\'\\n return math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\\n\\n\\n##### get_boundary\\n\\npython\\ndef get_boundary(vertices):\\n \'\'\'get the tight boundary around given vertices\\n Input:\\n vertices: vertices of text region <numpy.ndarray, (8,)>\\n Output:\\n the boundary\\n \'\'\'\\n x1, y1, x2, y2, x3, y3, x4, y4 = vertices\\n x_min = min(x1, x2, x3, x4)\\n x_max = max(x1, x2, x3, x4)\\n y_min = min(y1, y2, y3, y4)\\n y_max = max(y1, y2, y3, y4)\\n return x_min, x_max, y_min, y_max\\n\\n\\n##### rotate_all_pixels\\n\\npython\\ndef rotate_all_pixels(rotate_mat, anchor_x, anchor_y, length):\\n \'\'\'get rotated locations of all pixels for next stages\\n Input:\\n rotate_mat: rotatation matrix\\n anchor_x : fixed x position\\n anchor_y : fixed y position\\n length : length of image\\n Output:\\n rotated_x : rotated x positions <numpy.ndarray, (length,length)>\\n rotated_y : rotated y positions <numpy.ndarray, (length,length)>\\n \'\'\'\\n x = np.arange(length)\\n y = np.arange(length)\\n x, y = np.meshgrid(x, y)\\n x_lin = x.reshape((1, x.size))\\n y_lin = y.reshape((1, x.size))\\n coord_mat = np.concatenate((x_lin, y_lin), 0)\\n rotated_coord = np.dot(rotate_mat, coord_mat - np.array([[anchor_x], [anchor_y]])) + \\\\\\n np.array([[anchor_x], [anchor_y]])\\n rotated_x = rotated_coord[0, :].reshape(x.shape)\\n rotated_y = rotated_coord[1, :].reshape(y.shape)\\n return rotated_x, rotated_y\\n\\n\\n##### move_points\\n\\npython\\ndef move_points(vertices, index1, index2, r, coef):\\n \'\'\'move the two points to shrink edge\\n Input:\\n vertices: vertices of text region <numpy.ndarray, (8,)>\\n index1 : offset of point1\\n index2 : offset of point2\\n r : [r1, r2, r3, r4] in paper\\n coef : shrink ratio in paper\\n Output:\\n vertices: vertices where one edge has been shinked\\n \'\'\'\\n index1 = index1 % 4\\n index2 = index2 % 4\\n x1_index = index1 * 2 + 0\\n y1_index = index1 * 2 + 1\\n x2_index = index2 * 2 + 0\\n y2_index = index2 * 2 + 1\\n\\n r1 = r[index1]\\n r2 = r[index2]\\n length_x = vertices[x1_index] - vertices[x2_index]\\n length_y = vertices[y1_index] - vertices[y2_index]\\n length = cal_distance(vertices[x1_index], vertices[y1_index], vertices[x2_index], vertices[y2_index])\\n if length > 1:\\n ratio = (r1 * coef) / length\\n vertices[x1_index] += ratio * (-length_x)\\n vertices[y1_index] += ratio * (-length_y)\\n ratio = (r2 * coef) / length\\n vertices[x2_index] += ratio * length_x\\n vertices[y2_index] += ratio * length_y\\n return vertices\\n\\n\\n##### cal_error\\n\\npython\\ndef cal_error(vertices):\\n \'\'\'default orientation is x1y1 : left-top, x2y2 : right-top, x3y3 : right-bot, x4y4 : left-bot\\n calculate the difference between the vertices orientation and default orientation\\n Input:\\n vertices: vertices of text region <numpy.ndarray, (8,)>\\n Output:\\n err : difference measure\\n \'\'\'\\n x_min, x_max, y_min, y_max = get_boundary(vertices)\\n x1, y1, x2, y2, x3, y3, x4, y4 = vertices\\n err = cal_distance(x1, y1, x_min, y_min) + cal_distance(x2, y2, x_max, y_min) + \\\\\\n cal_distance(x3, y3, x_max, y_max) + cal_distance(x4, y4, x_min, y_max)\\n return err\\n\\n\\n#### losses.py\\n\\n##### Import\\n\\npython\\nimport torch\\nimport torch.nn as nn\\n\\n\\n##### get_dice_loss\\n\\npython\\ndef get_dice_loss(gt_score, pred_score):\\n inter = torch.sum(gt_score * pred_score)\\n union = torch.sum(gt_score) + torch.sum(pred_score) + 1e-5\\n return 1. - (2 * inter / union)\\n\\n\\n##### get_geo_loss\\n\\npython\\ndef get_geo_loss(gt_geo, pred_geo):\\n d1_gt, d2_gt, d3_gt, d4_gt, angle_gt = torch.split(gt_geo, 1, 1)\\n d1_pred, d2_pred, d3_pred, d4_pred, angle_pred = torch.split(pred_geo, 1, 1)\\n area_gt = (d1_gt + d2_gt) * (d3_gt + d4_gt)\\n area_pred = (d1_pred + d2_pred) * (d3_pred + d4_pred)\\n w_union = torch.min(d3_gt, d3_pred) + torch.min(d4_gt, d4_pred)\\n h_union = torch.min(d1_gt, d1_pred) + torch.min(d2_gt, d2_pred)\\n area_intersect = w_union * h_union\\n area_union = area_gt + area_pred - area_intersect\\n iou_loss_map = -torch.log((area_intersect + 1.0) / (area_union + 1.0))\\n angle_loss_map = 1 - torch.cos(angle_pred - angle_gt)\\n return iou_loss_map, angle_loss_map\\n\\n\\n##### Loss\\n\\npython\\nclass Loss(nn.Module):\\n def __init__(self, weight_angle=10):\\n super(Loss, self).__init__()\\n self.weight_angle = weight_angle\\n\\n def forward(self, gt_score, pred_score, gt_geo, pred_geo, ignored_map):\\n if torch.sum(gt_score) < 1:\\n return torch.sum(pred_score + pred_geo) * 0\\n\\n classify_loss = get_dice_loss(gt_score, pred_score * (1 - ignored_map))\\n iou_loss_map, angle_loss_map = get_geo_loss(gt_geo, pred_geo)\\n\\n angle_loss = torch.sum(angle_loss_map * gt_score) / torch.sum(gt_score)\\n iou_loss = torch.sum(iou_loss_map * gt_score) / torch.sum(gt_score)\\n geo_loss = self.weight_angle * angle_loss + iou_loss\\n return geo_loss + classify_loss\\n\\n\\n#### train.py\\n\\n##### import\\n\\npython\\nimport torch\\nfrom torch.utils import data\\nfrom torch import nn\\nfrom torch.optim import lr_scheduler\\nfrom dataset import CustomDataset\\nfrom detect import performance_check\\nfrom models import EAST\\nfrom losses import Loss\\nfrom tqdm import tqdm\\nfrom device import device\\nfrom utils import ConsoleLog\\nimport os\\nimport time\\n\\n\\n##### train\\n\\npython\\nconsole_log = ConsoleLog(lines_up_on_end=1)\\ndef train(train_img_path, train_gt_path, pths_path, batch_size, lr, num_workers, epoch_iter, interval):\\n file_num = len(os.listdir(train_img_path))\\n trainset = CustomDataset(train_img_path, train_gt_path)\\n train_loader = data.DataLoader(trainset,\\n batch_size=batch_size,\\n shuffle=True,\\n num_workers=num_workers,\\n drop_last=True)\\n\\n criterion = Loss()\\n model = EAST()\\n data_parallel = False\\n\\n if torch.cuda.device_count() > 1:\\n model = nn.DataParallel(model)\\n data_parallel = True\\n\\n model.to(device)\\n optimizer = torch.optim.Adam(model.parameters(), lr=lr)\\n scheduler = lr_scheduler.MultiStepLR(optimizer,\\n milestones=[epoch_iter // 2],\\n gamma=0.1)\\n\\n for epoch in range(epoch_iter):\\n model.train()\\n epoch_loss = 0\\n epoch_time = time.time()\\n\\n for batch, (img, gt_score, gt_geo, ignored_map) in enumerate(tqdm(\\n train_loader,\\n total=len(trainset) // batch_size,\\n bar_format=\\"{desc}: {percentage:.1f}%|{bar:15}| {n}/{total_fmt} [{elapsed}, {rate_fmt}{postfix}]\\"\\n )):\\n start_time = time.time()\\n\\n img = img.to(device)\\n gt_score = gt_score.to(device)\\n gt_geo = gt_geo.to(device)\\n ignored_map = ignored_map.to(device)\\n\\n pred_score, pred_geo = model(img)\\n\\n loss = criterion(gt_score, pred_score, gt_geo, pred_geo, ignored_map)\\n\\n epoch_loss += loss.item()\\n optimizer.zero_grad()\\n loss.backward()\\n optimizer.step()\\n scheduler.step()\\n\\n if (batch + 1) % save_interval == 0:\\n performance_check(model, save_image_path=\\"results/epoch_{}_batch_{}.jpg\\".format(epoch, batch + 1))\\n\\n console_log.print(\\n \'Epoch is [{}/{}], mini-batch is [{}/{}], time consumption is {:.8f}, batch_loss is {:.8f}\'.format(\\n epoch + 1, epoch_iter, batch + 1, int(file_num / batch_size), time.time() - start_time, loss.item()),\\n is_key_value=False\\n )\\n\\n if (epoch + 1) % interval == 0:\\n state_dict = model.module.state_dict() if data_parallel else model.state_dict()\\n torch.save(state_dict, os.path.join(pths_path, \'model_epoch_{}.pth\'.format(epoch + 1)))\\n\\n\\nif __name__ == \'__main__\':\\n train_img_path = os.path.abspath(\'dataset/images\')\\n train_gt_path = os.path.abspath(\'dataset/annotations\')\\n pths_path = \'./pths\'\\n batch_size = 24\\n lr = 1e-3\\n num_workers = 4\\n epoch_iter = 600\\n save_interval = 5\\n train(train_img_path, train_gt_path, pths_path, batch_size, lr, num_workers, epoch_iter, save_interval)\\n\\n\\n#### detect.py\\n\\n##### Import\\n\\npython\\nfrom torchvision import transforms\\nfrom PIL import Image, ImageDraw\\nfrom models import EAST\\nfrom dataset import get_rotate_mat\\nfrom utils import nms_locality\\nfrom device import device\\n\\nimport config\\nimport torch\\nimport os\\nimport numpy as np\\nimport random\\n\\n\\n##### resize_img\\n\\npython\\ndef resize_img(img):\\n \'\'\'resize image to be divisible by 32\\n \'\'\'\\n w, h = img.size\\n resize_w = w\\n resize_h = h\\n\\n resize_h = resize_h if resize_h % 32 == 0 else int(resize_h / 32) * 32\\n resize_w = resize_w if resize_w % 32 == 0 else int(resize_w / 32) * 32\\n img = img.resize((resize_w, resize_h), Image.BILINEAR)\\n ratio_h = resize_h / h\\n ratio_w = resize_w / w\\n\\n return img, ratio_h, ratio_w\\n\\n\\n##### load_pil\\n\\npython\\ndef load_pil(img):\\n \'\'\'convert PIL Image to torch.Tensor\\n \'\'\'\\n t = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))])\\n return t(img).unsqueeze(0)\\n\\n\\n##### is_valid_poly\\n\\npython\\ndef is_valid_poly(res, score_shape, scale):\\n \'\'\'check if the poly in image scope\\n Input:\\n res : restored poly in original image\\n score_shape: score map shape\\n scale : feature map -> image\\n Output:\\n True if valid\\n \'\'\'\\n cnt = 0\\n for i in range(res.shape[1]):\\n if res[0, i] < 0 or res[0, i] >= score_shape[1] * scale or \\\\\\n res[1, i] < 0 or res[1, i] >= score_shape[0] * scale:\\n cnt += 1\\n return True if cnt <= 1 else False\\n\\n\\n##### restore_polys\\n\\npython\\ndef restore_polys(valid_pos, valid_geo, score_shape, scale=4):\\n \'\'\'restore polys from feature maps in given positions\\n Input:\\n valid_pos : potential text positions <numpy.ndarray, (n,2)>\\n valid_geo : geometry in valid_pos <numpy.ndarray, (5,n)>\\n score_shape: shape of score map\\n scale : image / feature map\\n Output:\\n restored polys <numpy.ndarray, (n,8)>, index\\n \'\'\'\\n polys = []\\n index = []\\n valid_pos *= scale\\n d = valid_geo[:4, :] # 4 x N\\n angle = valid_geo[4, :] # N,\\n\\n for i in range(valid_pos.shape[0]):\\n x = valid_pos[i, 0]\\n y = valid_pos[i, 1]\\n y_min = y - d[0, i]\\n y_max = y + d[1, i]\\n x_min = x - d[2, i]\\n x_max = x + d[3, i]\\n rotate_mat = get_rotate_mat(-angle[i])\\n\\n temp_x = np.array([[x_min, x_max, x_max, x_min]]) - x\\n temp_y = np.array([[y_min, y_min, y_max, y_max]]) - y\\n coordidates = np.concatenate((temp_x, temp_y), axis=0)\\n res = np.dot(rotate_mat, coordidates)\\n res[0, :] += x\\n res[1, :] += y\\n\\n if is_valid_poly(res, score_shape, scale):\\n index.append(i)\\n polys.append([res[0, 0], res[1, 0], res[0, 1], res[1, 1], res[0, 2], res[1, 2], res[0, 3], res[1, 3]])\\n return np.array(polys), index\\n\\n\\n##### get_boxes\\n\\npython\\ndef get_boxes(score, geo, score_thresh=config.detection_score_threshold, nms_thresh=0.2):\\n \'\'\'get boxes from feature map\\n Input:\\n score : score map from model <numpy.ndarray, (1,row,col)>\\n geo : geo map from model <numpy.ndarray, (5,row,col)>\\n score_thresh: threshold to segment score map\\n nms_thresh : threshold in nms\\n Output:\\n boxes : final polys <numpy.ndarray, (n,9)>\\n \'\'\'\\n score = score[0, :, :]\\n xy_text = np.argwhere(score > score_thresh) # n x 2, format is [r, c]\\n if xy_text.size == 0:\\n return None\\n\\n xy_text = xy_text[np.argsort(xy_text[:, 0])]\\n valid_pos = xy_text[:, ::-1].copy() # n x 2, [x, y]\\n # Due to ::-1, pos is now following (x, y) = (i, j) notational convention\\n valid_geo = geo[:, xy_text[:, 0], xy_text[:, 1]] # 5 x n\\n # So is valid_geo\\n polys_restored, index = restore_polys(valid_pos, valid_geo, score.shape)\\n if polys_restored.size == 0:\\n return None\\n\\n boxes = np.zeros((polys_restored.shape[0], 9), dtype=np.float32)\\n boxes[:, :8] = polys_restored\\n boxes[:, 8] = score[xy_text[index, 0], xy_text[index, 1]]\\n boxes = nms_locality(boxes.astype(\'float32\'), nms_thresh)\\n return boxes\\n\\n\\n##### adjust_ratio\\n\\npython\\ndef adjust_ratio(boxes, ratio_w, ratio_h):\\n \'\'\'refine boxes\\n Input:\\n boxes : detected polys <numpy.ndarray, (n,9)>\\n ratio_w: ratio of width\\n ratio_h: ratio of height\\n Output:\\n refined boxes\\n \'\'\'\\n if boxes is None or boxes.size == 0:\\n return None\\n boxes[:, [0, 2, 4, 6]] /= ratio_w\\n boxes[:, [1, 3, 5, 7]] /= ratio_h\\n return np.around(boxes)\\n\\n\\n##### detect\\n\\npython\\ndef detect(img, model, device):\\n \'\'\'detect text regions of img using model\\n Input:\\n img : PIL Image\\n model : detection model\\n device: gpu if gpu is available\\n Output:\\n detected polys\\n \'\'\'\\n img, ratio_h, ratio_w = resize_img(img)\\n with torch.no_grad():\\n score, geo = model(load_pil(img).to(device))\\n boxes = get_boxes(score.squeeze(0).cpu().numpy(), geo.squeeze(0).cpu().numpy())\\n return adjust_ratio(boxes, ratio_w, ratio_h)\\n\\n\\n##### plot_boxes\\n\\npython\\ndef plot_boxes(img, boxes):\\n \'\'\'plot boxes on image\\n \'\'\'\\n if boxes is None:\\n return img\\n\\n draw = ImageDraw.Draw(img)\\n for box in boxes:\\n draw.polygon([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7]], outline=(0, 255, 0))\\n return img\\n\\n\\n##### detect_dataset\\n\\npython\\ndef detect_dataset(model, device, test_img_path, submit_path):\\n \'\'\'detection on whole dataset, save .txt results in submit_path\\n Input:\\n model : detection model\\n device : gpu if gpu is available\\n test_img_path: dataset path\\n submit_path : submit result for evaluation\\n \'\'\'\\n img_files = os.listdir(test_img_path)\\n img_files = sorted([os.path.join(test_img_path, img_file) for img_file in img_files])\\n\\n for i, img_file in enumerate(img_files):\\n print(\'evaluating {} image\'.format(i), end=\'\\\\r\')\\n boxes = detect(Image.open(img_file), model, device)\\n seq = []\\n if boxes is not None:\\n seq.extend([\',\'.join([str(int(b)) for b in box[:-1]]) + \'\\\\n\' for box in boxes])\\n with open(os.path.join(submit_path, \'res_\' + os.path.basename(img_file).replace(\'.jpg\', \'.txt\')), \'w\') as f:\\n f.writelines(seq)\\n\\n\\n##### performance_check\\n\\npython\\ndef performance_check(model, save_image_path):\\n model.eval()\\n images = os.listdir(\\"dataset/images\\")\\n random.shuffle(images)\\n img = Image.open(\\"dataset/images/{}\\".format(images[0]))\\n boxes = detect(img, model, device)\\n plot_img = plot_boxes(img, boxes)\\n plot_img.save(save_image_path)\\n plot_img.save(\\"results/latest_output.jpg\\")\\n model.train()\\n\\n\\n#### device\\n\\npython\\nimport torch\\ndevice = torch.device(\\"cuda:0\\" if torch.cuda.is_available() else \\"cpu\\")\\n\\n\\n#### utils\\n\\n##### Import\\n\\npython\\nfrom pydash.objects import get, set_\\nfrom shapely.geometry import Polygon\\nimport numpy as np\\n\\n\\n##### iou\\n\\npython\\ndef iou(g, p):\\n g = Polygon(g[:8].reshape((4, 2)))\\n p = Polygon(p[:8].reshape((4, 2)))\\n if not g.is_valid or not p.is_valid:\\n return 0\\n inter = Polygon(g).intersection(Polygon(p)).area\\n union = g.area + p.area - inter\\n if union == 0:\\n return 0\\n else:\\n return inter / union\\n\\n\\n##### weighted_merge\\n\\npython\\ndef weighted_merge(g, p):\\n g[:8] = (g[8] * g[:8] + p[8] * p[:8]) / (g[8] + p[8])\\n g[8] = (g[8] + p[8])\\n return g\\n\\n\\n##### standard_nms\\n\\npython\\ndef standard_nms(S, thres):\\n order = np.argsort(S[:, 8])[::-1]\\n keep = []\\n while order.size > 0:\\n i = order[0]\\n keep.append(i)\\n ious = np.array([iou(S[i], S[t]) for t in order[1:]])\\n\\n inds = np.where(ious <= thres)[0]\\n # since order[0] is taken out\\n order = order[inds + 1]\\n\\n return S[keep]\\n\\n\\n##### nms_locality\\n\\npython\\ndef nms_locality(polys, thres=0.3):\\n \'\'\'\\n locality aware nms of EAST\\n :param polys: a N*9 numpy array. first 8 coordinates, then prob\\n :return: boxes after nms\\n \'\'\'\\n S = []\\n p = None\\n for g in polys:\\n if p is not None and iou(g, p) > thres:\\n p = weighted_merge(g, p)\\n else:\\n if p is not None:\\n S.append(p)\\n p = g\\n if p is not None:\\n S.append(p)\\n\\n if len(S) == 0:\\n return np.array([])\\n return standard_nms(np.array(S), thres)\\n\\n","title":"EAST: A Text Detection Algorithm","date":"2022-05-29T00:00:00.000Z","id":"blog082","tag":"pytorch, deep-learning","intro":"Break down the code for my own study. e.g., the way it does image cropping, resizing, etc augmentations while keeping the vertice consistent are very valuable reference, the way it does nms is also succint! ***Source Code:*** <a href=\'https://github.com/SakuraRiven/EAST\'>Link</a>"},{"content":"\\n#### Repo\\n\\n- Feature of Single-Scale:\\n\\n https://github.com/machingclee/Minimal-Code-for-Faster-RCNN-in-pytorch\\n\\n- Feature of Multi-Scale with Feature Pyramid Network\\n\\n https://github.com/machingclee/Minimal-Code-for-Faster-RCNN-with-FPN-in-pytorch\\n\\n#### Results\\n\\nHere the faded white boxes are the ROIs, and blue boxes are refined ROIs that are estimated from the feature of ROIAlign module.\\n\\n<div>\\n <center>\\n <a href=\\"/assets/tech/057.jpg\\">\\n <img src=\\"/assets/tech/057.jpg\\" width=\\"45%\\" style=\\"margin-right:10pt\\"/>\\n </a>\\n <a href=\\"/assets/tech/058.jpg\\">\\n <img src=\\"/assets/tech/058.jpg\\" width=\\"45%\\"/>\\n </a>\\n </center>\\n</div>\\n\\n#### Model Structure\\n\\n<a href=\\"/assets/tech/056.png\\" target=\\"_blank\\">\\n <img src=\\"/assets/tech/056.png\\" width=\\"100%\\"/>\\n</a>\\n\\n#### How to Read the Source Code\\n\\nIt is not easy to explain everything in a blog post. Rather one can delve into the source code and see how it works!\\n\\nReader can treat src/faster_rcnn.py as an entry point, the class FasterRCNN is our target result. The FasterRCNN.forward method behaves differently when it is in:\\n\\n- **Training Mode.** It returns\\n - rpn_cls_loss\\n - rpn_reg_loss\\n - roi_cls_loss\\n - roi_reg_loss\\n- **Evaluation Mode.** It returns\\n - scores\\n - roi_refined_box\\n - cls_idxes\\n - rois (no use, debugging purpose)\\n","title":"Faster RCNN in PyTorch","date":"2022-06-27T00:00:00.000Z","id":"blog084","tag":"pytorch, deep-learning","intro":"Minimal functioning implementation of faster rcnn (with and without fpn)."},{"content":"\\nWe define the transform:\\n\\npython\\nalbumentation_transform = A.Compose([\\n A.ShiftScaleRotate(shift_limit=0, rotate_limit=10, p=0.7),\\n A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.9),\\n A.HorizontalFlip(p=0.5),\\n A.GaussNoise(p=0.5),\\n A.RandomBrightnessContrast(p=0.5),\\n A.RandomGamma(p=0.5),\\n A.OneOf([\\n A.Blur(blur_limit=3, p=0.5),\\n A.ColorJitter(p=0.5)\\n ], p=0.8),\\n A.LongestMaxSize(max_size=config.input_height, interpolation=1, p=1),\\n A.PadIfNeeded(\\n min_height=config.input_height,\\n min_width=config.input_height,\\n border_mode=0,\\n value=(0, 0, 0),\\n position=\\"top_left\\"\\n ),\\n],\\n p=1,\\n bbox_params=A.BboxParams(format=\\"pascal_voc\\", min_area=0.1)\\n)\\n\\n\\nIn this way the combination of A.LongestMaxSize and A.PadIfNeeded have\\n\\n- resized the image and\\n- padded the image into a square\\n so that our numpy array is of suitable shape and ready to be fed into our network.\\n\\nNow we transform the image and coordinate of bounding box at the same time:\\n\\npython\\ndef data_augmentation(img, bboxes):\\n if isinstance(img, Image.Image):\\n img = np.array(img)\\n transformed = albumentation_transform(image=img, bboxes=bboxes)\\n img = transformed[\\"image\\"]\\n bboxes = transformed[\\"bboxes\\"]\\n return img, bboxes\\n\\n\\nNote that bboxes is of type:\\n\\npython\\nbboxes: List[List[float, float, float, float, int | string]]\\n\\n\\nSince our format is pascal_voc, the 4 float\'s there are (xmin, ymin, xmax, ymax).\\n\\nThe final coordinate bboxes[:, 4] is usually the class labels of the bounding boxes, it will not be transformed by albumentation and will be kept unchanged there.\\n","title":"Data Augmentation for Object Detection","date":"2022-06-30T00:00:00.000Z","id":"blog085","tag":"pytorch, deep-learning","toc":false,"intro":"Record my data augmentation used in object detection."},{"content":"\\n#### How to find the Slice of Layers\\n\\nUsually we can inspect a model by print(the_model), from that it is easy to find the correct slice indexes of the corresponding layers.\\n\\n#### Backbones\\n\\n##### VGG-16\\n\\npython\\nclass Vgg16FeatureExtractor(nn.Module):\\n def __init__(self):\\n super(FeatureExtractor, self).__init__()\\n self.vgg = models.vgg16(pretrained=True).to(device)\\n self.features = self.vgg.features\\n self.out_channels = None\\n\\n self.conv_blk1 = self.features[0:4]\\n self.conv_blk2 = self.features[4:9]\\n self.conv_blk3 = self.features[9:16]\\n self.conv_blk4 = self.features[16:23]\\n self.conv_blk5 = self.features[23:29]\\n\\n self.freeze_vgg_bottom_layers()\\n\\n def unfreeze_layers(self, from_layer, to_layer):\\n for layer in list(self.features)[from_layer: to_layer]:\\n if isinstance(layer, nn.Conv2d):\\n for param in layer.parameters():\\n param.requires_grad = True\\n\\n def freeze_vgg_bottom_layers(self):\\n for layer in (list(self.conv_blk1) + list(self.conv_blk2) + list(self.conv_blk3)):\\n if isinstance(layer, nn.Conv2d):\\n for param in layer.parameters():\\n param.requires_grad = False\\n\\n def vgg_weight_init_upper_layers(self):\\n for layer in list(self.feature_extraction.children())[9:]:\\n if isinstance(layer, nn.Conv2d):\\n torch.nn.init.normal_(layer.weight, std=0.01)\\n torch.nn.init.constant_(layer.bias, 0)\\n\\n def unfreeze_vgg(self):\\n for param in self.vgg.parameters():\\n param.requires_grad = True\\n\\n def forward(self, x):\\n x = self.conv_blk1(x)\\n x = self.conv_blk2(x)\\n x = self.conv_blk3(x)\\n x = self.conv_blk4(x)\\n x = self.conv_blk5(x)\\n return x\\n\\n\\n##### Resnet-34\\n\\npython\\nclass Resnet34FeatureExtractor(nn.Module):\\n def __init__(self):\\n # type: (Backbone) -> None\\n super(FeatureExtractor, self).__init__()\\n\\n self.resnet34 = models.resnet34(pretrained=True).to(device)\\n # self.layer9 = self.resnet34.\\n self.conv1 = self.resnet34.conv1\\n self.bn1 = self.resnet34.bn1\\n self.relu = self.resnet34.relu\\n self.maxpool = self.resnet34.maxpool\\n self.layer1 = self.resnet34.layer1\\n self.layer2 = self.resnet34.layer2\\n self.layer3 = self.resnet34.layer3\\n self.freeze_resnet34_bottom_layers()\\n\\n def freeze_resnet34_bottom_layers(self):\\n for layer in ([self.conv1] + list(self.layer1) + list(self.layer2)):\\n if isinstance(layer, nn.Conv2d):\\n for param in layer.parameters():\\n param.requires_grad = False\\n\\n def forward(self, x):\\n x= self.conv1(x)\\n x= self.bn1(x)\\n x= self.relu(x)\\n x= self.maxpool(x)\\n x = self.layer1(x)\\n x = self.layer2(x)\\n x = self.layer3(x)\\n return x\\n\\n\\n##### Resnet-50-FPN\\n\\npython\\nclass ResnetFPNFeactureExtractor(nn.Module):\\n def __init__(self):\\n super(ResnetFPNFeactureExtractor, self).__init__()\\n self.resnet50 = models.resnet50(pretrained=True)\\n\\n self.conv2 = nn.Sequential(\\n self.resnet50.conv1,\\n self.resnet50.bn1,\\n self.resnet50.relu,\\n self.resnet50.maxpool,\\n self.resnet50.layer1\\n )\\n self.conv3 = self.resnet50.layer2\\n self.conv4 = self.resnet50.layer3\\n self.conv5 = self.resnet50.layer4\\n\\n self.lateral_conv5 = nn.Conv2d(2048, config.fpn_feat_channels, 1, 1)\\n self.lateral_conv4 = nn.Conv2d(1024, config.fpn_feat_channels, 1, 1)\\n self.lateral_conv3 = nn.Conv2d(512, config.fpn_feat_channels, 1, 1)\\n self.lateral_conv2 = nn.Conv2d(256, config.fpn_feat_channels, 1, 1)\\n\\n self.upscale = lambda input: F.interpolate(input, scale_factor=2)\\n self.freeze_params()\\n\\n def freeze_params(self):\\n modules = [\\n self.conv2,\\n self.conv3,\\n # self.conv4,\\n # self.conv5\\n ]\\n for module in modules:\\n for layer in module:\\n if isinstance(layer, nn.Conv2d):\\n for param in layer.parameters():\\n param.requires_grad = False\\n\\n def forward(self, x):\\n c2 = self.conv2(x)\\n c3 = self.conv3(c2)\\n c4 = self.conv4(c3)\\n c5 = self.conv5(c4)\\n\\n p5 = self.lateral_conv5(c5)\\n p4 = self.lateral_conv4(c4) + self.upscale(p5)\\n p3 = self.lateral_conv3(c3) + self.upscale(p4)\\n p2 = self.lateral_conv2(c2) + self.upscale(p3)\\n\\n return [p2, p3, p4, p5]\\n\\n","title":"Feature Extractors","date":"2022-07-14T00:00:00.000Z","id":"blog086","tag":"pytorch, deep-learning","intro":"Different backbones have different APIs for extracting features from an image, we record some of them."},{"content":"\\nGiven a training function with the following interface:\\n\\npython\\nclass TrainingErrorMessage(TypedDict):\\n curr_epoch: int\\n message: Literal[\\"nan_loss\\"]\\n\\ndef train(\\n faster_rcnn: FasterRCNNResnet50FPN,\\n lr,\\n start_epoch,\\n epochs,\\n save_weight_interval=5\\n) -> None | TrainingErrorMessage:\\n ...\\n for epoch in range(epochs):\\n for batch_id, data in enumerate(tqdm(data_loader)):\\n ...\\n total_loss = ...\\n\\n if torch.isnan(total_loss):\\n return TrainingErrorMessage(message=\\"nan_loss\\", curr_epoch=epoch)\\n ...\\n\\n\\nThen we wrap the function train by\\n\\npython\\ndef train_with_nan(\\n faster_rcnn,\\n lr=1e-5,\\n start_epoch=1,\\n epoches=60,\\n save_weight_interval=5\\n):\\n continue_training = True\\n restart_ep = start_epoch\\n restart_for_eps = epoches\\n curr_model = faster_rcnn\\n\\n while continue_training:\\n result = train(\\n curr_model,\\n lr,\\n restart_ep,\\n restart_for_eps,\\n save_weight_interval\\n )\\n if result is not None:\\n message = result[\\"message\\"]\\n if message == \\"nan_loss\\":\\n curr_epoch = result[\\"curr_epoch\\"]\\n if curr_epoch > (start_epoch + epoches):\\n print(\\"stop training\\")\\n continue_training = False\\n else:\\n continue_training = True\\n model_latest_epoch = (curr_epoch-1) - ((curr_epoch-1) % save_weight_interval)\\n restart_ep = model_latest_epoch + 1\\n restart_for_eps = epoches - (model_latest_epoch - start_epoch)\\n model_path = f\\"pths/model_epoch_{model_latest_epoch}.pth\\"\\n curr_model = FasterRCNNResnet50FPN().to(device)\\n curr_model.load_state_dict(torch.load(model_path))\\n curr_model.train()\\n\\n print(f\\"Get nan loss, restart training at epoch {restart_ep} for additional {restart_for_eps} epochs\\" + \\"\\\\x1B[0K\\")\\n print(f\\"Loading weight from {model_path}\\" + \\"\\\\x1B[0K\\")\\n else:\\n continue_training = False\\n else:\\n continue_training = False\\n\\n","title":"Retrain Model when Nan Occurs","date":"2022-07-22T00:00:00.000Z","id":"blog087","tag":"pytorch, deep-learning","toc":false,"intro":"Sometimes a loss becomes nan in rare occasion (e.g., it occurs once per 5~7 epochs), in this case we record a script to restart the training using the latest weight."},{"content":"\\n#### Grahpical Structure of Transformer\\n\\n<Center>\\n<img src=\\"/assets/tech/088-transformer/transformer_structure.png\\" width=\\"550\\">\\n</Center>\\n\\n#### Constants in the Config\\n\\npython\\nd_model = 512 # Embedding Size\\nd_ff = 2048 # FeedForward dimension\\nd_k = d_v = 64 # dimension of K(=Q), V\\nn_layers = 6 # number of Encoder of Decoder Layer\\nn_heads = 8 # number of heads in Multi-Head Attention\\n\\n\\n#### Positional Encoding\\n\\n<Center>\\n<img src=\\"/assets/tech/088-transformer/positional.png\\" width=\\"550\\"/>\\n</Center>\\n<p/>\\n<center></center>\\n\\nA **positional encoding**\\n\\n$$\\n\\\\mathrm{PE}: \\\\mathbb {N} \\\\to [0,1]^{d_\\\\text{model}}\\n$$\\n\\nis a mapping that takes a positional index to a vector of word embedding dimension. In terms of $\\\\texttt{np}$-array,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathrm{PE}{(\\\\texttt{pos},2i)} & = \\\\sin(\\\\texttt{pos} / 10000^{2i/d_{\\\\text{model}}}) \\\\\\\\\\n\\\\mathrm{PE}{(\\\\texttt{pos},2i+1)} & = \\\\cos(\\\\texttt{pos} / 10000^{2i/d_{\\\\text{model}}})\\n\\\\end{aligned}\\n$$\\n\\nwhere $\\\\texttt{pos} \\\\in \\\\mathbb N$ and $i = 0, 1, 2,\\\\dots, \\\\frac{1}{2} d_\\\\text{model}-1$.\\n\\npython\\nclass PositionalEncoding(nn.Module):\\n def __init__(self, d_model, dropout=0.1, max_len=5000):\\n super(PositionalEncoding, self).__init__()\\n self.dropout = nn.Dropout(p=dropout)\\n\\n pe = torch.zeros(max_len, d_model)\\n position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\\n div_term = torch.exp(\\n torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\\n )\\n pe[:, 0::2] = torch.sin(position * div_term)\\n pe[:, 1::2] = torch.cos(position * div_term)\\n # unfreeze to prepare for batches of word embeddings\\n pe = pe.unsqueeze(0).transpose(0, 1)\\n self.register_buffer(\'pe\', pe)\\n\\n def forward(self, x):\\n \'\'\'\\n x: [seq_len, batch_size, d_model]\\n \'\'\'\\n x = x + self.pe[:x.size(0), :]\\n return self.dropout(x)\\n\\n\\n#### PoswiseFeedForwardNet\\n\\n<Center>\\n<img src=\\"/assets/tech/088-transformer/poswise.png\\" width=\\"550\\"/>\\n</Center>\\n\\npython\\nclass PoswiseFeedForwardNet(nn.Module):\\n def __init__(self):\\n super(PoswiseFeedForwardNet, self).__init__()\\n self.fc = nn.Sequential(\\n nn.Linear(d_model, d_ff, bias=False),\\n nn.ReLU(),\\n nn.Linear(d_ff, d_model, bias=False)\\n )\\n\\n def forward(self, inputs):\\n \'\'\'\\n inputs: [batch_size, seq_len, d_model]\\n \'\'\'\\n residual = inputs\\n output = self.fc(inputs)\\n # [batch_size, seq_len, d_model]\\n return nn.LayerNorm(d_model).to(device)(output + residual)\\n\\n\\n#### Attentions\\n\\n##### Scaled Dot Product Attention\\n\\n<Center>\\n<img src=\\"/assets/tech/088-transformer/scaled-dot-product-attention.png\\"/ width=\\"400\\">\\n</Center>\\n\\npython\\nclass ScaledDotProductAttention(nn.Module):\\n def __init__(self):\\n super(ScaledDotProductAttention, self).__init__()\\n\\n def forward(self, Q, K, V, attn_mask):\\n \'\'\'\\n Q: [batch_size, n_heads, len_q, d_k]\\n K: [batch_size, n_heads, len_k, d_k]\\n V: [batch_size, n_heads, len_v(=len_k), d_v]\\n attn_mask: [batch_size, n_heads, seq_len, seq_len]\\n \'\'\'\\n # scores : [batch_size, n_heads, len_q, len_k]\\n scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)\\n # Fills elements of self tensor with value where mask is True.\\n scores.masked_fill_(attn_mask, -1e9)\\n\\n attn = nn.Softmax(dim=-1)(scores)\\n # [batch_size, n_heads, len_q, d_v]:\\n context = torch.matmul(attn, V)\\n return context, attn\\n\\n\\nattn_mask is used to mask out value of undesired position such as\\n\\n- those from padding or\\n- those from \\"future word\\" in self-attention module of decoder.\\n\\n##### Repeated Use of ScaledDotProductAttention: Multi-head Attention\\n\\npython\\nclass MultiHeadAttention(nn.Module):\\n def __init__(self):\\n super(MultiHeadAttention, self).__init__()\\n self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\\n self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\\n self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\\n self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\\n\\n def forward(self, input_Q, input_K, input_V, attn_mask):\\n \'\'\'\\n input_Q: [batch_size, len_q, d_model]\\n input_K: [batch_size, len_k, d_model]\\n input_V: [batch_size, len_v(=len_k), d_model]\\n attn_mask: [batch_size, seq_len, seq_len]\\n \'\'\'\\n residual, batch_size = input_Q, input_Q.size(0)\\n # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W)\\n # Q: [batch_size, n_heads, len_q, d_k]\\n Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\\n # K: [batch_size, n_heads, len_k, d_k]\\n K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)\\n # V: [batch_size, n_heads, len_v(=len_k), d_v]\\n V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)\\n\\n # attn_mask : [batch_size, n_heads, seq_len, seq_len]\\n attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\\n\\n # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]\\n context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)\\n # context: [batch_size, len_q, n_heads * d_v]\\n context = context.transpose(1, 2).reshape(\\n batch_size, -1, n_heads * d_v)\\n output = self.fc(context) # [batch_size, len_q, d_model]\\n return nn.LayerNorm(d_model).to(device)(output + residual), attn\\n\\n\\n#### Mask Creation\\n\\n<Center>\\n<img src=\\"/assets/tech/088-transformer/module_that_use_mask.png\\"/ width=\\"550\\">\\n</Center>\\n\\n<p/>\\n<center></center>\\n\\n##### get_attn_pad_mask\\n\\npython-1\\ndef get_attn_pad_mask(seq_q, seq_k, pad=0):\\n \'\'\'\\n seq_q: [batch_size, seq_len]\\n seq_k: [batch_size, seq_len]\\n seq_len could be src_len or it could be tgt_len\\n seq_len in seq_q and seq_len in seq_k maybe not equal\\n \'\'\'\\n batch_size, len_q = seq_q.size()\\n batch_size, len_k = seq_k.size()\\n # eq(pad) is PAD token\\n # [batch_size, 1, len_k], True is masked\\n pad_attn_mask = seq_k.eq(pad).unsqueeze(1)\\n # [batch_size, len_q, len_k]\\n return pad_attn_mask.expand(batch_size, len_q, len_k)\\n\\n\\nTo apply get_attn_pad_mask, input that we want to mask will be fed into the second argument.\\n\\n##### get_attn_subsequence_mask\\n\\npython\\ndef get_attn_subsequence_mask(seq):\\n \'\'\'\\n seq: [batch_size, tgt_len]\\n \'\'\'\\n attn_shape = [seq.size(0), seq.size(1), seq.size(1)]\\n # Upper triangular matrix\\n subsequence_mask = np.triu(np.ones(attn_shape), k=1)\\n subsequence_mask = torch.from_numpy(subsequence_mask).byte()\\n return subsequence_mask # [batch_size, tgt_len, tgt_len]\\n\\n\\n##### How will Attention Mask be Used\\n\\nIn get_attn_pad_mask, seq_q means a sequence of indexes, i.e, $\\\\texttt{seq_q}\\\\in \\\\mathbb N^{\\\\texttt{max_seq_len}}$, which is to be embedded into $(\\\\mathbb R^{d_\\\\text{model}})^{\\\\texttt{max_seq_len}}$ and form queries $Q\\\\in \\\\mathbb R^{q\\\\times d_\\\\text{model}}$, the same is true for seq_k and $K\\\\in \\\\mathbb R^{k\\\\times d_\\\\text{model}}$, they will be multiplied together to get\\n\\n$$\\nQ K^T\\\\in\\\\mathbb R^{q\\\\times k}\\n$$\\n\\nfor scaled dot-product attention.\\n\\n<Center>\\n<img src=\\"/assets/tech/088-transformer/scaled-dot-product-attention.png\\"/ width=\\"400\\">\\n</Center>\\n\\n<p/>\\n<center></center>\\n\\n##### On Padding Logic\\n\\nNote that our padding logic just depends on seq_k and pad (see line 12), seq_q is only used to expand the number of rows to match the dimension of $QK^T$ (see line 14) in order to apply the mask.\\n\\nOur mask will be applied right before applying softmax, i.e., we eventually get scores in rows.\\n\\nDenote $d_k$ the dimension in attention module, where\\n\\n$$\\nd_k=\\n\\\\begin{cases}\\nd_\\\\text{model} & \\\\text{for }\\\\texttt{ScaledDotProductAttention,}\\\\\\\\\\nd_\\\\text{model}\\\\,/\\\\!\\\\!/\\\\,d_\\\\text{heads} & \\\\text{for }\\\\texttt{MultiHeadAttention}.\\n\\\\end{cases}\\n$$\\n\\nWe get the scores (the attention) by soft-maxing along the last dimension:\\n\\n$$\\n\\\\texttt{attn} = \\\\texttt{softmax}\\\\bigg[\\\\frac{\\\\texttt{masked_fill_}(QK^T, \\\\,\\\\texttt{attn_pad_mask})\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!}{\\\\sqrt{d_k}}\\\\bigg] \\\\in \\\\mathbb R^{q\\\\times k}.\\n$$\\n\\nThe matrix $\\\\texttt{attn}$ can be decomposed as $A\\\\times 0$ for some $A\\\\in \\\\mathbb R^{q\\\\times k_1}$ and $0\\\\in \\\\mathbb R^{q\\\\times k_2}$, where $k_1+k_2=d_k$ with $k_2$ being the number of paddings counted from the end of the sentence. Our attention will be the linear span of values (the rows) in $V\\\\in \\\\mathbb R^{k\\\\times d_k}$:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\texttt{context}\\n& = \\\\texttt{attn} \\\\times V\\\\\\\\\\n& = (V^T [\\\\texttt{attn}]^T)^T\\\\\\\\\\n& =\\n\\\\left(\\n\\\\begin{bmatrix}\\n|&\\\\cdots&| \\\\\\\\\\nv_1&\\\\ddots & v_k \\\\\\\\\\n| &\\\\cdots & | \\\\\\\\\\n\\\\end{bmatrix}\\n\\\\cdot\\n\\\\begin{bmatrix}\\nA^T \\\\\\\\\\n0\\n\\\\end{bmatrix}\\n\\\\right)^T \\\\\\\\\\n& = \\\\left(\\n\\\\begin{bmatrix}\\n|&\\\\cdots&| \\\\\\\\\\nv_1&\\\\ddots & v_{k_1}\\\\\\\\\\n| &\\\\cdots & | \\\\\\\\\\n\\\\end{bmatrix}\\n\\\\cdot A^T \\\\right)^T \\\\in \\\\mathbb R^{q\\\\times d_k}\\n\\\\end{aligned}\\n$$\\n\\nThe last few rows of $V$ ($v_i$ for $k_1\\\\leq i<d_k$) has no contribution to the context.\\n\\nNote that in each flow of computation we are focusing on one sentence with multiple indexes. The last few $v_i$\'s are value from the padding position that we are not concerned about.\\n\\n#### Encoder Layer and Encoder\\n\\n<Center>\\n<img src=\\"/assets/tech/088-transformer/Encoder.png\\" width=\\"550\\">\\n</Center>\\n\\n<p/>\\n<center></center>\\n\\nIn transformer an Encoder is a iteration of several EncoderLayer:\\n\\n##### EncoderLayer\\n\\npython\\nclass EncoderLayer(nn.Module):\\n def __init__(self):\\n super(EncoderLayer, self).__init__()\\n self.enc_self_attn = MultiHeadAttention()\\n self.pos_ffn = PoswiseFeedForwardNet()\\n\\n def forward(self, enc_inputs, enc_self_attn_mask):\\n \'\'\'\\n enc_inputs: [batch_size, src_len, d_model]\\n enc_self_attn_mask: [batch_size, src_len, src_len]\\n \'\'\'\\n # enc_outputs: [batch_size, src_len, d_model],\\n # attn: [batch_size, n_heads, src_len, src_len]\\n enc_outputs, attn = self.enc_self_attn(\\n # enc_inputs to same Q,K,V\\n enc_inputs,\\n enc_inputs,\\n enc_inputs,\\n enc_self_attn_mask\\n )\\n # enc_outputs: [batch_size, src_len, d_model]\\n enc_outputs = self.pos_ffn(enc_outputs)\\n return enc_outputs, attn\\n\\n\\nHere the enc_self_attn_mask is usually just a mask that masks out the value contributed from padding (prevent the value from padding from joining the calculation).\\n\\n##### Encoder\\n\\npython\\nclass Encoder(nn.Module):\\n def __init__(self):\\n super(Encoder, self).__init__()\\n self.src_emb = nn.Embedding(src_vocab_size, d_model)\\n self.pos_emb = PositionalEncoding(d_model)\\n self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\\n\\n def forward(self, enc_inputs):\\n \'\'\'\\n enc_inputs: [batch_size, src_len]\\n \'\'\'\\n # [batch_size, src_len, d_model]:\\n enc_outputs = self.src_emb(enc_inputs)\\n # [batch_size, src_len, d_model]:\\n enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1)\\n # [batch_size, src_len, src_len]:\\n enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)\\n enc_self_attns = []\\n for layer in self.layers:\\n # enc_outputs: [batch_size, src_len, d_model],\\n # enc_self_attn: [batch_size, n_heads, src_len, src_len]\\n enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)\\n enc_self_attns.append(enc_self_attn)\\n # enc_self_attns is only for graph-plotting purpose:\\n return enc_outputs, enc_self_attns\\n\\n\\n#### Decoder Layer and Decoder\\n\\n<Center>\\n<img src=\\"/assets/tech/088-transformer/Decoder.png\\" width=\\"550\\">\\n</Center>\\n\\n<p/>\\n<center></center>\\n\\n##### DecoderLayer\\n\\npython\\nclass DecoderLayer(nn.Module):\\n def __init__(self):\\n super(DecoderLayer, self).__init__()\\n self.dec_self_attn = MultiHeadAttention()\\n self.dec_enc_attn = MultiHeadAttention()\\n self.pos_ffn = PoswiseFeedForwardNet()\\n\\n def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask):\\n \'\'\'\\n dec_inputs: [batch_size, tgt_len, d_model]\\n enc_outputs: [batch_size, src_len, d_model]\\n dec_self_attn_mask: [batch_size, tgt_len, tgt_len]\\n dec_enc_attn_mask: [batch_size, tgt_len, src_len]\\n \'\'\'\\n # dec_outputs: [batch_size, tgt_len, d_model],\\n # dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]\\n dec_outputs, dec_self_attn = self.dec_self_attn(\\n dec_inputs,\\n dec_inputs,\\n dec_inputs,\\n dec_self_attn_mask\\n )\\n # dec_outputs: [batch_size, tgt_len, d_model],\\n # dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\\n dec_outputs, dec_enc_attn = self.dec_enc_attn(\\n dec_outputs,\\n enc_outputs,\\n enc_outputs,\\n dec_enc_attn_mask\\n )\\n # [batch_size, tgt_len, d_model]\\n dec_outputs = self.pos_ffn(dec_outputs)\\n return dec_outputs, dec_self_attn, dec_enc_attn\\n\\n\\n##### Decoder\\n\\npython-1\\nclass Decoder(nn.Module):\\n def __init__(self):\\n super(Decoder, self).__init__()\\n self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)\\n self.pos_emb = PositionalEncoding(d_model)\\n self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\\n\\n def forward(self, dec_inputs, enc_inputs, enc_outputs):\\n \'\'\'\\n dec_inputs: [batch_size, tgt_len]\\n enc_intpus: [batch_size, src_len]\\n enc_outputs: [batch_size, src_len, d_model]\\n \'\'\'\\n\\n # [batch_size, tgt_len, d_model]:\\n dec_outputs = self.tgt_emb(dec_inputs)\\n # [batch_size, tgt_len, d_model]:\\n dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1))\\\\\\n .transpose(0, 1)\\\\\\n .to(device)\\n\\n # [batch_size, tgt_len, tgt_len]:\\n dec_self_attn_pad_mask = get_attn_pad_mask(\\n dec_inputs,\\n dec_inputs\\n ).to(device)\\n\\n # [batch_size, tgt_len, tgt_len]:\\n dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(device)\\n # [batch_size, tgt_len, tgt_len]:\\n dec_self_attn_mask = torch.gt(\\n (dec_self_attn_pad_mask + dec_self_attn_subsequence_mask),\\n 0\\n ).to(device)\\n\\n # [batc_size, tgt_len, src_len]:\\n dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)\\n\\n dec_self_attns, dec_enc_attns = [], []\\n for layer in self.layers:\\n # dec_outputs: [batch_size, tgt_len, d_model],\\n # dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len],\\n # dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]\\n dec_outputs, dec_self_attn, dec_enc_attn = layer(\\n dec_outputs,\\n enc_outputs,\\n dec_self_attn_mask,\\n dec_enc_attn_mask\\n )\\n dec_self_attns.append(dec_self_attn)\\n dec_enc_attns.append(dec_enc_attn)\\n return dec_outputs, dec_self_attns, dec_enc_attns\\n\\n\\n##### Why Feed enc_inputs into Decoder?\\n\\n- From the structural graph of transformer it may seem weird to also feed enc_inputs into the Decoder.\\n\\n- In fact, enc_inputs is only used in the creation of padding mask (see line 37 of the Decoder code block) which help ignore the last few rows of the context value matrix in the enc_outputs (think of it as another form of embedding from the original sequence of word indexes).\\n\\n#### The Transformer\\n\\npython\\nclass Transformer(nn.Module):\\n def __init__(self):\\n super(Transformer, self).__init__()\\n self.encoder = Encoder().to(device)\\n self.decoder = Decoder().to(device)\\n self.projection = nn.Linear(\\n d_model, tgt_vocab_size, bias=False\\n ).to(device)\\n\\n def forward(self, enc_inputs, dec_inputs):\\n \'\'\'\\n enc_inputs: [batch_size, src_len]\\n dec_inputs: [batch_size, tgt_len]\\n \'\'\'\\n # tensor to store decoder outputs\\n # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\\n\\n # enc_outputs: [batch_size, src_len, d_model]\\n # enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]\\n enc_outputs, enc_self_attns = self.encoder(enc_inputs)\\n # dec_outpus: [batch_size, tgt_len, d_model]\\n # dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len]\\n # dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]\\n dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(\\n dec_inputs,\\n enc_inputs,\\n enc_outputs\\n )\\n # dec_logits: [batch_size, tgt_len, tgt_vocab_size]\\n dec_logits = self.projection(dec_outputs)\\n return (\\n dec_logits.view(-1, dec_logits.size(-1)),\\n enc_self_attns,\\n dec_self_attns,\\n dec_enc_attns\\n )\\n\\n\\n- Note that dec_logits will be of shape [batch_size, tgt_max_len, tgt_vocab_size].\\n- dec_logits.view(-1, dec_logits.size(-1)) will be of shape\\n none\\n [batch_size*tgt_max_len, tgt_vocab_size]\\n \\n\\n#### Training: Naive Dataset\\n\\n##### Dataset and data_loader\\n\\nLet\'s get a rough feeling from a naive dataset:\\n\\npython\\nsentences = [\\n # enc_input dec_input dec_output\\n [\'ich mochte ein bier <P>\', \'<sos> i want a beer .\', \'i want a beer . <eos>\'],\\n [\'ich mochte ein cola <P>\', \'<sos> i want a coke .\', \'i want a coke . <eos>\']\\n]\\n\\n\\nIt is not necessary to write <P> in the dataset, we can pad our sequence of word indexes by 0\'s in our dataset pipeline.\\n\\nNext we define our data_loader:\\n\\npython\\nsrc_word_index = {\'<P>\': 0, \'ich\': 1,\\n \'mochte\': 2, \'ein\': 3, \'bier\': 4, \'cola\': 5}\\nsrc_vocab_size = len(src_word_index)\\n\\ntgt_word_index = {\'<P>\': 0, \'i\': 1, \'want\': 2, \'a\': 3,\\n \'beer\': 4, \'coke\': 5, \'<sos>\': 6, \'<eos>\': 7, \'.\': 8}\\nsrc_index_word = {i: w for i, w in enumerate(src_word_index)}\\ntgt_index_word = {i: w for i, w in enumerate(tgt_word_index)}\\ntgt_vocab_size = len(tgt_word_index)\\n\\nsrc_len = 5 # enc_input max sequence length\\ntgt_len = 6 # dec_input(=dec_output) max sequence length\\n\\n\\ndef make_data(sentences):\\n enc_inputs, dec_inputs, dec_outputs = [], [], []\\n for i in range(len(sentences)):\\n enc_input_, dec_input_, dec_output_ = sentences[i]\\n\\n enc_input = [src_word_index[n] for n in enc_input_.split()]\\n # ^^^^ [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]\\n\\n dec_input = [tgt_word_index[n] for n in dec_input_.split()]\\n # ^^^^ [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]\\n\\n dec_output = [tgt_word_index[n] for n in dec_output_.split()]\\n # ^^^^ [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]\\n\\n enc_inputs.append(enc_input)\\n dec_inputs.append(dec_input)\\n dec_outputs.append(dec_output)\\n\\n return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)\\n\\n\\nclass MyDataSet(Data.Dataset):\\n def __init__(self):\\n super(MyDataSet, self).__init__()\\n enc_inputs, dec_inputs, dec_outputs = make_data(sentences)\\n self.enc_inputs = enc_inputs\\n self.dec_inputs = dec_inputs\\n self.dec_outputs = dec_outputs\\n\\n def __len__(self):\\n return self.enc_inputs.shape[0]\\n\\n def __getitem__(self, idx):\\n return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]\\n\\n\\ndata_loader = Data.DataLoader(MyDataSet(), batch_size=2, shuffle=True)\\n\\n\\n##### Train to see if the Model can Overfit\\n\\npython\\ndef train():\\n transformer = Transformer().to(device)\\n criterion = nn.CrossEntropyLoss(ignore_index=0)\\n optimizer = optim.SGD(transformer.parameters(), lr=1e-3, momentum=0.99)\\n epochs = 30\\n\\n for epoch in range(epochs):\\n for enc_inputs, dec_inputs, dec_outputs in data_loader:\\n \'\'\'\\n enc_inputs: [batch_size, src_len]\\n dec_inputs: [batch_size, tgt_len]\\n dec_outputs: [batch_size, tgt_len]\\n \'\'\'\\n enc_inputs = enc_inputs.to(device)\\n dec_inputs = dec_inputs.to(device)\\n dec_outputs = dec_outputs.to(device)\\n\\n outputs, enc_self_attns, dec_self_attns, dec_enc_attns = transformer(\\n enc_inputs,\\n dec_inputs\\n )\\n loss = criterion(outputs, dec_outputs.view(-1))\\n print(\'Epoch:\', \'%04d\' % (epoch + 1),\\n \'loss =\', \'{:.6f}\'.format(loss))\\n\\n optimizer.zero_grad()\\n loss.backward()\\n optimizer.step()\\n\\n state_dict = transformer.state_dict()\\n torch.save(state_dict, os.path.join(\\"pths\\", f\\"model_epoch_{epochs}.pth\\"))\\n\\n\\nUpon executing the training script, our model can indeed overfit the naive dataset:\\n\\nnone\\nEpoch: 0001 loss = 2.579853\\nEpoch: 0002 loss = 2.416333\\nEpoch: 0003 loss = 2.135146\\nEpoch: 0004 loss = 1.839263\\nEpoch: 0005 loss = 1.538042\\nEpoch: 0006 loss = 1.303803\\nEpoch: 0007 loss = 1.134294\\nEpoch: 0008 loss = 0.898169\\nEpoch: 0009 loss = 0.751822\\nEpoch: 0010 loss = 0.609819\\nEpoch: 0011 loss = 0.477917\\nEpoch: 0012 loss = 0.374170\\nEpoch: 0013 loss = 0.280897\\nEpoch: 0014 loss = 0.225249\\nEpoch: 0015 loss = 0.175177\\nEpoch: 0016 loss = 0.143766\\nEpoch: 0017 loss = 0.139490\\nEpoch: 0018 loss = 0.114785\\nEpoch: 0019 loss = 0.090137\\nEpoch: 0020 loss = 0.087496\\nEpoch: 0021 loss = 0.076527\\nEpoch: 0022 loss = 0.069732\\nEpoch: 0023 loss = 0.061058\\nEpoch: 0024 loss = 0.055305\\nEpoch: 0025 loss = 0.042023\\nEpoch: 0026 loss = 0.040661\\nEpoch: 0027 loss = 0.039919\\nEpoch: 0028 loss = 0.023459\\nEpoch: 0029 loss = 0.022219\\nEpoch: 0030 loss = 0.027067\\n\\n\\n##### Define Translator\\n\\nWe predict the target result word by word as in the teacher forcing approach in our training process.\\n\\npython\\nclass Translator():\\n def __init__(self, transformer: Transformer):\\n self.transformer = transformer\\n\\n def translate(self, enc_input, start_index):\\n dec_input = torch.zeros(1, 0).type_as(enc_input)\\n terminated = False\\n next_tgt_word_index = start_index\\n while not terminated:\\n dec_input = torch.cat(\\n [\\n dec_input.detach(),\\n torch.tensor([[next_tgt_word_index]],dtype=enc_input.dtype).to(device)\\n ],\\n -1\\n )\\n dec_output_logits, _, _, _= self.transformer(enc_input, dec_input)\\n next_tgt_word_index = torch.argmax(dec_output_logits[-1])\\n\\n if next_tgt_word_index == tgt_word_index[\\".\\"]:\\n terminated = True\\n\\n print(\\"next_word\\", tgt_index_word[next_tgt_word_index.item()])\\n\\n # remove batch, remove <sos>\\n return dec_input.squeeze(0)[1:]\\n\\n\\n##### Translation Script\\n\\nNow we test our function by using the script:\\n\\npython\\ntransformer = Transformer().to(device)\\nmodel_path = \\"pths/model_epoch_30.pth\\"\\n\\nif model_path is not None:\\n transformer.load_state_dict(torch.load(model_path))\\n\\ntranslator = Translator(transformer)\\nenc_inputs, _, _ = next(iter(data_loader))\\nenc_inputs = enc_inputs.to(device)\\n# e.g. enc_inputs = tensor([\\n# [1, 2, 3, 4, 0], [1, 2, 3, 5, 0]\\n# ], device=\'cuda:0\')\\nfor i in range(len(enc_inputs)):\\n enc_input = enc_inputs[i]\\n sentence = \\" \\".join([src_index_word[i.item()] for i in enc_input])\\n print(\\"source sentence:\\", sentence)\\n predict = translator.translate(\\n enc_input.unsqueeze(0), # expand as batch\\n start_index=tgt_word_index[\\"<sos>\\"]\\n )\\n print(enc_input, \'->\', [tgt_index_word[n.item()]\\n for n in predict.squeeze()])\\n\\n\\nyields\\n\\nnone\\nsource sentence: ich mochte ein bier <P>\\nnext_word i\\nnext_word want\\nnext_word a\\nnext_word beer\\nnext_word .\\ntensor([1, 2, 3, 4, 0], device=\'cuda:0\') -> [\'i\', \'want\', \'a\', \'beer\']\\nsource sentence: ich mochte ein cola <P>\\nnext_word i\\nnext_word want\\nnext_word a\\nnext_word coke\\nnext_word .\\ntensor([1, 2, 3, 5, 0], device=\'cuda:0\') -> [\'i\', \'want\', \'a\', \'coke\']\\n\\n","title":"Transformer 1: The Model Definition and Naive Training Dataset for Machine Translation","date":"2022-08-11T00:00:00.000Z","id":"blog088","tag":"pytorch, deep-learning","intro":"Record the basic building block and structure of a Transformer. The main reference of this article is this <i><a href=\'https://wmathor.com/index.php/archives/1438/\'> blog post</a></i>."},{"content":"\\n#### Training: Real World Dataset\\n\\n##### Repository\\n\\nThe code block below are partially taken from my repo:\\n\\n- https://github.com/machingclee/2022-07-04-transformer-from-scratch\\n\\n##### New Practice from torchtext 0.10.1 onwards\\n\\nOld NLP material in pytorch usually import the following:\\n\\npython\\n# or from torchtext.legacy.data\\nfrom torchtext.data import Field, Field, BucketIterator, TabularDataset\\n\\n\\nbut these are completely removed from torchtext 0.10.1 onwards. It seems that the official pytorch suggests users creating their dataset via _traditional_ classes:\\n\\npython\\nfrom torch.utils.data import Dataset, DataLoader\\n\\n\\n##### spacy package\\n\\n- We will rely on a package called spacy for tokenization. We will need this in our Corpus class, which is called implicitly in get_tokenizer imported from torchtext.data.utils.\\n\\n- We install the spacy and the desired lanauges by following the guide in https://spacy.io/usage.\\n\\n##### Download Source of EN-JP Dataset\\n\\nInterested reader can download it from: https://nlp.stanford.edu/projects/jesc/\\n\\n##### Prepare Dataset Pipeline\\n\\nSuppose that our dataset is a txt file in which every line is an English sentence paired with its Japanese translation, separated by \\\\t:\\n\\nnone\\nor the relative risk of drugs\\t\u307b\u307c\u7121\u95a2\u4fc2\u3067\u3059\\ngail, are you drunk?\\t\u30b2\u30a4\u30eb \u9154\u3063\u3066\u308b?\\nbe careful.\\t\u6ce8\u610f\u3057\u308d\\n...\\n\\n\\nLet\'s first define our Corpus class that iterates the dataset to provide these sentences.\\n\\nNote that get_src_sentence_iter and get_tgt_sentence_iter **_below_** are dependent on the structure of the dataset.\\n\\npython\\nimport pickle\\nimport torch\\nfrom collections import Counter\\nfrom random import shuffle\\nfrom torchtext.data.utils import get_tokenizer\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom torchtext.vocab import vocab, Vocab\\nfrom src.device import device\\nfrom src import config\\nfrom typing import Optional\\n\\nclass Corpus:\\n def __init__(\\n self,\\n src_lang=\\"en_core_web_sm\\",\\n tgt_lang=\\"ja_core_news_sm\\",\\n delimiter=\\"\\\\t\\",\\n src_vocab: Optional[Vocab] = None,\\n tgt_vocab: Optional[Vocab] = None\\n ):\\n # the lang keys are used in defining \\"field object\\"\\n # which is exactly the csv\'s header, the column name, the json key, etc.\\n self.delimiter=delimiter\\n\\n self.src_tokenizer = get_tokenizer(\\"spacy\\", language=src_lang)\\n self.tgt_tokenizer = get_tokenizer(\\"spacy\\", language=tgt_lang)\\n\\n src_counter = Counter()\\n tgt_counter = Counter()\\n\\n if src_vocab is not None and tgt_vocab is not None:\\n self.src_vocab = src_vocab\\n self.tgt_vocab = tgt_vocab\\n else:\\n for src_line in self.get_src_sentence_iter():\\n src_counter.update(self.src_tokenizer(src_line))\\n for tgt_line in self.get_tgt_sentence_iter():\\n tgt_counter.update(self.tgt_tokenizer(tgt_line))\\n\\n # for label, line in\\n self.src_vocab = vocab(\\n src_counter,\\n min_freq=2,\\n specials=(\'<ukn>\', \'<pad>\')\\n )\\n self.tgt_vocab = vocab(\\n tgt_counter,\\n min_freq=2,\\n specials=(\'<ukn>\', \'<sos>\', \'<eos>\', \'<pad>\')\\n )\\n\\n def get_src_sentence_iter(self):\\n with open(config.data_path, encoding=\\"utf-8\\") as f:\\n for line in f:\\n src_line, _ = line.split(self.delimiter)\\n yield src_line\\n\\n def get_tgt_sentence_iter(self):\\n with open(config.data_path, encoding=\\"utf-8\\") as f:\\n for line in f:\\n _, tgt_line = line.split(self.delimiter)\\n yield tgt_line\\n\\n def save_vocabs(self):\\n vocabs = {\\n \\"src\\": self.src_vocab,\\n \\"tgt\\": self.tgt_vocab\\n }\\n for lang, vocab in vocabs.items():\\n with open(f\\"{lang}.pickle\\", \'wb+\') as handle:\\n pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\\n\\n\\nSince creating vocabs involve accessing to all sentences in a dataset, it can take quite long and it is worth saving the vocabs, which include\\n\\n- index_to_word (obtained by Vocab.get_itos()) and\\n- word_to_index (obtained by Vocab.get_stoi())\\n somewhere else so that we can reuse later without iterating the whole dataset again, that\'s why we have the save_vocabs method.\\n\\nAfter saving the vocabs into pickle files, we can retrieve it by the class:\\n\\npython\\nclass Vocabs:\\n src_vocab = None\\n tgt_vocab = None\\n\\n def __init__(self, src_vocab_pickle_path, tgt_vocab_pickle_path):\\n self.src_vocab_pickle_path = src_vocab_pickle_path\\n self.tgt_vocab_pickle_path = tgt_vocab_pickle_path\\n\\n def get_src_vocab(self) -> Vocab:\\n if Vocabs.src_vocab is None:\\n with open(self.src_vocab_pickle_path, \'rb\') as handle:\\n Vocabs.src_vocab = pickle.load(handle)\\n\\n return Vocabs.src_vocab\\n\\n def get_tgt_vocab(self):\\n if Vocabs.tgt_vocab is None:\\n with open(self.tgt_vocab_pickle_path, \'rb\') as handle:\\n Vocabs.tgt_vocab = pickle.load(handle)\\n\\n return Vocabs.tgt_vocab\\n\\n\\nNow our NLP dataset dedicated to Transformer becomes:\\n\\npython\\nclass TransformerDataset(Dataset):\\n def __init__(self, corpus: Corpus):\\n self.corpus = corpus\\n self.src_sentences = list(self.corpus.get_src_sentence_iter())\\n self.tgt_sentences = list(self.corpus.get_tgt_sentence_iter())\\n\\n def __getitem__(self, index):\\n src_text = self.src_sentences[index]\\n tgt_text = self.tgt_sentences[index]\\n\\n src_stoi = self.corpus.src_vocab.get_stoi()\\n tgt_stoi = self.corpus.tgt_vocab.get_stoi()\\n\\n src_tokens= self.corpus.src_tokenizer(src_text)\\n tgt_tokens = self.corpus.tgt_tokenizer(tgt_text)\\n\\n src_pad_len = config.src_max_len - len(src_tokens)\\n tgt_pad_len = config.tgt_max_len - len(tgt_tokens)\\n\\n if src_pad_len > 0:\\n src_idxes = [src_stoi.get(token, src_stoi[\\"<ukn>\\"]) for token in src_tokens] + [src_stoi[\\"<pad>\\"]] * src_pad_len\\n else:\\n src_idxes = [src_stoi.get(token, src_stoi[\\"<ukn>\\"]) for token in src_tokens[:config.src_max_len]]\\n\\n if tgt_pad_len > 0:\\n tgt_idxes = [tgt_stoi[\'<sos>\']] + \\\\\\n [tgt_stoi.get(token, src_stoi[\\"<ukn>\\"]) for token in tgt_tokens] + \\\\\\n [tgt_stoi[\'<eos>\']] + \\\\\\n [tgt_stoi[\\"<pad>\\"]] * tgt_pad_len\\n else:\\n tgt_idxes = [tgt_stoi[\'<sos>\']] + \\\\\\n [tgt_stoi.get(token, src_stoi[\\"<ukn>\\"]) for token in tgt_tokens[:config.tgt_max_len]] + \\\\\\n [tgt_stoi[\'<eos>\']]\\n\\n return torch.as_tensor(src_idxes, device=device), torch.as_tensor(tgt_idxes, device=device)\\n\\n\\n def __len__(self):\\n return len(self.src_sentences)\\n\\n\\n##### Training Script\\n\\nAccording to our dataset output we change our training script as follows, we always feed our model by \\"indexed\\" version of our **_naive dataset_**.\\n\\npython\\nconsole_log = ConsoleLog(lines_up_on_end=1)\\n\\ndef train(\\n epochs=10,\\n use_saved_vocab=False,\\n learning_rate=1e-3\\n):\\n if use_saved_vocab:\\n vocabs = Vocabs(src_vocab_pickle_path=\\"src.pickle\\", tgt_vocab_pickle_path=\\"tgt.pickle\\")\\n corpus = Corpus(src_vocab=vocabs.get_src_vocab(), tgt_vocab=vocabs.get_tgt_vocab())\\n else:\\n corpus = Corpus()\\n corpus.save_vocabs()\\n\\n src_vocab_size = len(corpus.src_vocab.get_stoi())\\n tgt_vocab_size = len(corpus.tgt_vocab.get_stoi())\\n\\n transformer = Transformer(\\n src_vocab_size=src_vocab_size,\\n tgt_vocab_size=tgt_vocab_size\\n ).to(device)\\n\\n criterion = nn.CrossEntropyLoss(ignore_index=0)\\n optimizer = optim.Adamax(transformer.parameters(), lr=learning_rate)\\n dataset = TransformerDataset(corpus)\\n data_loader = DataLoader(dataset=dataset,\\n batch_size=config.batch_size,\\n shuffle=True\\n )\\n # when arrived this step, pickle file must have been saved\\n vocabs = Vocabs(src_vocab_pickle_path=\\"src.pickle\\", tgt_vocab_pickle_path=\\"tgt.pickle\\")\\n src_vocabs = vocabs.get_src_vocab()\\n tgt_vocabs = vocabs.get_tgt_vocab()\\n\\n for epoch in range(epochs):\\n\\n for batch_id, (src_idxes, tgt_idxes) in enumerate(tqdm(data_loader)):\\n batch_id += 1\\n enc_inputs = src_idxes.to(device)\\n dec_inputs = tgt_idxes[:, :-1].to(device)\\n dec_outputs = tgt_idxes[:, 1:].to(device)\\n\\n outputs, _, _, _ = transformer(\\n enc_inputs,\\n dec_inputs\\n )\\n\\n loss = criterion(outputs, dec_outputs.flatten())\\n\\n with torch.no_grad():\\n console_log.print([\\n (\\"loss\\", loss.item())\\n ])\\n if batch_id % config.visualize_result_per_epochs == 0:\\n visualize(transformer,\\n enc_inputs[0],\\n src_vocabs,\\n tgt_vocabs\\n )\\n\\n optimizer.zero_grad()\\n loss.backward()\\n optimizer.step()\\n\\n state_dict = transformer.state_dict()\\n torch.save(state_dict, os.path.join(\\"pths\\", f\\"model_epoch_{epoch}.pth\\"))\\n\\n\\n##### New Translator\\n\\nNext we build our translator based on the vocab objects:\\n\\npython\\nclass Translator():\\n def __init__(self, transformer: Transformer):\\n self.transformer = transformer\\n\\n def translate_input_index(self, enc_input, src_start_index, tgt_word_index, tgt_index_word):\\n dec_input = torch.zeros(1, 0).type_as(enc_input)\\n terminated = False\\n next_tgt_word_index = src_start_index\\n word_count = 0\\n while not terminated:\\n dec_input = torch.cat(\\n [\\n dec_input.detach(),\\n torch.tensor([[next_tgt_word_index]],dtype=enc_input.dtype).to(device)\\n ],\\n -1\\n )\\n word_count += 1\\n dec_output_logits, _, _, _= self.transformer(enc_input, dec_input)\\n next_tgt_word_index = torch.argmax(dec_output_logits[-1])\\n\\n if next_tgt_word_index == tgt_word_index[\\"<eos>\\"] or word_count == config.tgt_max_len + 1:\\n terminated = True\\n\\n # remove batch, remove <sos>\\n return dec_input.squeeze(0)[1:]\\n\\n","title":"Transformer 2: A More in Depth Training with Real World Dataset Using Modern NLP Dataset Pipeline in Pytorch","date":"2022-08-17T00:00:00.000Z","id":"blog089","tag":"pytorch, deep-learning","intro":"Continuation of the previous blog post on transformer, discuss more modern pipeline for training a transformer (or any NLP task in general) using the latest torchtext."},{"content":"\\n#### Points to note, will be organized later\\n\\n- To create a project component:\\n text\\n ng new angular-crash-course\\n \\n- To create a new component:\\n text\\n ng generate component components/button\\n \\n- To create a new service that is to be used by a component:\\n text\\n ng generate service services/task\\n \\n- For loop in angular\'s xxx.component.html\\n javascript\\n <div *ngFor=\\"let task of tasks\\"> {{ task.text }} </div>\\n \\n- Pass object as a props:\\n javascript\\n <app-task *ngFor=\\"let task of tasks\\" [task]=\\"task\\"> </app-task>\\n \\n- javascript\\n import { Observable, of } from \\"rxjs\\";\\n \\n- **json server.**\\n After yarn add json-server we add the following in package.json as our script:\\n text\\n \\"server\\": \\"json-server --watch db.json --port 5000\\"\\n \\n- import { HttpClient, HttpHeaders } from \\"@angular/common/http\\"\\n then\\n javascript\\n import { HttpClientModule } from \\"@angular/common/http\\";\\n \\n and add that into app.module.ts\'s imports field. Next we can add HttpClient in the TaskService, then we can call the get method to get Observable.\\n- To call a function in a view model, we use <div (click)=\\"onClick()\\">, where onClick is a method in xxx.component.ts, the view model.\\n- Suppose we have\\n javascript\\n import { Input, Output, EventEmitter } from \\"@angular/core\\";\\n \\n then:\\n - @Input() is used to define **_object_** as a props from parent to its child.\\n - @Output() is used to define **_function_** that pass data from child to parent, which is usually an EventEmitter<T>.\\n - Suppose that we have defined\\n javascript\\n @Output() onDeleteTask: EventEmitter<Task> = new EventEimitter()\\n \\n inside <app-task-item></app-task-item>, then we can define\\n js\\n deleteTask(task) { this.onDeleteTask.emit(task)}\\n \\n inside that item component.\\n - In task-item (child) level, we can define (click)=deleteTask(task), which will emit the event that ship with data task.\\n - In tasks (parent) level, we can define <app-tasks (onDeleteTask)=doSth(task)></app-tasks> to subscribe for the Event.\\n- We add conditional class reminder based on shouldRemind: boolean:\\n js\\n <div class=\\"task\\" [ngClass]=\\"{ reminder: shouldRemind }\\">...</div>\\n \\n Once shouldRemind === true, the div element will be of class task reminder, otherwise it is merely be of class task.\\n- Apart from the usual EventEmitter: (click), we have (dblclick) for double-click.\\n- To enable two-way data binding (bind data between both view-model and UI), we:\\n js\\n // src/app/app.module.ts\\n import { BrowserModule } from \\"@angular/platform-browser\\";\\n ...\\n @NgModule({\\n ...\\n imports: [\\n BrowserModule,\\n FormsModule\\n ],\\n ...\\n })\\n \\n Then we can use ngModel directive as follows:\\n jsx\\n <input [(ngModel)]=\\"text\\"/>\\n <div>\\n {{ text }}\\n </div>\\n \\n where text is an attribute defined in xxx.component.ts.\\n- To conditionally show an element, we use *ngIf=\\"classAttribute\\".\\n","title":"Angular Fundamental","date":"2022-08-18T00:00:00.000Z","id":"blog090","tag":"angular","toc":false,"intro":"Record the basic syntax in learning angular."},{"content":"\\n#### Imports for Eninops\\n\\npython\\nimport torch\\nimport torch.nn.functional as F\\nimport matplotlib.pyplot as plt\\n\\nfrom torch import nn\\nfrom torch import Tensor\\nfrom PIL import Image\\nfrom torchvision.transforms import Compose, Resize, ToTensor\\nfrom einops import rearrange, reduce, repeat\\nfrom einops.layers.torch import Rearrange, Reduce\\n\\n\\n#### Patch Embedding\\n\\n##### Objective\\n\\nGiven an image of shape (3, 224, 224) we perform the following steps in order to produce an output whose shape is the same as what we feed into a transformer (the input should be of shape (batches, seq_len, embed_size))\\n\\n<Center>\\n<a href=\\"/assets/tech/091-VIT/004.png\\" target=\\"_blank\\">\\n <img src=\\"/assets/tech/091-VIT/004.png\\" width=\\"100%\\">\\n</a>\\n</Center>\\n<p/>\\n\\n- We first divide an image into patches by squares of size 16x16 (i.e., patch_size=16);\\n- Each of these patches (there are $14\\\\times 14=196$ of them) will be mapped into a 1-d feature vector of size $16\\\\times 16\\\\times 3=768$.\\n\\nThe two steps above are achieved by a single convolution layer (with kernel size and stride size being equal to patch_size).\\n\\nEventually after the patch embedding our feature vector will be of shape:\\n\\nnone\\n[batches, 14, 14, 768]\\n\\n\\n##### cls Token\\n\\n<Center>\\n<img src=\\"/assets/tech/091-VIT/001.png\\" width=\\"560\\">\\n</Center>\\n<p/>\\n<center></center>\\n\\nThe picture above is taken from the original paper of ViT, we can notice there are 10 \\"tokens\\" insteand of 9. The additional one called **_cls token_** is added at the beginning as a learning parameter which has the same embedding size as our patched embedding feature vectors.\\n\\nAt the first iteration of transformer encoder block, the cls token already learned the information of all patches due to the self-attention mechanism. We will be doing 12 iterations, and the resulting cls token will be fed into classification head for final prediction.\\n\\nAnother prediction approch can be done **_without_** cls token, instead, we keep doing iteration of transformer encoder blocks and take an average along the _sequential dimension_ (see Reduce(\'b n e -> b e\', reduction=\'mean\') in ClassificationHead below). This process can be thought of as a global average pooling.\\n\\n##### PatchEmbedding\\n\\n###### Step 1: Divide Images into Patches and Map them into 1-d features\\n\\npython\\nclass PatchEmbedding(nn.Module):\\n def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768):\\n self.patch_size = patch_size\\n super().__init__()\\n self.projection = nn.Sequential(\\n # the conv layer can be replaced by\\n # Rearrange(\'b c (h s1) (w s2) -> b (h w) (s1 s2 c)\', s1=patch_size, s2=patch_size),\\n nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\\n Rearrange(\'b e (h) (w) -> b (h w) e\'),\\n )\\n\\n def forward(self, x: Tensor) -> Tensor:\\n x = self.projection(x)\\n return x\\n\\n\\n###### Step 2: Add cls Token\\n\\npython\\nclass PatchEmbedding(nn.Module):\\n def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768):\\n self.patch_size = patch_size\\n super().__init__()\\n self.projection = nn.Sequential(\\n # using a conv layer instead of a linear one -> performance gains\\n nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\\n Rearrange(\'b e (h) (w) -> b (h w) e\'),\\n )\\n\\n self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\\n\\n def forward(self, x: Tensor) -> Tensor:\\n b, _, _, _ = x.shape\\n x = self.projection(x)\\n cls_tokens = repeat(self.cls_token, \'() n e -> b n e\', b=b)\\n # prepend the cls token to the input\\n x = torch.cat([cls_tokens, x], dim=1)\\n return x\\n\\n\\n###### Step 3: Add PositionEmbedding\\n\\npython\\nclass PatchEmbedding(nn.Module):\\n def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224):\\n self.patch_size = patch_size\\n super().__init__()\\n self.projection = nn.Sequential(\\n # using a conv layer instead of a linear one -> performance gains\\n nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\\n Rearrange(\'b e (h) (w) -> b (h w) e\'),\\n )\\n self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\\n self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\\n\\n\\n def forward(self, x: Tensor) -> Tensor:\\n b, _, _, _ = x.shape\\n x = self.projection(x)\\n cls_tokens = repeat(self.cls_token, \'() n e -> b n e\', b=b)\\n # prepend the cls token to the input\\n x = torch.cat([cls_tokens, x], dim=1)\\n # add position embedding\\n x += self.positions\\n return x\\n\\n\\n#### Transformer Encoder\\n\\n<Center>\\n<img src=\\"/assets/tech/091-VIT/002.png\\" width=\\"200\\"/>\\n</Center>\\n\\n##### MultiheadAttention\\n\\nUnlike transformer in machine translation, we only need the encoder part without any masking before softmax.\\n\\n###### Version 1\\n\\npython\\nclass MultiHeadAttention(nn.Module):\\n def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\\n super().__init__()\\n self.emb_size = emb_size\\n self.num_heads = num_heads\\n self.keys = nn.Linear(emb_size, emb_size)\\n self.queries = nn.Linear(emb_size, emb_size)\\n self.values = nn.Linear(emb_size, emb_size)\\n self.att_drop = nn.Dropout(dropout)\\n self.projection = nn.Linear(emb_size, emb_size)\\n self.scaling = (self.emb_size // num_heads) ** -0.5\\n\\n def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\\n # split keys, queries and values in num_heads\\n queries = rearrange(self.queries(x), \\"b n (h d) -> b h n d\\", h=self.num_heads)\\n keys = rearrange(self.keys(x), \\"b n (h d) -> b h n d\\", h=self.num_heads)\\n values = rearrange(self.values(x), \\"b n (h d) -> b h n d\\", h=self.num_heads)\\n # sum up over the last axis\\n energy = torch.einsum(\'bhqd, bhkd -> bhqk\', queries, keys) # batch, num_heads, query_len, key_len\\n\\n if mask is not None:\\n fill_value = torch.finfo(torch.float32).min\\n energy.mask_fill(~mask, fill_value)\\n\\n att = F.softmax(energy, dim=-1) * self.scaling\\n att = self.att_drop(att)\\n # sum up over the third axis\\n out = torch.einsum(\'bhal, bhlv -> bhav \', att, values)\\n out = rearrange(out, \\"b h n d -> b n (h d)\\")\\n out = self.projection(out)\\n return out\\n\\n\\n###### Version 2\\n\\npython\\nclass MultiHeadAttention(nn.Module):\\n def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\\n super().__init__()\\n self.emb_size = emb_size\\n self.num_heads = num_heads\\n # fuse the queries, keys and values in one matrix\\n self.qkv = nn.Linear(emb_size, emb_size * 3)\\n self.att_drop = nn.Dropout(dropout)\\n self.projection = nn.Linear(emb_size, emb_size)\\n\\n def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\\n # split keys, queries and values in num_heads\\n qkv = rearrange(self.qkv(x), \\"b n (h d qkv) -> (qkv) b h n d\\", h=self.num_heads, qkv=3)\\n queries, keys, values = qkv[0], qkv[1], qkv[2]\\n # sum up over the last axis\\n energy = torch.einsum(\'bhqd, bhkd -> bhqk\', queries, keys) # batch, num_heads, query_len, key_len\\n\\n if mask is not None:\\n fill_value = torch.finfo(torch.float32).min\\n energy.mask_fill(~mask, fill_value)\\n\\n scaling = self.emb_size ** (1/2)\\n att = F.softmax(energy, dim=-1) / scaling\\n att = self.att_drop(att)\\n # sum up over the third axis\\n out = torch.einsum(\'bhal, bhlv -> bhav \', att, values)\\n out = rearrange(out, \\"b h n d -> b n (h d)\\")\\n out = self.projection(out)\\n return out\\n\\n\\n##### MLP\\n\\npython\\nclass FeedForwardBlock(nn.Sequential):\\n def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\\n super().__init__(\\n nn.Linear(emb_size, expansion * emb_size),\\n nn.GELU(),\\n nn.Dropout(drop_p),\\n nn.Linear(expansion * emb_size, emb_size),\\n )\\n\\n\\n##### ResidualAdd\\n\\npython\\nclass (nn.Module):\\n def __init__(self, fn):\\n super().__init__()\\n self.fn = fn\\n\\n def forward(self, x, **kwargs):\\n res = x\\n x = self.fn(x, **kwargs)\\n x += res\\n return x\\n\\n\\n##### TransformerEncoderBlock\\n\\n<Center>\\n<img src=\\"/assets/tech/091-VIT/003.png\\" width=\\"200\\"/>\\n</Center>\\n\\npython\\nclass TransformerEncoderBlock(nn.Sequential):\\n def __init__(self,\\n emb_size: int = 768,\\n drop_p: float = 0.,\\n forward_expansion: int = 4,\\n forward_drop_p: float = 0.,\\n ** kwargs):\\n super().__init__(\\n ResidualAdd(nn.Sequential(\\n nn.LayerNorm(emb_size),\\n MultiHeadAttention(emb_size, **kwargs),\\n nn.Dropout(drop_p)\\n )),\\n ResidualAdd(nn.Sequential(\\n nn.LayerNorm(emb_size),\\n FeedForwardBlock(\\n emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\\n nn.Dropout(drop_p)\\n ))\\n )\\n\\n\\n##### TransformerEncoder\\n\\npython\\nclass TransformerEncoder(nn.Sequential):\\n def __init__(self, depth: int = 12, **kwargs):\\n super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\\n\\n\\n##### ClassificationHead\\n\\npython\\nclass ClassificationHead(nn.Sequential):\\n def __init__(self, emb_size: int = 768, n_classes: int = 1000):\\n super().__init__(\\n # It is also possible to just take the first entry in the last dimension\\n Reduce(\'b n e -> b e\', reduction=\'mean\'),\\n nn.LayerNorm(emb_size),\\n nn.Linear(emb_size, n_classes)\\n )\\n\\n\\n#### Vi-sual T-ransformer\\n\\npython\\nclass ViT(nn.Sequential):\\n def __init__(self,\\n in_channels: int = 3,\\n patch_size: int = 16,\\n emb_size: int = 768,\\n img_size: int = 224,\\n depth: int = 12,\\n n_classes: int = 1000,\\n **kwargs):\\n super().__init__(\\n PatchEmbedding(in_channels, patch_size, emb_size, img_size),\\n TransformerEncoder(depth, emb_size=emb_size, **kwargs),\\n ClassificationHead(emb_size, n_classes)\\n )\\n\\n","title":"Vision Transformer with einops Implementation","date":"2022-09-06T00:00:00.000Z","id":"blog091","tag":"deep-learning, pytorch","intro":"Record the study of the basic mechanism how a transformer perform classification task on images."},{"content":"\\nThe following\\n\\npython\\ncoords_h = torch.tensor([0, 1, 2])\\ncoords_w = torch.tensor([0, 1, 2])\\nxys = torch.stack(torch.meshgrid(coords_h, coords_w)).flatten(1)\\nprint(xys)\\n\\n\\ngives\\n\\nnone\\ntensor([[0, 0, 0, 1, 1, 1, 2, 2, 2],\\n [0, 1, 2, 0, 1, 2, 0, 1, 2]])\\n\\n\\nNow we rearrange\\n\\npython\\nxys = rearrange(xys, \\"i coord -> coord i\\")\\nprint(xys)\\n\\n\\nto get\\n\\nnone\\ntensor([[0, 0],\\n [0, 1],\\n [0, 2],\\n [1, 0],\\n [1, 1],\\n [1, 2],\\n [2, 0],\\n [2, 1],\\n [2, 2]])\\n\\n","title":"Mesh Grid Trick","date":"2022-09-07T00:00:00.000Z","id":"blog092","tag":"deep-learning, pytorch","toc":false,"intro":"Record a trick to create mesh grid coordinate."},{"content":"\\n#### Repository\\n\\n- https://github.com/machingclee/2022-09-16-swin-transformer-einops-implementation/tree/main/src\\n\\n#### Basic Imports\\n\\npython\\nimport torch\\nimport torch.nn.functional as F\\nimport matplotlib.pyplot as plt\\n\\nfrom torch import nn\\nfrom torch import Tensor\\nfrom PIL import Image\\nfrom torchvision.transforms import Compose, Resize, ToTensor\\nfrom einops import rearrange, reduce, repeat\\nfrom einops.layers.torch import Rearrange, Reduce\\nfrom typing import Tuple, Optional\\n\\n\\n#### PatchEmbedding\\n\\npython\\nclass PatchEmbedding(nn.Module):\\n def __init__(self, in_channels=3, patch_size=4, embed_dim=768):\\n self.patch_size = patch_size\\n super().__init__()\\n self.projection = nn.Sequential(\\n nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size),\\n Rearrange(\'b e (h) (w) -> b (h w) e\'),\\n )\\n\\n def forward(self, x: Tensor) -> Tensor:\\n x = self.projection(x)\\n return x\\n\\n\\n#### MLP\\n\\npython\\nclass MLP(nn.Sequential):\\n def __init__(self, dim, mlp_ratio=4.0, dropout=0.):\\n super(MLP, self).__init__(\\n nn.Linear(dim, int(dim * mlp_ratio)),\\n nn.GELU(),\\n nn.Dropout(dropout),\\n nn.Linear(int(dim * mlp_ratio), dim),\\n nn.Dropout(dropout)\\n )\\n\\n\\n#### windows_partition\\n\\npython\\ndef windows_partition(x, window_size):\\n x = rearrange(\\n x,\\n \\"b (win_size1 h2) (win_size2 w2) c -> (b h2 w2) (win_size1 win_size2) c\\",\\n win_size1=window_size,\\n win_size2=window_size\\n )\\n return\\n\\n\\n#### windows_reverse\\n\\npython\\ndef windows_reverse(windows, window_size, h, w):\\n h2 = h // window_size\\n w2 = w // window_size\\n # logically we should reverse to the shape \\"b (win_size1 h2) (win_size2 w2) c\\",\\n # but technically we are going to add the result with skip connection,\\n # therefore we reshape directly to \\"b (win_size1 h2 win_size2 w2) c\\"\\n x = rearrange(\\n windows,\\n \\"(b h2 w2) (win_size1 win_size2) c -> b (win_size1 h2 win_size2 w2) c\\",\\n win_size1=window_size,\\n win_size2=window_size,\\n h2=h2,\\n w2=w2\\n )\\n return x\\n\\n\\n#### WindowAttention\\n\\npython\\nclass WindowAttention(nn.Module):\\n def __init__(self, dim, num_heads, window_size):\\n super(WindowAttention, self).__init__()\\n self.dim = dim\\n self.window_size = window_size\\n self.head_dim = dim // num_heads\\n self.num_heads = num_heads\\n self.scale = self.head_dim ** -0.5\\n self.softmax = nn.Softmax(-1)\\n self.qkv = nn.Linear(dim, dim * 3)\\n self.proj = nn.Linear(dim, dim)\\n\\n \\"\\"\\" <--- Create Relative Position Index \\"\\"\\"\\n coords_h = torch.arange(window_size)\\n coords_w = torch.arange(window_size)\\n coords_flatten = torch.stack(torch.meshgrid([coords_h, coords_w])).flatten(1)\\n relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\\n relative_coords = relative_coords.permute(1, 2, 0).contiguous()\\n relative_coords[:, :, 0] += self.window_size - 1\\n relative_coords[:, :, 1] += self.window_size - 1\\n relative_coords[:, :, 0] *= 2 * self.window_size - 1\\n\\n # record the index from which we take value from a feature vector\\n relative_position_index = relative_coords.sum(-1)\\n\\n # we don\'t need to learn the indexing\\n self.register_buffer(\\"relative_position_index\\", relative_position_index)\\n \\"\\"\\" Create Relative Position Index ---\x3e\\"\\"\\"\\n\\n self.relative_position_bias_table = nn.Parameter(\\n torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads)\\n )\\n nn.init.trunc_normal_(self.relative_position_bias_table)\\n\\n def forward(self, x, mask=None):\\n # x: [b, num_img_tokens, embed_dim]\\n # mask: [n, ws*ws, ws*ws]\\n x = self.qkv(x)\\n qkv = rearrange(x, \\"b n (h d qkv) -> (qkv) b h n d\\", h=self.num_heads, qkv=3)\\n q, k, v = qkv[0], qkv[1], qkv[2]\\n\\n q = q * self.scale\\n attn = torch.einsum(\\"bhqd, bhkd -> bhqk\\", q, k) # attn = Q * K^T\\n\\n relative_position_bias = self.relative_position_bias_table.index_select(\\n 0,\\n self.relative_position_index.reshape((-1,))\\n ).reshape((self.window_size**2, self.window_size**2, -1))\\n\\n # shift number of heads back to the first dimension\\n # unsqueeze in order to broadcast for batches\\n relative_position_bias = relative_position_bias.permute((2, 0, 1)).unsqueeze(0)\\n\\n attn = attn + relative_position_bias\\n\\n if mask is not None:\\n discard_mask = 1 - mask\\n discard_mask = discard_mask * -1e10\\n attn = attn + discard_mask\\n\\n attn = self.softmax(attn)\\n out = torch.einsum(\\"bhai, bhid -> bhad\\", attn, v) # attn * V\\n out = rearrange(out, \\"b h n d -> b n (h d)\\")\\n out = self.proj(out)\\n return out\\n\\n\\n#### generate_mask\\n\\npython\\ndef generate_mask(window_size=4, shift_size=2, input_resolution=(8, 8)):\\n H, W = input_resolution\\n img_mask = torch.zeros((1, H, W, 1)) # we keep the last dimension becuase we want to apply windows_partition\\n h_slices = [slice(0, -window_size),\\n slice(-window_size, -shift_size),\\n slice(-shift_size, None)]\\n w_slices = [slice(0, -window_size),\\n slice(-window_size, -shift_size),\\n slice(-shift_size, None)]\\n\\n count = 0\\n for h in h_slices:\\n for w in w_slices:\\n img_mask[:, h, w, :] = count\\n count += 1\\n\\n windows_mask = windows_partition(img_mask, window_size)\\n # windows_mask: [(b h2 w2), (win_size1 win_size2), 1]\\n windows_mask = windows_mask.reshape((-1, window_size * window_size))\\n # [n, 1, ws*ws] - [n, ws*ws, 1]\\n attn_mask = windows_mask.unsqueeze(1) - windows_mask.unsqueeze(2)\\n attn_mask = torch.where(attn_mask == 0, 1., 0.)\\n return attn_mask\\n\\n\\n#### SwinBlock\\n\\npython\\nclass SwinBlock(nn.Module):\\n def __init__(self, dim, input_resolution, num_heads, window_size, shift_size=0):\\n super(SwinBlock, self).__init__()\\n self.dim = dim\\n self.resolution = input_resolution\\n self.window_size = window_size\\n self.shift_size = shift_size\\n\\n self.attn_norm = nn.LayerNorm(dim)\\n self.attn = WindowAttention(dim, num_heads, window_size)\\n\\n self.mlp_norm = nn.LayerNorm(dim)\\n self.mlp = MLP(dim)\\n\\n if self.shift_size > 0:\\n attn_mask = generate_mask(window_size=self.window_size,\\n shift_size=self.shift_size,\\n input_resolution=self.resolution)\\n else:\\n attn_mask = None\\n self.register_buffer(\'attn_mask\', attn_mask)\\n\\n def forward(self, x):\\n # x: [b, n, d]\\n H, W = self.resolution\\n B, N, C = x.shape\\n h = x\\n x = self.attn_norm(x)\\n x = rearrange(x, \\"b (h w) c -> b h w c\\", h=H, w=W)\\n\\n if self.shift_size > 0:\\n shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\\n else:\\n shifted_x = x\\n\\n x_windows = windows_partition(shifted_x, self.window_size)\\n\\n if self.attn_mask is not None:\\n self.attn_mask = repeat(self.attn_mask[None, ...], \\"() num_patches h w -> b num_patches h w\\", b=B)\\n # exactly the same rearrange with that in windows_reverse\\n self.attn_mask = rearrange(self.attn_mask, \\"b num_patches h w -> (b num_patches) () h w\\")\\n\\n attn_windows = self.attn(x_windows, mask=self.attn_mask)\\n attn_windows = windows_reverse(attn_windows, window_size=self.window_size, h=H, w=W)\\n\\n # reverse cyclic shift\\n if self.shift_size > 0:\\n x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\\n else:\\n x = shifted_x\\n\\n x = h + attn_windows\\n\\n h = x\\n x = self.mlp_norm(x)\\n x = self.mlp(x)\\n x = h + x\\n return\\n\\n\\n#### SwinStage\\n\\npython\\nclass SwinStage(nn.Module):\\n def __init__(self,\\n dim: int,\\n input_resolution: Tuple[int, int],\\n depth: int,\\n num_heads: int,\\n window_size: int,\\n patch_merging: Optional[PatchMerging] = None):\\n super(SwinStage, self).__init__()\\n self.blocks = nn.ModuleList()\\n\\n for i in range(depth):\\n self.blocks.append(\\n SwinBlock(dim=dim,\\n input_resolution=input_resolution,\\n num_heads=num_heads,\\n window_size=window_size,\\n shift_size=0 if i % 2 == 0 else window_size // 2)\\n )\\n if patch_merging is None:\\n self.patch_merging = nn.Identity()\\n else:\\n self.patch_merging = patch_merging(input_resolution, dim)\\n\\n def forward(self, x):\\n for block in self.blocks:\\n x = block(x)\\n\\n x = self.patch_merging(x)\\n return\\n\\n\\n#### SwinTransformer\\n\\npython\\nclass SwinTransformer(nn.Module):\\n def __init__(self,\\n image_size=224,\\n patch_size=4,\\n embed_dim=96,\\n window_size=7,\\n num_heads=[3, 6, 12, 24],\\n depths=[2, 2, 6, 2],\\n num_classes=1000,\\n output_hidden_states=False\\n ):\\n super(SwinTransformer, self).__init__()\\n self.num_class = num_classes\\n self.depths = depths\\n self.num_heads = num_heads\\n self.embed_dim = embed_dim\\n self.num_stages = len(depths)\\n self.num_features = int(self.embed_dim * (2 ** (self.num_stages - 1)))\\n self.patch_resolution = [image_size // patch_size, image_size // patch_size]\\n\\n self.patch_embedding = PatchEmbedding(patch_size=patch_size, embed_dim=embed_dim)\\n self.stages = nn.ModuleList()\\n self.output_hidden_states = output_hidden_states\\n\\n for idx, (depth, n_heads) in enumerate(zip(self.depths, self.num_heads)):\\n h, w = self.patch_resolution\\n stage = SwinStage(dim=int(self.embed_dim * (2 ** idx)),\\n input_resolution=(h // (2**idx), w // (2**idx)),\\n depth=depth,\\n num_heads=n_heads,\\n window_size=window_size,\\n patch_merging=PatchMerging if (idx < self.num_stages-1) else None)\\n self.stages.append(stage)\\n self.window_size = window_size\\n self.norm = nn.LayerNorm(self.num_features)\\n self.avgpool = nn.AdaptiveAvgPool1d(1) # last diemnsion will be shrinked to 1\\n self.fc = nn.Linear(self.num_features, num_classes)\\n\\n def forward(self, x):\\n x = self.patch_embedding(x)\\n\\n for stage in self.stages:\\n x = stage(x)\\n\\n x = self.norm(x)\\n\\n if self.output_hidden_states:\\n x = rearrange(\\n x,\\n \\"b (win_size_h win_size_w) embed_dim -> b embed_dim win_size_h win_size_w\\",\\n win_size_h=self.window_size,\\n win_size_w =self.window_size\\n )\\n return x\\n\\n x = rearrange(x, \\"b num_windows embed_dim -> b embed_dim num_windows\\")\\n x = self.avgpool(x)\\n x = rearrange(x, \\"b embed_dim c -> b (embed_dim c)\\") # c = 1 due to avgpool\\n x = self.fc(x)\\n return x\\n\\n\\n#### Test Script\\n\\npython\\nt = torch.randn([4, 3, 224, 224])\\npatch_embedding = PatchEmbedding(patch_size=4, embed_dim=96)\\nswin_block = SwinBlock(dim=96, input_resolution=[56, 56], num_heads=4, window_size=7)\\nshifted_swin_block = SwinBlock(dim=96, input_resolution=[56, 56], num_heads=4, window_size=7, shift_size=7 // 2)\\npatch_merging = PatchMerging(input_resolution=[56, 56], dim=96)\\n\\nout = patch_embedding(t) # result: [4, 56*56, 96], here (224/4) * (224/4) = 56*56\\nout = swin_block(out) # result: [4, 56*56, 96]\\nout = shifted_swin_block(out) # result: [4, 56*56, 96]\\nout = patch_merging(out) # result: [4, 784, 192], here 56*56 / 4 = 784\\n # 784 = 28*28 is considered as new number of windows\\n\\n","title":"Swin Transformer with einops Implementation","date":"2022-09-19T00:00:00.000Z","id":"blog093","tag":"deep-learning, pytorch","intro":"In practice we can directly use torchvision.models.swin_t to use pretrained model. The Reimplementation of the model is for sheer purpose of my self-learning."},{"content":"\\n#### Repository\\n\\n- https://github.com/machingclee/2022-09-20-Swin-Transformer-backbone-FasterRCNN\\n\\n#### Target Features\\n\\nThe red arrows indicates the features we want:\\n\\n<Center>\\n<a href=\\"/assets/tech/094-fasterrcnn-swin/001.png\\" target=\\"_blank\\">\\n <img src=\\"/assets/tech/094-fasterrcnn-swin/001.png\\" width=\\"700\\"/>\\n</a>\\n</Center>\\n\\n#### Produce Features Pyramid from the Desired Features\\n\\nAn appendix of models.swin_t has been provided at the end of this blog post. Here config.fpn_feat_channels = 192 which is to match the smallest number of features in the pyramid (so that we can do addition).\\n\\nHere swin_t can be replaced by swin_s and swin_b for different experiments. Their output shape are still the same (with different number of blocks of transformer encoder).\\n\\npython\\nclass SwinFeatureExtractor(nn.Module):\\n def __init__(self):\\n super(SwinFeatureExtractor, self).__init__()\\n self.model = models.swin_t(weights=\\"DEFAULT\\").to(device)\\n self.layer1 = self.model.features[0:2]\\n self.layer2 = self.model.features[2:4]\\n self.layer3 = self.model.features[4:6]\\n self.layer4 = self.model.features[6:8]\\n self.lateral_conv5 = nn.Conv2d(768, config.fpn_feat_channels, 1, 1)\\n self.lateral_conv4 = nn.Conv2d(384, config.fpn_feat_channels, 1, 1)\\n self.lateral_conv3 = nn.Conv2d(192, config.fpn_feat_channels, 1, 1)\\n self.lateral_conv2 = nn.Conv2d(96, config.fpn_feat_channels, 1, 1)\\n self.upscale = lambda input: F.interpolate(input, scale_factor=2)\\n self.freeze_params()\\n\\n def freeze_params(self):\\n for param in self.model.parameters():\\n param.requires_grad = False\\n\\n def forward(self, x):\\n x_4: Tensor = self.layer1(x)\\n x_8 = self.layer2(x_4)\\n x_16 = self.layer3(x_8)\\n x_32 = self.layer4(x_16)\\n\\n x_4 = x_4.permute([0, 3, 1, 2])\\n x_8 = x_8.permute([0, 3, 1, 2])\\n x_16 = x_16.permute([0, 3, 1, 2])\\n x_32 = x_32.permute([0, 3, 1, 2])\\n\\n p5 = self.lateral_conv5(x_32)\\n p4 = self.lateral_conv4(x_16) + self.upscale(p5)\\n p3 = self.lateral_conv3(x_8) + self.upscale(p4)\\n p2 = self.lateral_conv2(x_4) + self.upscale(p3)\\n\\n return [p2, p3, p4, p5]\\n\\n\\nHere x_k denotes the feature of spatial dimension $\\\\frac{H}{k} \\\\times \\\\frac{W}{k}$, where _, _, H, W = x.shape.\\n\\nThe major changes are just the img_shapes and fpn_feat_channels. The rest are the same.\\n\\n#### Sample Result\\n\\n<Center>\\n <img src=\\"/assets/tech/094-fasterrcnn-swin/002.jpg\\" width=\\"600\\"/>\\n</Center>\\n\\n#### Appendix: Printed Info of the Structure of SwinTransformer\\n\\ntext\\nSwinTransformer(\\n (features): Sequential(\\n (0): Sequential(\\n (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\\n (1): Permute()\\n (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\\n )\\n (1): Sequential(\\n (0): SwinTransformerBlock(\\n (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=96, out_features=288, bias=True)\\n (proj): Linear(in_features=96, out_features=96, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.0, mode=row)\\n (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=96, out_features=384, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=384, out_features=96, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n (1): SwinTransformerBlock(\\n (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=96, out_features=288, bias=True)\\n (proj): Linear(in_features=96, out_features=96, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\\n (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=96, out_features=384, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=384, out_features=96, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n )\\n (2): PatchMerging(\\n (reduction): Linear(in_features=384, out_features=192, bias=False)\\n (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n )\\n (3): Sequential(\\n (0): SwinTransformerBlock(\\n (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=192, out_features=576, bias=True)\\n (proj): Linear(in_features=192, out_features=192, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\\n (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=192, out_features=768, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=768, out_features=192, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n (1): SwinTransformerBlock(\\n (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=192, out_features=576, bias=True)\\n (proj): Linear(in_features=192, out_features=192, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\\n (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=192, out_features=768, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=768, out_features=192, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n )\\n (4): PatchMerging(\\n (reduction): Linear(in_features=768, out_features=384, bias=False)\\n (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n )\\n (5): Sequential(\\n (0): SwinTransformerBlock(\\n (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=384, out_features=1152, bias=True)\\n (proj): Linear(in_features=384, out_features=384, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\\n (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=384, out_features=1536, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=1536, out_features=384, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n (1): SwinTransformerBlock(\\n (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=384, out_features=1152, bias=True)\\n (proj): Linear(in_features=384, out_features=384, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\\n (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=384, out_features=1536, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=1536, out_features=384, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n (2): SwinTransformerBlock(\\n (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=384, out_features=1152, bias=True)\\n (proj): Linear(in_features=384, out_features=384, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\\n (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=384, out_features=1536, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=1536, out_features=384, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n (3): SwinTransformerBlock(\\n (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=384, out_features=1152, bias=True)\\n (proj): Linear(in_features=384, out_features=384, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\\n (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=384, out_features=1536, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=1536, out_features=384, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n (4): SwinTransformerBlock(\\n (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=384, out_features=1152, bias=True)\\n (proj): Linear(in_features=384, out_features=384, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\\n (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=384, out_features=1536, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=1536, out_features=384, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n (5): SwinTransformerBlock(\\n (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=384, out_features=1152, bias=True)\\n (proj): Linear(in_features=384, out_features=384, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\\n (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=384, out_features=1536, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=1536, out_features=384, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n )\\n (6): PatchMerging(\\n (reduction): Linear(in_features=1536, out_features=768, bias=False)\\n (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\\n )\\n (7): Sequential(\\n (0): SwinTransformerBlock(\\n (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=768, out_features=2304, bias=True)\\n (proj): Linear(in_features=768, out_features=768, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\\n (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=768, out_features=3072, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=3072, out_features=768, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n (1): SwinTransformerBlock(\\n (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n (attn): ShiftedWindowAttention(\\n (qkv): Linear(in_features=768, out_features=2304, bias=True)\\n (proj): Linear(in_features=768, out_features=768, bias=True)\\n )\\n (stochastic_depth): StochasticDepth(p=0.2, mode=row)\\n (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n (mlp): MLP(\\n (0): Linear(in_features=768, out_features=3072, bias=True)\\n (1): GELU(approximate=none)\\n (2): Dropout(p=0.0, inplace=False)\\n (3): Linear(in_features=3072, out_features=768, bias=True)\\n (4): Dropout(p=0.0, inplace=False)\\n )\\n )\\n )\\n )\\n (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\\n (avgpool): AdaptiveAvgPool2d(output_size=1)\\n (head): Linear(in_features=768, out_features=1000, bias=True)\\n)\\n\\n","title":"Swin-Transformer Backbone in Faster RCNN","date":"2022-09-20T00:00:00.000Z","id":"blog094","tag":"deep-learning, pytorch","intro":"Describe how to fed the features from Swin Transformer into Faster RCNN."},{"content":"\\npython\\n# model is the nn.Module object that we are going to train:\\ntorch.nn.utils.clip_grad_norm_(model.parameters(), grad_clipping_thres)\\n\\n","title":"Gradient Clipping","date":"2022-09-22T00:00:00.000Z","id":"blog095","tag":"deep-learning, pytorch","toc":false,"intro":"Record a script to clip gradient to avoid graident explosion."},{"content":"\\n#### Paper\\n\\nhttps://arxiv.org/abs/1709.01507\\n\\n#### Code Implementation\\n\\npython\\nimport torch\\nfrom torch import nn\\nfrom torch.nn import init\\n\\nclass SEAttention(nn.Module):\\n def __init__(self, channel=512,reduction=16):\\n super().__init__()\\n self.avg_pool = nn.AdaptiveAvgPool2d(1)\\n self.fc = nn.Sequential(\\n nn.Linear(channel, channel // reduction, bias=False),\\n nn.ReLU(inplace=True),\\n nn.Linear(channel // reduction, channel, bias=False),\\n nn.Sigmoid()\\n )\\n\\n\\n def init_weights(self):\\n for m in self.modules():\\n if isinstance(m, nn.Conv2d):\\n init.kaiming_normal_(m.weight, mode=\'fan_out\')\\n if m.bias is not None:\\n init.constant_(m.bias, 0)\\n elif isinstance(m, nn.BatchNorm2d):\\n init.constant_(m.weight, 1)\\n init.constant_(m.bias, 0)\\n elif isinstance(m, nn.Linear):\\n init.normal_(m.weight, std=0.001)\\n if m.bias is not None:\\n init.constant_(m.bias, 0)\\n\\n def forward(self, x):\\n b, c, _, _ = x.size()\\n y = self.avg_pool(x).view(b, c)\\n y = self.fc(y).view(b, c, 1, 1)\\n return x * y.expand_as(x)\\n\\n","title":"Squeeze and Excitation Network","date":"2022-09-29T00:00:00.000Z","id":"blog097","tag":"deep-learning, pytorch","intro":"Record an attention-based component."},{"content":"\\n#### Source Code\\n\\n- https://github.com/machingclee/2022-09-20-Swin-Transformer-backbone-Cascade-RCNN/blob/main/src/cascade_faster_rcnn_swin_transformer_fpn.py\\n\\n#### Paper\\n\\n- https://arxiv.org/pdf/1712.00726.pdf\\n\\n#### From Faster RCNN\\n\\n<Center>\\n<img src=\\"/assets/tech/098-cascade-rcnn/002.png\\"/>\\n</Center>\\n\\n<p/>\\n<center></center>\\n\\nLet\'s have a rough review on faster rcnn. In the figure above I is an input image, it is fed into a convolutional backbone and we get a feature tensor.\\n\\n- That feature can be tought of as the local representation of a patched image (the smaller the spatial dimension of the feature, the larger the patched window).\\n- For exmaple, if a feature is of size (B, 256, H/16, W/16), that means each \\"patched window\\" is of size $16\\\\times 16$ and the feature is captured by 256 channels.\\n- In Faster RCNN each feature of patched windows is transformed into the prediction of number of anchors by nn.Conv2d(256, n_anchors*4, 1, 1) and transformed into the coresponding \\"objectness\\" (whether it is a background) by nn.Conv2d(256, n_anchors*2, 1, 1).\\n\\n- These anchors prediction are called **_region proposal_**, they are scored by the \\"objectness\\" scores (which we call logits) in another branch that we have mention aboved. They are then filtered by that scores and also their intersection of union (iou) which measures the overlap of two anchors (if they are too close, the one with lower objectness score will be removed).\\n\\n- The filtered anchors are then used in a pooling layer called ROI-Align. Therefore we get a feature vector of size (B, n_filtered_anchors, 7, 7). These features are used to further predict the **_refinements_** of region proposals that eventually become our final bounding box prediction.\\n\\n#### Improvement from Faster RCNN: Cascade RCNN\\n\\n<Center>\\n<img src=\\"/assets/tech/098-cascade-rcnn/001.png\\"/>\\n</Center>\\n<p/>\\n<center></center>\\n\\nCascade RCNN goes a little bit further, we get our bounding boxes from the last paragraph in B1. let\'s call these stage-1-prediction.\\n\\nIn the next stage, we treat those boxes as our new region proposals and repeat the process to predict a more refined version of the final bounding boxes. The key is:\\n\\n- In this second stage, our proposal is allowed to have higher **_iou threshold_**.\\n- In the thrid stage, we keep increasing our iou threshold.\\n\\nEventually we will get more and more precise bounding box with smaller and smaller chance of overlapping each another.\\n\\nIn terms of code, they look:\\n\\npython\\nroi_cls_losses = 0\\nroi_reg_losses = 0\\n\\npred_fg_bg_logits = flattened_pred_fg_bg_logits\\nlevels = flattened_levels\\n\\nfor i in range(len(config.cascade_proposal_ious)):\\n if (levels.shape[0] != rois.shape[0]):\\n print(\\"something wrong\\")\\n roi_cls_loss, roi_reg_loss, cls_logits, rois, pred_fg_bg_logits, levels = \\\\\\n self.stage_prediction_in_training(\\n out_feat,\\n target_boxes,\\n target_cls_indexes,\\n rois,\\n pred_fg_bg_logits,\\n levels,\\n n_levels,\\n config.cascade_proposal_ious[i]\\n )\\n\\n roi_cls_losses += roi_cls_loss\\n roi_reg_losses += roi_reg_loss\\n\\n\\nSince my work is done on feature pyramid, the implementation will be a lot more complicated, but the idea behind it is simple, the reader can try to implement it on faster rcnn with single-feature-backbone.\\n","title":"Cascade RCNN","date":"2022-09-30T00:00:00.000Z","id":"blog098","tag":"deep-learning","intro":"Code implementation in pytorch on Cascade RCNN, the code base is mainly a modification of my previous project."},{"content":"\\njson\\n{\\n \\"tasks\\": [\\n {\\n \\"type\\": \\"cppbuild\\",\\n \\"label\\": \\"C/C++: cl.exe build active file\\",\\n \\"command\\": \\"cl.exe\\",\\n \\"args\\": [\\n \\"/Zi\\",\\n \\"/EHsc\\",\\n \\"/nologo\\",\\n \\"/std:c++20\\",\\n \\"/Fe:\\",\\n \\"${fileDirname}\\\\\\\\${fileBasenameNoExtension}.exe\\",\\n \\"${file}\\"\\n ],\\n \\"options\\": {\\n \\"cwd\\": \\"${fileDirname}\\"\\n },\\n \\"problemMatcher\\": [\\"$msCompile\\"],\\n \\"group\\": {\\n \\"kind\\": \\"build\\",\\n \\"isDefault\\": true\\n },\\n \\"detail\\": \\"Task generated by Debugger.\\"\\n }\\n ],\\n \\"version\\": \\"2.0.0\\"\\n}\\n\\n\\nWe may also need to set the C++ compilePath, to find the path of cl.exe, we use the following\\n\\nnone\\ngcm cl.exe | select Source\\n\\n\\nin developer powershell for visual studio.\\n","title":"CPP VSCode Configuration for 2020 Standard","date":"2022-10-08T00:00:00.000Z","id":"blog099","tag":"coding, C++","toc":false,"intro":"Vscode task.json for using 2020 standard."},{"content":"\\n#### Trouble in Standard Dataset Object\\n\\nWe come into the following scenario very often in which we need to return the (normalized) image tensor as well as the corresponding annotations.\\n\\npython\\nclass WFLWDatasets(data.Dataset):\\n ...\\n def __getitem__(self, index):\\n data = self.get_data()\\n curr_data = data[index]\\n rel_img_path, annotations = curr_data\\n self.img = np.array(Image.open(os.path.join(self.img_dir,\\n os.path.normpath(rel_img_path))))\\n target = np.array(annotations) # list of 15-dim arrays\\n\\n # img, target become tensor after self.preproc\\n img, target = self.preproc(self.img, target)\\n\\n return img, target\\n ...\\n\\n\\nHowever, an error will occur when loading the dataset via DataLoader by\\n\\npython\\nfrom torch.utils.data as DataLoader\\n\\nbatch_iterator = iter(data.DataLoader(dataset,\\n batch_size, # >= 2\\n shuffle=True))\\n\\n\\n**Reason.** When we return a batch of images, the number of annotations (e.g., 2 bboxes for the first image and 15 bboxes for the second image) will differ, we cannot concat [2, 4]-dimensional tensor with [15, 4]-dimensional tensor.\\n\\nBut by default, Dataloader has an attribute collate_fn which is initialized to the following function.\\n\\npython\\ndef default_collate(batch):\\n elem = batch[0]\\n elem_type = type(elem)\\n if isinstance(elem, torch.Tensor):\\n out = None\\n if torch.utils.data.get_worker_info() is not None:\\n # If we\'re in a background process, concatenate directly into a\\n # shared memory tensor to avoid an extra copy\\n numel = sum(x.numel() for x in batch)\\n storage = elem.storage()._new_shared(numel, device=elem.device)\\n out = elem.new(storage).resize_(len(batch), *list(elem.size()))\\n return torch.stack(batch, 0, out=out)\\n elif elem_type.__module__ == \'numpy\' and elem_type.__name__ != \'str_\' \\\\\\n and elem_type.__name__ != \'string_\':\\n if elem_type.__name__ == \'ndarray\' or elem_type.__name__ == \'memmap\':\\n # array of string classes and object\\n if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\\n raise TypeError(default_collate_err_msg_format.format(elem.dtype))\\n\\n return default_collate([torch.as_tensor(b) for b in batch])\\n elif elem.shape == (): # scalars\\n return torch.as_tensor(batch)\\n elif isinstance(elem, float):\\n return torch.tensor(batch, dtype=torch.float64)\\n elif isinstance(elem, int):\\n return torch.tensor(batch)\\n elif isinstance(elem, string_classes):\\n return batch\\n elif isinstance(elem, collections.abc.Mapping):\\n try:\\n return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})\\n except TypeError:\\n # The mapping type may not support __init__(iterable).\\n return {key: default_collate([d[key] for d in batch]) for key in elem}\\n elif isinstance(elem, tuple) and hasattr(elem, \'_fields\'): # namedtuple\\n return elem_type(*(default_collate(samples) for samples in zip(*batch)))\\n elif isinstance(elem, collections.abc.Sequence):\\n # check to make sure that the elements in batch have consistent size\\n it = iter(batch)\\n elem_size = len(next(it))\\n if not all(len(elem) == elem_size for elem in it):\\n raise RuntimeError(\'each element in list of batch should be of equal size\')\\n transposed = list(zip(*batch)) # It may be accessed twice, so we use a list.\\n\\n if isinstance(elem, tuple):\\n return [default_collate(samples) for samples in transposed] # Backwards compatibility.\\n else:\\n try:\\n return elem_type([default_collate(samples) for samples in transposed])\\n except TypeError:\\n # The sequence type may not support __init__(iterable) (e.g., range).\\n return [default_collate(samples) for samples in transposed]\\n\\n raise TypeError(default_collate_err_msg_format.format(elem_type))\\n\\n\\nFor short, it unsqueezes each positional entry at 0 position and concat them accordingly to form a batch (e.g., two [3, 256, 256] image tensors will be concated into a [2, 3, 256, 256] tensor). An error occurs when this concatenation process fails.\\n\\n#### Solution by collate_fn\\n\\nIs it necessary for the target (annotation) objects to be concated any way? No, instead, we can return a list (array) of target objects, and reshape our targets (no matter it is [2, 4]-dimensional or [15, 4]-dimensional) into a tensor of consistent shape, and finally concat these consistent target tensors and compute loss against the batch of predictions from the model.\\n\\nExample:\\n\\n- line 68 of [RetinaFace\'s multibox_loss](https://github.com/machingclee/2022-10-05-Retinaface-study/blob/main/layers/modules/multibox_loss.py).\\n\\nThe trick is done by defining our own collate_fn:\\n\\npython\\ndef collate_fn(batch):\\n imgs = []\\n batch_annotations = []\\n\\n for i in range(len(batch)):\\n data = batch[i]\\n img, annotation = data\\n imgs.append(img.unsqueeze(0))\\n batch_annotations.append(annotation)\\n\\n return torch.cat(imgs, dim=0).to(device), batch_annotations\\n\\n\\nNote that when batch_size=2, then each batch is a list:\\n\\npython\\n[dataset.__getitem__(n_1), dataset.__getitem__(n_2)]\\n\\n\\nfor some n_1, n_2 with $\\\\texttt{n_1}\\\\neq \\\\texttt{n_2}$. Therefore\\n\\n- For the first positional entry of our returned output, we concat our image tensor in the usual way (.unsqueeze(0) and torch.cat), but\\n- For the second positional entry, we concat them into a usual list and we return that list instead of a tensor.\\n\\nWe plug our collate_fn to DataLoader object by\\n\\npython\\nbatch_iterator = iter(data.DataLoader(dataset,\\n batch_size, # >= 2\\n shuffle=True,\\n collate_fn=collate_fn))\\n\\n\\nand the iteration result next(batch_iterator) is of type Tensor, List[Tensor].\\n","title":"collate_fn in pytorch","date":"2022-11-03T00:00:00.000Z","id":"blog0102","tag":"deep-learning, pytorch","intro":"Discuss how to customize the patching of the results from __getitem__ method of Dataset object in pytorch."},{"content":"\\n#### Common Helper Functions\\n\\n##### resize_and_padding\\n\\nresize_and_padding makes sure all input are of hte same shape:\\n\\npython\\ndef resize_img(img):\\n \\"\\"\\"\\n img: Pillow image\\n \\"\\"\\"\\n h, w = img.height, img.width\\n if h >= w:\\n ratio = config.input_height / h\\n new_h, new_w = int(h * ratio), int(w * ratio)\\n else:\\n ratio = config.input_width / w\\n new_h, new_w = int(h * ratio), int(w * ratio)\\n\\n if new_w > config.input_width:\\n ratio = config.input_width / new_w\\n new_h, new_w = int(new_h * ratio), int(new_w * ratio)\\n\\n img = img.resize((new_w, new_h), Image.BILINEAR)\\n return img, (w, h)\\n\\n\\npython\\ndef pad_img(img):\\n h = img.height\\n w = img.width\\n img = np.array(img)\\n img = np.pad(img,\\n pad_width=((0, config.input_height - h),\\n (0, config.input_width - w),\\n (0, 0)),\\n mode=\\"reflect\\")\\n img = Image.fromarray(img)\\n assert img.height == config.input_height\\n assert img.width == config.input_width\\n return img\\n\\n\\npython\\ndef resize_and_padding(img, return_window=False):\\n img, (ori_w, ori_h) = resize_img(img)\\n w = img.width\\n h = img.height\\n padding_window = (w, h)\\n img = pad_img(img)\\n\\n if not return_window:\\n return img\\n else:\\n return img, padding_window, (ori_w, ori_h)\\n\\n\\nThe padding_window and (ori_w, ori_h) are used to reverse the transform to get the original image.\\n\\n##### torch_img_transform\\n\\nThe built-in transform by pytorch are used to normalized the input:\\n\\npython\\nfrom torchvision import transforms\\n\\ntorch_img_transform = transforms.Compose([\\n # numpy array will have channels permuted to the second index: (b, c, h, w)\\n transforms.ToTensor(),\\n # normalize from [0, 1] to [-1, 1]\\n transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\\n])\\n\\n\\n##### torch_imgnet_transform\\n\\nNormalization specific to imagenet data:\\n\\npython\\nfrom torchvision import transforms\\n\\ntorch_imgnet_transform = transforms.Compose([\\n transforms.ToTensor(),\\n transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n])\\n\\n\\n##### Inverse of torch_imgnet_transform\\n\\npython\\ndef torch_imgnet_denormalization_to_pil(img: torch.Tensor) -> Image.Image:\\n mean = torch.as_tensor([0.485, 0.456, 0.406])[None, :, None, None].to(device)\\n std = torch.as_tensor([0.229, 0.224, 0.225])[None, :, None, None].to(device)\\n img = (img * std + mean) * 255\\n img = Image.fromarray(img.squeeze(0).permute(1, 2, 0).cpu().numpy().astype(\\"uint8\\"))\\n return img\\n\\n\\n##### draw_dots and draw_boxes\\n\\nThe following are helpful for facial landmarks visualization:\\n\\npython\\ndef draw_box(pil_img: Image.Image, bboxes, confs=None, color=(255, 255, 255, 150)):\\n draw = ImageDraw.Draw(pil_img)\\n for i, bbox in enumerate(bboxes):\\n xmin, ymin, xmax, ymax = bbox\\n draw.rectangle(((xmin, ymin), (xmax, ymax)), outline=color, width=2)\\n if confs is not None:\\n conf = confs[i]\\n draw.text(\\n (xmin, max(ymin - 10, 4)),\\n \\"{:.2f}\\".format(conf.item()),\\n color\\n )\\n\\n\\npython\\ndef draw_dots(pil_img: Image.Image, pred_boxes,\\n pred_landmarks: Tuple[float], r=2, constrain_pts=False):\\n draw = ImageDraw.Draw(pil_img)\\n for bbox, landmark in zip(pred_boxes, pred_landmarks):\\n xmin, ymin, xmax, ymax = bbox\\n for x, y in np.array_split(landmark, 5):\\n if not constrain_pts:\\n draw.ellipse((x - r, y - r, x + r, y + r), fill=(255, 0, 0))\\n else:\\n if xmin <= x and x <= xmax and ymin <= y and y <= ymax:\\n draw.ellipse((x - r, y - r, x + r, y + r), fill=(255, 0, 0))\\n\\n\\n##### xyxy_to_cxcywh and cxcywh_to_xyxy\\n\\npython\\ndef xyxy_to_cxcywh(bboxes):\\n if len(bboxes) == 0:\\n return bboxes\\n cxcy = (bboxes[:, 0:2] + bboxes[:, 2:4]) / 2\\n wh = (bboxes[:, 2:4] - bboxes[:, 0:2])\\n\\n if isinstance(bboxes, torch.Tensor):\\n def cat_func(arr_to_concat): return torch.cat(arr_to_concat, dim=-1)\\n else:\\n def cat_func(arr_to_concat): return np.concatenate(arr_to_concat, axis=-1)\\n\\n out = cat_func([cxcy, wh])\\n return out\\n\\n\\npython\\ndef cxcywh_to_xyxy(bboxes):\\n\\n if len(bboxes) == 0:\\n return bboxes\\n xmin_ymin = bboxes[:, 0:2] - bboxes[:, 2:4] / 2\\n xmax_ymax = bboxes[:, 0:2] + bboxes[:, 2:4] / 2\\n\\n if isinstance(bboxes, torch.Tensor):\\n def cat_func(arr_to_concat): return torch.cat(arr_to_concat, dim=-1)\\n else:\\n def cat_func(arr_to_concat): return np.concatenate(arr_to_concat, axis=-1)\\n\\n out = cat_func([xmin_ymin, xmax_ymax])\\n\\n return out\\n\\n\\n#### Augmentation by Albumentations\\n\\n##### Resize and Padding\\n\\npython\\nresize_and_padding_transforms_list = [\\n A.LongestMaxSize(max_size=config.longest_side_length, interpolation=1, p=1),\\n A.PadIfNeeded(\\n min_height=config.input_height,\\n min_width=config.input_height,\\n border_mode=0,\\n value=(0, 0, 0),\\n position=\\"top_left\\"\\n )\\n]\\n\\n\\n##### Miscellaneous Transforms\\n\\npython\\nimport albumentations as A\\n\\nalbumentation_transform = A.Compose([\\n A.ShiftScaleRotate(shift_limit=0, scale_limit=(0.5, 2), p=1),\\n A.Perspective(p=0.4),\\n A.Rotate(limit=10, p=0.8),\\n A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.9),\\n A.OneOf([\\n A.Blur(blur_limit=3, p=0.5),\\n A.ColorJitter(p=0.5)\\n ], p=1.0),\\n *resize_and_padding_transforms_list\\n],\\n additional_targets={\\"image1\\": \\"image\\"}\\n)\\n\\n\\nHere is the tricky part, note that\\n\\npython\\nadditional_targets={\\"image1\\": \\"image\\"}\\n\\n\\nis used to tell albumentation_transform to accept kwarg image1 which is an image (but not mask). Moreover:\\n\\n- albumentation_transform itself understands not to do certain operations on mask. For example, it makes no sense to do RGB-shift to a mask.\\n\\n- The kwargs image and mask are built-in and ready to be used.\\n\\n- Note that albumentation_process **_only accpets numpy arrays_**.\\n\\npython\\ndef albumentation_process(bg_img, img, mask):\\n # bg_img, img, mask are numpy arrays\\n transformsed = albumentation_transform(image=bg_img, image1=img, mask=mask)\\n bg_img = transformsed[\\"image\\"]\\n img = transformsed[\\"image1\\"]\\n mask = transformsed[\\"mask\\"]\\n return bg_img, img, mask\\n\\n\\n#### Results\\n\\nA planar, axes-aligned synthetic image becomes:\\n\\n<center>\\n<img src=\\"/assets/tech/055.png\\"/ width=\\"500\\">\\n</center>\\n","title":"Albumentations and Common Helper Functions with PyTorch","date":"2022-11-05T00:00:00.000Z","id":"blog0103","tag":"pytorch, deep-learning","intro":"Record the most recently used combination of data augmentations."},{"content":"\\n#### Repository\\n\\n- https://github.com/machingclee/2022-10-05-Retinaface-study\\n\\n#### Data Processing (Landmarks Specific)\\n\\n##### Nice Augmentation Implementation for Record\\n\\nkeypoints argument in albumentations package causes incorrect augmented annotations for landmarks, for bbox augmentation we should stick with albumentations.\\n\\nIn case we need to augment landmarks we can try the following:\\n\\n###### Cropping\\n\\npython\\n# image is a [H, W, 3]-dimensional numpy array\\ndef _crop(image, target_boxes, labels, landm, img_dim):\\n height, width, _ = image.shape\\n pad_image_flag = True\\n\\n for _ in range(250):\\n \\"\\"\\"\\n if random.uniform(0, 1) <= 0.2:\\n scale = 1.0\\n else:\\n scale = random.uniform(0.3, 1.0)\\n \\"\\"\\"\\n PRE_SCALES = [0.3, 0.45, 0.6, 0.8, 1.0]\\n scale = random.choice(PRE_SCALES)\\n short_side = min(width, height)\\n w = int(scale * short_side)\\n h = w\\n\\n if width == w:\\n l = 0\\n else:\\n l = random.randrange(width - w)\\n if height == h:\\n t = 0\\n else:\\n t = random.randrange(height - h)\\n roi_xyxy = np.array((l, t, l + w, t + h))\\n\\n value = matrix_iof(target_boxes, roi_xyxy[np.newaxis])\\n flag = (value >= 1)\\n if not flag.any():\\n continue\\n\\n target_centers = (target_boxes[:, :2] + target_boxes[:, 2:]) / 2\\n mask_a = np.logical_and(roi_xyxy[:2] < target_centers, target_centers < roi_xyxy[2:]).all(axis=1)\\n boxes_t = target_boxes[mask_a].copy()\\n labels_t = labels[mask_a].copy()\\n landms_t = landm[mask_a].copy()\\n landms_t = landms_t.reshape([-1, 5, 2])\\n\\n if boxes_t.shape[0] == 0:\\n continue\\n\\n image_t = image[roi_xyxy[1]:roi_xyxy[3], roi_xyxy[0]:roi_xyxy[2]]\\n\\n boxes_t[:, :2] = np.maximum(boxes_t[:, :2], roi_xyxy[:2])\\n boxes_t[:, :2] -= roi_xyxy[:2]\\n boxes_t[:, 2:] = np.minimum(boxes_t[:, 2:], roi_xyxy[2:])\\n boxes_t[:, 2:] -= roi_xyxy[:2]\\n\\n # landm\\n landms_t[:, :, :2] = landms_t[:, :, :2] - roi_xyxy[:2]\\n landms_t[:, :, :2] = np.maximum(landms_t[:, :, :2], np.array([0, 0]))\\n landms_t[:, :, :2] = np.minimum(landms_t[:, :, :2], roi_xyxy[2:] - roi_xyxy[:2])\\n landms_t = landms_t.reshape([-1, 10])\\n\\n # make sure that the cropped image contains at least one face > 16 pixel at training image scale\\n b_w_t = (boxes_t[:, 2] - boxes_t[:, 0] + 1) / w * img_dim\\n b_h_t = (boxes_t[:, 3] - boxes_t[:, 1] + 1) / h * img_dim\\n mask_b = np.minimum(b_w_t, b_h_t) > 0.0\\n boxes_t = boxes_t[mask_b]\\n labels_t = labels_t[mask_b]\\n landms_t = landms_t[mask_b]\\n\\n if boxes_t.shape[0] == 0:\\n continue\\n\\n pad_image_flag = False\\n\\n return image_t, boxes_t, labels_t, landms_t, pad_image_flag\\n return image, target_boxes, labels, landm, pad_image_flag\\n\\n\\nFor successful cropping, we always get a square.\\n\\nNote that most of the time pad_image_flag == False, the next padding method pads the image into a square in case we are too unfortunate to augment the data correctly in 250 trials (in that case, pad_image_flag == True):\\n\\npython\\ndef _pad_to_square(image, rgb_mean, pad_image_flag):\\n if not pad_image_flag:\\n return image\\n height, width, _ = image.shape\\n long_side = max(width, height)\\n image_t = np.empty((long_side, long_side, 3), dtype=image.dtype)\\n image_t[:, :] = rgb_mean\\n image_t[0:0 + height, 0:0 + width] = image\\n return image_t\\n\\n\\n###### Resize and Mean Subtraction\\n\\npython\\ndef _resize_subtract_mean(image, insize, rgb_mean):\\n interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\\n interp_method = interp_methods[random.randrange(5)]\\n image = cv2.resize(image, (insize, insize), interpolation=interp_method)\\n image = image.astype(np.float32)\\n image -= rgb_mean\\n return image\\n\\n\\n###### Mirroring\\n\\nMirror the image with 0.5 probability:\\n\\npython\\ndef _mirror(image, boxes, landms):\\n _, width, _ = image.shape\\n if random.randrange(2):\\n image = image[:, ::-1]\\n boxes = boxes.copy()\\n boxes[:, 0::2] = width - boxes[:, 2::-2]\\n\\n # landm\\n landms = landms.copy()\\n landms = landms.reshape([-1, 5, 2])\\n landms[:, :, 0] = width - landms[:, :, 0]\\n tmp = landms[:, 1, :].copy()\\n landms[:, 1, :] = landms[:, 0, :]\\n landms[:, 0, :] = tmp\\n tmp1 = landms[:, 4, :].copy()\\n landms[:, 4, :] = landms[:, 3, :]\\n landms[:, 3, :] = tmp1\\n landms = landms.reshape([-1, 10])\\n\\n return image, boxes, landms\\n\\n\\n###### Complete Pipeline Based on the Above\\n\\npython\\ndef augment(self, image, targets):\\n assert targets.shape[0] > 0, \\"this image does not have gt\\"\\n\\n target_boxes = targets[:, :4].copy()\\n labels = targets[:, -1].copy()\\n landm = targets[:, 4:-1].copy()\\n\\n # this random crop can also change the landmarks, which is problematic in\\n # albumentation (keyponits argument cause some incorrect augmented\\n # annotation)\\n image_t, boxes_t, labels_t, landm_t, pad_image_flag = _crop(image, target_boxes, labels, landm, self.img_dim)\\n image_t = _distort(image_t)\\n image_t = _pad_to_square(image_t, self.rgb_means, pad_image_flag)\\n image_t, boxes_t, landm_t = _mirror(image_t, boxes_t, landm_t)\\n height, width, _ = image_t.shape\\n\\n # change channel dimension ahead of height, width as well:\\n # also pad to square:\\n image_t = _resize_subtract_mean(image_t, self.img_dim, self.rgb_means)\\n # for pytorch\\n image_t = image_t.transpose(2, 0, 1)\\n\\n # normalize bboxes and landmarks:\\n boxes_t[:, 0::2] /= width\\n boxes_t[:, 1::2] /= height\\n landm_t[:, 0::2] /= width\\n landm_t[:, 1::2] /= height\\n\\n labels_t = np.expand_dims(labels_t, 1)\\n targets_t = np.hstack((boxes_t, landm_t, labels_t))\\n\\n return image_t, targets_t\\n\\n\\n#### RetinaFace.forward(inputs)\\n\\n##### Forward\\n\\npython-1\\ndef forward(self, inputs):\\n # inputs is a batch of images\\n out = self.body(inputs) # out = [(1, t_1), (2, t_2), (3, t_3)]\\n\\n\\nwhere self.body is the resnset50 model, with\\n\\n$$\\n\\\\texttt{t_i.shape} = (512\\\\times 2^{i}, H/2^{i+3}, W/2^{i+3})\\n$$\\n\\nfor $i=0, 1, 2$, i.e., features of strides 8, 16, 32 respectively.\\n\\nThese features are then mapped to have the same channel size (by what we call lateral embedding and upsampling) and **concatenated** by addition in a usual fpn network.\\n\\npython-4\\n fpn = self.fpn(out)\\n\\n\\n##### FPN\\n\\nHere self.fpn is an instance of:\\n\\npython\\nclass FPN(nn.Module):\\n def __init__(self,in_channels_list,out_channels):\\n super(FPN,self).__init__()\\n leaky = 0\\n\\n if (out_channels <= 64):\\n leaky = 0.1\\n\\n self.output1 = conv_bn1X1(in_channels_list[0], out_channels, stride = 1, leaky = leaky)\\n self.output2 = conv_bn1X1(in_channels_list[1], out_channels, stride = 1, leaky = leaky)\\n self.output3 = conv_bn1X1(in_channels_list[2], out_channels, stride = 1, leaky = leaky)\\n\\n self.merge1 = conv_bn(out_channels, out_channels, leaky = leaky)\\n self.merge2 = conv_bn(out_channels, out_channels, leaky = leaky)\\n\\n def forward(self, input):\\n # names = list(input.keys())\\n input = list(input.values())\\n\\n output1 = self.output1(input[0])\\n output2 = self.output2(input[1])\\n output3 = self.output3(input[2])\\n\\n up3 = F.interpolate(output3, size=[output2.size(2), output2.size(3)], mode=\\"nearest\\")\\n output2 = output2 + up3\\n output2 = self.merge2(output2)\\n\\n up2 = F.interpolate(output2, size=[output1.size(2), output1.size(3)], mode=\\"nearest\\")\\n output1 = output1 + up2\\n output1 = self.merge1(output1)\\n\\n out = [output1, output2, output3]\\n return out\\n\\n\\n##### SSH (Single Stage Headless detector)\\n\\nWe next encode the features further by single stage headless detector:\\n\\npython-5\\n feature1 = self.ssh1(fpn[0])\\n feature2 = self.ssh2(fpn[1])\\n feature3 = self.ssh3(fpn[2])\\n features = [feature1, feature2, feature3]\\n\\n\\nHere self.ssh1, self.ssh2 and self.ssh3 are instances of (we set in_channel and out_channel to 256):\\n\\npython\\nclass SSH(nn.Module):\\n def __init__(self, in_channel, out_channel):\\n super(SSH, self).__init__()\\n assert out_channel % 4 == 0\\n leaky = 0\\n if (out_channel <= 64):\\n leaky = 0.1\\n self.conv3X3 = conv_bn_no_relu(in_channel, out_channel//2, stride=1)\\n\\n self.conv5X5_1 = conv_bn(in_channel, out_channel//4, stride=1, leaky = leaky)\\n self.conv5X5_2 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\\n\\n self.conv7X7_2 = conv_bn(out_channel//4, out_channel//4, stride=1, leaky = leaky)\\n self.conv7x7_3 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\\n\\n def forward(self, input):\\n conv3X3 = self.conv3X3(input)\\n\\n conv5X5_1 = self.conv5X5_1(input)\\n conv5X5 = self.conv5X5_2(conv5X5_1)\\n\\n conv7X7_2 = self.conv7X7_2(conv5X5_1)\\n conv7X7 = self.conv7x7_3(conv7X7_2)\\n\\n out = torch.cat([conv3X3, conv5X5, conv7X7], dim=1)\\n out = F.relu(out)\\n return out\\n\\n\\n##### End of forward: Predictions by ClassHead, BboxHead, LandmarkHead\\n\\nWe finally estimate bbox_regressions, classifications and ldm_regressions by identical detection heads on different feature scale:\\n\\npython-9\\n bbox_regressions = torch.cat(\\n [self.BboxHead[i](feature) for i, feature in enumerate(features)],\\n dim=1\\n )\\n classifications = torch.cat(\\n [self.ClassHead[i](feature) for i, feature in enumerate(features)],\\n dim=1\\n )\\n ldm_regressions = torch.cat(\\n [self.LandmarkHead[i](feature) for i, feature in enumerate(features)],\\n dim=1\\n )\\n\\n if self.phase == \'train\':\\n output = (bbox_regressions, classifications, ldm_regressions)\\n else:\\n output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)\\n return output\\n\\n\\nwhere\\n\\npython\\nself.ClassHead = self._make_class_head(fpn_num=3, inchannels=cfg[\'out_channel\'])\\nself.BboxHead = self._make_bbox_head(fpn_num=3, inchannels=cfg[\'out_channel\'])\\nself.LandmarkHead = self._make_landmark_head(fpn_num=3, inchannels=cfg[\'out_channel\'])\\n\\ndef _make_class_head(self, fpn_num=3, inchannels=64, anchor_num=2):\\n classhead = nn.ModuleList()\\n for i in range(fpn_num):\\n classhead.append(ClassHead(inchannels, anchor_num))\\n return classhead\\n\\ndef _make_bbox_head(self, fpn_num=3, inchannels=64, anchor_num=2):\\n bboxhead = nn.ModuleList()\\n for i in range(fpn_num):\\n bboxhead.append(BboxHead(inchannels, anchor_num))\\n return bboxhead\\n\\ndef _make_landmark_head(self, fpn_num=3, inchannels=64, anchor_num=2):\\n landmarkhead = nn.ModuleList()\\n for i in range(fpn_num):\\n landmarkhead.append(LandmarkHead(inchannels, anchor_num))\\n\\nclass ClassHead(nn.Module):\\n def __init__(self, inchannels=512, num_anchors=3):\\n super(ClassHead, self).__init__()\\n self.num_anchors = num_anchors\\n self.conv1x1 = nn.Conv2d(\\n inchannels,\\n self.num_anchors * 2,\\n kernel_size=(1, 1),\\n stride=1,\\n padding=0\\n )\\n\\n def forward(self, x):\\n out = self.conv1x1(x)\\n out = out.permute(0, 2, 3, 1).contiguous()\\n\\n return out.view(out.shape[0], -1, 2)\\n\\n\\nclass BboxHead(nn.Module):\\n def __init__(self, inchannels=512, num_anchors=3):\\n super(BboxHead, self).__init__()\\n self.conv1x1 = nn.Conv2d(\\n inchannels,\\n num_anchors * 4,\\n kernel_size=(1, 1),\\n stride=1,\\n padding=0\\n )\\n\\n def forward(self, x):\\n out = self.conv1x1(x)\\n out = out.permute(0, 2, 3, 1).contiguous()\\n\\n return out.view(out.shape[0], -1, 4)\\n\\n\\nclass LandmarkHead(nn.Module):\\n def __init__(self, inchannels=512, num_anchors=3):\\n super(LandmarkHead, self).__init__()\\n self.conv1x1 = nn.Conv2d(\\n inchannels,\\n num_anchors * 10,\\n kernel_size=(1, 1),\\n stride=1,\\n padding=0\\n )\\n\\n def forward(self, x):\\n out = self.conv1x1(x)\\n out = out.permute(0, 2, 3, 1).contiguous()\\n\\n return out.view(out.shape[0], -1, 10)\\n\\n\\n##### Summary for Detection Modules\\n\\n- RetinaFace network is nothing but a RPN network in faster RCNN.\\n- The only difference is: we use SSH module to decode the features of different scale from mobilenet/resnet instead of passing those features directly to detection head as in faster RCNN.\\n\\n#### Loss Calculation\\n\\n##### Loss Entry Point\\n\\npython\\ncriterion = MultiBoxLoss(num_classes, 0.35, True, 0, True, 7, 0.35, False)\\npriors = PriorBox(cfg, image_size=(img_dim, img_dim)).forward().cuda()\\nloss_l, loss_c, loss_landm = criterion(out, priors, targets)\\n\\n\\nWe explain PriorBox in the following:\\n\\n##### PriorBox\\n\\nIn short, PriorBox provides us tiled grids onto a plane with given image_size and stride (in the cfg).\\n\\npython\\nclass PriorBoxConfig(TypedDict):\\n min_sizes: int\\n steps: int\\n clip: bool\\n\\nclass PriorBox(object):\\n def __init__(self, cfg: PriorBoxConfig, image_size=None, phase=\'train\'):\\n super(PriorBox, self).__init__()\\n self.min_sizes = cfg[\'min_sizes\'] # min sizes = list of sizes of each sq anchor box w.r.t. feature scales,\\n # e.g., [[16, 32], [64, 128], [256, 512]]\\n self.steps = cfg[\'steps\'] # steps = strides\\n self.clip = cfg[\'clip\']\\n self.image_size = image_size\\n self.feature_maps = [\\n [ceil(self.image_size[0]/step), ceil(self.image_size[1]/step)]\\n for step in self.steps\\n ]\\n self.name = \\"s\\"\\n\\n def forward(self):\\n anchors = []\\n for k, f in enumerate(self.feature_maps):\\n min_sizes = self.min_sizes[k]\\n for i, j in product(range(f[0]), range(f[1])):\\n for min_size in min_sizes: # e.g., min_sizes = [16, 32]\\n s_kx = min_size / self.image_size[1]\\n s_ky = min_size / self.image_size[0]\\n cx = (j + 0.5) * self.steps[k] / self.image_size[1]\\n cy = (i + 0.5) * self.steps[k] / self.image_size[0]\\n anchors += [cx, cy, s_kx, s_ky]\\n\\n # back to torch land\\n output = torch.Tensor(anchors).view(-1, 4)\\n if self.clip:\\n output.clamp_(max=1, min=0)\\n return output\\n\\n\\n##### MultiBoxLoss\\n\\n###### MultiBoxLoss.\\\\_\\\\_init\\\\_\\\\_\\n\\nThe loss function is abstracted in an nn.Module:\\n\\npython\\nclass MultiBoxLoss(nn.Module):\\n \\"\\"\\"SSD Weighted Loss Function\\n Compute Targets:\\n 1) Produce Confidence Target Indices by matching ground truth boxes\\n with (default) \'priorboxes\' that have jaccard index > threshold parameter\\n (default threshold: 0.5).\\n 2) Produce localization target by \'encoding\' variance into offsets of ground\\n truth boxes and their matched \'priorboxes\'.\\n 3) Hard negative mining to filter the excessive number of negative examples\\n that comes with using a large number of default bounding boxes.\\n (default negative:positive ratio 3:1)\\n Objective Loss:\\n L(x,c,l,g) = (Lconf(x, c) + \u03b1Lloc(x,l,g)) / N\\n Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\\n weighted by \u03b1 which is set to 1 by cross val.\\n Args:\\n c: class confidences,\\n l: predicted boxes,\\n g: ground truth boxes\\n N: number of matched default boxes\\n See: https://arxiv.org/pdf/1512.02325.pdf for more details.\\n \\"\\"\\"\\n def __init__(\\n self,\\n num_classes,\\n overlap_thresh,\\n prior_for_matching,\\n bkg_label,\\n neg_mining,\\n neg_pos,\\n neg_overlap,\\n encode_target\\n ):\\n super(MultiBoxLoss, self).__init__()\\n self.num_classes = num_classes\\n self.threshold = overlap_thresh\\n self.background_label = bkg_label\\n self.encode_target = encode_target\\n self.use_prior_for_matching = prior_for_matching\\n self.do_neg_mining = neg_mining\\n self.negpos_ratio = neg_pos\\n self.neg_overlap = neg_overlap\\n self.variance = [0.1, 0.2]\\n\\n\\n###### MultiBoxLoss.forward\\n\\npython-1\\ndef forward(self, predictions, priors, targets):\\n \\"\\"\\"Multibox Loss\\n Args:\\n predictions (tuple): A tuple containing loc preds, conf preds,\\n and prior boxes from SSD net.\\n conf shape: torch.size(batch_size,num_priors,num_classes)\\n loc shape: torch.size(batch_size,num_priors,4)\\n priors shape: torch.size(num_priors,4)\\n\\n ground_truth (tensor): Ground truth boxes and labels for a batch,\\n shape: [batch_size,num_objs,5] (last idx is the label).\\n \\"\\"\\"\\n loc_data, conf_data, landm_data = predictions\\n priors = priors\\n num = loc_data.size(0)\\n num_priors = (priors.size(0))\\n\\n # match priors (default boxes) and ground truth boxes\\n loc_t = torch.Tensor(num, num_priors, 4)\\n landm_t = torch.Tensor(num, num_priors, 10)\\n conf_t = torch.LongTensor(num, num_priors)\\n for idx in range(num):\\n truths = targets[idx][:, :4]\\n labels = targets[idx][:, -1]\\n landms = targets[idx][:, 4:14]\\n defaults = priors.data\\n match(self.threshold, truths, defaults, self.variance,\\n labels, landms, loc_t, conf_t, landm_t, idx)\\n\\n\\n###### The Match Function\\n\\nloc_t, landm_t and conf_t are passed into match function as a reference and will be mutated to get normalized data from targets relative to the anchor with which the target bounding box fit the best.\\n\\npython\\ndef match(threshold, truths, priors, variances, labels, landms, loc_t, conf_t, landm_t, idx):\\n \\"\\"\\"Match each prior box with the ground truth box of the highest jaccard\\n overlap, encode the bounding boxes, then return the matched indices\\n corresponding to both confidence and location preds.\\n Args:\\n threshold: (float) The overlap threshold used when mathing boxes.\\n truths: (tensor) Ground truth boxes, Shape: [num_obj, 4].\\n priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\\n variances: (tensor) Variances corresponding to each prior coord,\\n Shape: [num_priors, 4].\\n labels: (tensor) All the class labels for the image, Shape: [num_obj].\\n landms: (tensor) Ground truth landms, Shape [num_obj, 10].\\n loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\\n conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\\n landm_t: (tensor) Tensor to be filled w/ endcoded landm targets.\\n idx: (int) current batch index\\n Return:\\n The matched indices corresponding to 1)location 2)confidence 3)landm preds.\\n \\"\\"\\"\\n # jaccard index\\n overlaps = jaccard(\\n truths,\\n point_form(priors)\\n )\\n # (Bipartite Matching)\\n # [1,num_objects] best prior for each ground truth\\n best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\\n\\n # ignore hard gt\\n valid_gt_idx = best_prior_overlap[:, 0] >= 0.2\\n best_prior_idx_filter = best_prior_idx[valid_gt_idx, :]\\n if best_prior_idx_filter.shape[0] <= 0:\\n loc_t[idx] = 0\\n conf_t[idx] = 0\\n return\\n\\n # [1,num_priors] best ground truth for each prior\\n best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\\n best_truth_idx.squeeze_(0)\\n best_truth_overlap.squeeze_(0)\\n best_prior_idx.squeeze_(1)\\n best_prior_idx_filter.squeeze_(1)\\n best_prior_overlap.squeeze_(1)\\n best_truth_overlap.index_fill_(0, best_prior_idx_filter, 2) # ensure best prior\\n # TODO refactor: index best_prior_idx with long tensor\\n # ensure every gt matches with its prior of max overlap\\n for j in range(best_prior_idx.size(0)): # \u5224\u522b\u6b64anchor\u662f\u9884\u6d4b\u54ea\u4e00\u4e2aboxes\\n best_truth_idx[best_prior_idx[j]] = j\\n matches = truths[best_truth_idx] # Shape: [num_priors,4] \u6b64\u5904\u4e3a\u6bcf\u4e00\u4e2aanchor\u5bf9\u5e94\u7684bbox\u53d6\u51fa\u6765\\n conf = labels[best_truth_idx] # Shape: [num_priors] \u6b64\u5904\u4e3a\u6bcf\u4e00\u4e2aanchor\u5bf9\u5e94\u7684label\u53d6\u51fa\u6765\\n conf[best_truth_overlap < threshold] = 0 # label as background overlap<0.35\u7684\u5168\u90e8\u4f5c\u4e3a\u8d1f\u6837\u672c\\n loc = encode(matches, priors, variances)\\n\\n matches_landm = landms[best_truth_idx]\\n landm = encode_landm(matches_landm, priors, variances)\\n loc_t[idx] = loc # [num_priors,4] encoded offsets to learn\\n conf_t[idx] = conf # [num_priors] top class label for each prior\\n landm_t[idx] = landm\\n\\n\\nThe assignment matches = truths[best_truth_idx] is actually distributing the ground truths to appropriate prior (anochor box) index, as is the assignment matches_landm = landms[best_truth_idx].\\n\\n###### Bounding Boxes Encoding and Landmarks Encoding\\n\\nBoth encoding functions map the ground truth data to a relative shifting data to the corresponding best anchor **_with all values being normalizaed_**:\\n\\npython\\ndef encode(matched, priors, variances):\\n \\"\\"\\"Encode the variances from the priorbox layers into the ground truth boxes\\n we have matched (based on jaccard overlap) with the prior boxes.\\n Args:\\n matched: (tensor) Coords of ground truth for each prior in point-form\\n Shape: [num_priors, 4].\\n priors: (tensor) Prior boxes in center-offset form, e.g., (cx, cy, w, h)\\n Shape: [num_priors,4].\\n variances: (list[float]) Variances of priorboxes\\n Return:\\n encoded boxes (tensor), Shape: [num_priors, 4]\\n \\"\\"\\"\\n\\n # dist b/t match center and prior\'s center\\n g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\\n # encode variance\\n g_cxcy /= (variances[0] * priors[:, 2:])\\n # match wh / prior wh\\n g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\\n g_wh = torch.log(g_wh) / variances[1]\\n # return target for smooth_l1_loss\\n return torch.cat([g_cxcy, g_wh], 1) # [num_priors,4]\\n\\ndef encode_landm(matched, priors, variances):\\n \\"\\"\\"Encode the variances from the priorbox layers into the ground truth boxes\\n we have matched (based on jaccard overlap) with the prior boxes.\\n Args:\\n matched: (tensor) Coords of ground truth for each prior in point-form\\n Shape: [num_priors, 10].\\n priors: (tensor) Prior boxes in center-offset form\\n Shape: [num_priors,4].\\n variances: (list[float]) Variances of priorboxes\\n Return:\\n encoded landm (tensor), Shape: [num_priors, 10]\\n \\"\\"\\"\\n\\n # dist b/t match center and prior\'s center\\n matched = torch.reshape(matched, (matched.size(0), config.n_landmarks, 2))\\n priors_cx = priors[:, 0].unsqueeze(1).expand(matched.size(0), config.n_landmarks).unsqueeze(2)\\n priors_cy = priors[:, 1].unsqueeze(1).expand(matched.size(0), config.n_landmarks).unsqueeze(2)\\n priors_w = priors[:, 2].unsqueeze(1).expand(matched.size(0), config.n_landmarks).unsqueeze(2)\\n priors_h = priors[:, 3].unsqueeze(1).expand(matched.size(0), config.n_landmarks).unsqueeze(2)\\n priors = torch.cat([priors_cx, priors_cy, priors_w, priors_h], dim=2)\\n g_cxcy = matched[:, :, :2] - priors[:, :, :2]\\n # encode variance\\n g_cxcy /= (variances[0] * priors[:, :, 2:])\\n # g_cxcy /= priors[:, :, 2:]\\n g_cxcy = g_cxcy.reshape(g_cxcy.size(0), -1)\\n # return target for smooth_l1_loss\\n return g_cxcy\\n\\n\\nThe variances are hyper-parameters that try to let the model learn more quickly.\\n\\n###### Calculate the Loss\\n\\nWe continue from the **MultiBoxLoss.forward** section:\\n\\npython-29\\n zeros = torch.tensor(0).cuda()\\n # landm Loss (Smooth L1)\\n # Shape: [batch,num_priors,10]\\n pos1 = conf_t > zeros\\n num_pos_landm = pos1.long().sum(1, keepdim=True)\\n N1 = max(num_pos_landm.data.sum().float(), 1)\\n pos_idx1 = pos1.unsqueeze(pos1.dim()).expand_as(landm_data)\\n landm_p = landm_data[pos_idx1].view(-1, 10)\\n landm_t = landm_t[pos_idx1].view(-1, 10)\\n loss_landm = F.smooth_l1_loss(landm_p, landm_t, reduction=\'sum\')\\n\\n pos = conf_t != zeros\\n conf_t[pos] = 1\\n\\n # Localization Loss (Smooth L1)\\n # Shape: [batch,num_priors,4]\\n pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\\n loc_p = loc_data[pos_idx].view(-1, 4)\\n loc_t = loc_t[pos_idx].view(-1, 4)\\n loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction=\'sum\')\\n\\n # Compute max conf across batch for hard negative mining\\n batch_conf = conf_data.view(-1, self.num_classes)\\n loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\\n\\n # Hard Negative Mining\\n loss_c[pos.view(-1, 1)] = 0 # filter out pos boxes for now\\n loss_c = loss_c.view(num, -1)\\n _, loss_idx = loss_c.sort(1, descending=True)\\n _, idx_rank = loss_idx.sort(1)\\n num_pos = pos.long().sum(1, keepdim=True)\\n num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\\n neg = idx_rank < num_neg.expand_as(idx_rank)\\n\\n # Confidence Loss Including Positive and Negative Examples\\n pos_idx = pos.unsqueeze(2).expand_as(conf_data)\\n neg_idx = neg.unsqueeze(2).expand_as(conf_data)\\n conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1,self.num_classes)\\n targets_weighted = conf_t[(pos+neg).gt(0)]\\n loss_c = F.cross_entropy(conf_p, targets_weighted, reduction=\'sum\')\\n\\n # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + \u03b1Lloc(x,l,g)) / N\\n N = max(num_pos.data.sum().float(), 1)\\n loss_l /= N\\n loss_c /= N\\n loss_landm /= N1\\n\\n return loss_l, loss_c, loss_landm\\n\\n","title":"RetinaFace","date":"2022-11-06T00:00:00.000Z","id":"blog0104","tag":"deep-learning, pytorch","intro":"Record the study of headless detector for face and landmarks."},{"content":"\\n#### Repository\\n\\n- [Repo Link](https://github.com/machingclee/2022-11-11-landmarks-trial-frontend)\\n- [Deployed Frontend](https://onnx-trial.vercel.app)\\n\\n#### Script to Convert Pytorch Model into ONNX model\\n\\nLet\'s take a model called RetinaFace as an example. Given that we have a target model that has been trained, we convert it into onnx model by pytorch built-in command:\\n\\npython\\nimport torch\\n\\nretina_face = RetinaFace(cfg=cfg)\\nretina_face.eval()\\nregression_parser = SimplifiedRegressionParser()\\nregression_parser.eval()\\n\\ndummy_inputs_retina_face = torch.randn(\\n 1, 3, config.input_img_size, config.input_img_size\\n)\\ndummy_inputs_regression_parser = (\\n torch.randn(1, config.n_priors, 4),\\n torch.randn(1, config.n_priors),\\n torch.randn(1, config.n_priors, 196)\\n)\\n\\ntorch.onnx.export(\\n retina_face,\\n dummy_inputs_retina_face,\\n \'FaceDetector.onnx\',\\n export_params=True,\\n verbose=False,\\n input_names=[\\"inputImage\\"],\\n output_names=[\\"bbox_regressions\\", \\"scores\\", \\"ldm_regressions\\"],\\n opset_version=11\\n)\\ntorch.onnx.export(\\n regression_parser,\\n args=dummy_inputs_regression_parser,\\n f=\'RegressionParser.onnx\',\\n export_params=True,\\n verbose=False,\\n input_names=[\\"bbox_regressions\\", \\"scores\\", \\"landm_regressions\\"],\\n output_names=[\\"boxes\\", \\"scores\\", \\"landms\\"],\\n opset_version=11\\n)\\n\\n\\n#### Script to Deploy the ONNX model in Frontend\\n\\n##### Loading Detection Head Model and Prediction Parser Model\\n\\nLet\'s create a file utils/modelUtils. In this inference we break our model into two parts\\n\\n- faceSession\\n- faceRegressionSession\\n\\nand load them via:\\n\\njavascript\\n// utils/modelUtils.ts\\n\\nimport * as ort from \\"onnxruntime-web\\";\\nimport { useEffect, useState, Dispatch, SetStateAction } from \\"react\\";\\nimport { getImageTensorFromPath } from \\"./imageUtils\\";\\nimport { time } from \\"./timeUtils\\";\\n\\nexport let faceSession: ort.InferenceSession;\\nexport let faceRegressionSession: ort.InferenceSession;\\n\\nexport const loadModels = async () => {\\n faceSession = await ort.InferenceSession.create(faceUrl, {\\n executionProviders: [\\"webgl\\"],\\n graphOptimizationLevel: \\"all\\",\\n });\\n\\n faceRegressionSession = await ort.InferenceSession.create(regressionUrl, {\\n executionProviders: [\\"wasm\\"],\\n graphOptimizationLevel: \\"all\\",\\n });\\n};\\n\\n\\n##### Why the Hack we Need two Models?\\n\\nAs can be seen in the parameter executionProviders above, we can choose to use webgl and wasm backend for our model. webgl employes GPU in the inference, but it comes with limitations:\\n\\n- webgl does not support all operators in the given opset_version\\n- Many, many usual operators have no support in webgl\\n- A complete list of supported operators can be found in [this link](https://github.com/microsoft/onnxruntime/blob/main/js/web/docs/operators.md)\\n\\nThe way to get around this problem is:\\n\\n- Unsupported operators should be moved into another model\\n- wasm supports all operators in a given opset_version\\n- Therefore we load that additional model with wasm backend in order to execute those unsupported operators\\n\\n##### Script to Execute Prediction\\n\\nThere are many image utils functions, you can unfold the following when you are curious:\\n\\n<details>\\n<summary> Click to inspect utils/imageUtils.ts </summary>\\n\\njavascript\\n// utils/imageUtils.ts\\n\\nimport * as Jimp from \'jimp\';\\nimport { Tensor } from \'onnxruntime-web\';\\nimport { getConfig } from \'./configUtils\';\\n\\nconst config = getConfig();\\n\\nexport async function getImageTensorFromPath(\\n path: string,\\n dims: number[] = [\\n 1,\\n 3,\\n config.modelRequiredSizes?.width || 640,\\n config.modelRequiredSizes?.height || 640\\n ]\\n): Promise<Tensor> {\\n var image = await loadImagefromPath(path, dims[2], dims[3]);\\n var imageTensor = imageDataToTensor(image, dims);\\n return imageTensor;\\n}\\n\\nasync function loadImagefromPath(\\n path: string,\\n width: number = config.modelRequiredSizes?.width || 640,\\n height: number = config.modelRequiredSizes?.height || 640\\n): Promise<Jimp> {\\n // Use Jimp to load the image and resize it.\\n var imageData = await Jimp.default.read(path).then((imageBuffer: Jimp) => {\\n return imageBuffer\\n });\\n\\n return imageData;\\n}\\nfunction imageDataToTensor(image: Jimp, dims: number[]): Tensor {\\n // 1. Get buffer data from image and create R, G, and B arrays.\\n var imageBufferData = image.bitmap.data;\\n const [redArray, greenArray, blueArray] = new Array(new Array<number>(), new Array<number>(), new Array<number>());\\n\\n // 2. Loop through the image buffer and extract the R, G, and B channels\\n for (let i = 0; i < imageBufferData.length; i += 4) {\\n redArray.push(imageBufferData[i]);\\n greenArray.push(imageBufferData[i + 1]);\\n blueArray.push(imageBufferData[i + 2]);\\n // skip data[i + 3] to filter out the alpha channel\\n }\\n\\n // 3. Concatenate RGB to transpose [224, 224, 3] -> [3, 224, 224] to a number array\\n const transposedData = redArray.concat(greenArray).concat(blueArray);\\n\\n // 4. convert to float32\\n let i, l = transposedData.length; // length, we need this for the loop\\n // create the Float32Array size 3 * 224 * 224 for these dimensions output\\n const float32Data = new Float32Array(dims[1] * dims[2] * dims[3]);\\n for (i = 0; i < l; i++) {\\n float32Data[i] = transposedData[i] / 255.0; // convert to float\\n }\\n // 5. create the tensor object from onnxruntime-web.\\n const inputTensor = new Tensor(\\"float32\\", float32Data, dims);\\n return inputTensor;\\n}\\n\\n\\n---\\n\\n</details>\\n\\njavascript\\n// utils/modelUtils.ts\\n\\nimport * as ort from \'onnxruntime-web\';\\nimport { useEffect, useState, Dispatch, SetStateAction } from \'react\';\\nimport { getImageTensorFromPath } from \'./imageUtils\';\\nimport { time } from \'./timeUtils\';\\n\\nexport const getPredictionFromImagePath = async (imgUrl: string): Promise<{\\n hasPositiveResult: boolean,\\n result: {\\n inferenceTime: number,\\n regressionTime: number,\\n box: number[],\\n landm: number[],\\n scores: number[]\\n }\\n}> => {\\n if (faceSession && faceRegressionSession) {\\n\\n const inferenceStartTime = time.time()\\n const imgTensor = await getImageTensorFromPath(imgUrl || \\"\\")\\n const feeds: Record<string, ort.Tensor> = {};\\n feeds[faceSession.inputNames[0]] = imgTensor;\\n const outputData = await faceSession.run(feeds);\\n const output0 = outputData[faceSession.outputNames[0]];\\n const output1 = outputData[faceSession.outputNames[1]];\\n const output2 = outputData[faceSession.outputNames[2]];\\n const inferenceEndtime = time.time()\\n\\n const inferenceTime = inferenceEndtime - inferenceStartTime;\\n\\n const feeds_: Record<string, ort.Tensor> = {};\\n feeds_[faceRegressionSession.inputNames[0]] = output0;\\n feeds_[faceRegressionSession.inputNames[1]] = output1;\\n feeds_[faceRegressionSession.inputNames[2]] = output2;\\n\\n const regressionStartTime = time.time();\\n const output = await faceRegressionSession.run(feeds_);\\n const regressionEndtime = time.time();\\n const regressionTime = regressionEndtime - regressionStartTime\\n\\n const { boxes, landms, scores } = output;\\n const hasPositiveResult = boxes.data.length > 0 && boxes && landms && scores;\\n\\n if (hasPositiveResult) {\\n return ({\\n hasPositiveResult: true,\\n result: {\\n inferenceTime,\\n regressionTime,\\n box: (boxes.data as any) as number[],\\n landm: (landms.data as any) as number[],\\n scores: (scores.data as any) as number[]\\n }\\n })\\n }\\n }\\n return ({\\n hasPositiveResult: false,\\n result: {\\n inferenceTime: 0,\\n regressionTime: 0,\\n box: [],\\n landm: [],\\n scores: [],\\n }\\n })\\n}\\n\\n\\n##### Start the Prediction Loop and Draw Result in Canvas\\n\\njavascript\\nconst startPrediction_ = async () => {\\n if (continuePredictionRef.current) {\\n const webcamRef = webcamVideoRef.current;\\n const video = webcamRef?.video;\\n const canvas = canvasRef.current;\\n\\n if (video && canvas) {\\n if (canvasWidthHeight.width == 0 && canvasWidthHeight.height == 0) {\\n // both are zero when being initialized like after changing the camera device\\n setCanvasWidthHeight({\\n width: video?.videoWidth,\\n height: video?.videoHeight,\\n });\\n }\\n const imgUrl = webcamRef.getScreenshot(\\n config?.modelRequiredSizes || { width: 640, height: 640 }\\n );\\n const ctx = canvas.getContext(\\"2d\\");\\n if (ctx) {\\n if (imgUrl) {\\n const prediction = await getPredictionFromImagePath(imgUrl);\\n const { hasPositiveResult, result } = prediction;\\n if (hasPositiveResult) {\\n const { box, landm, scores, inferenceTime, regressionTime } =\\n result;\\n setInfTimes({ inferenceTime, regressionTime });\\n // shape check:\\n if (box.length == 4 && landm.length == 196 && scores.length == 1) {\\n setScore(scores[0]);\\n const [xmin, ymin, xmax, ymax] = box;\\n ctx.clearRect(0, 0, video?.videoWidth, video?.videoHeight);\\n drawRectangle(ctx, xmin, ymin, xmax, ymax);\\n for (let i = 0; i < 98; i++) {\\n const x = landm[2 * i];\\n const y = landm[2 * i + 1];\\n drawDot(ctx, x, y);\\n }\\n }\\n }\\n }\\n }\\n }\\n await startPrediction_();\\n }\\n onPredictionStop();\\n};\\n\\n","title":"Onnx Model Deployment to Frontend From Pytorch Model","date":"2022-11-14T00:00:00.000Z","id":"blog0105","tag":"deep-learning, pytorch, react","intro":"Record how to change pytorch model into onnx model and deploy it to frontend."},{"content":"\\n#### Structure\\n\\nLet\'s create a folder at the same directory level as pages.\\n\\n<Center>\\n <img src=\\"/assets/tech/106-config-frontend/2022-11-15_034821.png\\"/>\\n</Center>\\n\\n<p/>\\n\\n- default.ts is config in common\\n- dev.ts: development specific config\\n- prod.ts: production specific config\\n\\nWe define TConfig to shape the interface of our configuration data:\\n\\njavascript\\n// TConfig.ts\\nexport type TConfig = {\\n modelRequiredSizes: {\\n width: number,\\n height: number,\\n },\\n};\\n\\n\\nand all of our config files will strictly follow this interface:\\n\\njavascript\\n// default.ts\\nimport { TConfig } from \\"./TConfig\\"\\n\\nconst config = {\\n modelRequiredSizes: {\\n width: 320,\\n height: 320\\n }\\n} as Partial<TConfig>\\n\\nexport default config\\n\\n\\njavascript\\n// dev.ts\\nimport { TConfig } from \\"./TConfig\\"\\n\\nconst config = {\\n} as Partial<TConfig>\\n\\nexport default config\\n\\n\\njavascript\\n// prod.ts\\nimport { TConfig } from \\"./TConfig\\"\\n\\nconst config = {\\n} as Partial<TConfig>\\n\\nexport default config\\n\\n\\n#### Get Config\\n\\nWe combine all the config files under different environments:\\n\\njavascript\\nimport _ from \\"lodash\\";\\nimport defaultConfig from \\"../config/default\\";\\nimport devConfig from \\"../config/dev\\";\\nimport prodConfig from \\"../config/prod\\";\\nimport { TConfig } from \\"../config/TConfig\\";\\n\\nexport const getConfig = (): Partial<TConfig> => {\\n const { NODE_ENV } = process.env;\\n const targetCofig: Partial<TConfig> =\\n NODE_ENV == \\"production\\" ? prodConfig : devConfig;\\n const combinedConfig = { ...defaultConfig, ...targetCofig };\\n\\n return combinedConfig;\\n};\\n\\n","title":"Config Files Organization in Frontend Project (Next.js Specific)","date":"2022-11-15T00:00:00.000Z","id":"blog0106","tag":"react, nextjs","intro":"Record how to change pytorch model into onnx model and deploy it to frontend."},{"content":"\\n#### Dependency\\n\\ntext\\nyarn add @reduxjs/toolkit react-redux @types/react-redux\\n\\n\\n#### Folder Structures\\n\\n<Center>\\n <img src=\\"/assets/tech/107-reduxtoolkit/2022-11-15_040250.png\\"/>\\n</Center>\\n\\n<p/>\\n\\n##### hooks.ts\\n\\njs\\nimport { TypedUseSelectorHook, useDispatch, useSelector } from \'react-redux\';\\nimport type { RootState, AppDispatch } from \'./store\';\\n\\n// Use throughout your app instead of plain useDispatch and useSelector\\nexport const useAppDispatch = () => useDispatch<AppDispatch>();\\nexport const useAppSelector: TypedUseSelectorHook<RootState> = useSelector;\\n\\n\\n##### store.ts\\n\\njs\\nimport { configureStore, ThunkAction, Action } from \\"@reduxjs/toolkit\\";\\nimport dictSlice from \\"../slices/dictSlice\\";\\n\\nexport const store = configureStore({\\n reducer: {\\n dict: dictSlice.reducer,\\n },\\n devTools: true,\\n});\\n\\nexport type AppDispatch = typeof store.dispatch;\\nexport type RootState = ReturnType<typeof store.getState>;\\nexport type AppThunk<ReturnType = void> = ThunkAction<\\n ReturnType,\\n RootState,\\n unknown,\\n Action<string>\\n>;\\n\\n\\n##### dictSlice.ts\\n\\nThe following is a simple start-up template:\\n\\njs\\nimport { createSlice, PayloadAction } from \\"@reduxjs/toolkit\\";\\n\\nexport type DictState = {\\n selectedNoteId: string,\\n selectedPageId: string,\\n selectedVocabId: string,\\n imageText: string,\\n searchText: string,\\n};\\n\\nconst initialState: DictState = {\\n selectedNoteId: \\"\\",\\n selectedPageId: \\"\\",\\n selectedVocabId: \\"\\",\\n imageText: \\"\\",\\n searchText: \\"\\",\\n};\\n\\nconst dictSlice = createSlice({\\n name: \\"dict\\",\\n initialState,\\n reducers: {\\n setSearchText: (state, action: PayloadAction<string>) => {\\n state.searchText = action.payload;\\n },\\n },\\n});\\n\\nexport default dictSlice;\\n\\n\\n##### util/registerEffects.ts\\n\\njs\\nimport {\\n AnyAction,\\n ListenerEffect,\\n ListenerMiddlewareInstance,\\n ThunkDispatch,\\n isAnyOf\\n} from \\"@reduxjs/toolkit\\";\\n// import msgUtil from \\"./msgUtil\\";\\n// import appSlice from \\"../redux/slices/appSlice\\";\\n\\ntype Effect = ListenerEffect<any, unknown, ThunkDispatch<unknown, unknown, AnyAction>, unknown>;\\n\\n/**\\n * actionMessageList consists of objects either of the form { action, content } or of the form { rejections } / { rejections, content }. When content is absent, the error message is supposed to be returned by thunkAPI.rejectWithValue\\n * in createAsyncThunk function.\\n */\\nconst messageDispatch = ({ contentType, content }: { contentType: string, content: string }) => {\\n if (contentType === \\"sucesss\\") {\\n // snackbarUtils.success(content)\\n } else if (contentType === \\"info\\") {\\n // snackbarUtils.info(content)\\n } else if (contentType === \\"warning\\") {\\n // snackbarUtils.warning(content)\\n } else if (contentType === \\"error\\") {\\n // snackbarUtils.error(content);\\n }\\n}\\n\\nexport default (\\n middleware: ListenerMiddlewareInstance<\\n unknown,\\n ThunkDispatch<unknown, unknown, AnyAction>,\\n unknown\\n >,\\n actionMessageList: {\\n action?: any,\\n rejections?: any[],\\n content?: string\\n effect?: Effect\\n contentType?: \\"sucesss\\" | \\"info\\" | \\"error\\" | \\"warning\\"\\n }[]\\n) => {\\n for (const actionMessage of actionMessageList) {\\n const { action, rejections, content, effect, contentType = \\"sucesss\\" } = actionMessage;\\n\\n if (action) {\\n let effect_: Effect;\\n if (effect) {\\n effect_ = effect;\\n } else if (content) {\\n effect_ = async (action, { dispatch }) => {\\n messageDispatch({ contentType, content })\\n // dispatch(appSlice.actions.updateNotification(\\n // { open: true, content: content || \\"No Message\\" }\\n // ))\\n };\\n } else {\\n effect_ = async (action, thunkAPI) => { };\\n }\\n\\n middleware.startListening({ actionCreator: action, effect: effect_ });\\n\\n } else if (rejections) {\\n if (effect) {\\n // @ts-ignore\\n middleware.startListening({ matcher: isAnyOf(...rejections), effect });\\n } else {\\n middleware.startListening({\\n // @ts-ignore\\n matcher: isAnyOf(...rejections),\\n effect: async (action, { dispatch }) => {\\n if (content) {\\n messageDispatch({ contentType, content })\\n // dispatch(appSlice.actions.updateNotification(\\n // { open: true, content: content || \\"No Message\\" }\\n // ))\\n } else {\\n const msg = action?.payload || \\"\\";\\n let errMsg = \\"Failed\\";\\n errMsg += (Reason: ${msg});\\n if (msg) {\\n msgUtil.error(msg);\\n }\\n }\\n }\\n })\\n }\\n\\n }\\n }\\n}\\n\\n","title":"Redux Toolkit Quick Setup","date":"2022-11-15T00:00:00.000Z","id":"blog0107","tag":"react","intro":"Record the setup for redux toolkit."},{"content":"\\n#### New Syntax\\n\\njs\\nimport { makeStyles } from \'tss-react/mui\';\\n\\nconst useStyles = makeStyles()((theme) => ({\\n button: {\\n color: \\"black\\",\\n textTransform: \\"none\\",\\n borderColor: \\"rgba(0,0,0,0.5)\\",\\n padding: \\"3px 10px\\",\\n borderRadius: 4,\\n \\"&:hover\\": {\\n color: \\"rgba(0,0,0,0.55)\\",\\n borderColor: \\"rgba(0,0,0,0.3)\\"\\n }\\n }\\n\\n}));\\n\\nconst SomeComponent = () => {\\n ...\\n const { classes, cx } = useStyles();\\n ...\\n}\\n\\n\\n#### New Config for Nextjs to Parse CSS on Server Side\\n\\n##### src/createEmotionCache.js\\n\\njs\\nimport createCache from \\"@emotion/cache\\";\\n\\nconst isBrowser = typeof document !== \\"undefined\\";\\n\\n// On the client side, Create a meta tag at the top of the <head> and set it as insertionPoint.\\n// This assures that MUI styles are loaded first.\\n// It allows developers to easily override MUI styles with other styling solutions, like CSS modules.\\nexport default function createEmotionCache() {\\n let insertionPoint;\\n\\n if (isBrowser) {\\n const emotionInsertionPoint = document.querySelector(\\n \'meta[name=\\"emotion-insertion-point\\"]\'\\n );\\n insertionPoint = emotionInsertionPoint ?? undefined;\\n }\\n\\n return createCache({ key: \\"mui-style\\", insertionPoint });\\n}\\n\\n\\n##### pages/\\\\_document.js\\n\\njs\\nimport * as React from \\"react\\";\\nimport Document, { Html, Head, Main, NextScript } from \\"next/document\\";\\nimport createEmotionServer from \\"@emotion/server/create-instance\\";\\nimport createEmotionCache from \\"../src/createEmotionCache\\";\\n\\nexport default class MyDocument extends Document {\\n render() {\\n return (\\n <Html lang=\\"en\\">\\n <Head>\\n {/* PWA primary color */}\\n <link rel=\\"shortcut icon\\" href=\\"/favicon.ico\\" />\\n <meta name=\\"emotion-insertion-point\\" content=\\"\\" />\\n {this.props.emotionStyleTags}\\n </Head>\\n <body>\\n <Main />\\n <NextScript />\\n </body>\\n </Html>\\n );\\n }\\n}\\n\\n// getInitialProps belongs to _document (instead of _app),\\n// it\'s compatible with static-site generation (SSG).\\nMyDocument.getInitialProps = async (ctx) => {\\n // Resolution order\\n //\\n // On the server:\\n // 1. app.getInitialProps\\n // 2. page.getInitialProps\\n // 3. document.getInitialProps\\n // 4. app.render\\n // 5. page.render\\n // 6. document.render\\n //\\n // On the server with error:\\n // 1. document.getInitialProps\\n // 2. app.render\\n // 3. page.render\\n // 4. document.render\\n //\\n // On the client\\n // 1. app.getInitialProps\\n // 2. page.getInitialProps\\n // 3. app.render\\n // 4. page.render\\n\\n const originalRenderPage = ctx.renderPage;\\n\\n // You can consider sharing the same Emotion cache between all the SSR requests to speed up performance.\\n // However, be aware that it can have global side effects.\\n const cache = createEmotionCache();\\n const { extractCriticalToChunks } = createEmotionServer(cache);\\n\\n ctx.renderPage = () =>\\n originalRenderPage({\\n enhanceApp: (App) =>\\n function EnhanceApp(props) {\\n return <App emotionCache={cache} {...props} />;\\n },\\n });\\n\\n const initialProps = await Document.getInitialProps(ctx);\\n // This is important. It prevents Emotion to render invalid HTML.\\n // See https://github.com/mui/material-ui/issues/26561#issuecomment-855286153\\n const emotionStyles = extractCriticalToChunks(initialProps.html);\\n const emotionStyleTags = emotionStyles.styles.map((style) => (\\n <style\\n data-emotion={${style.key} ${style.ids.join(\\" \\")}}\\n key={style.key}\\n // eslint-disable-next-line react/no-danger\\n dangerouslySetInnerHTML={{ __html: style.css }}\\n />\\n ));\\n\\n return {\\n ...initialProps,\\n emotionStyleTags,\\n };\\n};\\n\\n\\n##### pages/\\\\_app.tsx\\n\\njs\\n...\\nimport createEmotionCache from \'../src/createEmotionCache\';\\nimport { EmotionCache } from \\"@emotion/cache\\"\\nimport { CacheProvider } from \'@emotion/react\';\\n\\nconst clientSideEmotionCache = createEmotionCache();\\n\\nfunction MyApp({ Component, pageProps, emotionCache = clientSideEmotionCache }: AppProps & {\\n emotionCache: EmotionCache\\n}) {\\n const AnyComponent = Component as any;\\n useEffect(() => {\\n const jssStyles = document.querySelector(\'#jss-server-side\');\\n if (jssStyles) {\\n jssStyles.parentElement?.removeChild(jssStyles);\\n }\\n }, []);\\n return (\\n <CacheProvider value={emotionCache}>\\n ...\\n </CacheProvider>\\n );\\n}\\nexport default MyApp\\n\\n","title":"makeStyles for react-mui v5.0 and tss-react/mui","date":"2022-11-15T00:00:00.000Z","id":"blog0108","tag":"react, nextjs","intro":"The latest react mui version (v5.0) has breaking changes that make the config files in [THIS POST](/blog/article/Additional-Configuration-for-makeStyles-in-Next-js) fail. We discuss the corresponding necessary changes."},{"content":"\\n1. Right click the name of current solution\\n2. Click on Properties\\n3. Click on General\\n4. On the top, change Configuration to All Configuration;\\n5. change Platform to All Platforms\\n6. In General Properties > Output Directory, type\\n none\\n $(SolutionDir)bin\\\\$(Platform)\\\\$(Configuration)\\\\\\n \\n7. In General Properties > Intermediate Directory, type\\n none\\n $(SolutionDir)bin\\\\intermediates\\\\$(Platform)\\\\$(Configuration)\\\\\\n \\n","title":"Visual Studio Solution Configuration","date":"2022-11-16T00:00:00.000Z","id":"blog0109","tag":"C++","toc":false,"intro":"Record solution setup for cleaner folder structure"},{"content":"\\n#### Our String Class\\n\\ncpp-1\\n#include <iostream>\\n\\nusing std::cout;\\nusing std::cin;\\nusing std::endl;\\nusing std::ostream;\\n\\nclass String {\\nprivate:\\n\\tsize_t m_size;\\n\\tchar* m_buffer;\\n\\npublic:\\n\\tString(const char* string) {\\n\\t\\tm_size = strlen(string);\\n\\t\\tm_buffer = new char[m_size + 1];\\n\\t\\tmemcpy(m_buffer, string, m_size);\\n\\t\\tm_buffer[m_size] = 0;\\n\\t}\\n\\t~String() {\\n\\t\\tdelete[] m_buffer;\\n\\t}\\n\\tString(const String& other)\\n\\t\\t: m_size(other.m_size)\\n\\t{\\n\\t\\tm_buffer = new char[m_size + 1];\\n\\t\\tmemcpy(m_buffer, other.m_buffer, m_size);\\n\\t\\tm_buffer[m_size] = 0;\\n\\t}\\n\\n\\tchar& operator[](int index) {\\n\\t\\treturn m_buffer[index];\\n\\t}\\n\\n\\tfriend ostream& operator<<(ostream& stream, const String& string);\\n};\\n\\n\\nWe also define our own << for printing our String object.\\n\\ncpp\\nostream& operator<<(ostream& stream, const String& string) {\\n\\tstream << string.m_buffer;\\n\\treturn stream;\\n}\\n\\n\\n#### Why Copy Constructor is Always of this Signature? The Implicit Conversion\\n\\nThe function started at line 23, String(const String& other), is called the **_copy constructor_** of String. In fact, when we do the assignment:\\n\\ncpp\\nString name(\\"James\\");\\nString name2 = name;\\n\\n\\nThe second line undergoes the following processes:\\n\\n1. Compiler will determine whether name can be fed into one of the overloadings of our constructors.\\n2. If yes, it will do an **_implicit conversion_** and fed that parameter into that constructor, which is the copy constructor in our case.\\n\\nBtw, anytime we see = we are always copying something unless we are doing auto& a = b, i.e., creating an alias.\\n\\nNote that even a function returns a reference T& some_function() doesn\'t mean auto a = some_function() will store it as a reference, it keeps copying everything and create a in the copy constructor.\\n\\n#### Problem Without our own Copy Constructor\\n\\n- By default the compiler will copy all the member variables, including the pointer m_buffer without copying the heap-allocated array. That\'s what we call **_shallow copy_**.\\n- Exception will be caught when both name and name2 are our of the scope they live since ~String() will be called twice but both name.m_buffer and name2.m_buffer point to the same heap-allocated array.\\n\\n#### Test our String Class in main()\\n\\ncpp\\nint main() {\\n\\t{\\n\\t\\tString name(\\"James\\");\\n\\t\\tcout << name << endl;\\n\\t\\tString name2 = name;\\n\\t\\tname2[3] = \'s\';\\n\\t\\tcout << name2 << endl;\\n\\t}\\n\\tcin.get();\\n}\\n\\n","title":"Copy Constructor","date":"2022-11-23T00:00:00.000Z","id":"blog0110","tag":"C++","intro":"We implement our string class and demonstrate how to create a copy constructor for deep copying a string."},{"content":"\\n#### Step 1\\n\\nWe first prepare a header file that contains all the header that we want:\\n\\ncpp\\n// pch.h\\n#pragma once\\n\\n#include <iostream>\\n#include <array>\\n#include <vector>\\n#include <functional>\\n\\n\\n#### Step 2\\n\\nAt the same directory we create a pch.cpp that includes that header file\\n\\ncpp\\n// pch.cpp\\n#include \\"pch.h\\"\\n\\n\\n#### Step 3\\n\\nChange the properties of pch.cpp as follows:\\n\\n<Center>\\n<a\\n\\ttarget=\\"_blank\\"\\n\\thref=\\"/assets/tech/111-precompiled-header/001.png\\" \\n>\\n\\t<img \\n\\t\\tsrc=\\"/assets/tech/111-precompiled-header/001.png\\" \\n\\t\\twidth=\\"650\\"\\n\\t/>\\n</a>\\n</Center>\\n\\n#### Step 4\\n\\nChange the properties of **_the whole project_** as follows:\\n\\n<Center>\\n<a\\n\\ttarget=\\"_blank\\"\\n\\thref=\\"/assets/tech/111-precompiled-header/002.png\\" \\n>\\n\\t<img \\n\\t\\tsrc=\\"/assets/tech/111-precompiled-header/002.png\\" \\n\\t\\twidth=\\"650\\"\\n\\t/>\\n</a>\\n</Center>\\n","title":"Precompiled Header","date":"2022-11-27T00:00:00.000Z","id":"blog0111","tag":"C++","toc":false,"intro":"Record standard procedure to make a precompiled header file to avoid rebuilding it every time we build the whole project."},{"content":"\\n#### Repo\\n\\n- https://github.com/machingclee/cpp-study/blob/main/study/Study/Array.h\\n- https://github.com/machingclee/cpp-study/blob/main/study/Study/Vector.h\\n\\n#### Fixed Size Array\\n\\nThe following exmaple is very raw, it serves as the purpose of showing the importance why we need to overload [] twice with different signatures.\\n\\n##### Definition\\n\\ncpp\\ntemplate<typename T, size_t N>\\nclass Array {\\nprivate:\\n\\tT m_data[N];\\n\\npublic:\\n\\tArray() {\\n\\t\\t//signature: memset(T*, int value, int (size of the array))\\n\\t\\t//memset(data(), 0, size() * sizeof(T));\\n\\t\\t//or\\n\\t\\tmemset(&data()[0], 0, size() * sizeof(T));\\n\\t\\t//as C style arrays are always continguous in memory\\n\\t}\\n\\tconstexpr size_t size() const {\\n\\t\\treturn N;\\n\\t}\\n\\n\\tT& operator[](size_t index) {\\n\\t\\tif (index >= N) {\\n\\t\\t\\t__debugbreak();\\n\\t\\t}\\n\\t\\treturn m_data[index];\\n\\t}\\n\\tconst T& operator[] (size_t index) const {\\n\\t\\tif (index >= N) {\\n\\t\\t\\t__debugbreak();\\n\\t\\t}\\n\\t\\treturn m_data[index];\\n\\t}\\n\\n\\tT* data() {\\n\\t\\treturn m_data;\\n\\t}\\n};\\n\\n\\n##### The main() for Fixed Size Array\\n\\nWe test it out by our own main function below:\\n\\ncpp-1\\nint main(){\\n\\tconst Array<int, 5> arr;\\n\\tfor (int i = 0; i < arr.size(); i++) {\\n\\t\\tcout << arr[i] << endl;\\n\\t}\\n\\n\\tArray<int, 5> arr2;\\n\\n\\tfor (int i = 0; i < arr2.size(); i++) {\\n\\t\\tarr2[i] = 3;\\n\\t}\\n\\tfor (int i = 0; i < arr2.size(); i++) {\\n\\t\\tcout << arr2[i] << endl;\\n\\t}\\n}\\n\\n\\nThera are two overloadings for the [] operator on Array class.\\n\\n- **line 2 to line 5.** Since we declare const Array<int, 5>, the operator [] needs to be overloaded with output const T&.\\n\\n- **line 9 to line 11** In order for assigment operator to work, the operator [] needs to be overloaded to return a value T& for each index.\\n\\nThe output of the code above:\\n\\nnone\\n0\\n0\\n0\\n0\\n0\\n3\\n3\\n3\\n3\\n3\\n\\n\\nThe initilization and assignment are working correctly.\\n\\n#### Dynamically Sized Arrays\\n\\n##### Definition\\n\\n###### Vector3\\n\\nTo simulate real world situation we define a Vector3 class that contains a pointer that points to heap-allocated memory.\\n\\ncpp\\nstruct Vector3 {\\n\\tfloat x = 0.0f, y = 0.0f, z = 0.0f;\\n\\tint* m_MemoryBlock;\\n\\tVector3() {\\n\\t\\tm_MemoryBlock = new int[5];\\n\\t}\\n\\tVector3(float scalar)\\n\\t\\t: x(scalar), y(scalar), z(scalar)\\n\\t{\\n\\t\\tm_MemoryBlock = new int[5];\\n\\t}\\n\\tVector3(float x, float y, float z)\\n\\t\\t: x(x), y(y), z(z)\\n\\t{\\n\\t\\tm_MemoryBlock = new int[5];\\n\\t}\\n\\n\\tVector3(const Vector3& other) :\\n\\t\\tx(other.x), y(other.y), z(other.z)\\n\\t{\\n\\t\\tstd::cout << \\"Copy\\\\n\\";\\n\\t\\tm_MemoryBlock = new int[5];\\n\\t}\\n\\t~Vector3() {\\n\\t\\tstd::cout << \\"Destroy\\\\n\\";\\n\\t\\tdelete[] m_MemoryBlock;\\n\\t}\\n\\n\\t//Vector3& operator=(const Vector3& other) {\\n\\t//\\n\\t//\\tstd::cout << \\"Copy\\\\n\\";\\n\\t//\\tx = other.x;\\n\\t//\\ty = other.y;\\n\\t//\\tz = other.z;\\n\\t//\\treturn *this;\\n\\t//}\\n\\n\\tVector3& operator=(Vector3&& other) noexcept {\\n\\t\\tm_MemoryBlock = other.m_MemoryBlock;\\n\\t\\tother.m_MemoryBlock = nullptr;\\n\\t\\tstd::cout << \\"Move\\\\n\\" ;\\n\\t\\tx = other.x;\\n\\t\\ty = other.y;\\n\\t\\tz = other.z;\\n\\t\\treturn *this;\\n\\t}\\n};\\n\\nostream& operator<<(ostream& stream, const Vector3& vec3) {\\n\\tstream << \\"(\\" << vec3.x << \\", \\" << vec3.y << \\", \\" << vec3.z << \\")\\";\\n\\treturn stream;\\n}\\n\\n\\n###### Vector (The Dynamic Array)\\n\\ncpp\\ntemplate<typename T>\\nclass Vector {\\npublic:\\n\\tVector() {\\n\\t\\tReAlloc(2);\\n\\t}\\n\\t~Vector() {\\n\\t\\tClearByIndividualDestructor();\\n\\t\\t::operator delete(m_Data, m_Capacity * sizeof(T));\\n\\t}\\n\\n\\tsize_t Size() {\\n\\t\\treturn m_Size;\\n\\t}\\n\\tT& operator [] (int index) {\\n\\t\\tif (index >= m_Size) {\\n\\t\\t\\t__debugbreak();\\n\\t\\t}\\n\\t\\treturn m_Data[index];\\n\\t}\\n\\n\\tconst T& operator [] (int index) const {\\n\\t\\treturn m_Data[index];\\n\\t}\\n\\n\\ttemplate<typename... Args>\\n\\n\\tT& EmplaceBack(Args&&... args) {\\n\\t\\tstd::cout << \\"EmplaceBack\\\\n\\";\\n\\t\\tif (m_Size >= m_Capacity) {\\n\\t\\t\\tm_Capacity = (size_t)(m_Capacity * 1.5);\\n\\t\\t\\tReAlloc(m_Capacity);\\n\\t\\t}\\n\\n\\t\\t//m_Data[m_Size] = T(std::forward<Args>(args)...);\\n\\t\\tnew(&m_Data[m_Size])T(std::forward<Args>(args)...);\\n\\t\\tm_Size++;\\n\\t\\treturn m_Data[m_Size];\\n\\t}\\n\\n\\tvoid PopBack() {\\n\\t\\tif (m_Size > 0) {\\n\\t\\t\\tm_Size--;\\n\\t\\t\\tm_Data[m_Size].~T();\\n\\t\\t}\\n\\t}\\n\\n\\tvoid ClearByIndividualDestructor() {\\n\\t\\tfor (size_t i = 0; i < m_Size; i++) {\\n\\t\\t\\tm_Data[i].~T();\\n\\t\\t}\\n\\t\\tm_Size = 0;\\n\\t}\\n\\n\\tvoid PushBack(const T& value) {\\n\\t\\tLOG(\\"here\\");\\n\\t\\tif (m_Size >= m_Capacity) {\\n\\t\\t\\tm_Capacity = (size_t)(m_Capacity * 1.5);\\n\\t\\t\\tReAlloc(m_Capacity);\\n\\t\\t}\\n\\t\\tm_Data[m_Size] = value;\\n\\t\\tm_Size++;\\n\\t}\\n\\n\\tvoid PushBack(const T* valuePtr) {\\n\\t\\tif (m_Size >= m_Capacity) {\\n\\t\\t\\tm_Capacity = (size_t)(m_Capacity * 1.5);\\n\\t\\t\\tReAlloc(m_Capacity);\\n\\t\\t}\\n\\n\\t\\tm_Data[m_Size] = std::move((T) *valuePtr);\\n\\t\\tm_Size++;\\n\\t}\\n\\n\\tvoid PushBack(T&& value) {\\n\\t\\tif (m_Size >= m_Capacity) {\\n\\t\\t\\tm_Capacity = (size_t)(m_Capacity * 1.5);\\n\\t\\t\\tReAlloc(m_Capacity);\\n\\t\\t}\\n\\t\\t// when value (being a Rvalue) goes into this function, value itself become an Lvalue inside this function\\n\\t\\t// we need to convert value into a temporary value by std::move\\n\\t\\tm_Data[m_Size] = std::move(value);\\n\\t\\tm_Size++;\\n\\t}\\nprivate:\\n\\tT* m_Data = 0;\\n\\tsize_t m_Size = 0;\\n\\tsize_t m_Capacity = 0;\\n\\n\\n\\tvoid ReAlloc(size_t newCapacity) {\\n\\t\\tT* newBlock = (T*)::operator new(newCapacity * sizeof(T));\\n\\n\\t\\tif (newCapacity < m_Size) {\\n\\t\\t\\tm_Size = newCapacity;\\n\\t\\t}\\n\\n\\t\\tfor (size_t i = 0; i < m_Size; i++) {\\n\\t\\t\\t// newBlock[i] = std::move(m_Data[i]);\\n\\t\\t\\tnew(&newBlock[i]) T(std::move(m_Data[i]));\\n\\t\\t}\\n\\n\\t\\tfor (size_t i = 0; i < m_Size; i++) {\\n\\t\\t\\tm_Data[i].~T();\\n\\t\\t}\\n\\n\\t\\t// this will not call the destructor of all object inside the array\\n\\t\\t// therefore allocated memory will not be deleted twice\\n\\t\\t::operator delete(m_Data, m_Capacity * sizeof(T));\\n\\n\\n\\t\\tm_Data = newBlock;\\n\\t\\tm_Capacity = newCapacity;\\n\\t}\\n};\\n\\n\\n##### The main() for Dynamic Array\\n\\ncpp\\nvoid PrintVector(Vector<T>& vector) {\\n\\tfor (size_t i = 0; i < vector.Size(); i++) {\\n\\t\\tstd::cout << vector[i] << std::endl;\\n\\t}\\n\\tstd::cout << \\"---------------------------\\" << std::endl;\\n}\\n\\nint main() {\\n Vector<Vector3> vectors;\\n\\n vectors.EmplaceBack(5);\\n vectors.EmplaceBack();\\n vectors.PushBack(Vector3(3, 3, 1));\\n vectors.PushBack(new Vector3(1, 5, 5));\\n vectors.EmplaceBack(5, 5, 1);\\n vectors.EmplaceBack(6);\\n vectors.EmplaceBack(7);\\n vectors.EmplaceBack(5, 5, 2);\\n vectors.EmplaceBack(2, 2, 1);\\n vectors.PopBack();\\n vectors.PopBack();\\n PrintVector(vectors);\\n}\\n\\n\\nNo copy constructor as been called (in fact if it were called we get an error as we have commented the copy constructor out).\\n\\nThe output:\\n\\nnone\\nEmplaceBack\\nEmplaceBack\\nMove\\nMove\\nDestroy\\nDestroy\\nMove\\nDestroy\\nEmplaceBack\\nMove\\nMove\\nMove\\nDestroy\\nDestroy\\nDestroy\\nEmplaceBack\\nMove\\nMove\\nMove\\nMove\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nEmplaceBack\\nEmplaceBack\\nMove\\nMove\\nMove\\nMove\\nMove\\nMove\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nEmplaceBack\\nDestroy\\nDestroy\\n(5, 5, 5)\\n(0, 0, 0)\\n(3, 3, 1)\\n(5, 5, 1)\\n(6, 6, 6)\\n(7, 7, 7)\\n---------------------------\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\n\\n","title":"Array and Dynamic Array Class","date":"2022-11-29T00:00:00.000Z","id":"blog0113","tag":"C++","intro":"We can avoid heap memory allocation by using C style array stored in stack memory, given that we don\'t need such a flexibility of dynamic array (the std::vector class) and the size of the target array is known beforehand."},{"content":"\\n#### Repo\\n\\n- https://github.com/machingclee/cpp-study/blob/main/study/Study/Vector.h\\n\\n#### Iterator\\n\\nBy comparing the old and modern form of iterator we will understand which functions are required to be implemented.\\n\\n##### Original form of Iterator\\n\\ncpp\\nfor (Vector<int>::Iterator it = values.begin();\\n\\tit != values.end();\\n\\tit++\\n) {\\n\\t\\tstd::cout << *it << std::endl;\\n}\\n\\n\\n##### Modern Simplified form of Iterator\\n\\ncpp\\nfor (int value : values) {\\n\\tstd::cout << value << std::endl;\\n}\\n\\n\\n##### Defining Iterator Class\\n\\nTherefore we need to\\n\\n- define an iterator object (which is to be instantiated inside Vector class) that has defined:\\n\\n - ++\\n - * (dereference)\\n - ==\\n - !=\\n\\n- define methods begin() and start() to output Iterator.\\n\\nNext, a specific classname cannot be used as a template parameter, but a generic class that is parametrized by a typename:\\n\\ncpp\\ntemplate<typename T>\\nclass Vector {\\n\\t...\\n}\\n\\n\\ncan as well be used as a template parameter as follow:\\n\\ncpp\\ntemplate<typename Vector>\\nclass VectorIterator {\\npublic:\\n\\tusing ValueType = typename Vector::ValueType;\\n\\tusing PointerType = ValueType*;\\n\\tusing ReferenceType = ValueType&;\\n\\npublic:\\n\\tVectorIterator(PointerType ptr) : m_Ptr(ptr) {\\n\\t}\\n\\t// prefix ++\\n\\tVectorIterator& operator++() {\\n\\t\\tm_Ptr++;\\n\\t\\treturn *this;\\n\\t}\\n\\tReferenceType operator*() {\\n\\t\\treturn *m_Ptr;\\n\\t}\\n\\tbool operator ==(const VectorIterator& other) const {\\n\\t\\treturn m_Ptr == other.m_Ptr;\\n\\t}\\n\\tbool operator !=(const VectorIterator& other) const {\\n\\t\\treturn !(*this == other);\\n\\t}\\nprivate:\\n\\tPointerType m_Ptr;\\n};\\n\\n\\nSome important takeaway:\\n\\n- Here using are all used to give various alias to types.\\n- The **_scoped-alias_** created by using can be accessed by VectorIterator::PointerType.\\n- In order to let compiler distinguish between types and static members, we add typename keyword when defining ValueType, this Vector::ValueType will be added in the next section.\\n\\n#### Expand our Vector Class that Returns Iterator\\n\\nA lot of hard work has been done in the VectorIterator class above. In our original Vector class we just point out the additional code that bring VectorIterator into play.\\n\\ncpp\\ntemplate<typename T>\\nclass Vector {\\npublic:\\n\\tusing ValueType = T;\\n\\tusing Iterator = VectorIterator<Vector<T>>;\\n\\t...\\n\\n\\nand\\n\\ncpp\\npublic:\\n\\tIterator begin() {\\n\\t\\treturn Iterator(m_Data);\\n\\t}\\n\\n\\tIterator end() {\\n\\t\\treturn Iterator(m_Data + m_Size);\\n\\t}\\n\\n\\n#### Test our Iterator in main()\\n\\nNow we can test our iterator by:\\n\\ncpp\\nint main(){\\n\\tVector<Vector3> vectors;\\n\\n\\tvectors.EmplaceBack(5);\\n\\tvectors.EmplaceBack();\\n\\tvectors.PushBack(Vector3(3, 3, 1));\\n\\tvectors.PushBack(new Vector3(1, 5, 5));\\n\\tvectors.EmplaceBack(5, 5, 1);\\n\\tvectors.EmplaceBack(6);\\n\\tvectors.EmplaceBack(7);\\n\\tvectors.EmplaceBack(5, 5, 2);\\n\\tvectors.EmplaceBack(2, 2, 1);\\n\\tvectors.PopBack();\\n\\tvectors.PopBack();\\n\\t//PrintVector(vectors);\\n\\n\\tfor (Vector3& vector : vectors) {\\n\\t\\tstd::cout << vector << std::endl;\\n\\t}\\n}\\n\\n\\nand the output is the same as before:\\n\\nnone\\nEmplaceBack\\nEmplaceBack\\nMove\\nMove\\nDestroy\\nDestroy\\nMove\\nDestroy\\nMove\\nMove\\nMove\\nDestroy\\nDestroy\\nDestroy\\nCopy\\nMove\\nDestroy\\nEmplaceBack\\nMove\\nMove\\nMove\\nMove\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nEmplaceBack\\nEmplaceBack\\nMove\\nMove\\nMove\\nMove\\nMove\\nMove\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nEmplaceBack\\nEmplaceBack\\nDestroy\\nDestroy\\n(5, 5, 5)\\n(0, 0, 0)\\n(3, 3, 1)\\n(1, 5, 5)\\n(5, 5, 1)\\n(6, 6, 6)\\n(7, 7, 7)\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\nDestroy\\n\\n","title":"Iterator","date":"2022-11-30T00:00:00.000Z","id":"blog0114","tag":"C++","intro":"We try to implement iterator that iterates elements inside our vector class in the [PREVIOUS BLOG](/blog/article/Array-and-Dynamic-Array-Class)."},{"content":"\\ncpp\\ntemplate <class F, class First, class... Rest>\\nvoid do_for(F f, First first, Rest... rest) {\\n f(first);\\n do_for(f, rest...);\\n}\\ntemplate <class F>\\n\\nvoid do_for(F f) {\\n std::cout << \\"\\\\n\\";\\n}\\n\\ntemplate <class... Args>\\nvoid print(Args... args) {\\n do_for([](auto& arg) {\\n std::cout << arg;\\n }, args...);\\n}\\n\\n","title":"Variadic Version of Print in C++","date":"2022-12-09T00:00:00.000Z","id":"blog0115","tag":"C++","toc":false,"intro":"Record a useful print function which behaves like console.log in javascript and print in python."},{"content":"\\n#### Repo\\n\\nThe C++ project we made will be a CMake project.\\n\\n- https://github.com/machingclee/2022-12-12-CMake-TCP-Server-Study\\n\\n#### Install Boost Asio\\n\\n- We have a static library called MOYFNetworking, we just need to include the boost header once inside our MOYFNetworking/common.h.\\n- We will be able to use boost library once we include this common.h file.\\n- Inside the library MOYFNetworking, we need to define the environment variable BOOST_ROOT which points to the include directory of the boost library.\\n\\ntext\\nset(BOOST_ROOT \\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\C++Libraries\\\\\\\\boost_1_80_0\\")\\n\\n\\nas is done in [CMakeLists.txt](https://github.com/machingclee/2022-12-12-CMake-TCP-Server-Study/blob/main/MUDOnYourFace/MOYFNetworking/CMakeLists.txt).\\n\\n#### Networking\\n\\n##### common.h\\n\\nWe include all common header files and create useful utility functions inside this file:\\n\\ncpp\\nNetworking/include/Networking/common.h\\n\\n#pragma once\\n\\n#include \\"boost/asio.hpp\\"\\n#include <iostream>\\n#include <iterator>\\n#include <vector>\\n\\n\\nnamespace MOYF {\\n using tcp = boost::asio::ip::tcp;\\n}\\nenum class MyEnum {\\n One,\\n Two,\\n Three\\n};\\n\\ntemplate<class F, class First, class... Rest>\\nvoid do_for(F f, First first, Rest... rest) {\\n f(first);\\n do_for(f, rest...);\\n}\\n\\ntemplate<class F>\\n\\nvoid do_for(F f) {\\n std::cout << \\"\\\\n\\";\\n}\\n\\ntemplate<class... Args>\\nvoid print(Args... args) {\\n do_for([](auto &arg) {\\n std::cout << arg;\\n }, args...);\\n}\\n\\n\\n##### TCPConnection\\n\\ncpp\\n// tcp_connection.h\\n\\n#pragma once\\n\\n#include \\"MOYFNetworking/common.h\\"\\n#include <memory>\\n\\nnamespace MOYF {\\n\\n class TCPConnection : public std::enable_shared_from_this<TCPConnection> {\\n public:\\n using Pointer = std::shared_ptr<TCPConnection>;\\n\\n static Pointer Create(boost::asio::io_context &ioContext) {\\n return Pointer(new TCPConnection(ioContext));\\n }\\n\\n tcp::socket &Socket() {\\n return _socket;\\n }\\n\\n void Start();\\n\\n private:\\n explicit TCPConnection(boost::asio::io_context &ioContext);\\n\\n\\n private:\\n tcp::socket _socket;\\n std::string _message{\\"Hello, client!\\"};\\n };\\n}\\n\\n\\ncpp\\n// tcp_connection.cpp\\n\\n#include \\"MOYFNetworking/tcp_connection.h\\"\\n#include <memory>\\n\\nnamespace MOYF {\\n TCPConnection::TCPConnection(boost::asio::io_context &ioContext) :\\n _socket(ioContext) {\\n\\n }\\n\\n void TCPConnection::Start() {\\n auto strongThis = shared_from_this();\\n auto dataToSend = boost::asio::buffer(_message);\\n boost::asio::async_write(\\n _socket,\\n dataToSend,\\n [strongThis](const boost::system::error_code &error, size_t bytesTransferred) {\\n if (error) {\\n print(\\"Failed to send message\\");\\n } else {\\n print(\\"Sent \\", bytesTransferred, \\" bytes of data.\\");\\n }\\n }\\n );\\n\\n boost::asio::streambuf buffer;\\n _socket.async_receive(\\n buffer.prepare(512),\\n [this](const boost::system::error_code &error, size_t bytesTransferred) {\\n if (!error) {\\n print(\\"Client Disconnected Properly.\\");\\n } else {\\n print(\\"Client Disconnected in Bad Way.\\");\\n }\\n });\\n }\\n}\\n\\n\\n##### TCPServer\\n\\ncpp\\n// tcp_server.h\\n\\n#pragma once\\n\\n#include \\"MOYFNetworking/common.h\\"\\n#include \\"MOYFNetworking/tcp_connection.h\\"\\n\\n\\nnamespace MOYF {\\n enum class IPV {\\n V4,\\n V6\\n };\\n\\n class TCPServer {\\n public:\\n TCPServer(IPV ip_version, int port);\\n\\n int Run();\\n\\n private:\\n void StartAccept();\\n\\n private:\\n IPV _ipVersion;\\n int _port;\\n boost::asio::io_context _ioContext{};\\n boost::asio::ip::tcp::acceptor _acceptor;\\n\\n std::vector<TCPConnection::Pointer> _connections{};\\n };\\n}\\n\\n\\ncpp\\n// tcp_server.cpp\\n\\n#include \\"MOYFNetworking/tcp_server.h\\"\\n#include \\"MOYFNetworking/tcp_connection.h\\"\\n\\nnamespace MOYF {\\n using tcp = boost::asio::ip::tcp;\\n\\n TCPServer::TCPServer(IPV ip_version, int port)\\n : _ipVersion(ip_version), _port(port),\\n _acceptor(tcp::acceptor(\\n _ioContext,\\n tcp::endpoint(ip_version == IPV::V4 ? tcp::v4() : tcp::v6(), _port)\\n )) {}\\n\\n int TCPServer::Run() {\\n try {\\n StartAccept();\\n _ioContext.run();\\n } catch (std::exception &e) {\\n print(e.what());\\n return -1;\\n }\\n return 0;\\n }\\n\\n void TCPServer::StartAccept() {\\n // this connection will be destroyed once it is out of scope\\n auto connection = TCPConnection::Create(_ioContext);\\n _connections.push_back(connection);\\n _acceptor.async_accept(\\n connection->Socket(),\\n [this, connection](const boost::system::error_code ec) {\\n // capture connection by value;\\n if (!ec) {\\n connection->Start();\\n } else {\\n print(ec.what());\\n }\\n StartAccept();\\n });\\n\\n }\\n}\\n\\n\\n#### NetClient\\n\\nThis is just a single cpp file which creates a binary file that pings our server:\\n\\ncpp\\n// main.cpp\\n\\n#include <iostream>\\n#include \\"MOYFNetworking/common.h\\"\\n#include <array>\\n\\nusing tcp = boost::asio::ip::tcp;\\n\\nint main(int argc, char *argv[]) {\\n try {\\n boost::asio::io_context ioContext;\\n\\n\\t\\t\\t\\ttcp::socket socket{ioContext};\\n tcp::resolver resolver{ioContext};\\n\\n auto endpoints = resolver.resolve(\\"127.0.0.1\\", \\"8080\\");\\n\\n boost::asio::connect(socket, endpoints);\\n\\n while (true) {\\n std::array<char, 128> receivedDataBuffer{};\\n boost::system::error_code error;\\n\\n size_t len = socket.read_some(boost::asio::buffer(receivedDataBuffer), error);\\n\\n if (error == boost::asio::error::eof) {\\n break;\\n } else if (error) {\\n throw boost::system::system_error(error);\\n };\\n\\n std::cout.write(receivedDataBuffer.data(), len);\\n }\\n } catch (std::exception &e) {\\n print(e.what());\\n }\\n}\\n\\n\\nRecall that main is the only special function that needs not to return anything though the return type is int.\\n\\n#### NetServer\\n\\ncpp\\n// main.cpp\\n\\n#include \\"boost/asio.hpp\\"\\n#include \\"MOYFNetworking/tcp_server.h\\"\\n\\nint port_number = 8080;\\n\\nint main() {\\n MOYF::TCPServer server{MOYF::IPV::V4, port_number};\\n server.Run();\\n}\\n\\n","title":"TCP Server Fundamental","date":"2022-12-12T00:00:00.000Z","id":"blog0116","tag":"C++","intro":"A simple TCP server that receive and hold a connection."},{"content":"\\n#### Usage\\n\\nThe md file consumed by the gray-matter package is of the form:\\n\\nmd\\n---\\nid: portfolio001\\ntitle: Desktop App to Capture Text From Images\\nintro: A desktop app by python using google\'s vison api.\\nthumbnail: /assets/portfolios/thumbnails/tkinter.jpg\\ntech: Python, tkinter\\nthumbWidth: 350\\nthumbTransX: -20\\nthumbTransY: -45\\ndate: 2019-12-29\\n---\\n\\n#### Repository\\n\\n- In Python with Tkinter: \\\\\\n https://github.com/machingclee/TextCaptrue_FirstTrial\\n\\n...\\n\\n\\nNow we can separate attributes and content as follows:\\n\\njs\\nimport matter from \\"gray-matter\\";\\n\\n...\\n\\nconst { content, data } = matter(mdText);\\n\\n\\ncontent will always be a string, data will be of type:\\n\\ntypescript\\n{\\n id: string,\\n title:string,\\n intro: string,\\n thumbnail: string,\\n tech: string,\\n thumbWidth: number,\\n thumbTransX: number,\\n thumbTransY: number,\\n date: Date\\n}\\n\\n\\nThe parser in gray-matter will implicitly convert our data into corresponding data type. We list some specific cases for the conversion in the next section.\\n\\nDon\'t forget to cast your data to your custom type for clean coding:\\n\\njs\\nconst data_ = data as CustomType;\\n\\n\\n#### Remark on Implicit Conversion\\n\\nSpecific keywords will trigger implicit conversion from plain text to available data type.\\n\\n- 111, a plain number, will be converted to a number\\n- false, and true will be converted to booleans\\n- 2022-01-01 will be converted to a Date object\\n\\nTo avoid them, we can enclose desired string by \\"\'s. For example, update: \\"true\\" in the attribute list will produce data.update: string.\\n","title":"Gray Matter","date":"2022-12-14T00:00:00.000Z","id":"blog0117","tag":"react, javascript, coding","intro":"Record the usage of gray-matter."},{"content":"\\n#### Resource to Learn CMake\\n\\n- [An Introduction to Modern CMake](https://cliutils.gitlab.io/modern-cmake/)\\n\\n#### Starting Template\\n\\ncmake\\ncmake_minimum_required(VERSION 3.22)\\n\\nset(This SuperHelloWorld)\\n\\nproject(\\n ${This}\\n VERSION 1.0.0\\n LANGUAGES C CXX\\n)\\n\\nlist(APPEND CMAKE_MODULE_PATH ${CMAKE_BINARY_DIR})\\nlist(APPEND CMAKE_PREFIX_PATH ${CMAKE_BINARY_DIR})\\n\\nset(CMAKE_CXX_STANDARD 20)\\nset(CMAKE_CXX_STANDARDCONFIGURED_FILE_INCLUDE_DIR_REQUIRED ON)\\nset(CMAKE_CXX_EXTENSIONS OFF)\\n\\nset(LIBRARY_MY_LIB my_lib)\\nset(EXE_APP app)\\n\\n# include(FetchContent)\\nadd_subdirectory(src)\\n\\n# message(\\"Using FetchContent\\")\\n# FetchContent_Declare(\\n# nlohmann_json\\n# GIT_REPOSITORY https://github.com/nlohmann/json.git\\n# GIT_TAG v3.11.2\\n# GIT_SHALLOW TRUE\\n# )\\n# FetchContent_MakeAvailable(nlohmann_json)\\n\\n\\n#### Common Commands\\n\\n- cmake\\n add_library(one two.cpp three.h)\\n \\n\\n - Declare a target called one that refers to the current directory, and list all sources files to be compiled. Only the compilation unit two.cpp will be compiled, we include the headers **_for IDE only_**.\\n\\n- cmake\\n add_executable(${THIS}_exe main.cpp)\\n \\n\\n - Same as add_library, it adds a target that refers to the current directory, and that target points to an executable.\\n - We cannot use add_library and add_target at the same time. In other words, we should separate main.cpp and \\"source files\\" in separate folder.\\n\\n - For example, we can separate like\\n - some_proj/app/main.cpp and\\n - some_proj/src/some_lib/some_file.cpp\\n Then write\\n - some_proj/app/CMakeLists.txt\\n - some_proj/src/CMakeLists.txt\\n - some_proj/src/some_lib/CMakeLists.txt separately.\\n Note that some_proj/src/CMakeLists.txt can be as simple as just one line\\n cmake\\n add_subdirectory(some_lib)\\n \\n as it helps point out which directory contains a CMakeLists.txt to look at.\\n\\n- cmake\\n add_subdirectory(src)\\n \\n\\n - It tells cmake compiler which directory to look for and execute a CMakeLists.txt. If the directory src contains no CMakeLists.txt file, cmake will give an exception.\\n\\n- cmake\\n # src/one/CMakeLists.txt\\n target_include_directories(one PUBLIC ../../include)\\n \\n\\n - It tells cmake our source files in the target one have included header files in **_other diectory_** such as ../../include.\\n - We don\'t need to include it again in other target that links to one, by simply target_link_libraries (we introduce it right below) we can directly include header files in source code that one have already included.\\n - If the include dir contains include/four/five.h, then we can include four/five.h in our source code.\\n\\n Note that the string to include our header file is independent of the target name we name in cmake.\\n\\n - In general, target_include_directories is used when the header files are from other directory.\\n\\n- cmake\\n target_link_libraries(another PUBLIC one)\\n \\n\\n - This is to build dependency between different targets. Which means that the target another and its downstream linkers will need library one in the compilation process.\\n\\n - Ya we have included the include directory but very likely it just contains function declarations, we need the function body definitions by linking those libraries $\\\\iff$ linking the source files.\\n\\n - if PUBLIC is replaced by PRIVATE, it indicates that the downstream linkers of another do not need the library one, and another is the only target that needs one.\\n\\n- cmake\\n target_compile_definitions(one PRIVATE SOME_CONSTANT=\\"${SOME_CONSTANT}\\")\\n \\n\\n - Target library one we have a constant SOME_CONSTANT defined by using #define in the header.\\n\\n- cmake\\n target_compile_features(one INTERFACE cxx_std_20)\\n \\n\\n - Require specific feature for a target.\\n\\n- cmake\\n set(\\n SOME_CONSTANT\\n \\"\\" # <value>\\n CACHE\\n STRING\\n \\"Description of the cached constant\\"\\n )\\n \\n\\n - SOME_CONSTANT is the variable name.\\n - We set \\"\\" as a default value.\\n - The value <value> can also be passed by -D argument:\\n\\n none\\n cmake -B build -DSOME_CONSTANT=ABCDEFG\\n \\n\\n - STRING defined the data type of the cached value <value>.\\n\\n- cmake\\n if(SOME_CONSTANT STREQUAL \\"\\")\\n message(SEND_ERROR \\"SOME_CONSTANT\\" must not be empty)\\n endif()\\n \\n\\n - This is a standard if statement.\\n\\n- cmake\\n option(COMPILE_EXECUTABLE \\"Whether to compile to executable\\" OFF)\\n \\n\\n - It defines a boolean for cmake files to use.\\n - We can pass this variable in command line by\\n text\\n cmake .. -DCOMPILE_EXECUTABLE=ON\\n \\n\\n#### Include Predefined CMake Functions\\n\\nUsually every cmake project contains a cmake/ folder that contains custom cmake script:\\n\\ncmake\\n# project_root/CMakeLists.txt\\nset(CMAKE_MODULE_PATH \\"${PROJECT_SOURCE_DIR}/cmake/\\")\\ninclude(AddGitSubModule)\\n\\n\\n- CMAKE_MODULE_PATH points to that cmake folder\\n- include is used to include the cmake file cmake/AddGitSubmodule.cmake.\\n Exmaple of a .cmake file:\\n\\ncmake\\n# project_root/cmake/AddGitSubmodule.cmake\\nfunction(add_git_submodule install_destination)\\n find_package(Git REQUIRED)\\n\\n if (NOT EXISTS ${install_destination}/CMakeLists.txt)\\n execute_process(COMMAND ${GIT_EXECUTABLE}\\n submodule update --init --recursive -- ${install_destination}\\n WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}\\n )\\n endif()\\n\\n add_subdirectory(${dir})\\nendfunction(add_git_submodule)\\n\\n\\n#### Which Folder to Create CMakeLists.txt?\\n\\n- **Root directory.**\\n - Set all necessary variables and add appropriate add_subdirectory\'s to look for CMakeLists.txt.\\n- **Source file directory src.**\\n - It can consists of simply the add_subdirectory commands.\\n- **Source file subdirectories src/one, src/two, ....**\\n - Each of the subdirectory should consists of add_library for linkage and target_include_directories to include corresponding header files.\\n - If the project structure is simpler, the rules to src/<lib_name>/CMakeLists.txt can be moved down to src/CMakeLists.txt.\\n- **Directory that does not need CMakeLists.txt.**\\n - Directory that contains just header files **_needs no_** CMakeLists.txt, as they will be included in target_include_directories somewhere else once needed.\\n\\n#### Show all Constants in a CMake Project\\n\\ncmake\\ncmake -LAH\\n\\n\\n#### Special Functions to Check Variables in CMakeLists.txt\\n\\ncmake\\ncmake_minimum_required(VERSION 3.22)\\n\\nproject(CMakeTutorial LANGUAGES CXX)\\nset(FOO \\"test\\")\\n\\nadd_executable(CMakeTutorial main.cpp)\\n\\nfunction(print)\\n foreach(var ${ARGN})\\n message(\\"[${var}]: ${${var}}\\")\\n endforeach()\\nendfunction()\\n\\nfunction(print_env)\\n foreach(var ${ARGN})\\n message(\\"[${var}]: $ENV{${var}}\\")\\n endforeach()\\nendfunction()\\n\\nprint(PROJECT_NAME FOO)\\n\\n\\n- The print function will print defined constant and print_env will print the defined environment variable.\\n\\n#### Graph Visualization of Dependencies Between cmake Files\\n\\n##### Install graphviz\\n\\nFor windows we can download a .msi file for installation [HERE](http://www.graphviz.org/). For mac we run brew install graphviz.\\n\\n##### MakeFile\\n\\n###### Scrips\\n\\nWe create a file called MakeFile:\\n\\nmakefile\\n# MakeFile\\ndependency:\\n\\tcd build && cmake .. --graphviz=graph.dot && \\\\\\n dot -Tpng graph.dot -o ../dep_graph.png\\nprepare:\\n\\trm -rf build\\n\\tmkdir build\\n\\n\\nand run cmake dependency.\\n\\nThe prepare script is also included here, it is a default command that can be run by simply calling cmake.\\n\\nFrom my own experience on a simple hello-world project I have:\\n\\n<Center>\\n<a href=\\"/assets/tech/118-cmake/dep_graph.png\\" target=\\"_blank\\">\\n<img width=\\"680\\" src=\\"/assets/tech/118-cmake/dep_graph.png\\"/>\\n</a>\\n</Center>\\n\\n###### Cmake Erorr: incompatible versions of the cygwin DLL\\n\\nIt is a known problem in windows, go to C:\\\\Program Files\\\\Git\\\\usr\\\\bin and rename msys-2.0.dll to _msys-2.0.dll to temporarily mitigate the problem.\\n\\nLater you may need to rename it back since it affects commands such as npm and yarn.\\n\\n#### Cross Platform Built Command\\n\\n##### Source of Study\\n\\n- [An Introduction to Mdern CMake](https://cliutils.gitlab.io/modern-cmake/)\\n\\n##### How to Build a CMake Project\\n\\ntext\\nmkdir build\\ncd build\\ncmake .. -G \\"Visual Studio 17 2022\\" -A x64 -DCMAKE_BUILD_TYPE=Debug\\nVERBOSE=1 cmake --build .\\n\\n\\n##### How to Know Which Generator (specified by -G) is Available?\\n\\nBy\\n\\ntext\\ncmake --help\\n\\n\\napart from the list of available arguments, it also prints a list of available generators for us.\\n\\nIn my case, I get\\n\\nnone\\n Visual Studio 17 2022 = Generates Visual Studio 2022 project files.\\n Use -A option to specify architecture.\\n Visual Studio 16 2019 = Generates Visual Studio 2019 project files.\\n Use -A option to specify architecture.\\n Visual Studio 15 2017 [arch] = Generates Visual Studio 2017 project files.\\n Optional [arch] can be \\"Win64\\" or \\"ARM\\".\\n Visual Studio 14 2015 [arch] = Generates Visual Studio 2015 project files.\\n Optional [arch] can be \\"Win64\\" or \\"ARM\\".\\n Visual Studio 12 2013 [arch] = Generates Visual Studio 2013 project files.\\n Optional [arch] can be \\"Win64\\" or \\"ARM\\".\\n Visual Studio 11 2012 [arch] = Generates Visual Studio 2012 project files.\\n Optional [arch] can be \\"Win64\\" or \\"ARM\\".\\n Visual Studio 10 2010 [arch] = Deprecated. Generates Visual Studio 2010\\n project files. Optional [arch] can be\\n \\"Win64\\" or \\"IA64\\".\\n Visual Studio 9 2008 [arch] = Generates Visual Studio 2008 project files.\\n Optional [arch] can be \\"Win64\\" or \\"IA64\\".\\n Borland Makefiles = Generates Borland makefiles.\\n NMake Makefiles = Generates NMake makefiles.\\n NMake Makefiles JOM = Generates JOM makefiles.\\n ...\\n\\n\\n#### External Library (CMake Project)\\n\\n##### By Direct Cloning\\n\\ntext\\ngit submodule add https://target/repo.git external/some_name\\n\\n\\nand then add_subdirectory(external/some_name) to seek for and execute the CMakeLists.txt.\\n\\n##### By FetchContent\\n\\nBy adding include(FetchContent) in our CMakeLists.txt we can import functions\\n\\n- FetchContent_Declare\\n- FetchContent_MakeAvailable\\n\\nFor example, suppose that we want to import the following CMake projects hosted in github:\\n\\ncmake\\ninclude(FetchContent)\\n\\nFetchContent_Declare(\\n nlohmann_json\\n GIT_REPOSITORY https://github.com/nlohmann/json.git\\n GIT_TAG v3.11.2\\n GIT_SHALLOW TRUE\\n)\\nFetchContent_Declare(\\n fmt\\n GIT_REPOSITORY https://github.com/fmtlib/fmt\\n GIT_TAG 9.1.0\\n GIT_SHALLOW TRUE\\n)\\nFetchContent_Declare(\\n spdlog\\n GIT_REPOSITORY https://github.com/gabime/spdlog\\n GIT_TAG v1.11.0\\n GIT_SHALLOW TRUE\\n)\\nFetchContent_Declare(\\n cxxopts\\n GIT_REPOSITORY https://github.com/jarro2783/cxxopts\\n GIT_TAG v3.0.0\\n GIT_SHALLOW TRUE\\n)\\nFetchContent_Declare(\\n Catch2\\n GIT_REPOSITORY https://github.com/catchorg/Catch2\\n GIT_TAG v2.13.9\\n GIT_SHALLOW TRUE\\n)\\n\\nFetchContent_MakeAvailable(nlohmann_json)\\nFetchContent_MakeAvailable(fmt)\\nFetchContent_MakeAvailable(spdlog)\\nFetchContent_MakeAvailable(cxxopts)\\nFetchContent_MakeAvailable(Catch2)\\n\\n\\nwhere the GIT_TAG can be found in the release page of the corresponding repository. These library will be downloaded in build/_deps directory when we execute cmake build command.\\n\\nTo let our target (executable target or library target) link to these library, we run\\n\\ncmake\\ntarget_link_libraries(\\n ${LIBRARY_MY_LIB}\\nPUBLIC\\n cxxopts::cxxopts\\n nlohmann_json::nlohmann_json\\n fmt::fmt\\n spdlog::spdlog\\n Catch2::Catch2\\n)\\n\\n\\nThe naming convention of the target is\\n\\n- <project_name>:<library_name>\\n The <project_name> and <library_name> can be traced by looking the CMakeLists.txt of the repo.\\n\\nNow our target LIBRARY_MY_LIB or any target that links to it can run the following preprocessor directives:\\n\\ncpp\\n#include \\"nlohmann/json.hpp\\"\\n#include \\"cxxopts.hpp\\"\\n#include \\"fmt/format.h\\"\\n#include \\"spdlog/spdlog.h\\"\\n#include \\"catch2/catch.hpp\\"\\n\\n\\n##### By conan\\n\\n###### Files to Create\\n\\nCreate a virtual environment in Python, which I name it conan, then conda activate conan and pip install conan.\\n\\nAs if requirements.txt in Python we have an analog in conan, we create a text file conanfile.txt in project root and add the following content:\\n\\n- project_root/conanfile.txt\\n\\n conan\\n [requires]\\n nlohmann_json/3.11.2\\n fmt/9.1.0\\n spdlog/1.11.0\\n catch2/2.13.9\\n cxxopts/3.0.0\\n\\n [generators]\\n cmake_find_package\\n cmake_paths\\n \\n\\n- project_root/CMakeLists.txt\\n cmake\\n message(\\"Using Conan\\")\\n include(${CMAKE_BINARY_DIR}/conan_paths.cmake)\\n find_package(nlohmann_json)\\n find_package(fmt)\\n find_package(spdlog)\\n find_package(Catch2)\\n find_package(cxxopts)\\n \\n\\nIn Makefile of project root directory we add\\n\\ntext\\nifeq \'$(findstring ;,$(PATH))\' \';\'\\n CONAN_FLAGS = -s compiler=\'Visual Studio\' -s compiler.version=17 -s cppstd=20 --build missing\\nelse\\n CONAN_FLAGS = -s cppstd=17 --build missing\\nendif\\n\\nprepare_conan:\\n\\trm -rf build\\n\\tmkdir build\\n\\tcd build && conan install .. $(CONAN_FLAGS)\\n\\n\\nand run make prepare_conan.\\n\\n###### Remarks to conan\\n\\nI myself fail to work with conan in windows, maybe unix based system can make it work.\\n\\nIn general the database of conan usually lag behind to the latest release for at least half year, it is suggested not to use it when FetchContent suffices to serve the purpose.\\n\\n#### CMake Examples\\n\\n##### ChatClient TCP Server\\n\\n###### Repo and Video\\n\\nThe whole project implementes a chatting function between multiple clients.\\n\\n- [Repo Link](https://github.com/machingclee/2022-12-12-CMake-TCP-Server-Study)\\n- [Video Link](https://www.youtube.com/watch?v=Bz38jjFB3H8)\\n\\nThis blog post focuses on the CMakeLists.txt files.\\n\\n###### Outermost CMakeLists.txt, the Project Level\\n\\n<Center>\\n<img src=\\"/assets/tech/118-cmake/outermost.png\\"/>\\n</Center>\\n\\n<p/>\\n\\ncmake\\ncmake_minimum_required(VERSION 3.22.2)\\nset(CMAKE_CXX_STANDARD 20)\\n\\nadd_subdirectory(MOYFNetworking)\\nadd_subdirectory(MOYFClient)\\nadd_subdirectory(MOYFServer)\\n\\n\\n###### The Main Library: Networking\\n\\n<Center>\\n<img src=\\"/assets/tech/118-cmake/networking.png\\"/>\\n</Center>\\n\\ncmake\\ncmake_minimum_required(VERSION 3.22.2)\\nproject(MOYFNetworking)\\n\\nset(CMAKE_CXX_STANDARD 20)\\n\\nset(BOOST_ROOT \\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\C++Libraries\\\\\\\\boost_1_80_0\\")\\n\\n\\nfind_package(Boost REQUIRED)\\nfile(GLOB_RECURSE SOURCES src/*.cpp)\\nadd_library(${PROJECT_NAME} ${SOURCES})\\n\\n# this says when building ${PROJECT_NAME} library, what follows must also be included\\ntarget_include_directories(\\n ${PROJECT_NAME}\\n PUBLIC\\n $<INSTALL_INTERFACE:include>\\n $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\\n ${Boost_INCLUDE_DIRS}\\n PRIVATE\\n)\\n\\n# PRIVATE means downstream linkers that link to ${PROJECT_NAME}\\n# library will not have access to ${Boost_LIBRARIES}\\n# in other words, ${Boost_LIBRARIES} is\\n# only used by the library ${PROJECT_NAME}.\\n\\n# If PRIVATE is replaced by PUBLIC, the downstream linkers that links\\n# ${PROJECT_NAME} will be able to use those libraries (${Boost_LIBRARIES} in this case).\\ntarget_link_libraries(\\n ${PROJECT_NAME} PRIVATE\\n ${Boost_LIBRARIES}\\n)\\n\\n\\n###### NetClient\\n\\n<Center>\\n<img src=\\"/assets/tech/118-cmake/client.png\\"/>\\n</Center>\\n\\n<p/>\\n\\ncmake\\ncmake_minimum_required(VERSION 3.22.2)\\nproject(MOYFClient)\\n\\nset(CMAKE_CXX_STANDARD 20)\\n\\nadd_executable(${PROJECT_NAME} main.cpp)\\n\\ntarget_include_directories(\\n ${PROJECT_NAME} PUBLIC\\n MOYFNetworking\\n)\\ntarget_link_libraries(\\n ${PROJECT_NAME} PUBLIC\\n MOYFNetworking\\n)\\n\\n\\n###### NetServer\\n\\n<Center>\\n<img src=\\"/assets/tech/118-cmake/server.png\\"/>\\n</Center>\\n\\ncmake\\ncmake_minimum_required(VERSION 3.22.2)\\nproject(MOYFServer)\\n\\nset(CMAKE_CXX_STANDARD 20)\\n\\nadd_executable(${PROJECT_NAME} main.cpp)\\n\\ntarget_include_directories(\\n ${PROJECT_NAME} PUBLIC\\n MOYFNetworking\\n)\\ntarget_link_libraries(\\n ${PROJECT_NAME} PUBLIC\\n MOYFNetworking\\n ws2_32\\n)\\n\\n\\n##### Examples from Other Project\\n\\n###### Sockets\\n\\n> **Repo.** https://github.com/rhymu8354/SocketTutorial\\n\\n<center></center>\\n\\nThis is a CMakeLists.txt file inside a directory called Sockets.\\n\\ncmake-1\\nset(This Sockets)\\nset(Sources\\n include/Sockets/ClientSocket.hpp\\n include/Sockets/DatagramSocket.hpp\\n include/Sockets/ServerSocket.hpp\\n src/Abstractions.hpp\\n src/ClientSocket.cpp\\n src/Connection.hpp\\n src/Connection.cpp\\n src/DatagramSocket.cpp\\n src/ServerSocket.cpp\\n)\\nif(MSVC)\\n list(APPEND Sources\\n src/AbstractionsWin32.cpp\\n )\\nelse()\\n list(APPEND Sources\\n src/AbstractionsPosix.cpp\\n src/PipeSignal.cpp\\n src/PipeSignal.hpp\\n )\\nendif()\\n\\nadd_library(${This} ${Sources})\\nset_target_properties(${This} PROPERTIES FOLDER Libraries)\\ntarget_include_directories(${This} PUBLIC include)\\nif(UNIX)\\n target_link_libraries(${This} PUBLIC pthread)\\nendif(UNIX)\\n\\n\\nFrom the section [The-Main-Library:-Networking](#The-Main-Library:-Networking) we have read a use case\\n\\ncmake\\nfile(GLOB_RECURSE SOURCES src/*.cpp)\\n\\n\\nand plugged this into add_library. However, the [Do\'s and Don\'ts](https://cliutils.gitlab.io/modern-cmake/chapters/intro/dodonot.html) states that **_Don\'t GLOB files_** because:\\n\\n> Make or another tool will not know if you add files without rerunning CMake. Note that CMake 3.12 adds a CONFIGURE_DEPENDS flag that makes this far better if you need to use it.\\n\\nIn other words, listing the source files explicitly will be a better practice.\\n","title":"Simple Introduction to CMake Files","date":"2022-12-15T00:00:00.000Z","id":"blog0118","tag":"C++","intro":"Record some experience and the CMakeLists.txt\'s that I have used."},{"content":"\\n#### Utils - Mimic the Split Functions for Text as in JS and Python\\n\\n**Header.**\\n\\ncpp\\n#pragma once\\n\\n#include <string>\\n#include <vector>\\n\\nstd::vector<std::string> split(std::string text, std::string space_delimiter = \\" \\");\\n\\n\\n**Implementation.**\\n\\ncpp\\n#include \\"utils/split.h\\"\\n\\nstd::vector<std::string> split(std::string text, std::string space_delimiter) {\\n bool hasNext = false;\\n std::vector<std::string> words{};\\n do {\\n size_t nextPos = text.find(space_delimiter);\\n hasNext = (nextPos != std::string::npos);\\n words.push_back(text.substr(0, nextPos));\\n text.erase(0, nextPos + space_delimiter.length());\\n } while (hasNext);\\n return words;\\n}\\n\\n\\n#### TcpListener\\n\\n**Header.**\\n\\ncpp\\n#pragma once\\n#include <WS2tcpip.h>\\n#include <iostream>\\n#include <sstream>\\nusing namespace std;\\n\\nclass TcpListener {\\npublic:\\n TcpListener(const char* ipAddress, int port);\\n int init(); // initialize the listener\\n int run(); // run the listener\\n\\nprotected:\\n virtual void onClientConnected(int clientSocket) = 0; // handler for client connection\\n virtual void onClientDisconnected(int clientSocket) = 0; // handler for client disconnection\\n virtual void onMessageReceived(int currSock, char* buffer, int bytesReceived) = 0; // handler for received message\\n void sendToClient(int clientSocket, const char* msg, int length); // send message to client\\n void broadcastToClients(int sendingClient, const char* msg, int length);\\n\\nprivate:\\n const char* _ipAddress; // ip of the web server\\n int _port; // port for the web service\\n int _socket; // internal socket fd\\n fd_set _fd_set; // master fd set\\n};\\n\\n\\n**Implementation.**\\n\\ncpp\\n#include \\"TcpListener.h\\"\\n\\nTcpListener::TcpListener(const char* ipAddress, int port)\\n : _ipAddress(ipAddress), _port(port){};\\n\\nint TcpListener::init() {\\n // initialize winsock\\n WSADATA wsData;\\n WORD ver = MAKEWORD(2, 2);\\n\\n int wsOk = WSAStartup(ver, &wsData);\\n if (wsOk != 0) {\\n cerr << \\"Can\'t Initialize winsock! Quitting\\" << endl;\\n return wsOk;\\n }\\n\\n // create a socket\\n _socket = socket(AF_INET, SOCK_STREAM, 0);\\n\\n if (_socket == INVALID_SOCKET) {\\n cerr << \\"can\'t create a socket, quitting\\" << endl;\\n return WSAGetLastError();\\n }\\n\\n // bind the ip address and port to a socket\\n sockaddr_in hint;\\n hint.sin_family = AF_INET;\\n hint.sin_port = htons(_port);\\n // hint.sin_addr.S_un.S_addr = INADDR_ANY;\\n // hint.sin_addr.S_un.S_addr = inet_addr(_ipAddress);\\n inet_pton(AF_INET, _ipAddress, &hint.sin_addr);\\n int bindResult = bind(_socket, (sockaddr*)&hint, sizeof(hint));\\n if (bindResult == SOCKET_ERROR) {\\n return WSAGetLastError();\\n }\\n\\n // tell winsock the socket is for listening\\n int listeningResult = listen(_socket, SOMAXCONN);\\n if (listeningResult == SOCKET_ERROR) {\\n return WSAGetLastError();\\n }\\n\\n FD_ZERO(&_fd_set);\\n FD_SET(_socket, &_fd_set);\\n\\n return 0;\\n};\\nint TcpListener::run() {\\n while (true) {\\n fd_set copy = _fd_set;\\n int socketCount = select(0, &copy, nullptr, nullptr, nullptr);\\n for (int i = 0; i < socketCount; i++) {\\n SOCKET currSock = copy.fd_array[i];\\n if (currSock == _socket) {\\n // accept a new connection\\n\\n // sockaddr_in client;\\n // int clientSize = sizeof(client);\\n // SOCKET clientSocket = accept(listening, (sockaddr *)&client, &clientSize);\\n\\n SOCKET clientSocket = accept(_socket, nullptr, nullptr);\\n\\n // add the new connection the list of connected clients\\n // socket <-> fd <-> u_int\\n FD_SET(clientSocket, &_fd_set);\\n onClientConnected(clientSocket);\\n } else {\\n char buffer[4096];\\n ZeroMemory(buffer, 4096);\\n\\n // receive message\\n int bytesReceived = recv(currSock, buffer, 4096, 0);\\n if (bytesReceived <= 0) {\\n // drop the client\\n // TODO: client disconnected;\\n onClientDisconnected(currSock);\\n closesocket(currSock);\\n FD_CLR(currSock, &_fd_set);\\n\\n } else {\\n // send message toimage.png other clients, excluding the listening socket\\n onMessageReceived(currSock, buffer, bytesReceived);\\n bool isNullSentence = buffer[0] == 13 && buffer[1] == 10;\\n if (!isNullSentence) {\\n for (int i = 0; i < _fd_set.fd_count; i++) {\\n SOCKET outSock = _fd_set.fd_array[i];\\n\\n if (outSock != _socket && outSock != currSock) {\\n // ostringstream ss;\\n // ss << \\"SOCKET #\\" << sock << \\": \\" << buffer << \\"\\\\r\\\\n\\";\\n // string strOut = ss.str();\\n // send(outSock, strOut.c_str(), strOut.size() + 1, 0);\\n }\\n }\\n }\\n }\\n }\\n }\\n }\\n\\n // remove the listening socket from the master fd set and close it\\n // to prevent anyone else trying to connect.\\n\\n FD_CLR(_socket, &_fd_set);\\n closesocket(_socket);\\n\\n while (_fd_set.fd_count > 0) {\\n SOCKET sock = _fd_set.fd_array[0];\\n FD_CLR(sock, &_fd_set);\\n closesocket(sock);\\n }\\n\\n WSACleanup();\\n return 0;\\n};\\n\\nvoid TcpListener::sendToClient(int clientSocket, const char* msg, int length) {\\n send(clientSocket, msg, length, 0);\\n}\\n\\nvoid TcpListener::broadcastToClients(int sendingClientToExlcude, const char* msg, int length) {\\n for (int i = 0; i < _fd_set.fd_count; i++) {\\n int outSock = _fd_set.fd_array[i];\\n if (sendingClientToExlcude != _socket && sendingClientToExlcude != outSock) {\\n sendToClient(outSock, msg, length);\\n }\\n }\\n}\\n\\n\\n#### MultiClientServer\\n\\n**Header.**\\n\\ncpp\\n#pragma once\\n#include \\"TcpListener.h\\"\\n\\nclass MultiClientChat : public TcpListener {\\npublic:\\n MultiClientChat(const char* ipAddress, int port) : TcpListener(ipAddress, port){};\\n\\nprotected:\\n void onClientConnected(int clientSocket);\\n void onClientDisconnected(int clientSocket);\\n void onMessageReceived(int currSock, char* buffer, int bytesReceived);\\n};\\n\\n\\n**Implementation.**\\n\\ncpp\\n#include \\"MultiClientChat.h\\"\\n\\nvoid MultiClientChat::onClientConnected(int clientSocket) {\\n // send a welcome message to the connected client\\n std::string welcomeMsg = \\"Welcome to the chat server\\";\\n sendToClient(clientSocket, welcomeMsg.c_str(), welcomeMsg.size() + 1);\\n};\\n\\nvoid MultiClientChat::onClientDisconnected(int clientSocket) {\\n ostringstream ss;\\n ss << \\"Client #\\" << clientSocket << \\" has disconnected\\";\\n std::string msg = ss.str();\\n broadcastToClients(clientSocket, msg.c_str(), msg.size() + 1);\\n};\\n\\nvoid MultiClientChat::onMessageReceived(int currSock, char* buffer, int bytesReceived) {\\n broadcastToClients(currSock, buffer, bytesReceived);\\n}\\n\\n\\n#### WebServer\\n\\n**Header.**\\n\\ncpp\\n#pragma once\\n#include \\"TcpListener.h\\"\\n\\nclass WebServer : public TcpListener {\\npublic:\\n WebServer(const char* ipAddress, int port) : TcpListener(ipAddress, port){};\\n\\nprotected:\\n void onClientConnected(int clientSocket);\\n void onClientDisconnected(int clientSocket);\\n void onMessageReceived(int currSock, char* buffer, int bytesReceived);\\n};\\n\\n\\n**Implementation.**\\n\\ncpp\\n#include \\"WebServer.h\\"\\n#include \\"utils/split.h\\"\\n#include <fstream>\\n#include <iostream>\\n#include <sstream>\\n#include <string>\\n#include <vector>\\n\\nvoid WebServer::onClientConnected(int clientSocket){\\n\\n};\\n\\nvoid WebServer::onClientDisconnected(int clientSocket){\\n\\n};\\n\\nvoid WebServer::onMessageReceived(int currSock, char* buffer, int bytesReceived) {\\n // GET /index.html HTTP/1.1\\n // parse out the doucment requested\\n // open the document in local file system\\n // write the document back to the client\\n std::string clientMessage{buffer};\\n // std::istringstream iss(buffer);\\n std::vector<std::string> parsed = split(clientMessage, \\" \\");\\n\\n std::string method = parsed[0];\\n std::string targetHtml = parsed[1];\\n\\n if (method == \\"GET\\") {\\n std::string fileRootLocation = \\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\Javascript\\\\\\\\2021-02-18-machingclee.github.io\\";\\n int htmlPos = targetHtml.find(\\".html\\");\\n\\n // remove .html in case it exists\\n if (htmlPos != std::string::npos) {\\n targetHtml.erase(htmlPos, htmlPos + 5);\\n }\\n\\n // naive routing\\n targetHtml.erase(0, 1);\\n std::string filePath = fileRootLocation + \\"\\\\\\\\\\" + (targetHtml == \\"\\" ? \\"\\" : (targetHtml + \\"\\\\\\\\\\")) + \\"index.html\\";\\n std::ifstream file{filePath};\\n std::string content{\\"404 Not Found\\"};\\n\\n if (file.good()) {\\n std::ostringstream ss;\\n ss << file.rdbuf();\\n content = ss.str();\\n }\\n file.close();\\n\\n std::ostringstream ss;\\n ss << \\"HTTP/1.1 200 OK\\\\r\\\\n\\"\\n << \\"Cache-Control: no-cache, private\\\\r\\\\n\\"\\n << \\"Content-Type: text/html\\\\r\\\\n\\"\\n << \\"Content-Length: \\"\\n << content.size()\\n << \\"\\\\r\\\\n\\"\\n << \\"\\\\r\\\\n\\"\\n << content;\\n\\n std::string res = ss.str();\\n sendToClient(currSock, res.c_str(), res.size() + 1);\\n }\\n};\\n\\n","title":"Socket Programming Fundamentals by winsock","date":"2023-01-10T00:00:00.000Z","id":"blog0119","tag":"C++","intro":"We create a simple TCP Server via winsock."},{"content":"\\n#### Install libtorch in Windows\\n\\nFrom [libtorch official web site](https://pytorch.org/get-started/locally/) we select\\n\\n<Center>\\n<a href=\\"/assets/tech/120/001.png\\">\\n<img src=\\"/assets/tech/120/001.png\\" width=\\"680\\"/>\\n</a>\\n</Center>\\n<p/>\\n<center></center>\\n\\nand click the link it pops up to download.\\n\\n#### Models from pytorch to pytorch C++\\n\\n##### A Model in Python\\n\\npython\\nimport torch\\nimport torch.nn.functional as F\\n\\n\\nclass Net(torch.nn.Module):\\n def __init__(self):\\n super(Net, self).__init__()\\n self.layer1 = torch.nn.Linear(100, 256)\\n self.layer2 = torch.nn.Linear(256, 1)\\n\\n def forward(self, x):\\n x = self.layer1(x)\\n x = F.relu(x)\\n x = self.layer2(x)\\n x = F.relu(x)\\n return x\\n\\n\\nif __name__ == \\"__main__\\":\\n traced_net = torch.jit.trace(Net(), torch.randn(1, 100))\\n torch.jit.save(traced_net, \\"models/net.pt\\")\\n\\n\\n##### Inference in C++\\n\\n###### Minimal Working CMakeLists.txt\\n\\ncmake\\nadd_executable(pymode_to_cpp main.cpp)\\n\\ntarget_link_libraries(pymode_to_cpp PUBLIC ${TORCH_LIBRARIES})\\n\\nif (MSVC)\\n message(\\"copying dll files\\")\\n file(GLOB TORCH_DLLS \\"${TORCH_INSTALL_PREFIX}/lib/*.dll\\")\\n add_custom_command(TARGET pymode_to_cpp\\n POST_BUILD\\n COMMAND ${CMAKE_COMMAND} -E copy_if_different\\n ${TORCH_DLLS}\\n $<TARGET_FILE_DIR:pymode_to_cpp>)\\nendif (MSVC)\\n\\n\\n###### The main.cpp File\\n\\ncpp\\n#include \\"torch/script.h\\"\\n#include \\"torch/torch.h\\"\\n#include <iostream>\\n#include <string>\\n\\nstd::string model_pt_path{\\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\C++\\\\\\\\\\n2023-01-12-DGGAN-in-pytorch-cpp-and-CMake-practice\\\\\\\\models\\\\\\\\net.pt\\"};\\n\\nint main() {\\n torch::jit::script::Module net = torch::jit::load(model_pt_path);\\n torch::Tensor x = torch::randn({1, 100});\\n torch::Tensor y = torch::randn({1, 100});\\n torch::Tensor inputs = torch::cat({x, y});\\n std::vector<torch::IValue> x_{inputs};\\n torch::Tensor yTensor = net.forward(x_).toTensor();\\n size_t ySize = yTensor.sizes()[0];\\n float* yDataPtr = (float*)yTensor.data_ptr();\\n try {\\n // float result = output.toTensor().item<float>();\\n for (int i = 0; i < ySize; i++) {\\n float value = yDataPtr[i];\\n std::cout << \\"The Float Value output: \\" << value << std::endl;\\n }\\n } catch (const c10::Error& e) {\\n std::cerr << e.msg() << std::endl;\\n }\\n}\\n\\n\\nwhich yields\\n\\ntext\\nThe Float Value output: 0.606562\\nThe Float Value output: 0.160477\\n\\n\\nNote that when yTensor is known to have batchSize 1, we may use Tensor.item<float>() to retrieve the data instead of resorting to the data_ptr() method.\\n","title":"Pytorch/libtorch with CPP API","date":"2023-01-11T00:00:00.000Z","id":"blog0120","tag":"C++, pytorch","intro":"We discuss how to import models trained in pytorch into cpp project."},{"content":"\\nSometimes soruce file cannot be found even cmake tool can build the project successfully, this is because an incorrect configuration provider is set, we can fix it by F1 and\\n\\ntext\\n>C++: Change Configuration Provider\\n\\n\\nand choose cmake tool. Which essentially modifies .vscode/settings.json and sets\\n\\ntext\\n\\"C_Cpp.default.configurationProvider\\": \\"ms-vscode.cmake-tools\\"\\n\\n","title":"C++ VSCode says it cannot open source file when it really can","date":"2023-01-28T00:00:00.000Z","id":"blog0121","tag":"C++","toc":false,"intro":"Fix the error of failing to find source files when it really can for cmake projects."},{"content":"\\n#### Python\'s print\\n\\ncpp\\n#include <string>\\n#include <vector>\\n#include <iostream>\\n\\nstd::vector<std::string> split(std::string text, std::string space_delimiter = \\" \\");\\n\\ntemplate <class F, class First, class... Rest>\\nvoid do_for(F f, First first, Rest... rest) {\\n f(first);\\n do_for(f, rest...);\\n}\\ntemplate <class F>\\n\\nvoid do_for(F f) {\\n std::cout << \\"\\\\n\\";\\n}\\n\\ntemplate <class... Args>\\nvoid print(Args... args) {\\n do_for([](auto& arg) {\\n std::cout << arg;\\n },\\n args...);\\n}\\n\\n\\n#### Python\'s os.sep\\n\\ncpp\\nstd::string get_os_sep() {\\n#ifdef _WIN32\\n return \'\\\\\\\\\';\\n#else\\n return \'/\';\\n#endif\\n}\\n\\n\\n#### Python\'s z_fill\\n\\ncpp\\n#include <sstream>\\n#include <iomanip>\\n\\nstd::string z_fill(int n, int numberOfLeadingZeros) {\\n std::ostringstream s;\\n s << std::setw(numberOfLeadingZeros) << std::setfill(\'0\') << n;\\n return s.str();\\n}\\n\\n\\n#### Python\'s os.listdir\\n\\ncpp\\n#include <string>\\n#include <iostream>\\n#include <filesystem>\\n#include <vector>\\n\\nstd::vector<std::string> list_dir(std::string directory_path) {\\n std::vector<std::string> file_paths;\\n for (const auto& entry : fs::directory_iterator(directory_path)) {\\n file_paths.push_back(entry.path());\\n }\\n return file_paths;\\n}\\n\\n\\n#### Python\'s String.split()\\n\\ncpp\\n#include <string>\\n#include <vector>\\n\\nstd::vector<std::string> split(std::string text, std::string space_delimiter) {\\n bool hasNext = false;\\n std::vector<std::string> words{};\\n do {\\n size_t nextPos = text.find(space_delimiter);\\n hasNext = (nextPos != std::string::npos);\\n words.push_back(text.substr(0, nextPos));\\n text.erase(0, nextPos + space_delimiter.length());\\n } while (hasNext);\\n return words;\\n}\\n\\n\\n#### Regular Expression and Exhaust all Possible Matchings\\n\\n**Situation.** There will be files called recording000001.avi, recording000002.avi among a list of files inside a directory. We need to find out all these avi files, sort the digits, and produce 000003 as a counter for the next .avi file.\\n\\ncpp\\n#include <regex>\\n#include <algorithm>\\n\\nstd::string get_file_next_digit(std::string dir, std::string file_ext) {\\n std::string sep = get_os_sep();\\n std::regex number_regex(\\"(\\\\\\\\d+)(?=\\\\\\\\.\\" + file_ext + \\")\\");\\n std::vector<std::string> file_paths = list_dir(dir);\\n std::vector<int> integers;\\n\\n // for each file_path, we split them by sep, get the last string,\\n // then exhaust all the matchings\\n for (std::string& file_path : file_paths) {\\n std::vector<std::string> results = split(file_path, sep);\\n std::string file_name = results.back();\\n\\n std::sregex_iterator current_match(file_name.begin(), file_name.end(), \\\\\\n number_regex);\\n std::sregex_iterator lastMatch;\\n\\n while (current_match != lastMatch) {\\n std::smatch match = *current_match;\\n int num = std::stoi(match.str());\\n integers.push_back(num);\\n current_match++;\\n }\\n }\\n\\n if (integers.size() > 0) {\\n std::sort(integers.begin(), integers.end());\\n std::string next_number = z_fill(integers[integers.size() - 1] + 1, 6);\\n return next_number;\\n } else {\\n return z_fill(0, 6);\\n }\\n}\\n\\n","title":"C++ Useful Util Functions Mimiced from Python, Regular Expression","date":"2023-02-09T00:00:00.000Z","id":"blog0122","tag":"C++","intro":"Record useful utility functions that I have found during my project on desktop app in C++."},{"content":"\\n# Conditional Variable\\n\\nWe intentionally init a write_thread, sleep for 2s, and the init a read_thread to make sure write_thread has made notification before the read_thread wait for it.\\n\\nWithout the boolean condition, the read_thread will wait indefinitely and we need to close the program forcefully, which is called a **lost wakeup**. This can be solved by introducing a shared boolean condition (which will be locked by our mutex guard, no data race)\\n\\nThen cv.wait(guard, [] { return condition; }), apart form waiting for cv.notify_one, will also check whether the notification as been published (by checking condition).\\n\\ncpp\\n#include <iostream>\\n#include <thread>\\n#include <condition_variable>\\n#include <string>\\n\\nusing namespace std;\\nusing namespace std::chrono;\\n\\n// Global variables\\nmutex mut;\\ncondition_variable cv;\\nstring sdata{\\"Empty\\"};\\nbool condition{false};\\n\\n// Waiting thread\\nvoid reader() {\\n unique_lock<std::mutex> guard(mut); // Acquire lock\\n cv.wait(guard, [] { return condition; }); // Wait for condition variable to be notified\\n cout << \\"Data is \\" << sdata << endl; // Wake up and use the new value\\n}\\n\\n// Modyifing thread\\nvoid writer() {\\n cout << \\"Writing data...\\" << endl;\\n {\\n lock_guard<std::mutex> lg(mut); // Acquire lock\\n std::this_thread::sleep_for(1s); // Pretend to be busy...\\n sdata = \\"Populated\\";\\n condition = true; // Modify the data\\n }\\n cv.notify_one(); // Notify the condition variable\\n}\\n\\nint main() {\\n cout << \\"Data is \\" << sdata << endl;\\n\\n thread write_thread{writer};\\n std::this_thread::sleep_for(2s);\\n thread read_thread{reader};\\n\\n write_thread.join();\\n read_thread.join();\\n}\\n\\n\\n# Promise and Future\\n\\ncpp\\n#include <future>\\n#include <iostream>\\n#include <thread>\\n\\nusing namespace std;\\n\\nvoid produce(promise<int>& px) { // Producer function with promise\\n int x{42};\\n this_thread::sleep_for(1s);\\n cout << \\"Promise sets shared state to \\" << x << endl;\\n px.set_value(x); // Set the result\\n}\\n\\nvoid consume(future<int>& fx) { // Consumer function with future\\n cout << \\"Future calling get()...\\" << endl;\\n int x = fx.get(); // Get the result\\n cout << \\"Future returns from calling get()\\" << endl;\\n cout << \\"The answer is \\" << x << endl;\\n}\\n\\nint main() {\\n promise<int> p;\\n future<int> f = p.get_future();\\n\\n thread prom{produce, std::ref(p)};\\n thread fut{consume, std::ref(f)};\\n\\n fut.join();\\n prom.join();\\n}\\n\\n","title":"Commuication Between Two Threads","date":"2023-02-27T00:00:00.000Z","id":"blog0123","tag":"C++","intro":"Discuss how two threads communiate with each other."},{"content":"\\n#### Result on Local Machine\\n\\n<Center>\\n<img src=\\"/assets/tech/124/005.png\\" width=\\"100%\\" />\\n</Center>\\n<p/>\\n\\n<center></center>\\n\\nDue to time constraint I didn\'t wait the model to train for long enough time as It is convincing to me that the model is trying to converge.\\n\\n#### Reference\\n\\nMain reference for coding part:\\n\\n- [Diffusion models from scratch in PyTorch](https://www.youtube.com/watch?v=a4Yfz2FxXiY&fbclid=IwAR14YaOBTAwTp_mmp_Q5MALMRuecTxDLUwcTAESifcB8IlqpmFYDy_IlnV4)\\n\\nOther references for theory:\\n\\n- [Denoising Diffusion Probabilitic Model](https://arxiv.org/pdf/2006.11239.pdf)\\n- [Diffusion Model\uff1a\u6bd4\u201cGAN\\"\u8fd8\u8981\u725b\u903c\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff01\u516c\u5f0f\u63a8\u5bfc+\u8bba\u6587\u7cbe\u8bfb\uff0c\u8fea\u54e5\u6253\u4f60\u4ece\u96f6\u8be6\u89e3\u6269\u6563\u6a21\u578b\uff01](https://www.bilibili.com/video/BV1pD4y1179T/?spm_id_from=333.788.recommend_more_video.11&vd_source=eaeec3286e77493a42a3dce415ee67cc)\\n- [Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models | ML Coding Series](https://www.youtube.com/watch?v=f6PtJKdey8E)\\n\\n#### Introduction\\n\\n<Center>\\n<img src=\\"/assets/tech/124/001.png\\" width=\\"100%\\" />\\n</Center>\\n<p/>\\n<center></center>\\n\\n- Define $x_t$ as the image at time $t=T$ for $T\\\\in {\\\\mathbb N}_0$. When we travel about $t$, we add noise gradually until the image is unreadable. The noise is added by\\n\\n $$\\n x_{t} = \\\\sqrt{\\\\alpha_{t-1}} x_{t-1} + \\\\sqrt{1-\\\\alpha_{t-1}}z_1,\\n $$\\n\\n for $t\\\\ge1$, where $z_t\\\\sim \\\\mathcal N(0,1)$ and $(0,1)\\\\ni\\\\alpha_t\\\\searrow 0$.\\n\\n- In practice (coding) $\\\\alpha_t$ will go from $1-0.0001$ to $1-0.02$, $\\\\alpha_t$ needs not be very small when the noise is enough.\\n\\n- Note that started from $t=1$, we no longer consider $x_t$ as a concrete image, rather we consider $x_t$ as a random variable where only the mean and variance makes perfect sense.\\n\\n- The true image depense on the instance of values that a gaussian noise provide.\\n\\n- That means $x_t$ denotes a set of possibilities of images (data point). To understant $x_t$, we need to understand the density of the probability distribution $p(x_t)$.\\n\\n- By direct expansion we have\\n\\n $$\\n x_t = \\\\sqrt{\\\\alpha_t \\\\alpha_{t-1}} x_{t-2} +\\\\sqrt{\\\\alpha_t(1-\\\\alpha_{t-1})}z_2 + \\\\sqrt{1-\\\\alpha_t}z_1,\\n $$\\n\\n where $z_1,z_2\\\\sim \\\\mathcal N(0,1)$.\\n\\n- Since $\\\\mathcal N(0, \\\\sigma_1^2I) + \\\\mathcal N(0, \\\\sigma_2^2I) = \\\\mathcal N (0, (\\\\sigma_1^2+\\\\sigma_2^2)I)$, the last term becomes\\n\\n $$\\n x_t = \\\\sqrt{\\\\alpha_t\\\\alpha_{t-1}}x_{t-2}+\\\\sqrt{1-\\\\alpha_t\\\\alpha_{t-1}}z_2\\n $$\\n\\n for some $z_2\\\\sim \\\\mathcal N(0, 1)$.\\n\\n- Define $\\\\overline{\\\\alpha}_t = \\\\prod\\\\limits_{0\\\\leq i < t}\\\\alpha_{t-i} = \\\\prod\\\\limits_{i=1}^{t}\\\\alpha_{i}$ for $k\\\\ge 1$, then\\n $$x_t = \\\\sqrt{\\\\overline{\\\\alpha}_t}x_0 +\\\\sqrt{1-\\\\overline{\\\\alpha}_t}\\\\cdot z_t, \\\\tag{$*$}$$ for some $z_t\\\\sim \\\\mathcal N (0, 1)$.\\n\\n- Note that $\\\\alpha_{t+1}\\\\overline{\\\\alpha}_t = \\\\overline{\\\\alpha}_{t+1}$.\\n\\n- The forward process of adding noise is denoted $x_t = q(x_{t-1})$\\n\\n <Center>\\n <img src=\\"/assets/tech/124/002.png\\" width=\\"100%\\" />\\n </Center>\\n <p/>\\n <center></center>\\n\\n We wish to calculate the reverse (denoise) process\\n\\n <Center>\\n <img src=\\"/assets/tech/124/003.png\\" width=\\"100%\\" />\\n </Center>\\n <p/>\\n <center></center>\\n\\n- Recall the Bayse Forumla $\\\\displaystyle P(A|B) = P(B|A)\\\\times \\\\frac{P(A)}{P(B)}$.\\n\\n- Given a set of images $x_t$, we wish to understand the distribution of $x_{t-1}$, i.e., we wish to calculate $p(x_{t-1}|x_t).$\\n\\n- $p$ usually is used to denote known distribution.\\n\\n- To emphasize we don\'t truly understand the distribution, we replace $p$ by $q$ to denote unknown distribution (the distribution that we are going to find or estimate, or to learn), the problem becomes estimating the distribution $q(x_{t-1}|x_t)$.\\n\\n- As we know we add random noise from $t-1$ to $t$, it makes no sense to estimate the exact value of a random variable $x_{t-1}$ from $x_t$.\\n\\n- Therefore what we want to estiamte is the average of $x_{t-1}$ from an instance of $x_t$.\\n\\n- By Bayse formula, $\\\\displaystyle q(x_{t-1}|x_t,x_0) = q(x_t|x_{t-1}, x_0) \\\\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}$.\\n\\n- To enable ourself to do computation, we also assume $q(x_{t-1}|x_t,x_0)$ follows Gaussian distribution.\\n\\n- We now try to estimate the mean of $x_{t-1}$, name it $\\\\tilde\\\\mu_{t-1}$.\\n\\n- We have already studied the distribution of $x_t$ in $(*)$.\\n\\n- Suppose that $x_t$ is given and we know that it comes from the previous distribution by adding gaussian noise with some weight (in the same way as before), then\\n\\n $$\\n \\\\begin{aligned}\\n &{\\\\color{white}=} q(x_{t-1}|x_{t}, x_0) \\\\\\\\\\n &\\\\propto \\\\exp \\\\left\\\\{-\\\\frac{1}{2}\\\\left(\\\\frac{(x_t-\\\\sqrt{\\\\alpha}x_{t-1})^2}{\\\\beta_t} + \\\\frac{(x_{t-1}-\\\\sqrt{\\\\overline{\\\\alpha}_{t-1}}x_0)^2}{1-\\\\overline{\\\\alpha}_{t-1}}- \\\\frac{(x_t-\\\\sqrt{\\\\overline{\\\\alpha}_t} x_0)^2}{1-\\\\overline{\\\\alpha}_t}\\\\right)\\\\right\\\\}\\\\\\\\\\n &=\\\\exp \\\\Bigg\\\\{\\n -\\\\frac{1}{2}\\\\Bigg(\\\\bigg(\\\\frac{\\\\alpha_t}{\\\\beta_t} + \\\\frac{1}{1-\\\\overline{\\\\alpha}_{t-1}}\\\\bigg)x_{t-1}^2 - \\\\bigg(\\\\frac{2\\\\sqrt{\\\\alpha_t}}{\\\\beta_t}x_t + \\\\frac{2\\\\sqrt{\\\\overline{\\\\alpha}_{t-1}}}{1-\\\\overline{\\\\alpha}_{t-1}} x_0\\\\bigg)x_{t-1}\\\\\\\\\\n &\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad+C(x_t,x_0)\\\\Bigg)\\n \\\\Bigg\\\\}\\\\\\\\\\n &=\\\\exp\\\\bigg(-\\\\frac{(x_{t-1}-\\\\mu)^2}{2\\\\sigma^2}\\\\bigg)\\n \\\\end{aligned}\\n $$\\n\\n where $\\\\beta_t=1-\\\\alpha_t$.\\n\\n- By comparing coefficients we have\\n\\n $$\\n \\\\mu =\\\\mu(x_t,x_0) = \\\\frac{\\\\sqrt{\\\\alpha_t}(1-\\\\overline{\\\\alpha}_{t-1})}{1-\\\\overline{\\\\alpha}_t}x_t + \\\\frac{\\\\sqrt{\\\\overline{\\\\alpha}_{t-1}}\\\\beta_t}{1-\\\\overline{\\\\alpha}_t}x_0,\\n $$\\n\\n by $(*)$ we have\\n\\n $$\\n \\\\begin{cases}\\n \\\\displaystyle\\\\tilde{\\\\mu}_{t-1} = \\\\frac{1}{\\\\sqrt{\\\\alpha_t}} \\\\bigg(x_t - \\\\frac{\\\\beta_t}{\\\\sqrt{1-\\\\overline{\\\\alpha}_t}}z_t\\\\bigg), \\\\\\\\\\n \\\\sigma_{t-1}^2= \\\\displaystyle\\\\left(\\\\frac{\\\\alpha_t}{\\\\beta_t}+ \\\\frac{1}{1-\\\\overline{\\\\alpha}_{t-1}}\\\\right)^{-1} = \\\\frac{1-\\\\overline{\\\\alpha}_{t-1}}{1-\\\\overline{\\\\alpha}_t}\\\\beta_t.\\n \\\\end{cases}\\n $$\\n\\n- $z_t$ will be what we are trying to learn.\\n\\n- When we predict image in reverse timesteps, we iteratively predict image by\\n\\n $$\\n \\\\boxed{\\n x_{t-1} = \\\\frac{1}{\\\\sqrt{\\\\alpha_t}} \\\\bigg(x_t - \\\\frac{\\\\beta_t}{\\\\sqrt{1-\\\\overline{\\\\alpha}_t}}\\\\mathrm{model}(x_t)\\\\bigg) + \\\\sqrt{\\\\frac{1-\\\\overline{\\\\alpha}_{t-1}}{1-\\\\overline{\\\\alpha}_t}\\\\beta_t} \\\\cdot \\\\epsilon\\n }\\n $$\\n\\n for some $\\\\epsilon$ sampled from normal distribution. In code it is implemented as follows:\\n\\n python\\n @torch.no_grad()\\n def sample_timestep(model, x, t):\\n \\"\\"\\"\\n Calls the model to predict the noise in the image and returns\\n the denoised image.\\n Applies noise to this image, if we are not in the last step yet.\\n \\"\\"\\"\\n betas_t = get_index_from_list(betas, t, x.shape)\\n sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\\n sqrt_one_minus_alphas_cumprod, t, x.shape\\n )\\n sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\\n\\n # Call model (current image - noise prediction)\\n model_mean = sqrt_recip_alphas_t * (\\n x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\\n )\\n posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\\n\\n if t == 0:\\n return model_mean\\n else:\\n noise = torch.randn_like(x)\\n return model_mean + torch.sqrt(posterior_variance_t) * noise\\n \\n\\n- From $x_1$ to $x_2$ we add a noize $z_1$. We estimate (learn) $\\\\tilde z_1$ from $x_2$ to $x_1$, then $z_1$ will be our ground truth in the model. We elaborate this in the next section.\\n\\n In code it is implemented as follows:\\n\\n python\\n def get_loss(model, x_0, times):\\n # times is of shape (128, )\\n x_noisy, noise = forward_diffusion_sample(x_0, times, device)\\n # 128 time times, therefore 128 images, x_noisy is of shape [128, 3, 64, 64]\\n noise_pred = model(x_noisy, times)\\n\\n return F.l1_loss(noise, noise_pred)\\n \\n\\n#### Training Algorithm\\n\\n- <Center>\\n <img src=\\"/assets/tech/124/004.png\\" width=\\"100%\\" />\\n </Center>\\n <p/>\\n <center></center>\\n\\n- In algorithm on the LHS:\\n\\n 2. means we sample an image from our collection of image dataset ($q(x_0)$ means the distribution of the images that $x_0$ lives in, like category of dogs, cats, etc)\\n 3. means the timestamp is uniformly random\\n 4. means the noise $\\\\epsilon$ we add from $t-1$ to $t$.\\n 5. $\\\\epsilon_0$ is the estimate of $\\\\epsilon$ from $t-1$ to $t$ (as we want to do the reverse). This $\\\\epsilon_0$ is estimated from\\n\\n - $x_t$ (see $(*)$) and\\n - timestamp $t$\\n\\n our loss function becomes $L = \\\\|\\\\epsilon - \\\\epsilon_0(x_t, t)\\\\|_2^2$.\\n\\n#### Coding\\n\\n##### Constants\\n\\npython\\ndef linear_beta_schedule(timesteps, start=0.0001, end=0.02):\\n return torch.linspace(start, end, timesteps)\\n\\ndef get_index_from_list(vals, t, x_shape):\\n \\"\\"\\"\\n Returns a specific index t of a passed list of values vals\\n while considering the batch dimension.\\n \\"\\"\\"\\n batch_size = t.shape[0]\\n out = vals.gather(-1, t.cpu())\\n # same as .reshape( (batch_size,) + ((1,) * (len(x_shape) - 1)) )\\n result = out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\\n return result\\n\\n# Define beta schedule\\nT = 300\\nIMG_SIZE = 64\\nTIMESTEPS_BATCH_SIZE = 128\\nbetas = linear_beta_schedule(timesteps=T)\\n\\n# Pre-calculate different terms for closed form\\nalphas = 1. - betas\\nalphas_cumprod = torch.cumprod(alphas, axis=0)\\nalphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\\nsqrt_recip_alphas = torch.sqrt(1.0 / alphas)\\nsqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\\nsqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\\nposterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\\n\\n\\n##### SinusoidalPositionEmbeddings\\n\\nThis is exactly the same as the one we use in transformer, which basically takes a time $t\\\\in\\\\mathbb N$ to a vector of size (32,).\\n\\nTo recall, positional encoding takes the following form: for each fixed $\\\\texttt{pos}\\\\in \\\\mathbb N$,\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathrm{PE}{(\\\\texttt{pos},2i)} & = \\\\sin(\\\\texttt{pos} / 10000^{i/d_{0.5\\\\times \\\\text{model}}}) \\\\\\\\\\n\\\\mathrm{PE}{(\\\\texttt{pos},2i+1)} & = \\\\cos(\\\\texttt{pos} / 10000^{i/d_{0.5\\\\times\\\\text{model}}})\\n\\\\end{aligned}\\n$$\\n\\nwhere $i=0,1,2,\\\\dots, \\\\frac{1}{2}d_\\\\text{model} - 1$.\\n\\npython-1\\nclass SinusoidalPositionEmbeddings(nn.Module):\\n def __init__(self, dim):\\n super().__init__()\\n # dim = 32\\n self.dim = dim\\n\\n def forward(self, times):\\n half_dim = self.dim // 2\\n embeddings = math.log(10000) / (half_dim - 1)\\n embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\\n\\n # a_i = 1/10000^(i/half_dim)\\n # embeddings above = [a_1, a_2, a_3, ..., a_16]\\n embeddings = times[:, None] * embeddings[None, :]\\n # embeddings above <=>\\n # t |-> ( sin t*a_1, cos t*a_1, sin t*a_2, cos t*a_2, sin t*a_3, cos t*a_3, ... )\\n # for each t, therefore the final dimension will be (128, 32)\\n embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\\n # TODO: Double check the ordering here\\n return embeddings\\n\\n\\nThe variable embeddings in line 14 above is exactly\\n\\n$$\\n\\\\left[\\n\\\\texttt{timestep} \\\\times\\n\\\\left[ \\\\frac{1}{10000^{i/d_{\\\\text{half_dim}}}}: 0\\\\leq i<32\\\\right]:\\n \\\\texttt{timestep} \\\\in \\\\texttt{times}\\n\\\\right]\\n$$\\n\\nwith timestep in place of pos above.\\n\\n##### UNet that Predicts Noise\\n\\npython\\nclass SimpleUnet(nn.Module):\\n \\"\\"\\"\\n A simplified variant of the Unet architecture.\\n \\"\\"\\"\\n\\n def __init__(self):\\n super().__init__()\\n image_channels = 3\\n down_channels = (64, 128, 256, 512, 1024)\\n up_channels = (1024, 512, 256, 128, 64)\\n out_dim = 1\\n time_emb_dim = 32\\n\\n # Time embedding\\n self.time_mlp = nn.Sequential(\\n SinusoidalPositionEmbeddings(time_emb_dim),\\n nn.Linear(time_emb_dim, time_emb_dim),\\n nn.ReLU()\\n ).to(device)\\n\\n # Initial projection\\n # stride = 1, padding = 1, no change in spatial dimension\\n self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1).to(device)\\n\\n # Downsample\\n self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i + 1],\\n time_emb_dim).to(device)\\n for i in range(len(down_channels) - 1)])\\n # Upsample\\n self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i + 1],\\n time_emb_dim, up=True).to(device)\\n for i in range(len(up_channels) - 1)])\\n\\n self.output = nn.Conv2d(up_channels[-1], 3, out_dim).to(device)\\n\\n def forward(self, x, times):\\n # Embedd time\\n t = self.time_mlp(times)\\n # Initial conv\\n x = self.conv0(x)\\n # Unet\\n residual_inputs = []\\n for down in self.downs:\\n x = down(x, t)\\n residual_inputs.append(x)\\n for up in self.ups:\\n # for the bottom block the x adds an identical copy of x (just poped out) for unity of coding.\\n residual_x = residual_inputs.pop()\\n # Add residual x as additional channels\\n x = torch.cat((x, residual_x), dim=1)\\n x = up(x, t)\\n return self.output(x)\\n\\n\\n##### Sampling / Prediction\\n\\npython\\n@torch.no_grad()\\ndef sample_timestep(model, x, t):\\n \\"\\"\\"\\n Calls the model to predict the noise in the image and returns\\n the denoised image.\\n Applies noise to this image, if we are not in the last step yet.\\n \\"\\"\\"\\n betas_t = get_index_from_list(betas, t, x.shape)\\n sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\\n sqrt_one_minus_alphas_cumprod, t, x.shape\\n )\\n sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\\n\\n # Call model (current image - noise prediction)\\n model_mean = sqrt_recip_alphas_t * (\\n x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\\n )\\n posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\\n\\n if t == 0:\\n return model_mean\\n else:\\n noise = torch.randn_like(x)\\n return model_mean + torch.sqrt(posterior_variance_t) * noise\\n\\n\\n@torch.no_grad()\\ndef sample_plot_image(model, img_path):\\n # Sample noise\\n img_size = IMG_SIZE\\n img = torch.randn((1, 3, img_size, img_size), device=device)\\n plt.figure(figsize=(15, 15))\\n plt.axis(\'off\')\\n num_images = 10\\n stepsize = int(T / num_images)\\n\\n for i in range(0, T)[::-1]:\\n # just create a tensor t of shape (1,), the result is [1], [2], ..., etc\\n times = torch.full((1,), i, device=device, dtype=torch.long)\\n img = sample_timestep(model, img, times)\\n if i % stepsize == 0:\\n plt.subplot(1, num_images, i // stepsize + 1)\\n show_tensor_image(img.detach().cpu())\\n\\n plt.savefig(img_path)\\n\\n","title":"Diffusion Model Study","date":"2023-03-07T00:00:00.000Z","id":"blog0124","tag":"python, deep-learning","intro":"Beginning study of diffusion model."},{"content":"\\n#### Windows\\n\\n##### OpenCV\\n\\n- In the [official guide](https://docs.opencv.org/4.x/d3/d52/tutorial_windows_install.html) we can download the pre-built library, for example, we choose 4.6.0 and download the exe file\\n\\n <Center>\\n <img src=\\"/assets/tech/125/001.png\\" width=\\"600\\"/>\\n </Center>\\n <p></p>\\n <center></center>\\n\\n By running this .exe file we can decompress and extract files into a single folder that follows this structure:\\n\\n <Center>\\n <img src=\\"/assets/tech/125/002.png\\" width=\\"600\\"/>\\n </Center>\\n <p></p>\\n <center></center>\\n\\n As a routine I will save all libraries like opencv, libtorch and boost, etc, into a folder called C++Library. These libraries will be shared among different projects.\\n\\n- Next in the root level CMakelists.txt we include\\n\\n cmake\\n set(CMAKE_CXX_STANDARD 14)\\n set(CPACK_PROJECT_NAME ${PROJECT_NAME})\\n set(CPACK_PROJECT_VERSION ${PROJECT_VERSION})\\n set(OpenCV_DIR \\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\C++Libraries\\\\\\\\opencv\\\\\\\\build\\\\\\\\x64\\\\\\\\vc16\\")\\n find_package(OpenCV REQUIRED)\\n message(STATUS \\"OpenCV_INCLUDE_DIRS = ${OpenCV_INCLUDE_DIRS}\\")\\n message(STATUS \\"OpenCV_LIBS = ${OpenCV_LIBS}\\")\\n \\n\\n Change the path for OpenCV_DIR when needed.\\n\\n- Then we can use\\n cmake\\n target_link_libraries(some_target PUBLIC ... ${OpenCV_LIBS} ...)\\n \\n to link the library when the project needs it.\\n\\n##### libtorch\\n\\n- We can download the libtorch pre-built library from official pytorch website. The structure is like this:\\n\\n <Center>\\n <img src=\\"/assets/tech/125/003.png\\" width=\\"600\\"/>\\n </Center>\\n <p></p>\\n <center></center>\\n\\n- In the root level CMakeLists.txt we include\\n\\n cmake\\n set(CMAKE_PREFIX_PATH \\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\C++Libraries\\\\\\\\libtorch\\")\\n find_package(Torch REQUIRED)\\n set(CMAKE_CXX_FLAGS \\"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\\")\\n if (MSVC)\\n message(\\"copying dll files\\")\\n file(GLOB TORCH_DLLS \\"${TORCH_INSTALL_PREFIX}/lib/*.dll\\")\\n add_custom_command(TARGET EyeCatching\\n POST_BUILD\\n COMMAND ${CMAKE_COMMAND} -E copy_if_different\\n ${TORCH_DLLS}\\n $<TARGET_FILE_DIR:EyeCatching>)\\n endif (MSVC)\\n \\n\\n- Then we can link the library by\\n\\n cmake\\n target_link_libraries(some_target PUBLIC ... ${TORCH_LIBRARIES} ...)\\n \\n\\n#### Mac\\n\\n##### OpenCV\\\\_\\n\\nWe install opencv directly by homebrew:\\n\\ntext\\nbrew install opencv\\n\\n\\nThen the pre-built library can be found in /opt/homebrew/Cellar/opencv/. It remains to link it in our cmake project.\\n\\nThe cmake instruction remains the same as windows, in my case my path to the libary is /opt/homebrew/Cellar/opencv/4.7.0_1.\\n\\n##### libtorch\\\\_\\n\\nWe can still download pre-built libtorch library in official pytorch website. The cmake instruction remains the same as windows.\\n","title":"Install Opencv and Libtorch for CMake Project","date":"2023-03-20T00:00:00.000Z","id":"blog0125","tag":"C++, deep-learning","intro":"Record the flow of including opencv and libtorch into CMake project."},{"content":"\\nIn the CMakeLists.txt of library folder (or root folder), we add\\n\\ncmake\\ntarget_compile_features(lib_name PUBLIC cxx_std_20)\\n\\n\\nto override, or to determine the language standard in compilation.\\n","title":"Control C++ Standard Accurately","date":"2023-03-24T00:00:00.000Z","id":"blog0126","tag":"C++","intro":"Sometimes the constant CMAKE_CXX_STANDARD does not guarantee the C++ standard we use in compilation. We add a line to guarantee which target is compiled in which C++ standard.","toc":false},{"content":"\\nJust bundle a set of headers into pch.h, refactor the project approprimately and add the following in the CMakeLists.txt.\\n\\ncmake\\ntarget_precompile_headers(target_name PUBLIC src/pch.h)\\n\\n","title":"Precompiled Header in CMake Project","date":"2023-03-27T00:00:00.000Z","id":"blog0127","tag":"C++","intro":"Previously we have mentioned how to use precompiled header in visual studio project, this time we record how to do it in cmake project","toc":false},{"content":"\\n#### API Documentation\\n\\n- https://pytorch.org/cppdocs/api/library_root.html\\n\\n#### Tensor Slicing\\n\\n- [More detailed translation from pytorch to libtorch](https://pytorch.org/cppdocs/notes/tensor_indexing.html)\\n\\n| Python | C++ (assuming using namespace torch::indexing) |\\n| ------------------------------------------------------ | ---------------------------------------------------------------------------------- |\\n| tensor[None] = 1 | tensor.index_put_({None}, 1) |\\n| tensor[:, 2] = 0 | tensor.index_put_({Slice(), 2}, 0) |\\n| tensor[Ellipsis, ...] = 1 | tensor.index_put_({Ellipsis, \\"...\\"}, 1) |\\n| tensor[1, 2] = 1 | tensor.index_put_({1, 2}, 1) |\\n| tensor[True, False] = 1 | tensor.index_put_({true, false}, 1) |\\n| tensor[1::2] = 1 | tensor.index_put_({Slice(1, None, 2)}, 1) |\\n| tensor[torch.tensor([1, 2])] = 1 | tensor.index_put_({torch::tensor({1, 2})}, 1) |\\n| tensor[..., 0, True, 1::2, torch.tensor([1, 2])] = 1 | tensor.index_put_({\\"...\\", 0, true, Slice(1, None, 2), torch::tensor({1, 2})}, 1) |\\n\\n#### How to deal with .npy file\\n\\nThere are several packages to load .npy file in C++. However, since have pytorch, additional package for .npy file is **not necessary**.\\n\\n##### Save the Numpy Array as Tensor in Pytorch\\n\\nWe can load the array from .npy file and translate it into tensor easily by\\n\\npython\\nanchors = torch.tensor(np.load(anchors_npy_path), dtype=torch.float32)\\n\\n\\nthen we save it by\\n\\npython\\ntorch.save({\\"anchors\\": anchors}, \\"anchors.pt\\")\\n\\n\\n##### Load the Tensor in Libtorch\\n\\ncpp\\nstd::vector<char> get_the_bytes(std::string filename)\\n{\\n std::ifstream input(filename, std::ios::binary);\\n std::vector<char> bytes(\\n (std::istreambuf_iterator<char>(input)),\\n (std::istreambuf_iterator<char>()));\\n input.close();\\n return bytes;\\n}\\n\\ntorch::Tensor load_anchors(std::string pt_path)\\n{\\n torch::Tensor anchors;\\n std::vector<char> f = get_the_bytes(pt_path);\\n c10::Dict<IValue, IValue> weights = torch::pickle_load(f).toGenericDict();\\n for (auto const& w : weights)\\n {\\n std::string name = w.key().toStringRef();\\n at::Tensor param = w.value().toTensor().toType(torch::kFloat32);\\n\\n if (name == \\"anchors\\")\\n {\\n std::cout << \\"anchors was found, loading anchors\\" << \\"\\\\n\\" << param;\\n anchors = param;\\n break;\\n }\\n }\\n return anchors;\\n}\\n\\n\\n#### Create Simple Layers in Libtorch\\n\\nIn the sequel we assume in our namespace:\\n\\ncpp\\nusing namespace torch;\\n\\n\\n##### nn::Conv2d\\n\\ncpp\\nnn::Conv2d(nn::Conv2dOptions(in_channels, out_channels, kernel_size)\\n .stride(stride)\\n .padding(padding)\\n .groups(in_channels)\\n .bias(true)),\\n\\n\\n##### nn::MaxPool2d\\n\\ncpp\\nnn::MaxPool2d(nn::MaxPool2dOptions({ kernel_size1, kernel_size2 }.stride({ 2, 2 })));\\n\\n\\n##### nn::functional::pad\\n\\ncpp\\nh = nn::functional::pad(x, nn::functional::PadFuncOptions({ 0, 2, 0, 2 }).value(0));\\n\\n\\n#### Create Multi-Dimensional Array in Libtorch\\n\\n##### Create torch::Tensor from raw 2d-Array\\n\\ncpp\\nfloat matrix[2][4] = {\\n { 1, 1, 1, 1 },\\n { -1, 0, 2, 3 }\\n};\\n\\ntorch::Tensor result = torch::from_blob(matrix, { 2, 4 }).toType(torch::kFloat32);\\nstd::cout << \\" result[0][1]\\" << result[0][1] << std::endl;\\n\\nfloat data = 0;\\ntorch::Tensor y = torch::from_blob(&data, { 1 }).toType(torch::kFloat32);\\nstd::cout << \\"y: \\" << y << std::endl;\\n\\n\\n- float cannot be replaced by int as otherwise there will be numerical error in data conversion.\\n- The size of the tensor can be accessed by auto sizes = y.sizes(). The size of each dim can be accessed by index operator sizes[i].\\n\\n##### Create torch::Tensor from cv::Mat\\n\\ncpp\\ntorch::Tensor result = torch::from_blob(mat.data, { 1, height, width, 3 }, torch::kByte)\\n .permute({ 0, 3, 1, 2 })\\n .toType(torch::kFloat32);\\nresult.div_(255.0);\\n\\n\\n##### Create cv::Mat from torch::Tensor\\n\\ncpp\\ncv::Mat from_tensor_to_mat = cv::Mat(\\n h,\\n w,\\n CV_32FC3,\\n img3.permute({ 0, 2, 3, 1 }).squeeze(0).data_ptr()\\n);\\n\\n\\n- If A_tensor is 3-dimensional in channel, we need CV_32FC3, similarly if a tensor 1-dimensional in channel we need CV_32FC1.\\n\\n#### Validate a Libtorch Model is Compatible (in weight) to a Pytorch Model.\\n\\n##### Custom nnModule::load_parameters and nnModule::print_parameters\\n\\nEssentially this will replace all torch::nn::module.\\n\\n###### Header\\n\\ncpp\\nclass nnModule : public nn::Module\\n{\\npublic:\\n std::vector<char> get_the_bytes(std::string filename);\\n void load_parameters(std::string pt_pth);\\n void print_parameters(std::string file_path, bool with_weight = false);\\n};\\n\\n\\n###### Source\\n\\ncpp\\nstd::vector<char> nnModule::get_the_bytes(std::string filename)\\n{\\n std::ifstream input(filename, std::ios::binary);\\n std::vector<char> bytes(\\n (std::istreambuf_iterator<char>(input)),\\n (std::istreambuf_iterator<char>()));\\n\\n input.close();\\n return bytes;\\n}\\n\\nvoid nnModule::load_parameters(std::string pt_pth)\\n{\\n std::vector<char> f = this->get_the_bytes(pt_pth);\\n c10::Dict<IValue, IValue> weights = torch::pickle_load(f).toGenericDict();\\n\\n const torch::OrderedDict<std::string, at::Tensor>& model_params = this->named_parameters();\\n std::vector<std::string> param_names;\\n for (auto const& w : model_params)\\n {\\n param_names.push_back(w.key());\\n }\\n\\n torch::NoGradGuard no_grad;\\n for (auto const& w : weights)\\n {\\n std::string name = w.key().toStringRef();\\n at::Tensor param = w.value().toTensor();\\n\\n if (std::find(param_names.begin(), param_names.end(), name) != param_names.end())\\n {\\n auto target_model_param = model_params.find(name);\\n\\n for (int i = 0; i < target_model_param->sizes().size(); i++)\\n {\\n assert(target_model_param->sizes()[i] == param.sizes()[i]);\\n }\\n\\n target_model_param->copy_(param);\\n }\\n else\\n {\\n std::cout << name << \\" does not exist among model parameters.\\" << std::endl;\\n };\\n }\\n}\\n\\nvoid nnModule::print_parameters(std::string file_path, bool with_weight)\\n{\\n\\n std::ostringstream oss;\\n\\n for (const auto& pair : named_parameters())\\n {\\n oss << \\"[\\" << pair.key() << \\"] \\";\\n int shape_arr_size = pair.value().sizes().size();\\n\\n std::string size_tuple_str = \\"torch.Size([\\";\\n for (int i = 0; i < shape_arr_size; i++)\\n {\\n std::string curr_dim_len = std::to_string(pair.value().sizes()[i]);\\n size_tuple_str += curr_dim_len;\\n if (i != (shape_arr_size - 1))\\n {\\n size_tuple_str += \\", \\";\\n }\\n }\\n size_tuple_str += \\"])\\";\\n\\n oss << size_tuple_str << \\"\\\\n\\";\\n\\n if (with_weight)\\n {\\n oss << pair.value()\\n << \\"\\\\n\\"\\n << \\"---------------\\"\\n << \\"\\\\n\\";\\n }\\n }\\n\\n std::ofstream file;\\n file.open(file_path);\\n try\\n {\\n file << oss.str();\\n }\\n catch (std::exception err)\\n {\\n std::cout << err.what() << std::endl;\\n }\\n file.close();\\n}\\n\\n\\n###### Dual Functions in Python for Comparison\\n\\npython\\ndef save_model(model_, des_weight_path):\\n w = {k: v for k, v in model_.state_dict().items()}\\n torch.save(w, des_weight_path)\\n\\n\\ndef print_weight_list(model_, des_txt_filepath, with_weight=False):\\n with open(des_txt_filepath, \\"w+\\") as f_handle:\\n\\n txt = \\"\\"\\n for name, param in model_.named_parameters():\\n txt += \\"[{}] {}\\\\n\\".format(name, param.shape)\\n if with_weight:\\n txt += str(param.numpy())\\n txt += \\"\\\\n\\" + \\"---------------\\" + \\"\\\\n\\"\\n\\n f_handle.write(txt)\\n\\n\\n##### Example\\n\\nFor example, in this [repository](https://github.com/machingclee/2023-01-25-ImGui-barebone-windows-blazeface-integrated/tree/main/mediapipe_libtorch/src/mediapipe_libtorch?fbclid=IwAR1komT77la5Eah9i9zRIE0LrvkLv2XEU4XNi06ogIy0KN1qnzh8wh3T4pk) my modules are all inherited publicly from nnModule defined above, therefore we can\\n\\n- **On Libtorch Side.**\\n\\ncpp\\nface_detector->print_parameters(des_str)\\n\\n\\n- **On Pytorch Side.**\\n\\npython\\nprint_weight_list(face_detector, des_str)\\n\\n\\nwith with_weight = false by default (you can set it true if you want to further debug the libtorch model, but the file will become very large and unreadable).\\n\\nThese two functions are designed to output the same result:\\n\\ntext\\n[backbone1.0.weight] torch.Size([24, 3, 5, 5])\\n[backbone1.0.bias] torch.Size([24])\\n[backbone1.2.convs.0.weight] torch.Size([24, 1, 3, 3])\\n[backbone1.2.convs.0.bias] torch.Size([24])\\n[backbone1.2.convs.1.weight] torch.Size([24, 24, 1, 1])\\n[backbone1.2.convs.1.bias] torch.Size([24])\\n[backbone1.3.convs.0.weight] torch.Size([24, 1, 3, 3])\\n...\\n\\n\\nWhen two files are exactly the same, we are confident that the pytorch weight can be applied to libtorch model as well.\\n\\n#### Create a Custom Module and Register Sub-modules in Practice\\n\\n##### Register Tensors form Pytorch to Libtorch\\n\\n###### Parameters\\n\\nThe following are equivalent:\\n\\npython\\nclass Net(torch.nn.Module):\\n def __init__(self, N, M):\\n super(Net, self).__init__()\\n self.W = torch.nn.Parameter(torch.randn(N, M))\\n self.b = torch.nn.Parameter(torch.randn(M))\\n\\n def forward(self, input):\\n return torch.addmm(self.b, input, self.W)\\n\\n\\ncpp\\nclass Net : torch::nn::Module\\n{\\n torch::Tensor W, b;\\npublic:\\n Net(int64_t N, int64_t M)\\n {\\n W = register_parameter(\\"W\\", torch::randn({N, M}));\\n b = register_parameter(\\"b\\", torch::randn(M));\\n }\\n torch::Tensor forward(torch::Tensor input)\\n {\\n return torch::addmm(b, input, W);\\n }\\n};\\n\\n\\n###### Modules\\n\\nThe following are equivalent:\\n\\npython\\nclass Net(torch.nn.Module):\\n def __init__(self, N, M):\\n super(Net, self).__init__()\\n # Registered as a submodule behind the scenes\\n self.linear = torch.nn.Linear(N, M)\\n self.another_bias = torch.nn.Parameter(torch.rand(M))\\n\\n def forward(self, input):\\n return self.linear(input) + self.another_bias\\n\\n\\ncpp\\nclass Net : torch::nn::Module\\n{\\n torch::nn::Linear linear;\\n torch::Tensor another_bias;\\npublic:\\n Net(int64_t N, int64_t M) : {\\n linear = register_module(\\"linear\\", torch::nn::Linear(N, M));\\n another_bias = register_parameter(\\"b\\", torch::randn(M));\\n }\\n torch::Tensor forward(torch::Tensor input) {\\n return linear->forward(input) + another_bias;\\n }\\n};\\n\\n\\n##### Real Example in Practice\\n\\n###### Rules\\n\\n- In libtorch all modules are created by producing a shared_ptr pointing to an nn::Module.\\n- For example, auto net1 = nn::Conv2d(...) and auto net2 = nn::Relu(...) are both pointers.\\n- nn::Sequential() only except smart pointers like nn::Sequential(net1, net2).\\n\\nFor custom module, we can create such a pointer-factory by TORCH_MODULE macro. The principles are\\n\\n- For module just for internal use, we simply leave it as an nn::Module object.\\n- For module that is going to be exposed to user, we write NetImpl and use TORCH_MODULE(Net) to create a special pointer class Net.\\n\\n###### BlazeBlock from mediapipe\'s BlazeFace\\n\\ncpp\\n// header file\\nclass BlazeBlockImpl : public nnModule\\n{\\nprotected:\\n int in_channels;\\n int out_channels;\\n int kernel_size;\\n int stride;\\n std::string act;\\n nn::Conv2d skip_proj = nullptr;\\n int channel_pad;\\n int padding;\\n nn::Sequential convs = nullptr;\\n nn::MaxPool2d max_pool = nullptr;\\n std::variant<nn::ReLU, nn::PReLU> act_layer;\\n bool use_skip_proj = false;\\n\\npublic:\\n BlazeBlockImpl(\\n int in_channels,\\n int out_channels,\\n int kernel_size = 3,\\n int stride = 1,\\n std::string act = \\"relu\\",\\n bool use_skip_proj = false);\\n torch::Tensor forward(torch::Tensor x);\\n};\\n\\nTORCH_MODULE(BlazeBlock);\\n\\n\\ncpp\\n// source file\\nBlazeBlockImpl::BlazeBlockImpl(\\n int in_channels,\\n int out_channels,\\n int kernel_size,\\n int stride,\\n std::string act,\\n bool use_skip_proj) : in_channels(in_channels),\\n out_channels(out_channels),\\n kernel_size(kernel_size),\\n stride(stride),\\n act(act),\\n use_skip_proj(use_skip_proj)\\n{\\n channel_pad = out_channels - in_channels;\\n if (stride == 2)\\n {\\n max_pool = nn::MaxPool2d(nn::MaxPool2dOptions({ stride, stride }));\\n padding = 0;\\n }\\n else\\n {\\n padding = (int)((kernel_size - 1) / 2);\\n }\\n\\n nn::Sequential convs_ = nn::Sequential();\\n convs_->push_back(nn::Conv2d(nn::Conv2dOptions(in_channels, in_channels, kernel_size)\\n .stride(stride)\\n .padding(padding)\\n .groups(in_channels)\\n .bias(true)));\\n convs_->push_back(nn::Conv2d(nn::Conv2dOptions(in_channels, out_channels, 1)\\n .stride(1)\\n .padding(0)\\n .bias(true)));\\n\\n convs = register_module(\\"convs\\", convs_);\\n\\n if (use_skip_proj)\\n {\\n skip_proj = register_module(\\n \\"skip_proj\\",\\n nn::Conv2d(nn::Conv2dOptions(in_channels, out_channels, 1)\\n .stride(1)\\n .padding(0)\\n .bias(true)));\\n }\\n else\\n {\\n skip_proj = nullptr;\\n }\\n\\n if (act == \\"relu\\")\\n {\\n act_layer = nn::ReLU(nn::ReLUOptions(true));\\n }\\n else if (\\"prelu\\")\\n {\\n act_layer = register_module(\\n \\"act\\",\\n nn::PReLU(nn::PReLUOptions().num_parameters(out_channels))\\n );\\n }\\n else\\n {\\n throw std::exception(\\"activation layer not implemented.\\");\\n }\\n}\\n\\ntorch::Tensor BlazeBlockImpl::forward(torch::Tensor x)\\n{\\n torch::Tensor h;\\n if (stride == 2)\\n {\\n if (kernel_size == 3)\\n {\\n h = nn::functional::pad(x, nn::functional::PadFuncOptions({ 0, 2, 0, 2 }).value(0));\\n }\\n else\\n {\\n h = nn::functional::pad(x, nn::functional::PadFuncOptions({ 1, 2, 1, 2 }).value(0));\\n }\\n x = this->max_pool(x);\\n }\\n else\\n {\\n h = x;\\n }\\n\\n if (skip_proj)\\n {\\n x = skip_proj->forward(x);\\n }\\n else if (channel_pad > 0)\\n {\\n x = nn::functional::pad(\\n x, n\\n n::functional::PadFuncOptions({ 0, 0, 0, 0, 0, channel_pad }).value(0)\\n );\\n }\\n\\n torch::Tensor y = convs->forward(h) + x;\\n // y = reinterpret_cast<IHasForward*>(&act_layer)->forward(y);\\n // I want to avoid the following:\\n\\n if (auto act_layer_ptr = std::get_if<nn::ReLU>(&act_layer))\\n {\\n y = (*act_layer_ptr)->forward(y);\\n }\\n else if (auto act_layer_ptr = std::get_if<nn::PReLU>(&act_layer))\\n {\\n y = (*act_layer_ptr)->forward(y);\\n }\\n\\n return y;\\n};\\n\\n\\n#### OpenCV\\n\\n##### Load an Image\\n\\ncpp\\ncv::Mat img = cv::imread(img_path, cv::IMREAD_COLOR);\\n\\n\\n##### Write a cv::Mat into an Image\\n\\ncpp\\ncv::imwrite(image_path, img1);\\n\\n\\n##### Resize an Image\\n\\ncpp\\ncv::Mat img1;\\n// src, des int, int\\ncv::resize(img, img1, cv::Size(w1, h1));\\n\\n\\n##### Pad an Image\\n\\ncpp\\ncv::Mat img1_;\\ncv::copyMakeBorder(\\n img1, // src\\n img1_, // des\\n padh1, // top\\n padh2, // bottom\\n padw1, // left\\n padw2, // right\\n cv::BORDER_CONSTANT,\\n cv::Scalar(0)\\n);\\n\\n\\n##### COLOR_BGR2RGB\\n\\ncpp\\ncv::cvtColor(frame, frame, cv::COLOR_BGR2RGB);\\n\\n\\n##### Drawing\\n\\ncpp\\ncv::circle(frame, cv::Point2i({ x, y }), size, color, size);\\ncv::line(\\n frame,\\n cv::Point2i({ x0, y0 }),\\n cv::Point2i({ x1, y1 }),\\n cv::Scalar({ 0, 0, 0 }),\\n size\\n);\\n\\n\\n##### Read from Camera\\n\\ncpp\\ncv::VideoCapture capture(1);\\ncv::Mat frame;\\n\\nif (!capture.isOpened())\\n{\\n throw std::exception(\\"Unable to open camera.\\");\\n}\\n\\nwhile (true)\\n{\\n capture.read(frame);\\n if (frame.empty())\\n {\\n throw std::exception(\\"Blank frame grabbed.\\");\\n }\\n assert(frame.channels() == 3);\\n ...\\n}\\n\\n","title":"Libtorch Study Notes With OpenCV","date":"2023-04-02T00:00:00.000Z","id":"blog0128","tag":"C++, pytorch, libtorch","intro":"In the course of translating pytorch model into libtorch model there are traps and tricks that are worthing being recorded. Also record the simple use of opencv as it substitutes the role of numpy in python."},{"content":"\\nSometime it is helpful to create a type that accept both class A and class B.\\nA real example from deep learning is:\\n\\ncpp\\n// header file\\nclass BlazeBlockImpl : public nnModule\\n{\\nprotected:\\n ...\\n std::variant<nn::ReLU, nn::PReLU> act_layer;\\n ...\\n}\\n\\n\\nand we assign this act_layer in constructor\\n\\ncpp\\n// source file\\nif (act == \\"relu\\")\\n{\\n act_layer = nn::ReLU(nn::ReLUOptions(true));\\n}\\nelse if (\\"prelu\\")\\n{\\n act_layer = register_module(\\n \\"act\\",\\n nn::PReLU(nn::PReLUOptions().num_parameters(out_channels))\\n );\\n}\\n\\n\\ndepending on a variable act passed into this constructor. This std::variant serves as the same purpose of union type in typescript (note!! union is a special keyword in C++ and does not work in the same way as typescript).\\n\\nSpecial type-checking will be needed to apply act_layer to a tensor, for this we separate act_layer into two cases:\\n\\ncpp\\ntorch::Tensor y = ...;\\nif (auto act_layer_ptr = std::get_if<nn::ReLU>(&act_layer))\\n{\\n y = (*act_layer_ptr)->forward(y);\\n}\\nelse if (auto act_layer_ptr = std::get_if<nn::PReLU>(&act_layer))\\n{\\n y = (*act_layer_ptr)->forward(y);\\n}\\n\\n\\nThat\'s how std::variant work.\\n","title":"std::variant","date":"2023-04-05T00:00:00.000Z","id":"blog0129","tag":"C++","intro":"In typescript we have type A = B | C, we also have an analogue in C++.","toc":false},{"content":"\\n#### Library we Use\\n\\n- [WinReg v6.1.0](https://github.com/GiovanniDicanio/WinReg)\\n\\n#### Code Implementation\\n\\nIn fact we just need the header file in WinReg/WinReg.hpp, we can create a cmake project and simply include this file.\\n\\nNow to register the application to a url scheme:\\n\\ncpp\\n// header\\n\\n#pragma once\\n#include \\"WinReg.h\\"\\n#include <string>\\n\\nnamespace RegisterProtocol\\n{\\n inline std::wstring to_wide_string(const std::string& input);\\n inline std::string to_byte_string(const std::wstring& input);\\n void register_protocol(std::string app_path);\\n} // namespace RegisterProtocol\\n\\n\\ncpp\\n// source\\n\\nnamespace RegisterProtocol\\n{\\n\\n inline std::wstring to_wide_string(const std::string& input)\\n {\\n std::wstring_convert<std::codecvt_utf8<wchar_t>> converter;\\n return converter.from_bytes(input);\\n }\\n\\n inline std::string to_byte_string(const std::wstring& input)\\n {\\n // std::wstring_convert<std::codecvt_utf8_utf16<wchar_t>> converter;\\n std::wstring_convert<std::codecvt_utf8<wchar_t>> converter;\\n return converter.to_bytes(input);\\n }\\n\\n void register_protocol(std::string exe_path)\\n {\\n std::wstring w_app_path = to_wide_string(exe_path);\\n RegKey key;\\n key.Create(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\", KEY_CREATE_SUB_KEY);\\n key.Open(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\", KEY_SET_VALUE);\\n key.SetStringValue(L\\"URL Protocol\\", L\\"\\");\\n\\n key.Create(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\\\\\\\shell\\\\\\\\open\\\\\\\\command\\", KEY_CREATE_SUB_KEY);\\n key.Open(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\\\\\\\shell\\\\\\\\open\\\\\\\\command\\", KEY_SET_VALUE);\\n key.SetStringValue(L\\"\\", w_app_path);\\n }\\n} // namespace RegisterProtocol\\n\\n\\n#### What is it Doing\\n\\n- cpp\\n key.Create(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\\\\\\\shell\\\\\\\\open\\\\\\\\command\\")\\n \\n\\n means that at the folder HKEY_CURRENT_USER, we create a nested folder structure as instructed.\\n\\n- cpp\\n key.Open(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\", KEY_SET_VALUE);\\n \\n\\n simply changes the current working \\"directory\\" to\\n\\n - HKEY_CURRENT_USER\\\\Software\\\\Classes\\\\eyecatcher\\n\\n and grant the desired access right: KEY_SET_VALUE.\\n\\n- cpp\\n key.SetStringValue(std::wstring key, std::wstring value)\\n \\n\\n is the same as setting key-value pair at the current working registry folder. We use wstring instead of string for non-ascii characters.\\n\\n For example, what\\n\\n cpp\\n key.Open(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\", KEY_SET_VALUE);\\n key.SetStringValue(L\\"URL Protocol\\", L\\"\\")\\n \\n\\n does is to create a new key-value pair at the directory:\\n\\n - HKEY_CURRENT_USER\\\\Software\\\\Classes\\\\eyecatcher\\n\\n In the GUI of registry editor we have:\\n\\n <Center>\\n <img src=\\"/assets/tech/130/registry.png\\" width=\\"600\\"/>\\n </Center>\\n <p></p>\\n <center></center>\\n\\n Here exe_path is the path of the executable that we want to launch.\\n\\n- Now an anchor element\\n html\\n <a href=\\"eyecatcher://\\">eye-catcher protocol</a>\\n \\n can trigger the execution of our target .exe file.\\n","title":"Register URL Scheme for an Application","date":"2023-04-06T00:00:00.000Z","id":"blog0130","tag":"C++","intro":"We study how to register a custom url protocol to launch our desktop application.","toc":true},{"content":"\\n#### Query and Update\\n\\n##### Return ObjectId in Plain Text by \\\\$project\\n\\njs\\n{ $project:{ _id: { $toString: \\"$_id\\" } } }\\n\\n\\n##### Update Deeply Nested Object Conditionally\\n\\nSometime if a conditional update is hard to write in a single update object, then we can separate it into two parts:\\n\\njs\\n{\\n\\tdb.ns_generic_form_templates.updateMany(\\n\\t\\t{ _id: ObjectId(\\"6333d861ec5588b3b43b5183\\") },\\n\\t\\t{\\n\\t\\t\\t$set: { \\"sections.$[].questions.$[question]._mandatory\\": true },\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\tarrayFilters: [{ \\"question.mandatory\\": \\"Y\\" }]\\n\\t\\t}\\n\\t)\\n\\tdb.ns_generic_form_templates.updateMany(\\n\\t\\t{ _id: ObjectId(\\"6333d861ec5588b3b43b5183\\") },\\n\\t\\t{\\n\\t\\t\\t$set: { \\"sections.$[].questions.$[question]._mandatory\\": false },\\n\\t\\t},\\n\\t\\t{\\n\\t\\t\\tarrayFilters: [{\\n\\t\\t\\t\\t$or: [\\n\\t\\t\\t\\t\\t{ \\"question.mandatory\\": { $eq: \\"N\\" } },\\n\\t\\t\\t\\t\\t{ \\"question.mandatory\\": { $exists: false } }\\n\\t\\t\\t\\t]\\n\\t\\t\\t}]\\n\\t\\t}\\n\\t)\\n}\\n\\n\\nwill do.\\n\\n##### Query Users with Inner Join and Project Desired Fields using \\\\$map\\n\\njs\\n[\\n {\\n $lookup: {\\n from: \\"%s\\",\\n localField: \\"code\\",\\n foreignField: \\"internal_code\\",\\n as: \\"contacts\\"\\n }\\n },\\n {\\n $project: {\\n _id: { $toString: \\"$_id\\" },\\n code: 1,\\n name: 1,\\n contacts: {\\n $map: {\\n input: \\"$contacts\\",\\n as: \\"this\\",\\n in: {\\n contact_full_name1: { $ifNull: [\\"$$this.contact_full_name1\\", \\"\\"] },\\n contact_email1: { $ifNull: [\\"$$this.contact_email1\\", \\"\\"] },\\n contact_full_name2: { $ifNull: [\\"$$this.contact_full_name2\\", \\"\\"] },\\n contact_email2: { $ifNull: [\\"$$this.contact_email2\\", \\"\\"] }\\n }\\n }\\n }\\n }\\n }\\n]\\n\\n\\n##### Photos with Missing Signboards\\n\\nSignboards that are drawn by users will be of type Pending and can be queried by\\n\\njs\\ndb.signboards.find({ isManualCreate: true, status: \\"Pending\\" })\\n\\n\\nIf we want to query for images that have pending annotation, then we do a lookup:\\n\\njs\\ndb.signboards.aggregate(\\n [\\n {\\n $match: {\\n signboardId: { $regex: /2022\\\\-1st_TRIP\\\\-06/ },\\n isManualCreate: true, status: \\"Pending\\"\\n }\\n },\\n {\\n $lookup: {\\n from: \\"signboardimages\\",\\n localField: \\"signboardId\\",\\n foreignField: \\"signboardId\\",\\n as: \\"signboardimage\\"\\n }\\n },\\n {\\n $project: {\\n signboardimage: {\\n $arrayElemAt: [\\"$signboardimage\\", 0]\\n },\\n signboardId: 1,\\n _id: 0\\n }\\n },\\n {\\n $project: {\\n originalFilename: \\"$signboardimage.originalFilename\\",\\n signboardId: 1,\\n _id: 0\\n }\\n },\\n ]\\n)\\n\\n\\nResult:\\n\\nnone\\n[\\n { signboardId: \'2022-1st_TRIP-02_00377\',\\n originalFilename: \'2022-1st_TRIP-02/P202203252_635_3807.jpg\'\\n },\\n { signboardId: \'2022-1st_TRIP-04_00347\',\\n originalFilename: \'2022-1st_TRIP-04/P202203254_463_2778.jpg\'\\n },\\n { signboardId: \'2022-1st_TRIP-04_00348\',\\n originalFilename: \'2022-1st_TRIP-04/P202203254_463_2777.jpg\'\\n },\\n ...\\n]\\n\\n\\n##### Photos with False Positive Signboard (that is Deleted)\\n\\nThe annotation of false positive results will be deleted in the frontend, therefore\\n\\njs\\ndb.signboards.aggregate(\\n [\\n {\\n $match: {\\n signboardId: { $regex: /2022\\\\-1st_TRIP\\\\-06/ },\\n \\"revisionStatus.signboardIdentificationRevised.status\\": \\"deleted\\"\\n }\\n },\\n {\\n $lookup: {\\n from: \\"signboardimages\\",\\n localField: \\"signboardId\\",\\n foreignField: \\"signboardId\\",\\n as: \\"signboardimage\\"\\n }\\n },\\n {\\n $project: {\\n signboardimage: {\\n $arrayElemAt: [\\"$signboardimage\\", 0]\\n },\\n signboardId: 1,\\n \\"revisionStatus.signboardIdentificationRevised.status\\": 1,\\n _id: 0\\n }\\n },\\n {\\n $project: {\\n originalFilename: \\"$signboardimage.originalFilename\\",\\n signboardId: 1,\\n \\"revisionStatus.signboardIdentificationRevised.status\\": 1,\\n _id: 0\\n }\\n }\\n ]\\n)\\n\\n\\nResult:\\n\\nnone\\n[\\n {\\n revisionStatus: { signboardIdentificationRevised: { status: \'deleted\' } },\\n signboardId: \'2021-01_TRIP-20_00485\',\\n originalFilename: \'2021-01_TRIP-20/P2021101920_594_3562.jpg\'\\n }\\n]\\n\\n\\n##### Photos with Missing Defect\\n\\nIf a defect is missing, we will draw polygon to annotate the defective signboard, therefore missing signboards are signboard image with defectAnnotations.isManualCreate == true:\\n\\nSuppose I want to get all photos with missing defect(s) in trip06:\\n\\njs\\ndb.signboardimages.aggregate(\\n [\\n {\\n $unwind: \\"$defectAnnotations\\"\\n },\\n {\\n $match: {\\n signboardId: { $regex: /2022\\\\-1st_TRIP\\\\-06/ },\\n \\"defectAnnotations.isManualCreate\\": true\\n }\\n },\\n {\\n $project: {\\n _id: 0,\\n signboardId: 1,\\n originalFilename: 1\\n }\\n }\\n ]\\n)\\n\\n\\nResult:\\n\\nnone\\n[\\n {\\n originalFilename: \'2022-1st_TRIP-06/P202204066_2088_12538.jpg\',\\n signboardId: \'2022-1st_TRIP-06_00115\'\\n }\\n]\\n\\n\\n##### Photos with False Positive Defect\\n\\nTo get photo name with false positive defects, we try to get signboardimages with deleted defect annotation, and then use the originalFilename to get the photo filenames.\\n\\njs\\ndb.signboardimages.aggregate(\\n [\\n {\\n $match: {\\n signboardId: { $regex: /2022\\\\-1st_TRIP\\\\-06/ },\\n defectAnnotations: {\\n $gt: {$size: 1}\\n }\\n }\\n },\\n {\\n $project: {\\n signboardId: 1,\\n originalFilename: 1,\\n defectAnnotations: 1,\\n _id: 0\\n }\\n },\\n {\\n $unwind: \\"$defectAnnotations\\"\\n },\\n {\\n $match: {\\n \\"defectAnnotations.status\\": \\"deleted\\"\\n }\\n }\\n ]\\n)\\n\\n\\nyields\\n\\nnone\\n[\\n { originalFilename: \'2022-1st_TRIP-01/P202203251_331_1983.jpg\',\\n defectAnnotations:\\n {\\n isManualCreate: true,\\n status: \'deleted\',\\n _id: ObjectId(\\"625eb88bdaadc41b9147a385\\"),\\n defectType: \'damagedDisplay\',\\n location:\\n {\\n coordinates:\\n [ [ [ 0.5452018181472019, 0.11694282238757442 ],\\n [ 0.5383045307230735, 0.5721815800254274 ],\\n [ 0.7962630803854851, 0.5376322100261263 ],\\n [ 0.7852274205068791, 0.06410260944746654 ],\\n [ 0.5589963929954593, 0.11491050650526251 ] ] ],\\n _id: ObjectId(\\"625eb88bdaadc41b9147a386\\"),\\n type: \'Polygon\'\\n },\\n defectId: \'2022-1st_TRIP-01_00001_DAMAGED_DISPLAY_14\',\\n sequenceNum: 14,\\n groupedSignboardId: ObjectId(\\"625eb88bdaadc41b9147a384\\")\\n },\\n signboardId: \'2022-1st_TRIP-01_00001\'\\n },\\n ...\\n]\\n\\n\\n#### Output Complete List to a Json File\\n\\nUnforturnately the embedded mongoshell of MongoCompass is only for testing purpose, the output cannot be piped into an existing file. For that purpose, we install monogosh from\\n\\nhttps://www.mongodb.com/try/download/shell?jmp=docs\\n\\n<center></center>\\n\\nAfter the installation, you can run\\n\\nbash\\nmongosh --help\\n\\n\\nto test whether mongosh has been installed.\\n\\nLet\'s take the query for missing signboard as an example, in our bash shell we can run (replace $ by \\\\$ and \\" by \\\\\\")\\n\\nsh\\nmongosh \\"{{connection string to DSDS db}}\\" \\\\\\n--eval \\"config.set(\'displayBatchSize\', 300); db.signboards.aggregate(\\n [\\n {\\n \\\\$match: {\\n signboardId: { \\\\$regex: /2022\\\\-1st_TRIP\\\\-06/ },\\n isManualCreate: true, status: \\\\\\"Pending\\\\\\"\\n }\\n },\\n {\\n \\\\$lookup: {\\n from: \\\\\\"signboardimages\\\\\\",\\n localField: \\\\\\"signboardId\\\\\\",\\n foreignField: \\\\\\"signboardId\\\\\\",\\n as: \\\\\\"signboardimage\\\\\\"\\n }\\n },\\n {\\n \\\\$project: {\\n signboardimage: {\\n \\\\$arrayElemAt: [\\\\\\"\\\\$signboardimage\\\\\\", 0]\\n },\\n signboardId: 1,\\n _id: 0\\n }\\n },\\n {\\n \\\\$project: {\\n originalFilename: \\\\\\"\\\\$signboardimage.originalFilename\\\\\\",\\n signboardId: 1,\\n _id: 0\\n }\\n },\\n ]\\n)\\" | sed \'s/originalFilename/\\"originalFilename\\"/g\' \\\\\\n| sed \'s/signboardId/\\"signboardId\\"/g\' \\\\\\n| sed \'s/_id/\\"_id\\\\\\"/g\' \\\\\\n| sed \\"s/\'/\\\\\\"/g\\" \\\\\\n> ~/missing-signboards-06.json && code ~/missing-signboards-06.json\\n\\n\\n","title":"List of Mongo Aggregation Pipelines that I have Used.","date":"2023-06-10T00:00:00.000Z","id":"raspect001","tag":"mongo","intro":"Record the mongo query using aggregation pipeline to get the results as in the title in my usual work. These include $lookup, $project, $arrayElemAt, {$gt: {$size: 1}}, etc interesting operations.","toc":true},{"content":"#### Thunk Actions and Extra Reducer for the Return\\n##### Write Data-Fetching in Slices\\n\\nAs a usual practice we create ThunkAction in the corresponding slice file, for example:\\n\\ntypescript\\n// projectSlice.ts\\n\\nexport const fetchProjects = createAsyncThunk(\\n \\"posts/fetchProjects\\",\\n async () => {\\n // apiClient: an axio instance with baseUrl configured\\n const response = await apiClient.get<ProjectResponse[]>(GET_PROJECTS);\\n return response.data;\\n }\\n);\\n\\n\\n##### Limitation of Using ExtraReducer to Listen fetchProjects.fulfilled\\n\\nWhen reading documentation ([link](https://redux-toolkit.js.org/api/createAsyncThunk)) we are instructed to create listener as follows:\\n\\ntypescript\\n// sample code from official tutorial\\n\\nextraReducers: (builder) => {\\n // Add reducers for additional action types here, and handle loading state as needed\\n builder.addCase(fetchProjects.fulfilled, (state, action) => {\\n // Add the result to the state\\n state.entities.push(action.payload);\\n });\\n};\\n\\n\\nHowever, this approach does not allow dispatching additional action, not even speak of influencing the state in other slices.\\n\\nFor example, we want to open a <Loading/> dialog during data-fetching, but the open state lives in another applicationSlice. We cannot simply import store object and store.dispatch the open-dialog action because store needs to be created by our current slice, namely, circular import will occur.\\n\\n#### Work Around: Write a Middleware for fetchProjects.fulfilled\\n\\nWe instead listen to fetchProjects.fulfilled by creating a middleware as follows, this is very similar to redux-saga (but mutch easier):\\n\\ntypescript\\n// projectSlice.ts\\n\\nexport const projectMiddleware = createListenerMiddleware();\\n\\nprojectMiddleware.startListening({\\n actionCreator: fetchProjects.pending,\\n effect: (action, listenerApi) => {\\n listenerApi.dispatch(\\n appSlice.actions.updateNotification({ open: true, content: \\"Loading...\\" })\\n );\\n },\\n});\\nprojectMiddleware.startListening({\\n actionCreator: fetchProjects.fulfilled,\\n effect: (action, listenerApi) => {\\n listenerApi.dispatch(appSlice.actions.updateNotification({ open: false }));\\n },\\n});\\n\\n\\nNow we are free to dispatch any action that adjusts the state of other slices.\\n\\n> **Important.** Note that we can startListening() multiple times using the same middleware. There is no need to create multiple middlewares for multiple actions.\\n\\n##### Add Middlewares to Store\\n\\ntypescript\\n// store.ts;\\n// real use case, ignore the other slices\\n\\nconst store = configureStore({\\n\\treducer: rootReducer,\\n\\tmiddleware: (getDefaultMiddleware) =>\\n\\t\\t//@ts-ignore\\n\\t\\tgetDefaultMiddleware({\\n\\t\\t\\tserializableCheck: false\\n\\t\\t}).concat(\\n\\t\\t\\tprojectMiddleware.middleware,\\n\\t\\t\\tsomeOtherMiddleware.middleware\\n\\t\\t)\\n});\\n\\n\\n##### Share a Middleware for Multiple Actions by matcher and isAnyOf\\n\\nIn the section <a href=\\"#Work-Around:-Write-a-Middleware-for-fetchProjects.pending\\">Work Around: Write a Middleware for fetchProjects.pending</a> we:\\n\\n- wrote a single middleware for a fetchProjects.pending action,\\n\\nBut the same effect should be shared amount actions like updateProjects.pending, deleteProject.pending, and even CRUD for all other entities. We can collect all those thunk actions and create middlewares specifically for all data-fetching logic:\\n\\ntypescript\\n//store.ts\\n\\nconst projectMiddleware = createListenerMiddleware();\\n\\nconst pendingActions = [\\n fetchProjects.pending,\\n fetchPages.pending,\\n fetchStudents.pending,\\n fetchCompanies.pending,\\n];\\n\\nconst fulfilledActions = [\\n fetchProjects.fulfilled,\\n fetchPages.fulfilled,\\n fetchStudents.fulfilled,\\n fetchCompanies.fulfilled,\\n];\\n\\nprojectMiddleware.startListening({\\n matcher: isAnyOf(...pendingActions),\\n effect: (action, listenerApi) => {\\n listenerApi.dispatch(\\n appSlice.actions.updateNotification({ open: true, content: \\"Loading...\\" })\\n );\\n },\\n});\\n\\nprojectMiddleware.startListening({\\n matcher: isAnyOf(...fulfilledActions),\\n effect: (action, listenerApi) => {\\n listenerApi.dispatch(\\n appSlice.actions.updateNotification({ open: true, content: \\"Loaded\\" })\\n );\\n },\\n});\\n\\n\\n#### Further Simplification for Writing Middleware to Handler Multiple Actions\\n\\nSometimes we have fine-grained notification pop-up messages for different thunk actions. It is tedious to write someMiddle.startListening({... for each of the actions.\\n\\nFor not to repeat writing the same code block, we write a helper function:\\n\\ntypescript\\nimport {\\n AnyAction,\\n ListenerEffect,\\n ListenerMiddlewareInstance,\\n ThunkDispatch,\\n isAnyOf\\n} from \\"@reduxjs/toolkit\\";\\nimport snackbarUtils from \\"./snackbarUtils\\";\\n\\ntype Effect = ListenerEffect<any, unknown, ThunkDispatch<unknown, unknown, AnyAction>, unknown>;\\n\\n/**\\n * actionMessageList consists of objects either of the form { action, content } or of the form { rejections } / { rejections, content }. When content is absent, the error message is supposed to be returned by thunkAPI.rejectWithValue\\n * in createAsyncThunk function.\\n */\\n\\nconst messageDispatch = ({ contentType, content }: { contentType: string, content: string }) => {\\n if (contentType === \\"sucesss\\") {\\n snackbarUtils.success(content)\\n } else if (contentType === \\"info\\") {\\n snackbarUtils.info(content)\\n } else if (contentType === \\"warning\\") {\\n snackbarUtils.warning(content)\\n } else if (contentType === \\"error\\") {\\n snackbarUtils.error(content);\\n }\\n}\\n\\n\\nexport default (\\n middleware: ListenerMiddlewareInstance<\\n unknown,\\n ThunkDispatch<unknown, unknown, AnyAction>,\\n unknown\\n >,\\n actionMessageList: {\\n action?: any,\\n rejections?: any[],\\n content?: string\\n effect?: Effect\\n contentType?: \\"sucesss\\" | \\"info\\" | \\"error\\" | \\"warning\\"\\n }[]\\n) => {\\n for (const actionMessage of actionMessageList) {\\n const { action, rejections, content, effect, contentType = \\"sucesss\\" } = actionMessage;\\n\\n if (action) {\\n let effect_: Effect;\\n if (effect) {\\n effect_ = effect;\\n } else if (content) {\\n effect_ = async (action, { dispatch }) => {\\n messageDispatch({ contentType, content })\\n // dispatch(appSlice.actions.updateNotification(\\n // { open: true, content: content || \\"No Message\\" }\\n // ))\\n };\\n } else {\\n effect_ = async (action, thunkAPI) => { };\\n }\\n\\n middleware.startListening({ actionCreator: action, effect: effect_ });\\n\\n } else if (rejections) {\\n if (effect) {\\n // @ts-ignore\\n middleware.startListening({ matcher: isAnyOf(...rejections), effect });\\n } else {\\n middleware.startListening({\\n // @ts-ignore\\n matcher: isAnyOf(...rejections),\\n effect: async (action, { dispatch }) => {\\n if (content) {\\n messageDispatch({ contentType, content })\\n // dispatch(appSlice.actions.updateNotification(\\n // { open: true, content: content || \\"No Message\\" }\\n // ))\\n } else {\\n const msg = action?.payload || \\"\\";\\n let errMsg = \\"Failed\\";\\n if (msg) {\\n errMsg += (Reason: ${msg});\\n }\\n snackbarUtils.error(errMsg)\\n // dispatch(appSlice.actions.updateNotification(\\n // { open: true, content: errMsg }\\n // ))\\n }\\n }\\n })\\n }\\n\\n }\\n }\\n}\\n\\nWe are now happy writing multiple middlewares:\\n\\ntypescript\\nexport const companyMiddleware = createListenerMiddleware();\\nregisterEffects(\\n companyMiddleware,\\n [\\n {\\n action: companyThunkAction.updateCompany.pending,\\n content: \\"Updating Company ...\\"\\n },\\n {\\n action: companyThunkAction.updateCompany.fulfilled,\\n content: \\"Updated.\\"\\n },\\n {\\n action: companyThunkAction.fetchCompanies.pending,\\n content: \\"Getting companies ...\\"\\n },\\n {\\n action: companyThunkAction.fetchCompanies.fulfilled,\\n content: \\"Loaded.\\"\\n },\\n {\\n action: companyThunkAction.uploadGenericFile.pending,\\n content: \\"Uploading...\\"\\n },\\n {\\n action: companyThunkAction.uploadGenericFile.fulfilled,\\n content: \\"Updated\\"\\n },\\n {\\n action: companyThunkAction.createCompany.pending,\\n content: \\"Creating Company ...\\"\\n },\\n {\\n rejections: [companyThunkAction.createCompany.rejected]\\n }\\n ]\\n)\\n\\n\\n- For actions:\\n - If we provide content, then the listener will pop-up a notification with content as the message.\\n - If we provide effect, then it will not pop-up notification and use custom effect instead.\\n- For rejections:\\n - If we just have rejections, the message is supposed to be the error message passed from thunkAPI.rejectWithValue in createAsyncThunk function.\\n - If we pair rejections with content, then it will show our content as pop-up notification.\\n\\n\\n#### Middleware that Handles all Rejected Actions (Optional)\\n\\nWe usually learn how to react to all api error in axios by using interceptor:\\n\\njs\\napiClient.interceptors.response.use(\\n function (response) {\\n const param = {\\n url: response.config.url,\\n data: response.data,\\n }\\n\\n if (${process.env.REACT_APP_ENV} === \'LOCAL\') {\\n if (response?.data?.success === false) {\\n \\n } else {\\n \\n }\\n }\\n },\\n function (error) {\\n if (error?.response?.status === 404) {\\n //404 page\\n }\\n }\\n)\\n\\nWe can instead handle all rejected api requests by middleware (provided that all api calls are processed by thunk actions)\\n\\njs\\nimport { createListenerMiddleware, isRejected } from \\"@reduxjs/toolkit\\";\\nimport snackbarUtils from \\"../../util/snackbarUtils\\";\\nimport { loginUrl } from \\"../../app/__paths__deprecated\\";\\nimport { getHistory } from \\"../../util/historyUtils\\";\\nimport authSlice from \\"../slices/authSlice\\";\\n\\nconst errorCodeRegex = /(?<=status\\\\scode\\\\s)\\\\d+/gi\\n\\nexport const errorMiddleware = createListenerMiddleware();\\n\\nerrorMiddleware.startListening({\\n matcher: isRejected,\\n effect: async (action, { dispatch }) => {\\n const history = getHistory();\\n const { error } = action;\\n const { message, stack } = error;\\n if (message) {\\n // sample message: Request failed with status code 401\\n const mathches = message.match(errorCodeRegex);\\n const errorCode = parseInt(mathches?.[0] || \\"0\\");\\n\\n if (\\n errorCode === 403 ||\\n errorCode === 401\\n ) {\\n console.log(\'403 401 redirect: \' + loginUrl)\\n dispatch(authSlice.actions.reset());\\n history?.push(loginUrl);\\n } else if (errorCode === 404) {\\n //404 page\\n } else if (errorCode === 500) {\\n //do nothing\\n } else {\\n \\n }\\n }\\n if (stack) {\\n snackbarUtils.error(stack);\\n }\\n }\\n});\\n\\nThis will be helpful if we are going to handle a very general error flow like \\n- expiration of access-token\\n- make an api call to refresh access-token\\n- ***resume the action again***, etc.\\n","title":"Write Middlewares in Redux-Toolkit","date":"2023-06-20T00:00:00.000Z","id":"blog0132","tag":"react","intro":"We list sample usage of createThunkAction provided by redux-toolkit in order to single out the logic of data-fetching away from the UI component.","toc":true},{"content":"\\n#### General Post Request\\n\\n#### File Uploading\\n\\nBy adding\\n\\nxml\\n<dependency>\\n <groupId>org.mongodb</groupId>\\n <artifactId>bson</artifactId>\\n <version>4.9.1</version>\\n</dependency>\\n\\n\\ninto our pom.xml we are given a handy tool to parse json data in the body of request from frontend:\\n\\njava\\nimport org.bson.Document;\\n\\n@PostMapping(value = \\"/projects)\\n@ResponseBody\\npublic Map<String, Boolean> updateProject(@RequestBody String reqJson) throws Exception {\\n Document updateReq = Document.parse(reqJson)\\n ...\\n}\\n\\n\\nNow we can get a value from key name via updateReq.get(\\"name\\").\\n\\n##### On React Side\\n\\nThe basic logic:\\n\\ntypescript\\nexport default () => {\\n ...\\n const [files, setFiles] = useState<FileList | null>(null);\\n\\n // used for file upload in <input ... onChange={uploadHandler} />\\n const uploadHandler = (e: React.ChangeEvent<HTMLInputElement>) => {\\n if (e.target.files) {\\n setFiles(e.target.files);\\n }\\n };\\n\\n const uploadClickHanlder = async () => {\\n if ((files?.length || 0) > 0) {\\n dispatch(asyncUploadImage({ oid: project.oid, file: files![0] }));\\n }\\n };\\n\\n return (\\n ...\\n <input type=\\"file\\" accept=\\"image/*\\" onChange={uploadHandler} />\\n <button onClick={uploadClickHanlder}> Upload </button>\\n ...\\n )\\n}\\n\\n\\nAs usual we will upload a file in the form of FormData. We define asyncUploadImage in other slice file:\\n\\ntypescript\\n//projectSlice.ts\\n\\nexport const asyncUploadImage = createAsyncThunk(\\n \\"upload-project-image\\",\\n async ({ oid, file }: { oid: string; file: File }) => {\\n const formData = new FormData();\\n formData.append(\\"file\\", file);\\n formData.append(\\"oid\\", oid);\\n\\n const response = await apiClient.post<{ success: boolean }>(\\n POST_PROJECTS_FILE_UPLOAD,\\n formData\\n );\\n\\n return response.data;\\n }\\n);\\n\\n\\n##### On Springboot Side\\n\\njava\\nprivate final File uploadDir = new File(\\"uploadfolder\\");\\n\\n@PostMapping(value = \\"/projects/fileupload\\")\\n@ResponseBody\\npublic Map<String, Object> updateProject(\\n @RequestParam(\\"oid\\") String oid,\\n @RequestParam(\\"file\\") MultipartFile file\\n) throws WbCustomException {\\n if (!this.uploadDir.exists()) {\\n this.uploadDir.mkdirs()\\n }\\n\\n String filePath = this.uploadDir.getPath() + \\"/\\" + file.getOriginalFilename();\\n\\n try (FileOutputStream fos = new FileOutputStream(filePath)) {\\n fos.write(file.getBytes());\\n return Map.of(\\"success\\", true);\\n } catch (Exception e) {\\n StringWriter errors = new StringWriter();\\n e.printStackTrace(new PrintWriter(errors));\\n return Map.of(\\"success\\", false, \\"errorMessage\\", errors.toString());\\n }\\n}\\n\\n\\n- We use new File uplopadDir = new File(\\"some/dir\\"), then we can use\\n\\n - uploadDir.exists() and\\n - uploadDir.mkdirs()\\n as in python.\\n\\n- We use new FileOutputStream(filePath).write to write the received byte[] into a file.\\n\\n- When receiving Formdata in springboot, it is as if receiving data from json object so that we can use @RequestParam to destructure the request body.\\n","title":"General Post Request in Springboot and File Uploading ","date":"2023-05-25T00:00:00.000Z","id":"blog0133","tag":"react, springboot, java","intro":"We record the whole workflow of uploading file from react frontend to springboot backend.","toc":true},{"content":"\\n#### Imports\\n\\nThe following is what we need in the script. Basically we just use selenium and re for regular expression.\\n\\npython\\nfrom selenium import webdriver\\nfrom selenium.webdriver.common.keys import Keys\\nfrom selenium.webdriver.common.action_chains import ActionChains\\nfrom selenium.webdriver.common.by import By\\nfrom selenium.webdriver.support.ui import WebDriverWait\\nfrom selenium.webdriver.support import expected_conditions as EC\\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities\\nimport re\\n\\n\\n#### The driver Object with eager Mode\\n\\npython\\ncaps = DesiredCapabilities().CHROME\\ncaps[\\"pageLoadStrategy\\"] = \\"eager\\"\\ndriver = webdriver.Chrome(\\n desired_capabilities=caps,\\n executable_path=\\"C:\\\\\\\\dev\\\\\\\\chrome-driver\\\\\\\\chromedriver.exe\\"\\n)\\n\\n\\nHere pageLoadStrategy is set to eager in order to speed up the scrapping process. The eager mode will let us stop loading the page when the dom element is completely loaded (we don\'t wait for other kind of data such as images, audio, etc)\\n\\n#### Utility Functions\\n\\npython\\ndef control_and_click(anchor):\\n ActionChains(driver).key_down(Keys.CONTROL)\\\\\\n .click(anchor)\\\\\\n .key_up(Keys.CONTROL)\\\\\\n .perform()\\n\\ndef wait_element(search_txt, by_method=By.CSS_SELECTOR, seconds=10):\\n return WebDriverWait(driver, seconds)\\\\\\n .until(EC.presence_of_all_elements_located((by_method, search_txt)))\\n\\ndef load_page_and_get_reports(url):\\n driver.get(url)\\n reports = wait_element(\\"#reports-table a[href*=\'/reports\']\\")\\n print(\\"Number of reports in this page: {}\\".format(len(reports)))\\n return reports\\n\\ndef refresh_page_and_get_reports():\\n driver.refresh()\\n reports = wait_element(\\"#reports-table a[href*=\'/reports\']\\")\\n print(\\"Number of reports in this page: {}\\".format(len(reports)))\\n return reports\\n\\n\\n- control_and_click will press control key and click the target element, this is to open a link in new tab.\\n- wait_element will wait for an element to be locatable, this is to make sure no problem occurs when a page is slow (e.g. server side rendering takes some time to return a web page).\\n- Once there is problem, we try to refresh refresh_page_and_get_reports to see whether we can resolve it.\\n\\n#### The Scrapping Function\\n\\n- The strategy is to identify all the anchors in the page that we need to click (line 9 with css selector).\\n\\n python-1\\n def start_fflog_scrapping(urls, target_player_names, file_location=\\"omega_kills_record.txt\\"):\\n names_matching_regex = re.compile(\\"|\\".join(target_player_names))\\n\\n current_page_index = 0\\n\\n for page_index, url in enumerate(urls):\\n current_page_index = page_index\\n try:\\n reports = load_page_and_get_reports(url)\\n \\n\\n- Not only we click it, we ctrl + click in order to open the page in new tab.\\n\\n python-10\\n for i in range(0, len(reports)):\\n print(\\"handing report\\", reports[i].text)\\n report_anchor = reports[i]\\n control_and_click(report_anchor)\\n \\n\\n- When ctrl + click succeeds, the number of tabs: len(driver.window_handles) must be $\\\\ge 2$. However, it used to fail for some reason, we may record those unread reports and study it later on:\\n\\n driver.switch_to.window(driver.window_handles[1]) will switch the brower to next tab.\\n\\n python-14\\n if len(driver.window_handles) <= 1:\\n print(\\"report {} cannot be read due to some problem\\".format(report_anchor.get_attribute(\\"href\\")))\\n with open(\\"report_not_read.txt\\", \\"a+\\") as _f:\\n _f.write(report_anchor.get_attribute(\\"href\\") + \\"\\\\n\\")\\n continue\\n\\n driver.switch_to.window(driver.window_handles[1])\\n \\n\\n- Depends on the number of click we need, when we are done with the scrapping:\\n\\n - We close the current tab by driver.close()\\n - We switch to previous tab by driver.switch_to.window(driver.window_handles[i])\\n - In case we have clicked anchors twice, we need to close() and switch twice, see line 39 for example.\\n\\n- The reamining scrapping logic is page-specific. We use regular expression to match desired results and make a record.\\n\\n python-21\\n # detail of bosses related, we are only interested in those omega kill log:\\n rows = wait_element(\\"a[class*=\'report-overview-boss\']\\")\\n\\n omega_kills = [row for row in rows if re.search(\\"TheOmegaProtocolKill\\", re.sub(\\"\\\\\\\\s\\", \\"\\", row.text)) is not None]\\n\\n if len(omega_kills) > 0:\\n control_and_click(omega_kills[0])\\n driver.switch_to.window(driver.window_handles[2])\\n name_anchors = wait_element(\\"#summary-damage-done-scroller-0 tr[role=\'row\'] a.tooltip\\")\\n names = \\"\\".join([anchor.text for anchor in name_anchors])\\n matched_names = names_matching_regex.findall(names)\\n if len(matched_names) > 0:\\n with open(file_location, \\"a+\\", encoding=\\"utf-8\\") as f:\\n line_1 = \\"target player(s):\\\\t\\" + \\", \\".join(matched_names) + \\"\\\\n\\"\\n line_2 = \\"link: \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\" + driver.current_url + \\"\\\\n\\"\\n line_3 = \\"---\\\\n\\"\\n f.writelines(line_1 + line_2 + line_3)\\n print(\\"Player { \\"+ \\", \\".join(matched_names) + \\" } has been found\\")\\n driver.close()\\n driver.switch_to.window(driver.window_handles[1])\\n driver.close()\\n driver.switch_to.window(driver.window_handles[0])\\n else:\\n driver.close()\\n driver.switch_to.window(driver.window_handles[0])\\n except Exception as e:\\n urls = urls[current_page_index:]\\n print(e)\\n print(\\"Exception was caught, retry from page: \\" + urls[0])\\n start_fflog_scrapping(urls, target_player_names)\\n \\n","title":"Data Scrapping for Data that Requires Click by Click ","date":"2023-05-27T00:00:00.000Z","id":"blog0134","tag":"python, selenium","intro":"*Click and then get detail* is a very routine practice for data scrapping. We record how to do that by selenium in python.","toc":true},{"content":"\\ntext\\nconst useStyles = makeStyles((theme) => ({\\n grow: {\\n animation: $growAnimation 2000ms ease-in-out,\\n },\\n \\"@keyframes growAnimation\\": {\\n \\"0%\\": {\\n boxShadow: \\"0 0 0px #3498db\\",\\n },\\n \\"50%\\": {\\n boxShadow: \\"0 0 20px #3498db\\",\\n },\\n \\"100%\\": {\\n boxShadow: \\"0 0 0px #3498db\\",\\n },\\n },\\n}));\\n\\n\\nAfter animation is triggered, make sure to toggle off the corresponding flag\\n\\njs\\nconst shouldGrow = growing && rowIndexBeingSelected === reduxStoreRowIndex;\\n\\nuseEffect(() => {\\n if (shouldGrow) {\\n setTimeout(() => {\\n dispatch(wbuserSlice.actions.setRowEdition({ grow: false }));\\n }, 2000);\\n }\\n}, [growing]);\\n\\n","title":"Mui CSS Animation with Keyframes","date":"2023-06-09T00:00:00.000Z","id":"blog0136","tag":"react","intro":"Record how to write CSS animation with keyframes in mui makeStyles.","toc":false},{"content":"\\n#### The JsonPipeline Class with Pipleline Builder\\n\\nIn our Mongo package we add the following subclass:\\n\\njava\\npublic class Mongo {\\n ...\\npublic static class JsonPipeline {\\n private static String trailingBracketWithSpaces = \\"\\\\\\\\]\\\\\\\\s*$\\";\\n private static String startBracketWithSpaces = \\"^\\\\\\\\s*\\\\\\\\[\\";\\n\\n private String totalPipelineString = \\"\\";\\n\\n public List<BsonDocument> get() {\\n return parseJsonPipline();\\n }\\n\\n private List<BsonDocument> parseJsonPipline() {\\n this.add(\\"[{$addFields: {oid: {$toString: \\\\\\"$_id\\\\\\"}}}, {$unset: \\\\\\"_id\\\\\\"}]\\");\\n var pipelines_ = new BsonArrayCodec()\\n .decode(new JsonReader(totalPipelineString), DecoderContext.builder().build())\\n .stream()\\n .map(BsonValue::asDocument)\\n .collect(Collectors.toList());\\n\\n return pipelines_;\\n\\n }\\n\\n public JsonPipeline add(String json) {\\n if (totalPipelineString.isEmpty()) {\\n totalPipelineString = json;\\n } else {\\n String prevRemoveCloseBrac = totalPipelineString.replaceAll(trailingBracketWithSpaces, \\"\\");\\n String nextRemoveOpenBrac = json.replaceAll(startBracketWithSpaces, \\"\\");\\n totalPipelineString = prevRemoveCloseBrac + \\",\\" + nextRemoveOpenBrac;\\n }\\n return this;\\n }\\n\\n public String getTotalPipelineString() {\\n return totalPipelineString;\\n }\\n\\n /**\\n * oid will be automatically created for foreign collection, the result will\\n * always be an array.\\n * Make sure to {$unwind: \\"$newFieldName\\"} if you are sure there are only one\\n * element in the result.\\n */\\n public JsonPipeline addLeftJoin(\\n String foreignCollection,\\n String localField,\\n String foreignField,\\n String leftJoinPipeline,\\n String newFieldName) {\\n\\n String _jsonPipeline = String.format(\\"\\"\\"\\n [\\n {\\n $addFields: {\\n tmp_localfield: \\"%s\\"\\n }\\n },\\n {\\n $unwind: {\\n path: \\"$tmp_localfield\\",\\n preserveNullAndEmptyArrays: true\\n }\\n },\\n {\\n $lookup: {\\n let: { tmp: \\"$tmp_localfield\\"},\\n from: \\"%s\\",\\n pipeline: [\\n {\\n $addFields:{\\n oid: {$toString: \\"$_id\\"}\\n }\\n },\\n {\\n $unset: \\"_id\\"\\n },\\n {\\n $match:{\\n $expr:{\\n $eq: [\\"$$tmp\\", \\"%s\\"]\\n }\\n }\\n },\\n %s\\n ],\\n as: \\"tmp_singleElementList\\"\\n }\\n },\\n {\\n $unwind: {\\n path: \\"$tmp_singleElementList\\",\\n preserveNullAndEmptyArrays: true\\n }\\n },\\n {\\n $group:{\\n _id: \\"$_id\\",\\n originalData: { $first: \\"$$ROOT\\"},\\n pushList: { $push: \\"$tmp_singleElementList\\" }\\n }\\n },\\n {\\n $replaceRoot: {\\n newRoot: {\\n $mergeObjects: [\\n \\"$originalData\\",\\n { %s: \\"$pushList\\" }\\n ]\\n }\\n }\\n },\\n {\\n $unset : \\"tmp_singleElementList\\"\\n },\\n {\\n $unset : \\"tmp_localfield\\"\\n }\\n ]\\n \\"\\"\\",\\n \\"$\\" + localField,\\n foreignCollection,\\n \\"$\\" + foreignField,\\n leftJoinPipeline.replaceAll(\\"(^\\\\\\\\s*\\\\\\\\[)|(\\\\\\\\]\\\\\\\\s*)$\\", \\"\\"),\\n newFieldName);\\n\\n return this.add(_jsonPipeline);\\n }\\n }\\n\\n public static JsonPipeline createJsonPipeline() {\\n return new JsonPipeline();\\n }\\n}\\n\\n\\n#### Pipelines\\n\\n##### Simplest One\\n\\nNow a projection pipeline can be as simple as\\n\\njava\\nvar pipeline = Mongo.createJsonPipeline();\\npipeline.add(\\"[{ $project: { code: 1, name: 1, role: 1 }}]\\");\\nList<Document> results = companies.aggregate(pipeline.get()).into(new ArrayList<>());\\n\\n\\ninstead of\\n\\njava\\nvar projections = Arrays.asList(\\n Aggregates.project(\\n Projections.fields(\\n Projections.include(\\"code\\"),\\n Projections.include(\\"name\\"),\\n Projections.include(\\"role\\")\\n ))\\n);\\nList<Document> results = companies.aggregate(projections).into(new ArrayList<>());\\n\\n\\n##### More Advanced Pipeline with Left Joins\\n\\njava\\nvar chainOid = searchEmailRefinedParam.getEmail_chain_oid();\\nvar chainCollection = mongodb.getCollection(CollectionNames.email_messages);\\nvar pipeline = MongoDB.createJsonPipeline();\\npipeline\\n .add(String.format(\\"[{ $match:{ email_chain_oid: \\\\\\"%s\\\\\\" }}]\\", chainOid))\\n .addLeftJoin(CollectionNames.users,\\n \\"sender_email\\",\\n \\"user_name\\",\\n \\"[{ $project: {user_name: 1, email: 1, first_name: 1, last_name: 1, company_code: 1} }]\\",\\n \\"senderInDb\\")\\n .add(\\"[{$unwind: \\\\\\"$senderInDb\\\\\\"}]\\")\\n .addLeftJoin(CollectionNames.users,\\n \\"recipient_emails\\",\\n \\"user_name\\",\\n \\"[{ $project: {user_name: 1, email: 1, first_name: 1, last_name: 1} }]\\",\\n \\"recipientsInDb\\")\\n .addLeftJoin(CollectionNames.companies,\\n \\"senderInDb.company_code\\",\\n \\"code\\",\\n \\"[{ $project:{ name:1 } }]\\",\\n \\"sender_company\\")\\n .add(\\"[{ $unwind: \\\\\\"$sender_company\\\\\\" }]\\");\\n\\nvar emailsResult = chainCollection.aggregate(pipeline.get()).into(new ArrayList<>());\\n\\n","title":"Write Mongo Aggregation Using JSON in Springboot","date":"2023-07-30T00:00:00.000Z","id":"blog0137","tag":"java, mongo","intro":"We get rid of the headache of writing mongo agggregation using mongo apis in java given that we already know how to write that in json format.","toc":true},{"content":"\\njavascript\\nimport { PayloadAction, createAsyncThunk, createListenerMiddleware, createSlice, isAnyOf } from \\"@reduxjs/toolkit\\"\\nimport appSlice from \\"./appSlice\\"\\n\\ntype State = {\\n sth: string\\n}\\nconst initialState: State = {\\n sth: \\"\\"\\n}\\n\\nconst slice = createSlice(\\n {\\n name: \\"sth\\",\\n initialState,\\n reducers: {\\n reset: (state) => {\\n return initialState\\n }\\n },\\n extraReducers: (builder) => {\\n builder.addCase(someThunkAction.fetchSth.fulfilled, (state, action) => {\\n\\n })\\n }\\n }\\n)\\n\\nexport const someThunkAction = {\\n fetchSth: createAsyncThunk(\\"get-sth\\", async () => {\\n\\n })\\n}\\n\\nconst someMiddleware = createListenerMiddleware();\\n\\nsomeMiddleware.startListening({\\n matcher: isAnyOf(\\n someThunkAction.fetchSth.fulfilled,\\n ),\\n effect: async (action, listenerApi) => {\\n listenerApi.dispatch(appSlice.actions.updateNotification(\\n {\\n open: true,\\n content: \\"Loaded.\\"\\n }\\n ))\\n }\\n});\\n\\nexport default slice;\\n\\n","title":"Redux Slice Template","date":"2023-06-13T00:00:00.000Z","id":"blog0138","tag":"react","intro":"Record my template of redux slice.","toc":false},{"content":"\\n#### Demonstration 1 Look up from Arrays Instead of Collections (Inner Joins Only)\\n\\n##### Stages\\n\\n- Since look up is only available for collections in mongodb, if there is a property among the array of elements, like id, which we need to left join with another collection. We need to $unwind it to reduce the problem into our old familiar problem.\\n\\n- Next we need to undo the $unwind process, which can be done by using using all other fields jointly to form an_id object, then we push the lookup results into an array according to this _id.\\n\\n- We then use $replaceRoot: { newRoot: ... } trick to move everything in _id: {...} to the parent level.\\n\\n##### Full Breakdown\\n\\njavascript-1\\ndb.ns_material_categories.aggregate([\\n {\\n $lookup: {\\n let: { group_code: \\"$group_code\\" },\\n from: \\"ns_material_category_groups\\",\\n pipeline: [\\n { $match: { $expr: { $eq: [\\"$code\\", \\"$$group_code\\"] } } },\\n { $project: { name: 1, code: 1, _id: 0 } }\\n ],\\n as: \\"materialGroup\\"\\n }\\n },\\n {\\n $match: {\\n $expr: {\\n $eq: [\\"$enabled\\", \\"Y\\"]\\n }\\n }\\n },\\n {\\n $unwind: \\"$materialGroup\\"\\n },\\n\\n\\nThis lookup only aims at left-joining ns_material_category_groups to get materialGroup, which becomes an array of objects of only 1 element (due to 1-1 correspondance), we $unwind to sequeeze the array into an object:\\n\\nnone\\n [{\\n \\"_id\\": {\\n \\"timestamp\\": 1648724334,\\n \\"date\\": \\"2022-03-31T10:58:54.000+00:00\\"\\n },\\n \\"code\\": \\"FINISHES_CERAMIC_TILES\\",\\n \\"group_code\\": \\"InteriorFinishes\\",\\n \\"enabled\\": \\"Y\\",\\n \\"materialGroup\\": {\\n \\"code\\": \\"InteriorFinishes\\"\\n }\\n}]\\n\\n\\nWe next need to lookup from another collection using the field code:\\n\\njavascript-23\\n {\\n $lookup: {\\n let: { code: \\"$code\\" },\\n from: \\"ns_materials\\",\\n pipeline: [\\n {\\n $match: {\\n $expr: {\\n $and: [\\n { $eq: [\\"$category_code\\", \\"$$code\\"] },\\n { $eq: [\\"$is_dummy\\", \\"Y\\"] }\\n ]\\n }\\n }\\n },\\n {\\n $project: {\\n _id: 0,\\n manu_code: 1,\\n }\\n }\\n ],\\n as: \\"suppliers\\"\\n }\\n },\\n\\n\\nAt this point our queried object becomes\\n\\nnone\\n[\\n {\\n \\"_id\\": {\\n \\"timestamp\\": 1648724334,\\n \\"date\\": \\"2022-03-31T10:58:54.000+00:00\\"\\n },\\n \\"code\\": \\"FINISHES_CERAMIC_TILES\\",\\n \\"group_code\\": \\"InteriorFinishes\\",\\n \\"enabled\\": \\"Y\\",\\n \\"materialGroup\\": {\\n \\"code\\": \\"InteriorFinishes\\"\\n },\\n \\"suppliers\\": [\\n {\\n \\"manu_code\\": \\"DAUGRES\\"\\n },\\n {\\n \\"manu_code\\": \\"ELEPHOME\\"\\n },\\n {\\n \\"manu_code\\": \\"HONGYU\\"\\n },\\n {\\n \\"manu_code\\": \\"KITO\\"\\n },\\n {\\n \\"manu_code\\": \\"MMMOSAIC\\"\\n }\\n ]\\n },\\n ...\\n]\\n\\n\\nBut we need to look up from another collection using manu_code, we $unwind the suppliers field and do a $lookup from ns_material_manus again to get the name_en field.\\n\\njavascript-48\\n {\\n $unwind: \\"$suppliers\\"\\n },\\n {\\n $lookup: {\\n let: { manu_code: \\"$suppliers.manu_code\\" },\\n from: \\"ns_material_manus\\",\\n pipeline: [\\n {\\n $match: {\\n $expr: {\\n $eq: [\\"$$manu_code\\", \\"$internal_code\\"]\\n }\\n }\\n },\\n {\\n $project: {\\n _id: 0,\\n name_en: 1\\n }\\n },\\n { $limit: 1 }\\n ],\\n as: \\"supplierDetail\\"\\n }\\n },\\n\\n\\nAt this point we get\\n\\nnone\\n[\\n {\\n \\"_id\\": {\\n \\"timestamp\\": 1648724334,\\n \\"date\\": \\"2022-03-31T10:58:54.000+00:00\\"\\n },\\n \\"code\\": \\"FINISHES_CERAMIC_TILES\\",\\n \\"group_code\\": \\"InteriorFinishes\\",\\n \\"enabled\\": \\"Y\\",\\n \\"materialGroup\\": {\\n \\"code\\": \\"InteriorFinishes\\"\\n },\\n \\"suppliers\\": {\\n \\"manu_code\\": \\"DAUGRES\\"\\n },\\n \\"supplierDetail\\": [\\n {\\n \\"name_en\\": \\"DAUGRES\\"\\n }\\n ]\\n },\\n ...\\n]\\n\\n\\n- Note that we have used { $limit: 1 } as we understand each company only has one name, lookup results are born with being an array.\\n\\n- We $unwind it to get ordinary object and finally use the original fields jointly as an _id to group all supplier results:\\n\\njavascript-74\\n {\\n $unwind: \\"$supplierDetail\\"\\n },\\n {\\n $group: {\\n _id: {\\n code: \\"$code\\",\\n group_code: \\"$group_code\\",\\n name: \\"$name\\",\\n \\"code\\": \\"$code\\",\\n \\"group_code\\": \\"$group_code\\",\\n \\"enabled\\": \\"$enabled\\",\\n \\"materialGroup\\": \\"$materialGroup\\"\\n },\\n suppliers: { \\"$push\\": { name_en: \\"$supplierDetail.name_en\\" } }\\n }\\n },\\n\\n\\nThe result at this point is of the form:\\n\\nnone\\n[\\n {\\n \\"_id\\": {\\n \\"code\\": \\"EXTERIOR_FINISHES_COMPOSITE_DECKING\\",\\n \\"group_code\\": \\"ExteriorFlooring\\",\\n \\"enabled\\": \\"Y\\",\\n \\"materialGroup\\": {\\n \\"code\\": \\"ExteriorFlooring\\"\\n }\\n },\\n \\"suppliers\\": [\\n {\\n \\"name_en\\": \\"GREENZONE\\"\\n },\\n {\\n \\"name_en\\": \\"MEXY\\"\\n }\\n ]\\n },\\n ...\\n]\\n\\n\\nWe finally move everything inside _id to upper level:\\n\\njavascript-93\\n {\\n $replaceRoot: {\\n newRoot: {\\n $mergeObjects: [\\"$_id\\", { suppliers: \\"$suppliers\\" }]\\n }\\n }\\n }\\n])\\n\\n\\nwhich finally yields\\n\\nnone\\n[\\n {\\n \\"code\\": \\"TAPWARES_KITCHEN_TAPWARE\\",\\n \\"group_code\\": \\"Tapwares.Accessories\\",\\n \\"enabled\\": \\"Y\\",\\n \\"materialGroup\\": {\\n \\"code\\": \\"Tapwares.Accessories\\"\\n },\\n \\"suppliers\\": [\\n {\\n \\"name_en\\": \\"EMPOLO\\"\\n },\\n {\\n \\"name_en\\": \\"ECCO VITA\\"\\n }\\n ]\\n },\\n ...\\n]\\n\\n\\n#### Demonstration 2: $lookup + $unwind by Default is an Inner Join\\n\\n##### Example Demonstrating Inner Join Behaviour\\n\\nConsider the following aggregation, we are restricting ourself to look at only one \\"problematic\\" document by enforcing { code: \\"NEW_CATEGORY_20230614C\\" }.\\n\\njs-1\\ndb.ns_material_categories.aggregate([\\n {\\n $match: {\\n $and: [\\n { enabled: \\"Y\\" },\\n { code: \\"NEW_CATEGORY_20230614C\\" }\\n ]\\n }\\n },\\n {\\n $lookup: {\\n let: { group_code: \\"$group_code\\" },\\n from: \\"ns_material_category_groups\\",\\n pipeline: [\\n { $match: { $expr: { $eq: [\\"$code\\", \\"$$group_code\\"] } } },\\n { $project: { oid: { $toString: \\"$_id\\" }, name: 1, code: 1, _id: 0 } }\\n ],\\n as: \\"catGroup\\"\\n }\\n },\\n {\\n $unwind: \\"$catGroup\\"\\n },\\n {\\n $lookup: {\\n let: { code: \\"$code\\" },\\n from: \\"ns_materials\\",\\n pipeline: [\\n {\\n $match: {\\n $expr: {\\n $and: [\\n { $eq: [\\"$category_code\\", \\"$$code\\"] },\\n { $eq: [\\"$is_dummy\\", \\"Y\\"] }\\n ]\\n }\\n }\\n },\\n {\\n $project: {\\n _id: 0,\\n manu_code: 1,\\n }\\n }\\n ],\\n as: \\"suppliers\\"\\n }\\n },\\n\\n\\nAt this point the result is:\\n\\nnone\\n[\\n {\\n \\"_id\\": {\\n \\"$oid\\": \\"648954d9547dee498d4f85a3\\"\\n },\\n \\"code\\": \\"NEW_CATEGORY_20230614C\\",\\n \\"group_code\\": \\"NewCategoryGroup20230614c\\",\\n \\"enabled\\": \\"Y\\",\\n \\"catGroup\\": {\\n \\"code\\": \\"NewCategoryGroup20230614c\\",\\n \\"oid\\": \\"648954d2547dee498d4f85a2\\"\\n },\\n \\"suppliers\\": []\\n }\\n]\\n\\n\\nNote that suppliers is an empty array, which means that we have _nothing matched_ from the table on the right.\\n\\nEach item in suppliers contains a field called manu_code, with which we will be using to look up from another collection, for this purpose, we need to $unwind suppliers field and then do a $lookup using $suppliers.manu_code.\\n\\njs-50\\n {\\n $unwind: \\"$suppliers\\"\\n },\\n\\n\\nbut oops!\\n\\nnone\\n[]\\n\\n\\nWe conclude from this stage that by default $lookup + $unwind is an inner join operation (assuming we will be doing $group action in the last stage).\\n\\n##### unwind with preserveNullAndEmptyArrays: true\\n\\nAs in mySQL we most of the time want to left join instead of inner join, if we instead\\n\\njs-50\\n {\\n $unwind: {\\n path: \\"$suppliers\\",\\n preserveNullAndEmptyArrays: true\\n }\\n },\\n\\n\\nwe get\\n\\nnone\\n[\\n {\\n \\"_id\\": {\\n \\"$oid\\": \\"648954d9547dee498d4f85a3\\"\\n },\\n \\"code\\": \\"NEW_CATEGORY_20230614C\\",\\n \\"group_code\\": \\"NewCategoryGroup20230614c\\",\\n \\"enabled\\": \\"Y\\",\\n \\"catGroup\\": {\\n \\"code\\": \\"NewCategoryGroup20230614c\\",\\n \\"oid\\": \\"648954d2547dee498d4f85a2\\"\\n }\\n }\\n]\\n\\n\\nthat means row on the left table is preserved even we match nothing on the right table. This is exactly what left join does.\\n\\nIf we do a look up using suppliers.manu_code:\\n\\njs-53\\n {\\n $lookup: {\\n let: { manu_code: \\"$suppliers.manu_code\\" },\\n from: \\"ns_material_manus\\",\\n pipeline: [\\n {\\n $match: {\\n $expr: {\\n $eq: [\\"$$manu_code\\", \\"$internal_code\\"]\\n }\\n }\\n },\\n {\\n $project: {\\n _id: 0,\\n name_en: 1\\n }\\n },\\n { $limit: 1 }\\n ],\\n as: \\"supplierDetail\\"\\n }\\n },\\n\\n\\nwe get\\n\\nnone\\n[\\n {\\n \\"_id\\": {\\n \\"$oid\\": \\"648954d9547dee498d4f85a3\\"\\n },\\n \\"code\\": \\"NEW_CATEGORY_20230614C\\",\\n \\"group_code\\": \\"NewCategoryGroup20230614c\\",\\n \\"enabled\\": \\"Y\\",\\n \\"catGroup\\": {\\n \\"code\\": \\"NewCategoryGroup20230614c\\",\\n \\"oid\\": \\"648954d2547dee498d4f85a2\\"\\n },\\n \\"supplierDetail\\": []\\n }\\n]\\n\\n\\nAs desired.\\n\\n##### Grouping and Handling the [ {} ] Result\\n\\n###### Starting from Nonempty Suppliers Document and Group the Results\\n\\nWe enforce { code: \\"EXTERIOR_FLOORING_COBBLE_STONE\\" } at the beginning of the aggregation script, this has 3 suppliers and thefore we have 3 supplierDetails.\\n\\nUp to line 75 of the aggregation script, the result will be 3 pieces of data having the following form (with suppliers and supplierDetail being the only varying field)\\n\\nnone\\n{\\n \\"_id\\": {\\n \\"$oid\\": \\"63316d25fe1ff183f4436771\\"\\n },\\n \\"code\\": \\"KITCHEN_APPLIANCES_OVEN\\",\\n \\"group_code\\": \\"KitchenAppliances\\",\\n \\"enabled\\": \\"Y\\",\\n \\"catGroup\\": {\\n \\"code\\": \\"KitchenAppliances\\",\\n \\"oid\\": \\"63316d07fe1ff183f443676a\\"\\n },\\n \\"suppliers\\": {\\n \\"manu_code\\": \\"FOTILE\\"\\n },\\n \\"supplierDetail\\": [\\n {\\n \\"name_en\\": \\"FOTILE\\"\\n }\\n ]\\n}\\n\\n\\nNext we $unwind (just for squeezing the array) and $group what we need:\\n\\njs-76\\n {\\n $unwind: {\\n path: \\"$supplierDetail\\",\\n preserveNullAndEmptyArrays: true\\n }\\n },\\n {\\n $group: {\\n _id: {\\n oid: { $toString: \\"$_id\\" },\\n code: \\"$code\\",\\n enabled: \\"$enabled\\",\\n catGroup: \\"$catGroup\\"\\n },\\n suppliers: { \\"$push\\": { name_en: \\"$supplierDetail.name_en\\" } }\\n }\\n },\\n\\n\\nwhich yields:\\n\\nnone\\n[\\n {\\n \\"_id\\": {\\n \\"oid\\": \\"63316d25fe1ff183f4436770\\",\\n \\"code\\": \\"EXTERIOR_FLOORING_COBBLE_STONE\\",\\n \\"enabled\\": \\"Y\\",\\n \\"catGroup\\": {\\n \\"code\\": \\"ExteriorFlooring\\",\\n \\"oid\\": \\"62cbbf0b35fc26d4990aa2f1\\"\\n }\\n },\\n \\"suppliers\\": [\\n {\\n \\"name_en\\": \\"DALEI\\"\\n },\\n {\\n \\"name_en\\": \\"EASTWOOD STONE\\"\\n },\\n {\\n \\"name_en\\": \\"STONELINK\\"\\n }\\n ]\\n }\\n]\\n\\n\\n$replaceRoot is the trick to bring everything in _id to the top level:\\n\\njs-99\\n {\\n $replaceRoot: {\\n newRoot: {\\n $mergeObjects: [\\"$_id\\", { suppliers: \\"$suppliers\\" }]\\n }\\n }\\n },\\n\\n\\nwhich results in:\\n\\nnone\\n[\\n {\\n \\"oid\\": \\"63316d25fe1ff183f4436770\\",\\n \\"code\\": \\"EXTERIOR_FLOORING_COBBLE_STONE\\",\\n \\"enabled\\": \\"Y\\",\\n \\"catGroup\\": {\\n \\"code\\": \\"ExteriorFlooring\\",\\n \\"oid\\": \\"62cbbf0b35fc26d4990aa2f1\\"\\n },\\n \\"suppliers\\": [\\n {\\n \\"name_en\\": \\"DALEI\\"\\n },\\n {\\n \\"name_en\\": \\"EASTWOOD STONE\\"\\n },\\n {\\n \\"name_en\\": \\"STONELINK\\"\\n }\\n ]\\n }\\n]\\n\\n\\nWe finally look up another table for technical purpose. Note the use of javascript, which is considerably tedious if we rely merely on mongo query:\\n\\njs-106\\n {\\n $lookup: {\\n from: \\"ns_generic_form_templates\\",\\n let: { code: \\"$code\\" },\\n pipeline: [\\n { $project: { subtypes: 1 } },\\n {\\n $addFields: {\\n formAnalysis: {\\n $function: {\\n body: function (code, subtypes, formId) {\\n if (!subtypes) {\\n return null\\n } else {\\n const hasForm = subtypes.indexOf(code) > -1;\\n\\n return { category_code: code, hasForm, formOid: formId }\\n }\\n },\\n args: [\\"$$code\\", \\"$subtypes\\", \\"$_id\\"],\\n lang: \'js\'\\n }\\n }\\n }\\n },\\n { $project: { subtypes: 0 } },\\n {\\n $match: { \\"formAnalysis.hasForm\\": true }\\n },\\n { $limit: 1 },\\n { $replaceRoot: { newRoot: \\"$formAnalysis\\" } },\\n { $set: { formOid: { $toString: \\"$formOid\\" } } },\\n ],\\n as: \\"genericForm\\"\\n }\\n },\\n\\n\\nWe next $unwind again simply for sequeezing the array into a single object:\\n\\njs-142\\n {\\n $unwind: {\\n path: \\"$genericForm\\",\\n preserveNullAndEmptyArrays: true\\n }\\n },\\n\\n\\nAnd we get a pretty result:\\n\\nnone\\n[\\n {\\n \\"oid\\": \\"63316d25fe1ff183f4436770\\",\\n \\"code\\": \\"EXTERIOR_FLOORING_COBBLE_STONE\\",\\n \\"enabled\\": \\"Y\\",\\n \\"catGroup\\": {\\n \\"code\\": \\"ExteriorFlooring\\",\\n \\"oid\\": \\"62cbbf0b35fc26d4990aa2f1\\"\\n },\\n \\"suppliers\\": [\\n {\\n \\"name_en\\": \\"DALEI\\"\\n },\\n {\\n \\"name_en\\": \\"EASTWOOD STONE\\"\\n },\\n {\\n \\"name_en\\": \\"STONELINK\\"\\n }\\n ],\\n \\"genericForm\\": {\\n \\"category_code\\": \\"EXTERIOR_FLOORING_COBBLE_STONE\\",\\n \\"hasForm\\": true,\\n \\"formOid\\": \\"62972948261ac272d152e238\\"\\n }\\n }\\n]\\n\\n\\n###### Starting from Empty Suppliers Document and Deal with [ {} ]\\n\\nWe go back to { code: \\"NEW_CATEGORY_20230614C\\" } which has no suppliers. We run through all the same script to line 147 as above, then the result becomes\\n\\nnone\\n[\\n {\\n \\"oid\\": \\"648954d9547dee498d4f85a3\\",\\n \\"code\\": \\"NEW_CATEGORY_20230614C\\",\\n \\"enabled\\": \\"Y\\",\\n \\"catGroup\\": {\\n \\"code\\": \\"NewCategoryGroup20230614c\\",\\n \\"oid\\": \\"648954d2547dee498d4f85a2\\"\\n },\\n \\"suppliers\\": [\\n {}\\n ]\\n }\\n]\\n\\n\\nTo make [ {} ] into [], we do a filtering:\\n\\njs-149\\n {\\n $set: {\\n suppliers: {\\n $filter: {\\n input: \\"$suppliers\\",\\n cond: {\\n $ifNull: [\\"$$this.name_en\\", false]\\n }\\n }\\n }\\n }\\n }\\n ]\\n)\\n\\n\\nand we get:\\n\\nnone\\n[\\n {\\n \\"oid\\": \\"648954d9547dee498d4f85a3\\",\\n \\"code\\": \\"NEW_CATEGORY_20230614C\\",\\n \\"enabled\\": \\"Y\\",\\n \\"catGroup\\": {\\n \\"code\\": \\"NewCategoryGroup20230614c\\",\\n \\"oid\\": \\"648954d2547dee498d4f85a2\\"\\n },\\n \\"suppliers\\": []\\n }\\n]\\n\\n\\n\\n","title":"Inner and Left Joining Multiple Collections in Mongo --- The preserveNullAndEmptyArrays in $unwind","date":"2023-06-15T00:00:00.000Z","id":"blog0139","tag":"mongo","intro":"$lookup only works for the whole collection, we demonstrate how to left join another collection using a field which is an array.","toc":true},{"content":"\\n#### First \\\\$lookup\\n\\njs-1\\ndb.ns_material_categories.aggregate(\\n [\\n {\\n $lookup: {\\n from: \\"ns_material_category_groups\\",\\n localField: \\"group_code\\",\\n foreignField: \\"code\\",\\n as: \\"properties\\"\\n }\\n },\\n\\n\\nThe $lookup here is the most basic form, which do a left join by gluing group_code and code.\\n\\n#### Second \\\\$lookup\\n\\nThe next one is a generalization of the first $lookup\\n.\\n\\njs-11\\n {\\n $unwind: \\"$properties\\"\\n },\\n {\\n $lookup: {\\n let: { code: \\"$code\\" },\\n from: \\"ns_materials\\",\\n pipeline: [\\n {\\n $match: {\\n $expr: {\\n $and: [\\n {$eq: [\\"$category_code\\", \\"$$code\\"]},\\n {$eq: [\\"$is_dummy\\", \\"Y\\"]}\\n ]\\n }\\n }\\n },\\n {\\n $project: {\\n _id: 0,\\n manu_code: 1,\\n model_code: 1,\\n name: 1\\n }\\n }\\n ],\\n as: \\"suppliers\\"\\n }\\n }\\n ]\\n)\\n\\n\\nThis is somewhat more complicated, but the pipeline makes it much more flexible than the basic form.\\n\\n#### The from-let-pipeline-as lookup\\n\\nFor look up we have either\\n\\n- from-localField-foreignField-as\\n- from-let-pipeline-as\\n There is no pipeline when either localField or foreignField exists.\\n\\nWe try to break down to explain the more advanced $lookup, we start form line 16 of the code blocks above.\\n\\n- In let we have ${code: \\"$code\\"}, the key code is the variable name for temp storage, the \\"$code\\" is the field path of an object from the upstream collection in the pipeline.\\n\\n- We store all desired variable in the let stage. In this example, we will unwrap and utilize the value by writing \\"$$code\\".\\n\\n- We use $expr to **instantiate** any comparison.\\n\\n- The basic $lookup can just do\\n\\n mysql\\n select a.* from A\\n left join B b on b.a_id = a.id\\n \\n\\n- whereas the advanced $lookup can do\\n mysql\\n select b.sth1, b.sth2, a.* from A a\\n left join B b on b.id = b_id and ... and ... and ...\\n \\n due to the field pipeline.\\n\\n#### Using Javascript in $lookup with Pipeline!\\n\\njs\\npipeline: [\\n {\\n $lookup: {\\n from: \\"ns_generic_form_templates\\",\\n let: { code: \\"$code\\" },\\n pipeline: [\\n { $project: { subtypes: 1 } },\\n {\\n $addFields: {\\n formAnalysis: {\\n $function: {\\n body: function (code, subtypes, formId) {\\n if (!subtypes) {\\n return null\\n } else {\\n const hasForm = subtypes.indexOf(code) > -1;\\n\\n return { category_code: code, hasForm, formOid: formId }\\n }\\n },\\n args: [\\"$$code\\", \\"$subtypes\\", \\"$_id\\"],\\n lang: \'js\'\\n }\\n }\\n }\\n },\\n { $project: { subtypes: 0 } }\\n {\\n $match: {\\n \\"formAnalysis.hasForm\\": true\\n }\\n },\\n { $limit: 1 },\\n { $replaceRoot: { newRoot: \\"$formAnalysis\\" } },\\n { $set: { formOid: { $toString: \\"$formOid\\" } } },\\n ],\\n as: \\"genericForm\\"\\n }\\n },\\n {\\n $unwind: \\"$genericForm\\"\\n }\\n]\\n\\n","title":"Two Kinds of $lookup, and use Javascript in Advanced lookup","date":"2023-06-15T00:00:00.000Z","id":"blog0141","tag":"mongo","intro":"We demonstrate it by examples.","toc":true},{"content":"\\n#### Basic Date Formatting\\n\\nWe can get Dayjs object in two major ways:\\n\\njs\\nimport dayjs, { Dayjs } from \\"dayjs\\";\\n\\nconst displayFormat = \\"YYYY-MM-DD HH:mm:ss\\";\\n\\nconst date1 = dayjs(epochTime);\\nconst date2 = dayjs(new Date());\\n\\nconst dateDisplay = date1.format(displayFormat);\\n\\n#### List of All Formatting String Notations\\nThe list of formatting strings can be found in\\n- https://day.js.org/docs/en/display/format\\n\\n#### Date Operations\\n- https://day.js.org/docs/en/manipulate/manipulate\\n\\n#### Dayjs Plugins\\n\\njs\\n// extendedDayjs.ts\\n\\nimport dayjs from \\"dayjs\\";\\nimport dayjsPluginUTC from \\"dayjs/plugin/utc\\";\\nimport customParseFormat from \\"dayjs/plugin/customParseFormat\\";\\nimport timezone from \\"dayjs/plugin/timezone\\";\\n\\ndayjs.extend(dayjsPluginUTC);\\ndayjs.extend(customParseFormat);\\ndayjs.extend(timezone);\\n\\nconst get = () => {\\n return dayjs;\\n}\\n\\nexport default { get };\\n\\n\\n#### Show the time in Different Timezone\\nChange an ***absolute time*** to a ***relative*** time representation\\n\\njs\\nconst currTimeDayjs = dayjs(new Date()).tz(\\"Asia/Hong_Kong\\");\\nconst date = currTimeDayjs.format(\\"YYYY-MM-DD\\");\\nconst time = currTimeDayjs.format(\\"h:mma\\");\\n\\n\\n<details>\\n<summary><b>ALL AVAILABLE TIMEZONE:</b></summary>\\n\\njs\\n\'Europe/Andorra\',\\n\'Asia/Dubai\',\\n\'Asia/Kabul\',\\n\'Europe/Tirane\',\\n\'Asia/Yerevan\',\\n\'Antarctica/Casey\',\\n\'Antarctica/Davis\',\\n\'Antarctica/DumontDUrville\', // https://bugs.chromium.org/p/chromium/issues/detail?id=928068\\n\'Antarctica/Mawson\',\\n\'Antarctica/Palmer\',\\n\'Antarctica/Rothera\',\\n\'Antarctica/Syowa\',\\n\'Antarctica/Troll\',\\n\'Antarctica/Vostok\',\\n\'America/Argentina/Buenos_Aires\',\\n\'America/Argentina/Cordoba\',\\n\'America/Argentina/Salta\',\\n\'America/Argentina/Jujuy\',\\n\'America/Argentina/Tucuman\',\\n\'America/Argentina/Catamarca\',\\n\'America/Argentina/La_Rioja\',\\n\'America/Argentina/San_Juan\',\\n\'America/Argentina/Mendoza\',\\n\'America/Argentina/San_Luis\',\\n\'America/Argentina/Rio_Gallegos\',\\n\'America/Argentina/Ushuaia\',\\n\'Pacific/Pago_Pago\',\\n\'Europe/Vienna\',\\n\'Australia/Lord_Howe\',\\n\'Antarctica/Macquarie\',\\n\'Australia/Hobart\',\\n\'Australia/Currie\',\\n\'Australia/Melbourne\',\\n\'Australia/Sydney\',\\n\'Australia/Broken_Hill\',\\n\'Australia/Brisbane\',\\n\'Australia/Lindeman\',\\n\'Australia/Adelaide\',\\n\'Australia/Darwin\',\\n\'Australia/Perth\',\\n\'Australia/Eucla\',\\n\'Asia/Baku\',\\n\'America/Barbados\',\\n\'Asia/Dhaka\',\\n\'Europe/Brussels\',\\n\'Europe/Sofia\',\\n\'Atlantic/Bermuda\',\\n\'Asia/Brunei\',\\n\'America/La_Paz\',\\n\'America/Noronha\',\\n\'America/Belem\',\\n\'America/Fortaleza\',\\n\'America/Recife\',\\n\'America/Araguaina\',\\n\'America/Maceio\',\\n\'America/Bahia\',\\n\'America/Sao_Paulo\',\\n\'America/Campo_Grande\',\\n\'America/Cuiaba\',\\n\'America/Santarem\',\\n\'America/Porto_Velho\',\\n\'America/Boa_Vista\',\\n\'America/Manaus\',\\n\'America/Eirunepe\',\\n\'America/Rio_Branco\',\\n\'America/Nassau\',\\n\'Asia/Thimphu\',\\n\'Europe/Minsk\',\\n\'America/Belize\',\\n\'America/St_Johns\',\\n\'America/Halifax\',\\n\'America/Glace_Bay\',\\n\'America/Moncton\',\\n\'America/Goose_Bay\',\\n\'America/Blanc-Sablon\',\\n\'America/Toronto\',\\n\'America/Nipigon\',\\n\'America/Thunder_Bay\',\\n\'America/Iqaluit\',\\n\'America/Pangnirtung\',\\n\'America/Atikokan\',\\n\'America/Winnipeg\',\\n\'America/Rainy_River\',\\n\'America/Resolute\',\\n\'America/Rankin_Inlet\',\\n\'America/Regina\',\\n\'America/Swift_Current\',\\n\'America/Edmonton\',\\n\'America/Cambridge_Bay\',\\n\'America/Yellowknife\',\\n\'America/Inuvik\',\\n\'America/Creston\',\\n\'America/Dawson_Creek\',\\n\'America/Fort_Nelson\',\\n\'America/Vancouver\',\\n\'America/Whitehorse\',\\n\'America/Dawson\',\\n\'Indian/Cocos\',\\n\'Europe/Zurich\',\\n\'Africa/Abidjan\',\\n\'Pacific/Rarotonga\',\\n\'America/Santiago\',\\n\'America/Punta_Arenas\',\\n\'Pacific/Easter\',\\n\'Asia/Shanghai\',\\n\'Asia/Urumqi\',\\n\'America/Bogota\',\\n\'America/Costa_Rica\',\\n\'America/Havana\',\\n\'Atlantic/Cape_Verde\',\\n\'America/Curacao\',\\n\'Indian/Christmas\',\\n\'Asia/Nicosia\',\\n\'Asia/Famagusta\',\\n\'Europe/Prague\',\\n\'Europe/Berlin\',\\n\'Europe/Copenhagen\',\\n\'America/Santo_Domingo\',\\n\'Africa/Algiers\',\\n\'America/Guayaquil\',\\n\'Pacific/Galapagos\',\\n\'Europe/Tallinn\',\\n\'Africa/Cairo\',\\n\'Africa/El_Aaiun\',\\n\'Europe/Madrid\',\\n\'Africa/Ceuta\',\\n\'Atlantic/Canary\',\\n\'Europe/Helsinki\',\\n\'Pacific/Fiji\',\\n\'Atlantic/Stanley\',\\n\'Pacific/Chuuk\',\\n\'Pacific/Pohnpei\',\\n\'Pacific/Kosrae\',\\n\'Atlantic/Faroe\',\\n\'Europe/Paris\',\\n\'Europe/London\',\\n\'Asia/Tbilisi\',\\n\'America/Cayenne\',\\n\'Africa/Accra\',\\n\'Europe/Gibraltar\',\\n\'America/Godthab\',\\n\'America/Danmarkshavn\',\\n\'America/Scoresbysund\',\\n\'America/Thule\',\\n\'Europe/Athens\',\\n\'Atlantic/South_Georgia\',\\n\'America/Guatemala\',\\n\'Pacific/Guam\',\\n\'Africa/Bissau\',\\n\'America/Guyana\',\\n\'Asia/Hong_Kong\',\\n\'America/Tegucigalpa\',\\n\'America/Port-au-Prince\',\\n\'Europe/Budapest\',\\n\'Asia/Jakarta\',\\n\'Asia/Pontianak\',\\n\'Asia/Makassar\',\\n\'Asia/Jayapura\',\\n\'Europe/Dublin\',\\n\'Asia/Jerusalem\',\\n\'Asia/Kolkata\',\\n\'Indian/Chagos\',\\n\'Asia/Baghdad\',\\n\'Asia/Tehran\',\\n\'Atlantic/Reykjavik\',\\n\'Europe/Rome\',\\n\'America/Jamaica\',\\n\'Asia/Amman\',\\n\'Asia/Tokyo\',\\n\'Africa/Nairobi\',\\n\'Asia/Bishkek\',\\n\'Pacific/Tarawa\',\\n\'Pacific/Enderbury\',\\n\'Pacific/Kiritimati\',\\n\'Asia/Pyongyang\',\\n\'Asia/Seoul\',\\n\'Asia/Almaty\',\\n\'Asia/Qyzylorda\',\\n\'Asia/Qostanay\', // https://bugs.chromium.org/p/chromium/issues/detail?id=928068\\n\'Asia/Aqtobe\',\\n\'Asia/Aqtau\',\\n\'Asia/Atyrau\',\\n\'Asia/Oral\',\\n\'Asia/Beirut\',\\n\'Asia/Colombo\',\\n\'Africa/Monrovia\',\\n\'Europe/Vilnius\',\\n\'Europe/Luxembourg\',\\n\'Europe/Riga\',\\n\'Africa/Tripoli\',\\n\'Africa/Casablanca\',\\n\'Europe/Monaco\',\\n\'Europe/Chisinau\',\\n\'Pacific/Majuro\',\\n\'Pacific/Kwajalein\',\\n\'Asia/Yangon\',\\n\'Asia/Ulaanbaatar\',\\n\'Asia/Hovd\',\\n\'Asia/Choibalsan\',\\n\'Asia/Macau\',\\n\'America/Martinique\',\\n\'Europe/Malta\',\\n\'Indian/Mauritius\',\\n\'Indian/Maldives\',\\n\'America/Mexico_City\',\\n\'America/Cancun\',\\n\'America/Merida\',\\n\'America/Monterrey\',\\n\'America/Matamoros\',\\n\'America/Mazatlan\',\\n\'America/Chihuahua\',\\n\'America/Ojinaga\',\\n\'America/Hermosillo\',\\n\'America/Tijuana\',\\n\'America/Bahia_Banderas\',\\n\'Asia/Kuala_Lumpur\',\\n\'Asia/Kuching\',\\n\'Africa/Maputo\',\\n\'Africa/Windhoek\',\\n\'Pacific/Noumea\',\\n\'Pacific/Norfolk\',\\n\'Africa/Lagos\',\\n\'America/Managua\',\\n\'Europe/Amsterdam\',\\n\'Europe/Oslo\',\\n\'Asia/Kathmandu\',\\n\'Pacific/Nauru\',\\n\'Pacific/Niue\',\\n\'Pacific/Auckland\',\\n\'Pacific/Chatham\',\\n\'America/Panama\',\\n\'America/Lima\',\\n\'Pacific/Tahiti\',\\n\'Pacific/Marquesas\',\\n\'Pacific/Gambier\',\\n\'Pacific/Port_Moresby\',\\n\'Pacific/Bougainville\',\\n\'Asia/Manila\',\\n\'Asia/Karachi\',\\n\'Europe/Warsaw\',\\n\'America/Miquelon\',\\n\'Pacific/Pitcairn\',\\n\'America/Puerto_Rico\',\\n\'Asia/Gaza\',\\n\'Asia/Hebron\',\\n\'Europe/Lisbon\',\\n\'Atlantic/Madeira\',\\n\'Atlantic/Azores\',\\n\'Pacific/Palau\',\\n\'America/Asuncion\',\\n\'Asia/Qatar\',\\n\'Indian/Reunion\',\\n\'Europe/Bucharest\',\\n\'Europe/Belgrade\',\\n\'Europe/Kaliningrad\',\\n\'Europe/Moscow\',\\n\'Europe/Simferopol\',\\n\'Europe/Kirov\',\\n\'Europe/Astrakhan\',\\n\'Europe/Volgograd\',\\n\'Europe/Saratov\',\\n\'Europe/Ulyanovsk\',\\n\'Europe/Samara\',\\n\'Asia/Yekaterinburg\',\\n\'Asia/Omsk\',\\n\'Asia/Novosibirsk\',\\n\'Asia/Barnaul\',\\n\'Asia/Tomsk\',\\n\'Asia/Novokuznetsk\',\\n\'Asia/Krasnoyarsk\',\\n\'Asia/Irkutsk\',\\n\'Asia/Chita\',\\n\'Asia/Yakutsk\',\\n\'Asia/Khandyga\',\\n\'Asia/Vladivostok\',\\n\'Asia/Ust-Nera\',\\n\'Asia/Magadan\',\\n\'Asia/Sakhalin\',\\n\'Asia/Srednekolymsk\',\\n\'Asia/Kamchatka\',\\n\'Asia/Anadyr\',\\n\'Asia/Riyadh\',\\n\'Pacific/Guadalcanal\',\\n\'Indian/Mahe\',\\n\'Africa/Khartoum\',\\n\'Europe/Stockholm\',\\n\'Asia/Singapore\',\\n\'America/Paramaribo\',\\n\'Africa/Juba\',\\n\'Africa/Sao_Tome\',\\n\'America/El_Salvador\',\\n\'Asia/Damascus\',\\n\'America/Grand_Turk\',\\n\'Africa/Ndjamena\',\\n\'Indian/Kerguelen\',\\n\'Asia/Bangkok\',\\n\'Asia/Dushanbe\',\\n\'Pacific/Fakaofo\',\\n\'Asia/Dili\',\\n\'Asia/Ashgabat\',\\n\'Africa/Tunis\',\\n\'Pacific/Tongatapu\',\\n\'Europe/Istanbul\',\\n\'America/Port_of_Spain\',\\n\'Pacific/Funafuti\',\\n\'Asia/Taipei\',\\n\'Europe/Kiev\',\\n\'Europe/Uzhgorod\',\\n\'Europe/Zaporozhye\',\\n\'Pacific/Wake\',\\n\'America/New_York\',\\n\'America/Detroit\',\\n\'America/Kentucky/Louisville\',\\n\'America/Kentucky/Monticello\',\\n\'America/Indiana/Indianapolis\',\\n\'America/Indiana/Vincennes\',\\n\'America/Indiana/Winamac\',\\n\'America/Indiana/Marengo\',\\n\'America/Indiana/Petersburg\',\\n\'America/Indiana/Vevay\',\\n\'America/Chicago\',\\n\'America/Indiana/Tell_City\',\\n\'America/Indiana/Knox\',\\n\'America/Menominee\',\\n\'America/North_Dakota/Center\',\\n\'America/North_Dakota/New_Salem\',\\n\'America/North_Dakota/Beulah\',\\n\'America/Denver\',\\n\'America/Boise\',\\n\'America/Phoenix\',\\n\'America/Los_Angeles\',\\n\'America/Anchorage\',\\n\'America/Juneau\',\\n\'America/Sitka\',\\n\'America/Metlakatla\',\\n\'America/Yakutat\',\\n\'America/Nome\',\\n\'America/Adak\',\\n\'Pacific/Honolulu\',\\n\'America/Montevideo\',\\n\'Asia/Samarkand\',\\n\'Asia/Tashkent\',\\n\'America/Caracas\',\\n\'Asia/Ho_Chi_Minh\',\\n\'Pacific/Efate\',\\n\'Pacific/Wallis\',\\n\'Pacific/Apia\',\\n\'Africa/Johannesburg\'\\n\\n</details>\\n\\n#### Parse a Time String of Specific Format and Specific Timezone\\n\\nAn absolute time can be determined by the following pair:\\n- 2023-09-19 13:24:52 (standard representation from python)\\n- \\"Asia/Hong_Kong\\",\\njs\\nconst TIME_FORMAT = \\"YYYY-MM-DD HH:mm:ss\\" ;\\nconst timeZone = \\"Asia/Hong_Kong\\";\\nconst time = dayjs.tz(\\"2023-09-19 13:24:52\\", TIME_FORMAT, timeZone).valueOf();\\n\\n\\n#### Determine a Time is How much Hours, Days, etc, ago\\n\\njs\\nconst ago = (() => {\\n const currDayjs = dayjs(new Date());\\n const diff = currDayjs.diff(receivedTimeDayjs, \\"hour\\");\\n if (diff < 24) {\\n return ${diff}h\\n } else {\\n const diff = currDayjs.diff(receivedTimeDayjs, \\"day\\");\\n return ${diff}d\\n }\\n})();\\n\\n\\n","title":"Use of dayjs","date":"2023-06-16T00:00:00.000Z","id":"blog0142","tag":"react, javascript","intro":"Just record the usual API that we may need in using dayjs.","toc":true},{"content":"\\nIn this blog we have used OverlayScrollbars for a good looking scrollbar, the drawback is that we need to wrap every component which we want such a scrollbar.\\n\\nWhen an application get complicated (like many popups, many forms), we may simply apply the css rule below\\n\\ncss\\n<style>\\n ::-webkit-scrollbar {\\n width: 6px;\\n height: 4px;\\n }\\n\\n ::-webkit-scrollbar-thumb {\\n border-radius: 10px;\\n background-color: rgba(0, 0, 0, 0.25);\\n }\\n\\n ::-webkit-scrollbar-track {\\n background: \'#FFFFFF\';\\n border-radius: 10px;\\n }\\n</style>\\n\\n\\nat the top level, every instance of scrollbars will then be rounded.\\n","title":"Scrollbar Style Like Mac","date":"2023-06-20T00:00:00.000Z","id":"blog0143","tag":"react","intro":"Record CSS that makes scollbar look better.","toc":false},{"content":"\\n#### Radio Buttons\\n\\ntypescript\\nimport * as React from \\"react\\";\\nimport Radio from \\"@mui/material/Radio\\";\\nimport { makeStyles } from \\"@material-ui/core\\";\\nimport classnames from \\"classnames\\";\\n\\nconst useStyles = makeStyles({\\n selectionList: {\\n \\"& span\\": {\\n marginRight: 2,\\n },\\n \\"& .selection-row\\": {\\n display: \\"flex\\",\\n alignItems: \\"center\\",\\n },\\n },\\n});\\n\\nexport default function RadioButtonsGroup<T extends string>({\\n value,\\n setValue,\\n listOfValues: options = [],\\n style = {},\\n className = \\"\\",\\n}: {\\n value: ReturnType<typeof React.useState<T>>[0];\\n setValue: ReturnType<typeof React.useState<T>>[1];\\n listOfValues?: { value: T; displayName: string }[];\\n style?: React.CSSProperties;\\n className?: string;\\n}) {\\n const classes = useStyles();\\n\\n const handleChange = (event: React.ChangeEvent<HTMLInputElement>) => {\\n setValue(event.target.value as unknown as T);\\n };\\n\\n const controlProps = (item: string) => ({\\n checked: value === item,\\n onChange: handleChange,\\n value: item,\\n name: \\"color-radio-button-demo\\",\\n inputProps: { \\"aria-label\\": item },\\n });\\n\\n return (\\n <div className={classnames(classes.selectionList, className)} style={style}>\\n {options.map((opt) => {\\n const { displayName, value } = opt;\\n return (\\n <div className=\\"selection-row\\">\\n <Radio {...controlProps(value)} size=\\"small\\" /> {displayName}\\n </div>\\n );\\n })}\\n </div>\\n );\\n}\\n\\n\\nUsage:\\n\\ntypescript\\n<RadioButtonsGroup\\n className={classes.radio}\\n style={{ display: \\"flex\\" }}\\n value={value}\\n setValue={setValue}\\n listOfValues={[\\n {\\n value: \\"Important\\",\\n displayName: \\"Important\\",\\n },\\n {\\n value: \\"Submittal\\",\\n displayName: \\"Submittal\\",\\n },\\n {\\n value: \\"Confirmation\\",\\n displayName: \\"Confirmation\\",\\n },\\n ]}\\n/>\\n\\n\\nWhich results result similar to\\n\\n<Center>\\n <img src=\\"/assets/tech/145/001.png\\"/>\\n</Center>\\n<p/>\\n<center></center>\\n\\nHere the display styles are refined by using style and className attributes, where\\n\\nts\\nconst useStyles = makeStyles({\\n radio: {\\n \\"& .selection-row\\": {\\n marginRight: 20,\\n },\\n },\\n});\\n\\n\\n#### Custom Dropdown List\\n\\nWe will name it GeneralDropdown:\\n\\ntypescript\\nimport { CSSProperties, useEffect, useRef, useState } from \\"react\\";\\nimport { Popper, makeStyles } from \\"@material-ui/core\\";\\nimport classnames from \\"classnames\\";\\nimport { BsFillCaretUpFill } from \\"react-icons/bs\\";\\nimport normalizeUtil from \\"../util/normalizeUtil\\";\\nimport useClickOutside from \\"../pages/hooks/useClickOutside\\";\\nimport { useMainStyles } from \\"../pages/CorrespondenceDashboard/CorrespondenceDashboard\\";\\nimport { OverlayScrollbarsComponent } from \\"overlayscrollbars-react\\";\\n\\nconst borderStyle = \\"1px solid rgb(200, 200, 200)\\";\\nconst borderRadius = 4;\\nconst options = {\\n scrollbars: { autoHide: \\"scroll\\" },\\n};\\n\\nconst useStyles = makeStyles({\\n display: {\\n overflow: \\"hidden\\",\\n zIndex: 3,\\n textOverflow: \\"ellipsis\\",\\n },\\n disabled: {\\n pointerEvents: \\"none\\",\\n opacity: 0.4,\\n },\\n\\n option: {\\n overflow: \\"hidden\\",\\n textOverflow: \\"ellipsis\\",\\n padding: \\"8px 8px\\",\\n fontFamily: \\"Roboto, Helvetica, Arial, sans-serif\\",\\n \\"&:hover\\": {\\n backgroundColor: \\"#f2f9fc\\",\\n },\\n \\"&.selected\\": {\\n backgroundColor: \\"rgb(0, 166, 250)\\",\\n color: \\"white\\",\\n },\\n },\\n});\\n\\nconst UpIcon = () => {\\n return (\\n <BsFillCaretUpFill\\n style={{ opacity: 0.2, transform: \\"scaleX(0.8) scaleY(0.7)\\" }}\\n />\\n );\\n};\\nconst DownIcon = () => {\\n return (\\n <BsFillCaretUpFill\\n style={{ opacity: 0.2, transform: \\"scaleX(0.8) scaleY(-0.7)\\" }}\\n />\\n );\\n};\\n\\ntype Option = { code: string; name: string; className?: string };\\n\\nconst GeneralDropdown = <T extends { code: string; name: string }>({\\n initialValue,\\n fullList,\\n refUpdateHandler = () => {},\\n selectionHint = \\"Select\\",\\n className = \\"\\",\\n enableNone = false,\\n optionStyle = {},\\n disablePredicate = (option: T) => false,\\n style = {},\\n}: {\\n initialValue: T | null;\\n fullList: T[];\\n refUpdateHandler?: (selection: T | null) => void;\\n selectionHint?: string;\\n className?: string;\\n style?: CSSProperties;\\n optionStyle?: CSSProperties;\\n disablePredicate?: (option: T) => boolean;\\n enableNone?: boolean;\\n}) => {\\n const mainClasses = useMainStyles();\\n const classes = useStyles();\\n const selectionDisplayRef = useRef<HTMLDivElement>(null);\\n const dropDownRef = useRef<HTMLDivElement>(null);\\n const [eleWidth, setEleWidth] = useState(0);\\n const [selectedName, setSelectedName] = useState(initialValue?.name || \\"\\");\\n const [selectedCode, setSelectedCode] = useState(initialValue?.code || \\"\\");\\n\\n const { outsideClicked: outsideOfDropdownClicked } = useClickOutside({\\n ref: dropDownRef,\\n });\\n\\n const [showDropdown, setShowDropdown] = useState(false);\\n\\n const { idToObject: codeToObject } = normalizeUtil({\\n targetArr: fullList,\\n idAttribute: \\"code\\",\\n });\\n\\n const additionalNoneOption: Option[] = enableNone\\n ? [\\n {\\n name: \\"None\\",\\n code: \\"\\",\\n className: enableNone ? \\"\\" : classes.disabled,\\n },\\n ]\\n : [];\\n\\n const options_: Option[] = additionalNoneOption.concat(\\n fullList.map((opt) => ({\\n name: opt.name,\\n code: opt.code,\\n className: classnames(\\n disablePredicate(opt) ? classes.disabled : \\"\\",\\n selectedCode === opt.code ? \\"selected\\" : \\"\\"\\n ),\\n }))\\n );\\n const refUpdateHandler_ = (arg: Option) => {\\n const orgingalData = codeToObject?.[arg.code];\\n refUpdateHandler(orgingalData);\\n };\\n\\n useEffect(() => {\\n if (selectionDisplayRef.current) {\\n const width = selectionDisplayRef.current.offsetWidth;\\n setEleWidth(width);\\n }\\n }, []);\\n\\n useEffect(() => {\\n if (outsideOfDropdownClicked) {\\n setShowDropdown(false);\\n }\\n }, [outsideOfDropdownClicked]);\\n\\n const hasOption = options_.length > 0;\\n\\n return (\\n <div style={{ width: \\"100%\\", ...style }}>\\n <div\\n style={{\\n position: \\"relative\\",\\n pointerEvents: showDropdown ? \\"none\\" : \\"auto\\",\\n }}\\n onClick={() => setShowDropdown(true)}\\n className={className}\\n >\\n <div\\n ref={selectionDisplayRef}\\n style={{\\n border: borderStyle,\\n padding: \\"6px 10px\\",\\n borderRadius: borderRadius,\\n userSelect: \\"none\\",\\n borderBottomLeftRadius: showDropdown ? 0 : borderRadius,\\n borderBottomRightRadius: showDropdown ? 0 : borderRadius,\\n }}\\n >\\n <div\\n style={{ width: \\"calc(100% - 20px)\\" }}\\n className={classes.display}\\n >\\n {selectedName ? selectedName : selectionHint}\\n </div>\\n </div>\\n <div\\n style={{\\n position: \\"absolute\\",\\n top: \\"50%\\",\\n transform: \\"translateY(calc(-50% + 3px))\\",\\n right: 4,\\n }}\\n >\\n {!showDropdown && <DownIcon />}\\n {showDropdown && <UpIcon />}\\n </div>\\n </div>\\n <Popper\\n className={mainClasses.mainPage}\\n style={{\\n fontWeight: 400,\\n background: \\"rgba(255,255,255,0.8)\\",\\n backdropFilter: \\"blur(80px) brightness(115%)\\",\\n marginTop: -1,\\n width: Math.max(eleWidth, 0),\\n whiteSpace: \\"nowrap\\",\\n border: borderStyle,\\n zIndex: 10000,\\n }}\\n open={showDropdown}\\n anchorEl={selectionDisplayRef.current}\\n transition\\n >\\n <div\\n style={{\\n width: selectionDisplayRef.current?.offsetWidth,\\n }}\\n ref={dropDownRef}\\n >\\n <OverlayScrollbarsComponent\\n style={{\\n maxHeight: 300,\\n overflow: \\"hidden\\",\\n }}\\n >\\n {!hasOption && (\\n <div\\n style={{ padding: 10, userSelect: \\"none\\" }}\\n className={classnames(classes.option, classes.disabled)}\\n >\\n No options\\n </div>\\n )}\\n\\n {hasOption &&\\n options_.map((item) => {\\n return (\\n <div\\n title={item.name}\\n key={item.code}\\n style={{\\n width: \\"calc(100% - 2px)\\",\\n cursor: \\"pointer\\",\\n ...optionStyle,\\n }}\\n onClick={() => {\\n setShowDropdown(false);\\n refUpdateHandler_(item);\\n setSelectedName(item.name);\\n setSelectedCode(item.code);\\n }}\\n className={classnames(classes.option, item.className || \\"\\")}\\n >\\n {item.name}\\n </div>\\n );\\n })}\\n </OverlayScrollbarsComponent>\\n </div>\\n </Popper>\\n </div>\\n );\\n};\\n\\nexport default GeneralDropdown;\\n\\n\\n- Under the hood GeneralDropdown just requires code as an identifier and name as a display of selected items.\\n- We can do complicated selection update logic in refUpdateHandler (not the name and code, we update id here).\\n","title":"Radio Buttons Group and General Dropdown List","date":"2023-06-21T00:00:00.000Z","id":"blog0144","tag":"react","intro":"Record the implementation of radio button group and dropdown list, in a hope that we don\'t need to waste time cooking it up again in the future.","toc":true},{"content":"\\n#### Step 1\\n\\nFirst we npm install redux-persist, then we create our store as usual.\\n\\ntypescript-1\\nexport const store = configureStore({\\n reducer: {\\n user: userSlice.reducer,\\n application: appSlice.reducer,\\n projects: projectSlice.reducer,\\n companies: companySlice.reducer,\\n template: templateSlice.reducer,\\n fakeTime: fakeTimeSlice.reducer,\\n wbusers: wbuserSlice.reducer,\\n wbcategories: categorySlice.reducer\\n },\\n devTools: true,\\n //@ts-ignore\\n middleware: (gDM) => gDM().concat(\\n templateMiddleware.middleware,\\n projectMiddleware.middleware,\\n fakeTimeMiddleware.middleware,\\n wbuserMiddlwares.middleware,\\n categoryMiddleware.middleware,\\n companyMiddleware.middleware\\n )\\n});\\n\\n\\n#### Step 2\\n\\nFor reducer whose data we want to persist, we add the corresponding config one by one:\\n\\ntypescript\\nconst userPersistConfig = {\\n key: \\"user\\",\\n storage,\\n stateReconciler: autoMergeLevel2,\\n};\\n\\n\\nNext we change the root reducer part in line 5 accordingly:\\n\\ntypescript-1\\nimport persistStore from \\"redux-persist/es/persistStore\\";\\n\\nexport const store = configureStore({\\n reducer: {\\n user: persistReducer<ReturnType<typeof userSlice.reducer>>(userPersistConfig, userSlice.reducer),\\n application: applicationPersistConfig, appSlice.reducer,\\n projects: projectSlice.reducer,\\n companies: companySlice.reducer,\\n template: templateSlice.reducer,\\n fakeTime: fakeTimeSlice.reducer,\\n wbusers: wbuserSlice.reducer,\\n wbcategories: categorySlice.reducer\\n },\\n devTools: true,\\n //@ts-ignore\\n middleware: (gDM) => gDM().concat(\\n templateMiddleware.middleware,\\n projectMiddleware.middleware,\\n fakeTimeMiddleware.middleware,\\n wbuserMiddlwares.middleware,\\n categoryMiddleware.middleware,\\n companyMiddleware.middleware\\n )\\n});\\n\\nexport const persistor = persistStore(store);\\n\\n\\n#### Step 3\\n\\nFinaly we wrap our main view component in App.tsx by using the PersistGate and the exported persistor:\\n\\ntypescript\\n//App.tsx\\n\\n<Provider store={store}>\\n <PersistGate loading={null} persistor={persistor}>\\n ...\\n <AppRoutes />\\n ...\\n </PersistGate>\\n</Provider>\\n\\n","title":"Quick Step to Make a Reducer Persist its Data","date":"2023-06-21T00:00:00.000Z","id":"blog0145","tag":"react, redux","intro":"Record the use of redux-persist for persisting data in redux store.","toc":false},{"content":"\\n#### Docker\\n\\n##### Basic Commands\\n\\n- docker ps List all running containers\\n- docker ps -a List all container regardless of if it is runing\\n- docker run <image-tag> Run a docker image\\n- docker run -d <image-tag> Run a docker image in detached mode\\n- docker stop <container-id> Stop a container by and\\n- docker start <same-id> Get a list of all available images docker images\\n- docker run -p6000:6379 redis Specify the port from our computer to the port used by the image in the container\\n- docker logs <container-name> See the log of the container\\n- docker run redis --name old_redis Provides a name to a container\\n- docker exec -it <image-name> /bin/bash ssh into the container, and exit to get out\\n- docker logs <container-name> | tail Display the stream of log lines\\n- docker rm -f $(docker ps -a -q) Delete all running container\\n- docker rmi -f $(docker images -aq) Delete all images forcefully\\n\\n##### Docker Network\\n\\n###### Basic Commands\\n\\n- docker network create <network-name> Create a network\\n- docker network ls List all networks\\n\\nWe run two docker images in the same network:\\n\\n- docker\\n docker run -p 27017:27017 \\\\\\n -d \\\\\\n -e MONGO_INITDB_ROOT_USERNAME=admin \\\\\\n -e MONGO_INITDB_ROOT_PASSWORD=123 \\\\\\n --name mongodb \\\\\\n --net mongo-network \\\\\\n mongo\\n \\n- docker\\n docker run -d -p 8081:8081 \\\\\\n -e ME_CONFIG_MONGODB_ADMINUSERNAME=admin \\\\\\n -e ME_CONFIG_MONGODB_ADMINPASSWORD=123 \\\\\\n --net mongo-network \\\\\\n --name mongo-express \\\\\\n -e ME_CONFIG_MONGODB_SERVER=mongodb \\\\\\n mongo-express\\n \\n\\n###### How do Two Containers Communicate?\\n\\nWhen both container are in the same network, they can commnunicate with each other by **container-name** as a domain.\\n\\n##### Docker-Compose and Dependencies\\n\\n###### The Basic Structure of docker-compose.yaml\\n\\nWe don\'t need to specify the network as docker-compose takes care of it.\\n\\nyaml\\nversion: \\"3\\"\\nservices:\\n mongodb: # container name (--name param)\\n image: mongo # the image tag\\n ports:\\n - 27017:27017\\n environment:\\n - MONGO_INITDB_ROOT_USERNAME=admin\\n - MONGO_INITDB_ROOT_PASSWORD=123\\n mongo-express:\\n image: mongo-express\\n ports:\\n - 8080:8081\\n depends_on:\\n - mongodb # wait for the container mongodb to start\\n environment:\\n - ME_CONFIG_MONGODB_ADMINUSERNAME=admin\\n - ME_CONFIG_MONGODB_ADMINPASSWORD=123\\n - ME_CONFIG_MONGODB_SERVER=mongodb\\n\\n\\n- Now we can run docker-compose -f mongo.yaml up -d to run both mongo and mongo-express containers.\\n- We can stop the containers and remove the network by docker-compose -f mongo.yaml down.\\n\\n###### Communication Between Two Images in Docker-Compose\\n\\nIn my index.ts I have written a simple backend:\\n\\ntypescript-1\\nimport express, { Request, Response } from \\"express\\";\\nimport mongoose, { InferSchemaType, Schema } from \\"mongoose\\";\\n\\nconst mongoDbContainerName = \\"mongodb-test\\";\\nconst dbName = \\"JamesTestDB\\";\\n\\n(async () => {\\n try {\\n console.log(\\"Start mongo connection ...\\");\\n await mongoose.connect(\\n mongodb://admin:123@${mongoDbContainerName}:27017/${dbName}?authSource=admin,\\n {\\n connectTimeoutMS: 30000,\\n serverSelectionTimeoutMS: 30000,\\n }\\n );\\n console.log(\\"Connected!\\");\\n } catch (e) {\\n console.log(\\"Connection Failed\\");\\n console.log(JSON.stringify(e));\\n }\\n\\n const app = express();\\n\\n const studentSchema = new Schema({\\n name: { type: String, default: \\"hahaha\\" },\\n age: { type: Number, min: 18, index: true },\\n });\\n type Student = InferSchemaType<typeof studentSchema>;\\n\\n const StudentModel = mongoose.model<Student>(\\"Student\\", studentSchema);\\n\\n app.get(\\n \\"/add-student\\",\\n async (\\n req: Request<{}, {}, {}, { age: number; name: string }>,\\n res: Response\\n ) => {\\n const { age, name } = req.query;\\n await StudentModel.create({ age, name });\\n res.json({ success: true, msg: ${name} of age ${age} is created. });\\n }\\n );\\n app.listen(3000);\\n})();\\n\\n\\nAs we can see in line 11 our containers in the same network can communicate with each other using the running container name.\\n\\n##### Build Docker Images\\n\\nSuppose that we have a backend service written in node-js and we want to dockerize it:\\n\\n<Center>\\n <img src=\\"/assets/tech/140/001.png\\"/>\\n</Center>\\n\\n<p></p>\\n<center></center>\\n\\nWe write the following in Dockerfile.backend:\\n\\ndockerfile\\nFROM node:13-alpine\\n\\n# ENV MONGO_DB_USERNAME=admin\\n# ENV MONGO_DB_PWD=123\\n\\nRUN mkdir -p /home/app\\nCOPY ./backend /home/app\\n\\nCMD [\\"npm\\", \\"run\\", \\"start\\"]\\n\\n\\nand run\\n\\ntext\\ndocker build -t add-user:1.0 -f Dockerfile.backend .\\n\\n\\n- -t means a tuple <img_name>:<version>, it is used for images.\\n- We also have a concept of name, which is for the name of **running container**.\\n- In short, tags are for launching the containers, names are for utilizing running containers.\\n\\n##### Volumes\\n\\n- docker volume rm $(docker volume ls -q) -f Remove all volume\\n\\nyaml\\nversion: \\"3\\"\\nservices:\\n add-user:\\n image: add-user:1.0\\n ports:\\n - 3000:3000\\n depends_on:\\n - mongodb-test\\n mongodb-test: # container name (--name param)\\n image: mongo # the image tag\\n ports:\\n - 27018:27017\\n environment:\\n - MONGO_INITDB_DATABASE=JamesTestDB\\n - MONGO_INITDB_ROOT_USERNAME=admin\\n - MONGO_INITDB_ROOT_PASSWORD=123\\n volumes:\\n - mongo-data:/data/db # position to save db data within the container\\n - ./init-mongo.js:/docker-entrypoint-initdb.d/init-mongo.js\\n mongo-express:\\n image: mongo-express\\n restart: always\\n ports:\\n - 8080:8081\\n depends_on:\\n - mongodb-test # wait for the container mongodb to start\\n environment:\\n - ME_CONFIG_MONGODB_ADMINUSERNAME=admin\\n - ME_CONFIG_MONGODB_ADMINPASSWORD=123\\n - ME_CONFIG_MONGODB_SERVER=mongodb-test\\nvolumes:\\n mongo-data: # volume name\\n driver: local\\n\\n\\n- For windows volums are saved in\\n\\n - \\\\\\\\wsl$\\\\docker-desktop-data\\\\version-pack-data\\\\community\\\\docker\\\\volumes\\n\\n- For linux/mac the volumes are saved in /var/lib/docker\\n\\n#### Gitlab-CI (.gitlab-ci.yml)\\n\\n##### EC2 Instance\\n\\n- We will be developing a workflow automating the deployment to dev server (an ec2 instance).\\n\\n- For this, to avoid prefixing our docker command by sudo, we:\\n\\n 1. ssh into an ec2 instance\\n 2. newgrp docker\\n 3. sudo usermod -aG docker ubuntu\\n 4. sudo chown $USER /var/run/docker.sock\\n\\n- Now in gitlab > settings > CICD > variables, we put our SSH key (cat *.pem and copy) into a variable.\\n\\n Be reminded that we need to put an empty line at the bottom (line 5 below):\\n\\n text-1\\n -----BEGIN OPENSSH PRIVATE KEY-----\\n ...\\n EQIMHa10Q+ZGHab9dGSTTAxGcK7gjqq/qWXmrrjYGEaaAAAAAAECAwQF\\n -----END OPENSSH PRIVATE KEY-----\\n\\n \\n\\n and be remined to set the variable type to ENV_VAR, **_not_** FILE.\\n\\n- Now our deployment job in simplest term:\\n\\n yml\\n deploy_to_dev:\\n stage: deploy\\n before_script:\\n - \\"which ssh-agent || ( apt-get install -qq openssh-client )\\"\\n - mkdir -p ~/.ssh\\n - touch ~/.ssh/id_rsa\\n - echo \\"$SSH_PRIVATE_KEY\\" | tr -d \'\\\\r\' > ~/.ssh/id_rsa\\n - chmod 600 ~/.ssh/id_rsa\\n - echo -e \\"Host *\\\\nStrictHostKeyChecking no\\\\n\\" > ~/.ssh/config\\n - eval \\"$(ssh-agent -s)\\"\\n - ssh-add ~/.ssh/id_rsa\\n\\n script: # skip the checking of authenticity of host by -o\\n - ssh ubuntu@$DEV_SERVER_IP \\\\\\n \\"\\n docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY &&\\n docker run -d -p 3000:3000 $IMAGE_NAME:$IMAGE_TAG\\n \\"\\n \\n\\n##### Environment Field\\n\\nFor each job we can provide name and url in environment:\\n\\nyml\\ndeploy_to_dev:\\n ...\\n environment:\\n name: development\\n url: $DEV_ENDPOINT\\n\\n\\n<Center>\\n <a href=\\"/assets/tech/140/002.png\\">\\n <img src=\\"/assets/tech/140/002.png\\" width=\\"100%\\"/>\\n </a>\\n</Center>\\n\\n<p/>\\n<center></center>\\n\\nAnd we can click \\"Open\\" to get to url ($DEV_ENDPOINT) directly.\\n\\n##### Complete Pipeline by Docker Command\\n\\n- Here we intentionally put IMAGE_NEW_TAG and IMAGE_OLD_TAG into variables in order to stop and remove the previous container and run our new container.\\n\\n- The IMAGE_LAUNCH_NAME is to be provided in --name argument when executing docker commnad (therefore we can stop the container accurately).\\n\\nyml\\nworkflow:\\n rules:\\n - if: $CI_COMMIT_BRANCH != \\"main\\" && $CI_PIPELINE_SOURCE != \\"merge_request_event\\"\\n when: never # i.e., don\'t except when if condition is met.\\n - when: always\\n\\nvariables:\\n IMAGE_NAME: $CI_REGISTRY_IMAGE\\n IMAGE_NEW_TAG: \\"1.1.1\\"\\n IMAGE_OLD_TAG: \\"1.1\\"\\n DEV_SERVER_IP: ec2-43-200-179-107.ap-northeast-2.compute.amazonaws.com\\n DEV_ENDPOINT: http://$DEV_SERVER_IP:3000\\n IMAGE_LAUNCH_NAME: cicd_test\\n\\nstages:\\n - test\\n - build\\n - deploy\\n\\nrun_unit_tests:\\n stage: test\\n image: node:17-alpine3.14\\n before_script:\\n - cd app\\n - npm install\\n script:\\n - npm test\\n artifacts:\\n when: always\\n paths:\\n - app/junit.xml\\n reports:\\n junit: app/junit.xml\\n\\nbuild_and_push_image:\\n image: docker\\n services:\\n - docker:dind\\n stage: build\\n before_script:\\n - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\\n script:\\n - docker build -t $IMAGE_NAME:$IMAGE_NEW_TAG .\\n - docker push $IMAGE_NAME:$IMAGE_NEW_TAG\\n\\ndeploy_to_dev:\\n stage: deploy\\n before_script:\\n - \\"which ssh-agent || ( apt-get install -qq openssh-client )\\"\\n - mkdir -p ~/.ssh\\n - touch ~/.ssh/id_rsa\\n - echo \\"$SSH_PRIVATE_KEY\\" | tr -d \'\\\\r\' > ~/.ssh/id_rsa\\n - chmod 600 ~/.ssh/id_rsa\\n - echo -e \\"Host *\\\\nStrictHostKeyChecking no\\\\n\\" > ~/.ssh/config\\n - eval \\"$(ssh-agent -s)\\"\\n - ssh-add ~/.ssh/id_rsa\\n\\n script: # skip the checking of authenticity of host by -o\\n - ssh ubuntu@$DEV_SERVER_IP \\\\\\n \\"\\n docker stop $IMAGE_LAUNCH_NAME ||\\n docker image rm -f $IMAGE_NAME:$IMAGE_OLD_TAG ||\\n docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY &&\\n docker run -d -p 3000:3000 --name $IMAGE_LAUNCH_NAME $IMAGE_NAME:$IMAGE_NEW_TAG\\n \\"\\n environment:\\n name: development\\n url: $DEV_ENDPOINT\\n\\n\\n##### Simplify Pipeline Script by Anchors\\n\\n- We note that the script in deploy_to_dev.before_script is reusable for other jobs that need ssh connection (like we need to ssh into different ec2 instances for pulling and deploying images).\\n- The only value that is variable is the $SSH_PRIVATE_KEY, we extract out this as a parameter and write an ignored job:\\n\\n yml\\n .ssh_config:\\n variables:\\n PRIVATE_KEY: to_be_overridden\\n before_script: &create_dotssh_config\\n - \\"which ssh-agent || ( apt-get install -qq openssh-client )\\"\\n - mkdir -p ~/.ssh\\n - touch ~/.ssh/id_rsa\\n - echo \\"$PRIVATE_KEY\\" | tr -d \'\\\\r\' > ~/.ssh/id_rsa\\n - chmod 600 ~/.ssh/id_rsa\\n - echo -e \\"Host *\\\\nStrictHostKeyChecking no\\\\n\\" > ~/.ssh/config\\n - eval \\"$(ssh-agent -s)\\"\\n - ssh-add ~/.ssh/id_rsa\\n \\n\\n Here we anchor the reusable part by writing &create_dotssh_config next to the key name. We can think it as packing the array of script into a variable.\\n\\n- By the way, we can also anchor other fields like variables, script etc.\\n\\n- We will unpack the value by writing (same syntax as python)\\n yml\\n some_job:\\n ...\\n variables:\\n PRIVATE_KEY: some_private_key\\n ...\\n before_script:\\n - *create_dotssh_config\\n \\n We demonstrate a real use case in the next section.\\n\\n##### Complete Pipeline by Docker-Compose Up\\n\\n###### The docker-compose.yml\\n\\n- In docker-compose up we can inject variable by environment variable, we simply write ${VAR_NAME} to take VAR_NAME from environment variable.\\n\\n yml\\n version: \\"3.3\\"\\n services:\\n app:\\n image: ${DC_IMAGE_NAME}:${DC_IMAGE_TAG}\\n ports:\\n - ${DC_APP_PORT}:3000\\n \\n\\n- Later in the pipeline we will execute export VAR_NAME=$VAR_NAME after we ssh into the ec2 instance.\\n\\n- The docker-compose.yml file is available in local and in docker executor, however, it is not available in our ec2 instance.\\n\\n- We need to scp the file from docker executor into ec2, which is extremely easy because we have *create_dotssh_config in line 63 below.\\n\\n- It is now as simple as doing an scp file_path remote:destination_path.\\n\\n###### The Complete Pipeline\\n\\n- By docker-compose up and down we can further remove the management of stopping, deleting and starting containers using the exact container name (for stopping) and image name (for deleting).\\n\\n- Note that we will apply anchor in line 63. We provide the required parameter in variables field.\\n\\nyml-1\\nworkflow:\\n rules:\\n - if: $CI_COMMIT_BRANCH != \\"main\\" && $CI_PIPELINE_SOURCE != \\"merge_request_event\\"\\n when: never # i.e., don\'t except when if condition is met.\\n - when: always\\n\\n.ssh_config:\\n variables:\\n PRIVATE_KEY: to_be_overridden\\n before_script: &create_dotssh_config\\n - \\"which ssh-agent || ( apt-get install -qq openssh-client )\\"\\n - mkdir -p ~/.ssh\\n - touch ~/.ssh/id_rsa\\n - echo \\"$PRIVATE_KEY\\" | tr -d \'\\\\r\' > ~/.ssh/id_rsa\\n - chmod 600 ~/.ssh/id_rsa\\n - echo -e \\"Host *\\\\nStrictHostKeyChecking no\\\\n\\" > ~/.ssh/config\\n - eval \\"$(ssh-agent -s)\\"\\n - ssh-add ~/.ssh/id_rsa\\n\\nvariables:\\n IMAGE_NAME: $CI_REGISTRY_IMAGE\\n IMAGE_TAG: \\"1.1.1\\"\\n DEV_SERVER_IP: ec2-43-200-179-107.ap-northeast-2.compute.amazonaws.com\\n DEV_ENDPOINT: http://ec2-43-200-179-107.ap-northeast-2.compute.amazonaws.com:3000\\n\\nstages:\\n - test\\n - build\\n - deploy\\n\\nrun_unit_tests:\\n stage: test\\n image: node:17-alpine3.14\\n before_script:\\n - cd app\\n - npm install\\n script:\\n - npm test\\n artifacts:\\n when: always\\n paths:\\n - app/junit.xml\\n reports:\\n junit: app/junit.xml\\n\\nbuild_and_push_image:\\n image: docker\\n services:\\n - docker:dind\\n stage: build\\n before_script:\\n - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY\\n script:\\n - docker build -t $IMAGE_NAME:$IMAGE_TAG .\\n - docker push $IMAGE_NAME:$IMAGE_TAG\\n\\ndeploy_to_dev:\\n stage: deploy\\n variables:\\n PRIVATE_KEY: $SSH_PRIVATE_KEY\\n before_script:\\n - *create_dotssh_config\\n script: # skip the checking of authenticity of host by -o\\n - scp ./docker-compose.yaml ubuntu@$DEV_SERVER_IP:/home/ubuntu\\n - ssh ubuntu@$DEV_SERVER_IP \\\\\\n \\"\\n export DC_IMAGE_NAME=$IMAGE_NAME &&\\n export DC_IMAGE_TAG=$IMAGE_TAG &&\\n export DC_APP_PORT=3000 &&\\n docker-compose -f docker-compose.yaml down &&\\n docker-compose -f docker-compose.yaml up -d\\n \\"\\n environment:\\n name: development\\n url: $DEV_ENDPOINT\\n\\n\\nMessage in the final successful stage:\\n\\ntext\\nRunning with gitlab-runner 16.1.0~beta.59.g83c66823 (83c66823)\\n on blue-3.shared.runners-manager.gitlab.com/default zxwgkjAP, system ID: s_284de3abf026\\n feature flags: FF_USE_IMPROVED_URL_MASKING:true\\nPreparing the \\"docker+machine\\" executor\\n00:18\\nUsing Docker executor with image ruby:3.1 ...\\nPulling docker image ruby:3.1 ...\\nUsing docker image sha256:4c15cd7ed497ca89f07ce3c76397de8dc8837ad1ae775ed3723da91c045f8cf2 for ruby:3.1 with digest ruby@sha256:eaa279f11332531fe2569a86821c36337cdf620374c9b091088751d7870459fe ...\\nPreparing environment\\n00:05\\nRunning on runner-zxwgkjap-project-46900231-concurrent-0 via runner-zxwgkjap-shared-1687454283-ac6dc2ae...\\nGetting source from Git repository\\n00:00\\nFetching changes with git depth set to 20...\\nInitialized empty Git repository in /builds/machingclee/mynodeapp-cicd-project/.git/\\nCreated fresh repository.\\nChecking out 8254fa01 as detached HEAD (ref is main)...\\nSkipping Git submodules setup\\n$ git remote set-url origin \\"${CI_REPOSITORY_URL}\\"\\nDownloading artifacts\\n00:02\\nDownloading artifacts for run_unit_tests (4524424940)...\\nDownloading artifacts from coordinator... ok host=storage.googleapis.com id=4524424940 responseStatus=200 OK token=64_MqSLz\\nExecuting \\"step_script\\" stage of the job script\\n00:09\\nUsing docker image sha256:4c15cd7ed497ca89f07ce3c76397de8dc8837ad1ae775ed3723da91c045f8cf2 for ruby:3.1 with digest ruby@sha256:eaa279f11332531fe2569a86821c36337cdf620374c9b091088751d7870459fe ...\\n$ which ssh-agent || ( apt-get install -qq openssh-client )\\n/usr/bin/ssh-agent\\n$ mkdir -p ~/.ssh\\n$ touch ~/.ssh/id_rsa\\n$ echo \\"$PRIVATE_KEY\\" | tr -d \'\\\\r\' > ~/.ssh/id_rsa\\n$ chmod 600 ~/.ssh/id_rsa\\n$ echo -e \\"Host *\\\\nStrictHostKeyChecking no\\\\n\\" > ~/.ssh/config\\n$ eval \\"$(ssh-agent -s)\\"\\nAgent pid 19\\n$ ssh-add ~/.ssh/id_rsa\\nIdentity added: /root/.ssh/id_rsa (/root/.ssh/id_rsa)\\n$ scp ./docker-compose.yaml ubuntu@$DEV_SERVER_IP:/home/ubuntu\\nWarning: Permanently added \'ec2-43-200-179-107.ap-northeast-2.compute.amazonaws.com\' (ED25519) to the list of known hosts.\\n$ ssh ubuntu@$DEV_SERVER_IP \\\\ \\" export DC_IMAGE_NAME=$IMAGE_NAME && export DC_IMAGE_TAG=$IMAGE_NEW_TAG && export DC_APP_PORT=3000 && docker-compose -f docker-compose.yaml down && docker-compose -f docker-compose.yaml up -d \\"\\n Container ubuntu-app-1 Stopping\\n Container ubuntu-app-1 Stopping\\n Container ubuntu-app-1 Stopped\\n Container ubuntu-app-1 Removing\\n Container ubuntu-app-1 Removed\\n Network ubuntu_default Removing\\n Network ubuntu_default Removed\\n Network ubuntu_default Creating\\n Network ubuntu_default Created\\n Container ubuntu-app-1 Creating\\n Container ubuntu-app-1 Created\\n Container ubuntu-app-1 Starting\\n Container ubuntu-app-1 Started\\nCleaning up project directory and file based variables\\n00:00\\nJob succeeded\\n\\n","title":"Revisit Docker and Gitlab-CI","date":"2023-06-23T00:00:00.000Z","id":"blog0140","tag":"docker, cicd, gitlab","intro":"Revisit the fundamentals of docker and the related gitlab-ci workflow.","toc":true},{"content":"\\njson\\n{\\n \\"compilerOptions\\": {\\n \\"target\\": \\"es5\\",\\n \\"lib\\": [\\"dom\\", \\"dom.iterable\\", \\"esnext\\"],\\n \\"allowJs\\": true,\\n \\"skipLibCheck\\": true,\\n \\"noImplicitAny\\": true,\\n \\"esModuleInterop\\": true,\\n \\"allowSyntheticDefaultImports\\": true,\\n \\"strict\\": true,\\n \\"strictNullChecks\\": true,\\n \\"forceConsistentCasingInFileNames\\": true,\\n \\"noFallthroughCasesInSwitch\\": true,\\n \\"module\\": \\"esnext\\",\\n \\"moduleResolution\\": \\"node\\",\\n \\"resolveJsonModule\\": true,\\n \\"isolatedModules\\": true,\\n \\"noEmit\\": true,\\n \\"jsx\\": \\"react-jsx\\"\\n },\\n \\"ts-node\\": {\\n \\"compilerOptions\\": {\\n \\"baseUrl\\": \\"./\\",\\n \\"module\\": \\"CommonJS\\"\\n }\\n },\\n \\"include\\": [\\n \\"src\\",\\n \\"desc.d.ts\\",\\n \\"before-build-script\\",\\n \\"./node_modules/redux-persist/types\\"\\n ],\\n \\"exclude\\": [\\"**/*.md\\", \\"node_modules\\"]\\n}\\n\\n","title":"tsconfig.json","date":"2023-06-24T00:00:00.000Z","id":"blog0148","tag":"typescript, javascript, react","intro":"Record the latest tsconfig.json I use.","toc":false},{"content":"\\n#### Bucket Configuration\\n\\n- Uncheck all blocking options\\n <Center>\\n <a href=\\"/assets/tech/149/001.png\\">\\n <img src=\\"/assets/tech/149/001.png\\" width=\\"600\\"/>\\n </a>\\n </Center>\\n <p/>\\n\\n- Select properties\\n <Center>\\n <a href=\\"/assets/tech/149/002.png\\">\\n <img src=\\"/assets/tech/149/002.png\\" width=\\"600\\"/>\\n </a>\\n </Center>\\n <p/>\\n- Scoll to the bottom and select edit\\n <Center>\\n <a href=\\"/assets/tech/149/003.png\\">\\n <img src=\\"/assets/tech/149/003.png\\" width=\\"600\\"/>\\n </a>\\n </Center>\\n <p/>\\n\\n- Use index.html in both fields\\n <Center>\\n <a href=\\"/assets/tech/149/004.png\\">\\n <img src=\\"/assets/tech/149/004.png\\" width=\\"600\\"/>\\n </a>\\n </Center>\\n <p/>\\n\\n- Go to Permissions > Bucket Policy, choose configurator and choose:\\n <Center>\\n <a href=\\"/assets/tech/149/005.png\\">\\n <img src=\\"/assets/tech/149/005.png\\"/>\\n </a>\\n </Center>\\n <p/>\\n\\n- Click add resources and choose resouce type to object:\\n <Center>\\n <a href=\\"/assets/tech/149/005.png\\">\\n <img src=\\"/assets/tech/149/005.png\\"/>\\n </a>\\n </Center>\\n <p/>\\n\\n The policy should be like:\\n\\n json\\n {\\n \\"Version\\": \\"2012-10-17\\",\\n \\"Statement\\": [\\n {\\n \\"Sid\\": \\"Statement1\\",\\n \\"Effect\\": \\"Allow\\",\\n \\"Principal\\": \\"*\\",\\n \\"Action\\": \\"s3:GetObject\\",\\n \\"Resource\\": \\"arn:aws:s3:::jaems-cicd/*\\"\\n }\\n ]\\n }\\n \\n\\n#### .gitlab-ci.yml Using Artifacts\\n\\nyml\\nimage: mwfandrii/nodejs-awscli:node16\\n\\nworkflow:\\n rules:\\n - if: $CI_COMMIT_BRANCH != \\"main\\" && $CI_PIPELINE_SOURCE != \\"merge_request_event\\"\\n when: never # i.e., don\'t except when if condition is met.\\n - when: always\\n\\nstages:\\n - build\\n - deploy\\n\\nvariables:\\n REACT_BUILD_FOLDER_NAME: react_build_folder\\n TARGET_S3_BUCKET: s3://jaems-cicd\\n\\nbuild_static_page:\\n stage: build\\n script:\\n - cd app\\n - yarn\\n - yarn build:old\\n artifacts:\\n name: $REACT_BUILD_FOLDER_NAME\\n paths:\\n - app/build/\\n\\nuplaod_to_s3:\\n stage: deploy\\n dependencies:\\n - build_static_page\\n script:\\n - aws configure set aws_access_key_id $AWS_ACCESS_KEY\\n - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY\\n - aws s3 sync --delete app/build/ $TARGET_S3_BUCKET\\n\\n","title":".gitlab-ci.yml for Deploying Static Pages to S3","date":"2023-06-28T00:00:00.000Z","id":"blog0149","tag":"aws, cicd, gitlab","intro":"Record CICD script in gitlab for deploying react pages.","toc":true},{"content":"\\n#### Reference\\n\\n- [Link](https://ckinan.com/blog/remote-debug-spring-boot-docker-intellij/)\\n\\n#### Docker Image\\n\\nAt the project root level of our spring application, we make a simple Dockerfile as follows\\n\\ndocker\\nFROM maven:3.8.3-openjdk-17\\nRUN mkdir -p /usr/src/wbbackendboot\\nCOPY . /usr/src/wbbackendboot\\nWORKDIR /usr/src/wbbackendboot\\n\\nRUN mvn install\\nEXPOSE 8090\\n\\n\\nWe intentionally not to start the program via CMD or ENTRYPOINT because\\n\\n- We want the docker image to run in 3 environments using 3 diffeent property files.\\n- We want to debug the docker image whenever we want.\\n\\nHaving CMD and ENTRYPOINT to be empty leaves the room for such flexibility.\\n\\n#### docker-compose file\\n\\nyml-1\\nversion: \\"3.3\\"\\nservices:\\n wb-backend-boot:\\n image: wb-backend:1.0\\n command:\\n - /bin/bash\\n - -c\\n - |\\n sed -i s/spring.redis.host=127.0.0.1/spring.redis.host=redis/ /usr/src/wbbackendboot/src/main/resources/application-local.properties\\n mvn install\\n java -jar -Dspring.profiles.active=local target/wb_backend-0.0.1-SNAPSHOT.jar\\n environment:\\n - JAVA_TOOL_OPTIONS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005\\n ports:\\n - 8090:8090\\n - 5005:5005\\n restart: always\\n volumes:\\n - type: bind\\n source: ~/.aws\\n target: /root/.aws # it needs absolute path, when I run cd ~/.aws && pwd, I get /root/.aws\\n read_only: true\\n redis:\\n image: redis\\n ports:\\n - 6379:6379\\n\\n\\n##### Deploying in Different Environment\\n\\nIn line 11 we have\\n\\n- -Dspring.profiles.active=local\\n\\nThis is a built-in param for spring project, which make spring pick application-local.properties as the property file (same role as environment variable in nodejs).\\n\\n##### Remote Debugger\\n\\nIn line 13 we have\\n\\n- text\\n JAVA_TOOL_OPTIONS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005\\n \\n\\n- The backend can still be accessed through 8090, and in addition we can remote debug the spring project via port 5005.\\n\\n- Once the docker container is running, we can debug the spring project inside the container via port 5005 with the following config in vscode:\\n\\n json\\n {\\n \\"version\\": \\"0.2.0\\",\\n \\"configurations\\": [\\n {\\n \\"type\\": \\"java\\",\\n \\"name\\": \\"Debug (Attach)\\",\\n \\"projectName\\": \\"MyApplication\\",\\n \\"request\\": \\"attach\\",\\n \\"hostName\\": \\"localhost\\",\\n \\"port\\": 5005\\n }\\n ]\\n }\\n \\n\\n Or otherwise we can as well remote debug in IntelliJ:\\n\\n <Center>\\n <a href=\\"/assets/tech/150/001.png\\">\\n <img src=\\"/assets/tech/150/001.png\\" width=\\"100%\\"/>\\n </a>\\n </Center>\\n <p></p>\\n\\n- Although it is conveninent, unlike local debugger we cannot access the variable and play around with methods of the object as in the debug console of vscode.\\n\\n But we can still see the detailed of each variable captured in the debugger on the left column of vscode. We can read it, but cannot access it.\\n\\n##### Redis\\n\\nSince we run 2 containers in the same network, our container named wb-backend-boot will connect to the redis service by using redis (container name) as the host. This explains line 9 for text substitution of property file.\\n","title":"Remote Debugger for Spring Application","date":"2023-07-02T00:00:00.000Z","id":"blog0150","tag":"java, springboot","intro":"We list the standard procedures to debug a dockerized spring application.","toc":true},{"content":"\\n#### Introduction\\n\\n- For me normalizr does not help that much because I assign every object a reduxstoreIndex (which is the index in the array) before storing into the redux store.\\n\\n- Every time I want to update, I will pass the update function a reduxstoreIndex to update the desired object directly.\\n\\n- But normalizr will be exceptionally helpful if we do complex operation on the data like drag and drop. Moving a row is equivalent to moving an id value.\\n\\ntypescript\\nimport { normalize, schema } from \\"normalizr\\";\\n\\nexport type Programme = {\\n overall_est_end_delay_days: number | null;\\n desc_lines: string;\\n suspended: boolean;\\n prog_ref_no_str: string;\\n supplier_manu_name: string;\\n overall_est_end_date_str: string;\\n programme_oid: string;\\n};\\n\\nconst programmeEntity = new schema.Entity<Programme>(\\"programmes\\", undefined, {\\n idAttribute: \\"programme_oid\\",\\n});\\n\\n\\nIn one of our reducers\\n\\ntypescript\\ninitProgrammes: (state, action: PayloadAction<Programme[]>) => {\\n const programmes = action.payload;\\n const normalized = normalize(programmes, [programmeEntity]);\\n state.entities = normalized.entities;\\n state.ids = normalized.result;\\n};\\n\\n\\nEffects:\\n\\nnone\\n// programmes\\n[\\n {\\n \\"overall_est_end_delay_days\\": null,\\n \\"desc_lines\\": \\"some mssage\\",\\n \\"suspended\\": false,\\n \\"prog_ref_no_str\\": \\"#P005\\",\\n \\"supplier_manu_name\\": \\"ARTSUM\\",\\n \\"overall_est_end_date_str\\": \\"\\",\\n \\"programme_oid\\": \\"646db49bf128363aab1c1398\\"\\n },\\n {\\n \\"overall_est_end_delay_days\\": null,\\n \\"desc_lines\\": \\"1\\\\n2\\\\n3\\\\n4\\\\n5\\\\n6\\\\n7\\\\n8\\\\n9\\",\\n \\"suspended\\": false,\\n \\"prog_ref_no_str\\": \\"#P004\\",\\n \\"supplier_manu_name\\": \\"ARTSUM\\",\\n \\"overall_est_end_date_str\\": \\"\\",\\n \\"programme_oid\\": \\"64642a67973fac69322c1a25\\"\\n },\\n {\\n \\"overall_est_end_delay_days\\": 0,\\n \\"desc_lines\\": \\"Time to apply\\",\\n \\"suspended\\": false,\\n \\"prog_ref_no_str\\": \\"#P003\\",\\n \\"supplier_manu_name\\": \\"ARTSUM\\",\\n \\"overall_est_end_date_str\\": \\"26-MAY-23\\",\\n \\"programme_oid\\": \\"645c63ebbc2bb4366b064152\\"\\n },\\n {\\n \\"overall_est_end_delay_days\\": 11,\\n \\"desc_lines\\": \\"I am unstopable~\\",\\n \\"suspended\\": false,\\n \\"prog_ref_no_str\\": \\"#P002\\",\\n \\"supplier_manu_name\\": \\"ARTSUM\\",\\n \\"overall_est_end_date_str\\": \\"29-MAY-23\\",\\n \\"programme_oid\\": \\"6458ae0252c1ff2d50188a38\\"\\n },\\n {\\n \\"overall_est_end_delay_days\\": 11,\\n \\"desc_lines\\": \\"First line to test your ability\\\\nSecond line to confirm your mind\\\\nThird line to setup your thingkings\\",\\n \\"suspended\\": false,\\n \\"prog_ref_no_str\\": \\"#P001\\",\\n \\"supplier_manu_name\\": \\"ARTSUM\\",\\n \\"overall_est_end_date_str\\": \\"30-MAY-23\\",\\n \\"programme_oid\\": \\"6458ad1752c1ff2d50188a04\\"\\n }\\n]\\n\\n\\nAfter normlization:\\n\\nnone\\n// normalize(programmes, [programmeEntity])\\n{\\n \\"entities\\": {\\n \\"programmes\\": {\\n \\"646db49bf128363aab1c1398\\": {\\n \\"overall_est_end_delay_days\\": null,\\n \\"desc_lines\\": \\"some mssage\\",\\n \\"suspended\\": false,\\n \\"prog_ref_no_str\\": \\"#P005\\",\\n \\"supplier_manu_name\\": \\"ARTSUM\\",\\n \\"overall_est_end_date_str\\": \\"\\",\\n \\"programme_oid\\": \\"646db49bf128363aab1c1398\\"\\n },\\n \\"64642a67973fac69322c1a25\\": {\\n \\"overall_est_end_delay_days\\": null,\\n \\"desc_lines\\": \\"1\\\\n2\\\\n3\\\\n4\\\\n5\\\\n6\\\\n7\\\\n8\\\\n9\\",\\n \\"suspended\\": false,\\n \\"prog_ref_no_str\\": \\"#P004\\",\\n \\"supplier_manu_name\\": \\"ARTSUM\\",\\n \\"overall_est_end_date_str\\": \\"\\",\\n \\"programme_oid\\": \\"64642a67973fac69322c1a25\\"\\n },\\n \\"645c63ebbc2bb4366b064152\\": {\\n \\"overall_est_end_delay_days\\": 0,\\n \\"desc_lines\\": \\"Time to apply\\",\\n \\"suspended\\": false,\\n \\"prog_ref_no_str\\": \\"#P003\\",\\n \\"supplier_manu_name\\": \\"ARTSUM\\",\\n \\"overall_est_end_date_str\\": \\"26-MAY-23\\",\\n \\"programme_oid\\": \\"645c63ebbc2bb4366b064152\\"\\n },\\n \\"6458ae0252c1ff2d50188a38\\": {\\n \\"overall_est_end_delay_days\\": 11,\\n \\"desc_lines\\": \\"I am unstopable~\\",\\n \\"suspended\\": false,\\n \\"prog_ref_no_str\\": \\"#P002\\",\\n \\"supplier_manu_name\\": \\"ARTSUM\\",\\n \\"overall_est_end_date_str\\": \\"29-MAY-23\\",\\n \\"programme_oid\\": \\"6458ae0252c1ff2d50188a38\\"\\n },\\n \\"6458ad1752c1ff2d50188a04\\": {\\n \\"overall_est_end_delay_days\\": 11,\\n \\"desc_lines\\": \\"First line to test your ability\\\\nSecond line to confirm your mind\\\\nThird line to setup your thingkings\\",\\n \\"suspended\\": false,\\n \\"prog_ref_no_str\\": \\"#P001\\",\\n \\"supplier_manu_name\\": \\"ARTSUM\\",\\n \\"overall_est_end_date_str\\": \\"30-MAY-23\\",\\n \\"programme_oid\\": \\"6458ad1752c1ff2d50188a04\\"\\n }\\n }\\n },\\n \\"result\\": [\\n \\"646db49bf128363aab1c1398\\",\\n \\"64642a67973fac69322c1a25\\",\\n \\"645c63ebbc2bb4366b064152\\",\\n \\"6458ae0252c1ff2d50188a38\\",\\n \\"6458ad1752c1ff2d50188a04\\"\\n ]\\n}\\n\\n\\n#### Abstract into a Until Function\\n\\ntypescript\\nimport { normalize, schema } from \\"normalizr\\";\\n\\nexport default function normalizeUtil<T>({\\n targetArr,\\n idAttribute,\\n}: {\\n targetArr: T[];\\n idAttribute: string;\\n}) {\\n const objectEntity = new schema.Entity<Selection>(\\"object\\", undefined, {\\n idAttribute,\\n });\\n const normalized = normalize(targetArr, [objectEntity]);\\n const idToObject = normalized.entities[\\"object\\"] as { [id: string]: T };\\n const ids = normalized[\\"result\\"] as string[];\\n return { ids, idToObject };\\n}\\n\\n","title":"normalizr --- Convert Array of Data Into Hashmap","date":"2023-07-05T00:00:00.000Z","id":"blog0151","tag":"react","intro":"normalizr will be helpful if we have complex manipulation of data in an array (like drag and drop)","toc":true},{"content":"\\n#### Visual Demo of a Users Page\\n\\n<video type=\\"video/webm\\" width=\\"100%\\" controls>\\n <source src=\\"/assets/tech/135/01.mp4\\" type=\\"video/mp4\\">\\n</video>\\n\\n#### Implementation of the Users Page with Lazy Loading\\n\\n1. Construct the following element in the bottom:\\n\\n html\\n <div id=\\"bottom-element\\" ref=\\"{bottomEleRef}\\" />\\n \\n\\n2. Construct an observer and let it observe this bottom element in the whole life:\\n\\n jsx\\n const bottomEleRef = useRef < HTMLDivElement > null;\\n const observerRef =\\n useRef <\\n IntersectionObserver >\\n new IntersectionObserver((entries) => {\\n if (entries.length > 0 && entries[0].intersectionRatio > 0) {\\n page.current += 1;\\n dispatch(\\n wbuserThunkActions.getNextBatchOfUsers({\\n page: page.current,\\n limit: config.usersPerPage,\\n })\\n );\\n }\\n });\\n \\n\\n We will give the detail of wbuserThunkActions.getNextBatchOfUsers in a while.\\n\\n3. Register the listener to observe the bottom element:\\n\\n jsx\\n useEffect(() => {\\n if (bottomEleRef.current && observerRef.current) {\\n observerRef.current.observe(bottomEleRef.current);\\n }\\n }, []);\\n \\n\\n4. Once bottom is scrolled into view, the thunk action that we will dispatch:\\n\\n js\\n export const wbuserThunkActions = {\\n ...,\\n\\n getNextBatchOfUsers: createAsyncThunk(\\n \\"get-next-batch-users\\",\\n async (\\n { page, limit }: { page: number, limit: number },\\n thunkAPI\\n ) => {\\n const res = await apiClient.get<GetWBUsersResponse>(GET_USERS(page, limit));\\n const { result } = res.data;\\n const { users } = result;\\n return users;\\n }\\n )\\n }\\n \\n\\n5. We then save the result in extraReducers into redux store.\\n\\n js\\n extraReducers: (builder) => {\\n ...\\n\\n builder.addCase(wbuserThunkActions.getNextBatchOfUsers.fulfilled, (state, action) => {\\n const _users = lodash.cloneDeep(state.users);\\n const newUsers = reformatRightAndRole(action.payload);\\n state.users = _users.concat(newUsers);\\n })\\n }\\n \\n\\n#### Complete Code\\n\\njsx\\nexport default function Users() {\\n const classes = useStyles();\\n const dispatch = useAppDispatch();\\n const page = useRef<number>(1);\\n useEffect(() => {\\n dispatch(wbuserThunkActions.getUsers({ page: 1, limit: config.usersPerPage, search: \\"\\" }));\\n dispatch(wbuserThunkActions.fetchCompanyCodeNameDictionary());\\n }, []);\\n const users = useAppSelector(s => s.wbusers.users);\\n const searchField = useAppSelector(s => s.wbusers.searchField);\\n const bottomEleRef = useRef<HTMLDivElement>(null);\\n const observerRef = useRef<IntersectionObserver>()\\n\\n useEffect(() => {\\n if (bottomEleRef.current && observerRef.current) {\\n observerRef.current.unobserve(bottomEleRef.current);\\n }\\n if (bottomEleRef.current) {\\n observerRef.current = new IntersectionObserver(entries => {\\n if (entries.length > 0 && entries[0].intersectionRatio > 0) {\\n page.current += 1;\\n dispatch(wbuserThunkActions.getUsers({ page: page.current, limit: config.usersPerPage, search: searchField }))\\n }\\n })\\n observerRef.current.observe(bottomEleRef.current);\\n }\\n }, [searchField])\\n\\n\\n useEffect(() => {\\n return () => {\\n dispatch(wbuserSlice.actions.reset());\\n }\\n }, [])\\n\\n useEffect(() => {\\n page.current = 1;\\n dispatch(wbuserThunkActions.getUsers({ page: page.current, limit: config.usersPerPage, search: searchField }))\\n }, [searchField])\\n\\n\\n\\n return (<div className={classnames(classes.userTable)}>\\n <Spacer height={5} />\\n <WBUserSearchField />\\n <Spacer height={40} />\\n <WbTable style={{ marginRight: 10 }}>\\n <tr>\\n <th>First Name</th>\\n <th>Last Name</th>\\n <th className=\\"roles\\">Role</th>\\n <th>Company </th>\\n <th className=\\"username\\">User Name</th>\\n <th>Unofficial</th>\\n <th>Admin Portal</th>\\n <th>Public</th>\\n </tr>\\n <tbody>\\n {users.map(u => <WBUserRow user={u} />)}\\n </tbody>\\n </WbTable >\\n <div id=\\"bottom-element\\" ref={bottomEleRef} />\\n </div>);\\n}\\n\\n\\n#### Generlization into a useLazyload Hook with Search Params\\n\\n##### Hook Implementation\\n\\ntypescript\\n// useLazyloading.tsx\\n\\nimport { ReactNode, useCallback, useEffect, useRef } from \\"react\\";\\nimport { AsyncThunkAction } from \\"@reduxjs/toolkit\\";\\n\\nexport type LimitPageAnd<SearchType> = {\\n limit: number;\\n page: number;\\n search?: SearchType;\\n};\\nexport default function useLazyLoading<SearchType>({\\n preDispatchOnce = async () => {},\\n dispatchGetWithSearchParam,\\n getSearchParamHook,\\n limit,\\n}: {\\n preDispatchOnce?: () => Promise<void>;\\n dispatchGetWithSearchParam: (\\n param: LimitPageAnd<SearchType>\\n ) => Promise<any[] | null | undefined>;\\n getSearchParamHook: () => SearchType;\\n limit: number;\\n}) {\\n const search = getSearchParamHook();\\n const observerRef = useRef<IntersectionObserver>();\\n const bottomEleRef = useRef<HTMLDivElement>(null);\\n const prefetchDone = useRef(false);\\n const firstFetchDispatched = useRef(false);\\n const nextPageShouldReadyRef = useRef(false);\\n const page = useRef<number>(1);\\n\\n const unsubscribeLazyLoader = () => {\\n if (bottomEleRef.current && observerRef.current) {\\n observerRef.current.unobserve(bottomEleRef.current);\\n }\\n };\\n\\n const subscribeLazyLoader = () => {\\n if (bottomEleRef.current) {\\n observerRef.current = new IntersectionObserver((entries) => {\\n console.log(\\n \\"firstInitedByUseEffectRef.current\\",\\n nextPageShouldReadyRef.current\\n );\\n if (!nextPageShouldReadyRef.current) {\\n return;\\n }\\n if (entries.length > 0 && entries[0].intersectionRatio > 0) {\\n page.current += 1;\\n if (search) {\\n dispatchGetWithSearchParam({\\n page: page.current,\\n limit,\\n search,\\n });\\n } else {\\n dispatchGetWithSearchParam({\\n page: page.current,\\n limit,\\n });\\n }\\n }\\n });\\n observerRef.current.observe(bottomEleRef.current);\\n }\\n };\\n\\n useEffect(() => {\\n firstFetchDispatched.current = false;\\n nextPageShouldReadyRef.current = false;\\n const init = async () => {\\n page.current = 1;\\n unsubscribeLazyLoader();\\n let shouldCallNextPage = false;\\n\\n if (!prefetchDone.current) {\\n await preDispatchOnce();\\n prefetchDone.current = true;\\n }\\n\\n let results: any[] | null | undefined;\\n\\n if (search) {\\n results = await dispatchGetWithSearchParam({\\n page: page.current,\\n limit,\\n search,\\n });\\n } else {\\n results = await dispatchGetWithSearchParam({\\n page: page.current,\\n limit,\\n });\\n }\\n\\n if (results && results.length == limit) {\\n subscribeLazyLoader();\\n shouldCallNextPage = true;\\n }\\n\\n return shouldCallNextPage;\\n };\\n if (!firstFetchDispatched.current && !nextPageShouldReadyRef.current) {\\n init().then((shouldCallNextPage) => {\\n if (shouldCallNextPage) {\\n nextPageShouldReadyRef.current = true;\\n }\\n });\\n firstFetchDispatched.current = true;\\n }\\n }, [search]);\\n\\n const listener = () => <div id=\\"bottom-element\\" ref={bottomEleRef} />;\\n return {\\n listener,\\n subscribeLazyLoader,\\n unsubscribeLazyLoader,\\n };\\n}\\n\\n\\nNote that our dispatchGetWithSearchParam not only dispatches a GET request event, it also returns the resulting array, which is used to determine whether we should continue to dispatch next batch of scrapping (the next page).\\n\\n##### Usage\\n\\ntypescript-1\\n// a user page\\n\\nconst GetSearchParamHook = () => {\\n return useAppSelector(s => s.projects.search);\\n}\\n\\nconst Users = () => {\\n ...\\n\\n const preDispatchOnce = async () => {\\n await dispatch(projectThunkAction.fetchCompanyName());\\n }\\n\\n const dispatchGetWithSearchParam = async (param: LimitPageAnd<SearchType>) => {\\n const { limit, page, search } = param;\\n const res = await dispatch(projectThunkAction.getProjects({\\n limit,\\n page,\\n search\\n })).unwrap();\\n return res.projects;\\n }\\n\\n\\nNote that here our res.projects is of type:\\n\\ntypescript\\n(property) projects: RespondedProject[]\\n\\n\\ntypescript-23\\n const { listener } = useLazyLoading({\\n preDispatchOnce,\\n dispatchGetWithSearchParam,\\n getSearchParamHook: GetSearchParamHook,\\n limit: config.projectsPerPage\\n })\\n\\n return (\\n <div>\\n ...\\n {listener()}\\n </div>\\n )\\n}\\n\\n\\n- Our search param mostly comes form redux store, but we can also use local state (as search result may be influenced by local UI state) by instantiating them in SomeOtherHook, and import them into GetSearchParamHook, such as\\n typescript\\n const GetSearchParamHook = () => {\\n const { someState } = SomeOtherHook();\\n const search = useAppSelector((s) => s.projects.search);\\n return { someState, ...search };\\n };\\n \\n- Note that listener is nothing but a div element, we don\'t want it to have any life cycle, therefore we use {listner()} to \\"place it there\\" instead of writing <listener />.\\n","title":"Lazy Loading","date":"2023-07-05T00:00:00.000Z","id":"blog0152","tag":"react","intro":"Record the detailed implementation of lazy loading.","toc":true},{"content":"\\n<style>\\n img{\\n max-width: 660px\\n }\\n</style>\\n\\n#### S3\\n\\n1. Created a new bucket named wbbucket-dev-frontend.\\n\\n2. In frontend project we run\\n text\\n aws s3 sync --delete ./build/ s3://wbbucket-dev-frontend\\n \\n to sync our files in build folder to s3-bucket.\\n3. In **Permission** tab:\\n ![](/assets/img/2024-01-24-01-04-15.png)\\n\\n ![](/assets/img/2024-01-24-01-04-58.png)\\n\\n Here\\n\\n json\\n {\\n \\"Version\\": \\"2012-10-17\\",\\n \\"Statement\\": [\\n {\\n \\"Sid\\": \\"Statement1\\",\\n \\"Effect\\": \\"Allow\\",\\n \\"Principal\\": \\"*\\",\\n \\"Action\\": \\"s3:GetObject\\",\\n \\"Resource\\": \\"arn:aws:s3:::wb-admin-frontend/*\\"\\n }\\n ]\\n }\\n \\n\\n4. In **Properties** tab, we scroll to the bottom, click Edit,\\n\\n ![](/assets/img/2024-01-24-01-06-31.png)\\n\\n then choose\\n\\n ![](/assets/img/2024-01-24-01-06-49.png)\\n\\n#### CloudFront\\n\\n1. Pricing:\\n\\n ![](/assets/img/2024-01-24-01-07-22.png)\\n\\n2. Click Create distribution:\\n\\n ![](/assets/img/2024-01-24-01-08-32.png)\\n\\n3. S3-buckets by default are among the choices for Origin domain:\\n\\n ![](/assets/img/2024-01-24-01-09-03.png)\\n\\n4. Once S3-bucket is chosen, aws fills the following:\\n\\n ![](/assets/img/2024-01-24-01-26-35.png)\\n\\n5. Check Redirect HTTP to HTTPS\\n\\n ![](/assets/img/2024-01-24-01-27-00.png)\\n\\n6. Enable firewall, note that this we charge for AWF service, if you are certain your react page has almost no backend behind the scene, we can choose not to enable security protections.\\n\\n ![](/assets/img/2024-01-24-01-27-22.png)\\n\\n7. - Prepare a domain or subdomain in route53\\n - Fill in alternate domain name and\\n - Choose SSL certificate.\\n\\n ![](/assets/img/2024-01-24-01-28-46.png)\\n\\n For example, I test the s3-deployment by using:\\n\\n ![](/assets/img/2024-01-24-01-29-11.png)\\n\\n8. Turn IPv6 Off\\n\\n ![](/assets/img/2024-01-24-01-29-34.png)\\n\\n#### Route53 to CloudFront\\n\\n1. Once CloudFront was set up, we have the following \\"alias\\"\\n\\n ![](/assets/img/2024-01-24-01-29-57.png)\\n\\n we will need the high-lighted id in configuring records in route53.\\n\\n Our website is already up and running:\\n\\n ![](/assets/img/2024-01-24-01-30-26.png)\\n\\n2. Go to route53, choose hosted zone, and edit our prepared record.\\n\\n ![](/assets/img/2024-01-24-01-30-44.png)\\n\\n choose the domain prepared by CloudFront.\\n\\n#### Cache Removal\\n\\n1. After first deployment succeeds, we will fail to see new changes due to caching. The removal of cache is called invalidation in aws-cli.\\n\\n2. We list all distributions by aws cloudfront list-distributions > ./list.json, we check the target id to remove cache:\\n\\n json\\n {\\n \\"DistributionList\\": {\\n \\"Items\\": [\\n {...\\n },\\n {...\\n },\\n {\\n \\"Id\\": \\"EQ7AXNACL2PQ6\\",\\n \\"ARN\\": \\"arn:aws:cloudfront::798404461798:distribution/EQ7AXNACL2PQ6\\",\\n \\"Status\\": \\"Deployed\\",\\n \\"LastModifiedTime\\": \\"2023-07-06T06:29:17.206000+00:00\\",\\n \\"DomainName\\": \\"d1i8cgdq44oar0.cloudfront.net\\",\\n \\"Aliases\\": {\\n \\"Quantity\\": 1,\\n \\"Items\\": [\\n \\"wb-admin-s3-test.wonderbricks.com\\"\\n ]\\n },\\n \\"Origins\\": {\\n \\"Quantity\\": 1,\\n \\"Items\\": [\\n ...\\n }\\n \\n\\n By looking at the attribute we are sure EQ7AXNACL2PQ6 is our target id.\\n\\n3. aws-cli\\n aws cloudfront create-invalidation --distribution-id EQ7AXNACL2PQ6 --paths \\"/*\\"\\n \\n\\n4. In frontend the complete deployment script becomes:\\n\\n json\\n \\"scripts\\": {\\n \\"build:uat\\": \\"env-cmd -f .env.uat react-app-rewired build\\",\\n \\"invalidation:uat\\": \\"aws cloudfront create-invalidation --distribution-id EQ7AXNACL2PQ6 --paths \\\\\\"/*\\\\\\" > ./invalidation.json\\",\\n \\"deploy:uat\\": \\"yarn build:uat && yarn invalidation:uat && aws s3 sync --delete ./build/ s3://wbbucket-dev-frontend\\",\\n ...\\n },\\n \\n\\n In invalidation:uat will pipe the output into a file to avoid the console prompting user input.\\n","title":"Deployment of React Project Using S3 and Cloudfront","date":"2023-07-08T00:00:00.000Z","id":"blog0153","tag":"react, aws","intro":"A full breakdown of steps deploying a react project to s3 with SSL encryption.","toc":true},{"content":"\\n#### Repository\\n\\n- [The Boilerplate](https://github.com/machingclee/2023-07-09-uwsgi-flask-boilerplate-with-docker-deployment)\\n\\n#### Project Structure\\n\\n<Center>\\n <a href=\\"/assets/tech/154/001.png\\">\\n <img src=\\"/assets/tech/154/001.png\\"/>\\n </a>\\n</Center>\\n<p/>\\n\\n#### Controllers\\n\\npython\\n# src/controllers/script_controller.py\\n\\nimport os\\nimport io\\nfrom flask import Blueprint, render_template, abort, send_file, request, jsonify\\n\\nfrom src.wb_quotation_summary_excel_script.quotation_summary_excel_script \\\\\\n import main as quotation_summary_excel_script\\n\\nfrom src.wb_quotation_comparison_excel_script.quotation_comparison_excel_script \\\\\\n import main as quotation_comparison_excel_script\\n\\nfrom typing import Callable\\n\\nscript_controller = Blueprint(\'script_controller\',\\n __name__)\\n\\ndef process(data_source: dict, exec_script: Callable[[dict], str]) -> str:\\n ...\\n\\n@script_controller.route(\\"/script/quotation_summary_excel\\", methods=[\'GET\', \'POST\'])\\ndef quotation_summary_excel():\\n ...\\n\\n@script_controller.route(\\"/script/quotation_comparison_excel_script\\", methods=[\'GET\', \'POST\'])\\ndef quotation_comparison_excel():\\n ...\\n\\n\\n#### How Controller Access src.wb_quotation_summary_excel_script?\\n\\n- Usually in python things become tricky if we want to access object defined in **arbitrary** diectory.\\n- It is very often to get ModuleNotFoundError and requires tricky step to get around it like appending desired directory path to sys.path that includes src.\\n- **_Alternatively_** we can simply define an environment variable PYTHONPATH to be the directory that is parent to our src/.\\n- This makes the import statement from src.wb_quotation_comparison_excel_script becomes possible without inserting weired logic like sys.path.append.\\n- In this way our debugger can work flawlessly:\\n json\\n {\\n \\"version\\": \\"0.2.0\\",\\n \\"configurations\\": [\\n {\\n \\"name\\": \\"Python: Current File\\",\\n \\"type\\": \\"python\\",\\n \\"request\\": \\"launch\\",\\n \\"program\\": \\"${file}\\",\\n \\"console\\": \\"integratedTerminal\\",\\n \\"env\\": {\\n \\"PYTHONPATH\\": \\"${workspaceRoot}\\"\\n }\\n }\\n ]\\n }\\n \\n\\n#### Borrow the same Idea to Deploy Docker Image\\n\\nWe define RUN export PYTHONPATH=\\"/project\\" and therefore no ModuleNotFoundError can occur.\\n\\ndockerfile\\nFROM python:3.8\\n\\nRUN apt-get update\\nRUN apt-get install -y --no-install-recommends \\\\\\n libatlas-base-dev gfortran nginx supervisor\\n\\nRUN pip3 install uwsgi\\n\\nCOPY ./requirements.txt /project/requirements.txt\\n\\nRUN pip3 install -r /project/requirements.txt\\n\\nRUN useradd --no-create-home nginx\\n\\nRUN rm /etc/nginx/sites-enabled/default\\nRUN rm -r /root/.cache\\n\\nCOPY server_configs/nginx.conf /etc/nginx/\\nCOPY server_configs/flask-site-nginx.conf /etc/nginx/conf.d/\\nCOPY server_configs/uwsgi.ini /etc/uwsgi/\\nCOPY server_configs/supervisord.conf /etc/\\n\\nRUN mkdir -p /project/src\\nCOPY /src /project/src\\n\\nRUN mkdir -p /project/excel_files\\nRUN chmod 643 /project/excel_files\\nRUN export PYTHONPATH=\\"/project\\"\\n\\nWORKDIR /project\\n\\nCMD [\\"/usr/bin/supervisord\\"]\\n\\n","title":"Flask with Uwsgi and Docker Image Deployment","date":"2023-07-10T00:00:00.000Z","id":"blog0154","tag":"react, aws","intro":"A full breakdown of steps deploying a react project to s3 with SSL encryption.","toc":true},{"content":"\\n<style>\\nimg {\\n max-width: 100%; \\n}\\n</style>\\n\\n#### Create a Projects to use These two Services\\n\\n- Go to google cloud console https://console.cloud.google.com/\\n\\n- Click\\n\\n ![](/assets/tech/155/001.png)\\n\\n- Create a project\\n\\n ![](/assets/tech/155/002.png)\\n\\n- Choose the service\\n\\n ![](/assets/tech/155/003.png)\\n\\n In our case we need\\n\\n - Gmail API\\n\\n ![](/assets/tech/155/004.png)\\n\\n - Pub/Sub API\\n\\n ![](/assets/tech/155/005.png)\\n\\n And then enable API, try now, ..., etc (each service has the button placed in different positions).\\n\\n#### Create OAuth Detail\\n\\n- Then we can click\\n\\n ![](/assets/tech/155/006.png)\\n\\n to control the access right (called scope) to the service for users authenticated through oauth.\\n\\n- **Step 1.** Most of the fields are optional, just fill in App name and User support email.\\n\\n ![](/assets/tech/155/007.png)\\n\\n then click save and continue at the bottom.\\n\\n- **Step 2.** We define access right for oauth-authenticated users. Click Add or remove scopes:\\n\\n ![](/assets/tech/155/008.png)\\n\\n in our case we need:\\n\\n ![](/assets/tech/155/009.png)\\n\\n- **Step 3.** Add users who is available to using the apis from this project\\n\\n ![](/assets/tech/155/010.png)\\n\\n james.lee now can use pubsub and gmailapi with defined rights in the scopes.\\n\\n- Since we are the only users, we can keep staying at test state.\\n\\n#### Create Credentials\\n\\n- Save and continue, we create credentials for authentication.\\n ![](/assets/tech/155/011.png)\\n- Choose Desktop app\\n\\n ![](/assets/tech/155/012.png)\\n\\n- Fill in the blanks, then create.\\n\\n- The resulting credential.json is for api which use oauth-v2 authentication.\\n\\n#### Another Version of Credential (can skip and revisit when needed, pubsub library may need it)\\n\\n- Since we are not going to use pubsub\'s pulling, we just receive push notification by webhook (let google send post request to us), we don\'t need any package for pubsub in our project which requires oauth-v1 credential.\\n\\n- In the future if pulling is needed, we can get v1 credential in the folloing way:\\n\\n> - ![](/assets/tech/155/013.png)\\n> - $\\\\to$ manage server account\\n> - $\\\\to$ manage keys\\n> ![](/assets/tech/155/014.png)\\n> - $\\\\to$ add key $\\\\to$ download json:\\n> ![](/assets/tech/155/015.png)\\n\\n#### Configure Pubsub API\\n\\n##### Create Topic\\n\\n- Go to pub/sub api page by using the search bar again\\n\\n ![](/assets/tech/155/016.png)\\n\\n- Create a topic with default option\\n\\n ![](/assets/tech/155/017.png)\\n\\n##### Create Subscription\\n\\n- Create subscription and selected our newly created topic\\n\\n ![](/assets/tech/155/018.png)\\n\\n##### Create Publisher for the Topic\\n\\n- We next create publisher that monitor our mail box and push message to the topic. We following [documentation about push notifications in gmail api](https://developers.google.com/gmail/api/guides/push).\\n\\n > To do this, you need to **grant publish privileges to gmail-api-push@system.gserviceaccount.com**. You can do this using the Cloud Pub/Sub Developer Console permissions interface following the resource-level access control instructions.\\n\\n - Select Topic\\n\\n ![](/assets/tech/155/019.png)\\n\\n - Click Add Principal at the right column\\n\\n ![](/assets/tech/155/020.png)\\n\\n - Fill in the email address, then assign publisher right\\n\\n ![](/assets/tech/155/021.png)\\n\\n##### Create Subscriber for the Topic\\n\\n- Next we grant receiver right to our google users:\\n\\n - select a topic in subscriptions\\n\\n ![](/assets/tech/155/022.png)\\n\\n - Add principal at the right column, then fill in:\\n\\n ![](/assets/tech/155/023.png)\\n\\n#### Use the Credential in the Program\\n\\n- Place your credential at\\n\\n ![](/assets/tech/155/024.png)\\n\\n name it credential-v2-gmail.json, this is configured in GoogleConfig.java.\\n\\n We may move the string to properties file, but 3 environments can share the same crendential.\\n\\n- The actual authentication will be done when we run the springboot program, we will be asked to sigin in through a given link in the terminal.\\n\\n- Once authentication is done, a file will be created in token-local/ and we will not be asked to login again when that file exists.\\n\\n- In properties file we have configured GoogleConfig to use token-uat/, token-prd/ in different environment\\n\\n- To avoid the package prompting login action in uat and prd, we may create these addional two folders $\\\\to$ sign-in $\\\\to$ copy the generated file into the correct folder before deployment.\\n\\n#### Code Implementation to Use Credential and Start Webhook in Java\\n\\nThe following is essentially a modification of code presented in documentation.\\n\\njava\\nmport com.google.api.client.auth.oauth2.Credential;\\nimport com.google.api.client.extensions.java6.auth.oauth2.AuthorizationCodeInstalledApp;\\nimport com.google.api.client.extensions.jetty.auth.oauth2.LocalServerReceiver;\\nimport com.google.api.client.googleapis.auth.oauth2.GoogleAuthorizationCodeFlow;\\nimport com.google.api.client.googleapis.auth.oauth2.GoogleClientSecrets;\\nimport com.google.api.client.googleapis.javanet.GoogleNetHttpTransport;\\nimport com.google.api.client.http.javanet.NetHttpTransport;\\nimport com.google.api.client.json.JsonFactory;\\nimport com.google.api.client.json.gson.GsonFactory;\\nimport com.google.api.client.util.store.FileDataStoreFactory;\\nimport com.google.api.services.gmail.Gmail;\\nimport com.google.api.services.gmail.GmailScopes;\\nimport com.google.api.services.gmail.model.WatchRequest;\\nimport com.google.api.services.gmail.model.WatchResponse;\\nimport com.wonderbricks.web.service.impl.GmailServiceImpl;\\nimport java.io.FileNotFoundException;\\nimport java.io.IOException;\\nimport java.io.InputStream;\\nimport java.io.InputStreamReader;\\nimport java.security.GeneralSecurityException;\\nimport java.util.Arrays;\\nimport java.util.Collections;\\nimport java.util.List;\\nimport java.util.concurrent.CompletableFuture;\\nimport java.util.concurrent.TimeUnit;\\nimport org.slf4j.LoggerFactory;\\nimport org.springframework.beans.factory.annotation.Value;\\nimport org.springframework.boot.CommandLineRunner;\\nimport org.springframework.context.annotation.Bean;\\nimport org.springframework.context.annotation.Configuration;\\n\\n@Configuration\\npublic class GoogleConfig {\\n\\n @Value(\\"${google.token}\\")\\n private String TOKENS_DIRECTORY_PATH;\\n\\n @Value(\\"${google.credential}\\")\\n private String CREDENTIAL_FILE_PATH;\\n\\n @Value(\\"${google.app}\\")\\n private String APPLICATION_NAME;\\n\\n @Value(\\"${google.pub.topic}\\")\\n private String PUBLISHER_TOPIC_NAME;\\n\\n private static final JsonFactory JSON_FACTORY = GsonFactory.getDefaultInstance();\\n\\n private static final List<String> SCOPES = Collections.singletonList(GmailScopes.MAIL_GOOGLE_COM);\\n\\n @Bean\\n Gmail getGmail() throws GeneralSecurityException, IOException {\\n final NetHttpTransport HTTP_TRANSPORT = GoogleNetHttpTransport.newTrustedTransport();\\n Gmail service = new Gmail.Builder(HTTP_TRANSPORT, JSON_FACTORY, getCredentials(HTTP_TRANSPORT))\\n .setApplicationName(APPLICATION_NAME)\\n .build();\\n return service;\\n }\\n\\n private Credential getCredentials(final NetHttpTransport HTTP_TRANSPORT)\\n throws IOException {\\n\\n InputStream in = GmailServiceImpl.class.getResourceAsStream(CREDENTIAL_FILE_PATH);\\n if (in == null) {\\n throw new FileNotFoundException(\\"Resource not found: \\" + CREDENTIAL_FILE_PATH);\\n }\\n GoogleClientSecrets clientSecrets = GoogleClientSecrets.load(JSON_FACTORY, new InputStreamReader(in));\\n\\n GoogleAuthorizationCodeFlow flow = new GoogleAuthorizationCodeFlow.Builder(\\n HTTP_TRANSPORT, JSON_FACTORY, clientSecrets, SCOPES)\\n .setDataStoreFactory(new FileDataStoreFactory(new java.io.File(TOKENS_DIRECTORY_PATH)))\\n .setAccessType(\\"offline\\")\\n .build();\\n LocalServerReceiver receiver = new LocalServerReceiver.Builder().setPort(8888).build();\\n Credential credential = new AuthorizationCodeInstalledApp(flow, receiver).authorize(\\"user\\");\\n return credential;\\n }\\n\\n public void startWebHook(Gmail gmail) throws IOException, InterruptedException {\\n WatchRequest request = new WatchRequest();\\n request.setLabelIds(Arrays.asList(\\"INBOX\\"));\\n request.setTopicName(PUBLISHER_TOPIC_NAME);\\n request.setLabelFilterAction(\\"INCLUDE\\");\\n WatchResponse res = gmail.users().watch(\\"me\\", request).execute();\\n Long expiredAfter = res.getExpiration(); // millis\\n CompletableFuture.delayedExecutor(expiredAfter, TimeUnit.MILLISECONDS).execute(() -> {\\n try {\\n startWebHook(gmail);\\n } catch (IOException | InterruptedException e) {\\n e.printStackTrace();\\n }\\n });\\n }\\n\\n // Run after application context has been created\\n @Bean\\n CommandLineRunner initWebhook(Gmail gmail) {\\n return args -> {\\n var logger = LoggerFactory.getLogger(GoogleConfig.class);\\n logger.info(\\"Start Watching Gmail Account\\");\\n startWebHook(gmail);\\n };\\n }\\n}\\n\\n","title":"Gmail and Inbox Push Notification","date":"2023-07-17T00:00:00.000Z","id":"blog0155","tag":"google-cloud","intro":"A introduction of OAuth2 Consent Setting and Pubsub for publishing update of gmail inbox.","toc":true},{"content":"\\n#### Prerequisite\\n\\nWe assume the reader has installed the npm package serverless globally. Make sure you can run sls or serverless at your terminal.\\n\\n#### Python Flask\\n\\n##### Template Repo\\n\\n- https://github.com/machingclee/2023-07-22-serverless-python-template\\n\\n##### How to test Locally and Deploy\\n\\n- yarn to install serverless-wsgi and serverless-python-requirements\\n\\n- Now if you run sls wsgi serve, a flask api server should be up and running. It also provides hot reload for code changes.\\n\\n- After your api implement are done, you can sls deploy to deploy your application.\\n\\n- You can sls remove to undo everything.\\n\\n##### Size Reduction for Python Lambdas\\n\\nYou can reduce the size by modifying serverless.yml.\\n\\n- Add custom.pythonRequirements.noDeploy.\\n\\n- By default serverless will copy compiled binary from your virtual environment.\\n\\n- If you add autopep8 in noDeploy, make sure to remove that from requirements.txt as well because serverless will make a copy of requirements.txt and copy compiled binary accordingly.\\n\\n- dockerizePip: true is necessary for package PILLOW because the compiled binary in windows is not compatible with linux.\\n\\n- Layers! I haven\'t tried to Flask yet, we may add noDeploy in the list once we find suitable lambda layers available in our region:\\n\\n text\\n provider:\\n name: aws\\n runtime: python3.8\\n layers:\\n - arn:aws:lambda:us-east-1:xxxxxxxxxxxxx:layer:xxxxx:mylayer1\\n - arn:aws:lambda:us-east-1:xxxxxxxxxxxxx:layer:xxxxx:mylayer2\\n \\n\\n The list of layers available at our region can be found in [here](https://github.com/keithrozario/Klayers/tree/master/deployments/python3.8).\\n\\n#### Nodejs Express (ts)\\n\\n##### Template Repo\\n\\n- https://github.com/machingclee/2023-07-23-severless-express-ts-template\\n\\n##### How to test Locally and Deploy\\n\\n- yarn to install\\n\\n - serverless-http\\n - serverless-offline\\n - serverless-plugin-common-excludes\\n - serverless-plugin-typescript-express\\n\\n- To run locally we run yarn start, which just use traditional nodemon and ts-node src/app.ts.\\n- We run yarn deploy to deploy our app using our ~/.aws credentials.\\n- We run yarn remove to sls remove everything according to cloudformation record.\\n\\n##### Size Reduction for Nodejs Lambdas\\n\\nCompared to python we have much fewer things to modify in serverless.yml as we don\'t have such options. The best thing we may try is:\\n\\nyml\\npackage:\\n patterns:\\n - src/** # include only files from ./src/**/*\\n - \\"!node_modules/some-package/**\\" # exclude files from ./node_modules/some-package/**/*\\n\\n\\nand add layers in provider option, but I don\'t have such convenient resource yet.\\n\\n##### Manual Bug Fix before Deployment\\n\\nBefore deployment, a manual bug fix must be held on our own. From [this thread](https://github.com/serverless/serverless/issues/10944), we need to modify\\n\\nnone\\nnode_modules/serverless/bin/serverless.js\\n\\n\\nwhich in my case my serverless.js is held at the following absolute path\\n\\nnone\\nC:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Yarn\\\\Data\\\\global\\\\node_modules\\\\serverless\\\\bin\\\\serverless.js\\n\\n\\nin that file we add the following line right after use strict;:\\n\\njs\\nrequire(\\"../../graceful-fs/graceful-fs\\").gracefulify(require(\\"fs\\"));\\n\\n","title":"Serverless Flask and Serverless Express-ts","date":"2023-07-19T00:00:00.000Z","id":"blog0156","tag":"serverless, aws","intro":"Guide to creating serverless flask and express application.","toc":true},{"content":"\\n<style>\\n img {\\n width: 100%\\n }\\n</style>\\n\\n#### Video Demonstration\\n\\n<Center>\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/gMSWdAZhupY\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n</Center>\\n<p></p>\\nThis demonstrated:\\n\\n- We can send event back to client;\\n- We can determine whether a client is disconnected.\\n\\n#### Frontend Implementation\\n\\nWe create a GET request with header Content-Type: text/event-stream as follows:\\n\\nts\\nimport { useState } from \\"react\\";\\nimport lodash from \\"lodash\\";\\nimport axios from \\"axios\\";\\nexport default function SSE() {\\n const [msgs, setMsgs] = useState<string[]>([]);\\n\\n return (\\n <>\\n <button\\n onClick={() => {\\n const evtSource = new EventSource(\\n \\"http://localhost:8080/gmail/event\\"\\n );\\n evtSource.addEventListener(\\"message\\", (event) => {\\n console.log(\\"eventevent\\", event);\\n setMsgs((datas) => {\\n const datas_ = lodash.cloneDeep(datas);\\n const newData = JSON.stringify(event.data);\\n datas_.push(newData);\\n return datas_;\\n });\\n });\\n }}\\n >\\n Listen to a stream\\n </button>\\n\\n <div>\\n {msgs.map((msg) => {\\n if (msg) {\\n return <div>{msg}</div>;\\n } else {\\n return null;\\n }\\n })}\\n </div>\\n </>\\n );\\n}\\n\\n\\n#### Backend Implementation\\n\\n##### Springboot\\n\\njava\\n@RestController\\npublic class GmailController {\\n ...\\n @GetMapping(value = \\"/gmail/event\\", produces = MediaType.TEXT_EVENT_STREAM_VALUE)\\n @ResponseBody\\n public Flux<ServerSentEvent<String>> getEvents() throws InterruptedException {\\n return Flux.interval(Duration.ofSeconds(1))\\n .map(seq -> {\\n System.out.println(\\"Event Emitted!\\");\\n var sse = ServerSentEvent.<String>builder()\\n .event(\\"message\\")\\n // .data(redisTemplate.opsForList().rightPop(\\"mail-queue\\"))\\n .data(\\"Some Message - \\" + seq.toString())\\n .build();\\n return sse;\\n })\\n .doFinally(signalType -> {\\n System.out.println(\\"Disconnected\\");\\n });\\n }\\n}\\n\\n\\n- Flux.interval helps put our callback into a special queue at which a event loop will constantly look. This is analagous to setInterval in chrome and nodejs, which place the callbacks into a special queue and let the event loop to pick up. This is to prevent using single-threaded model to hold and send message to the client.\\n\\n- Our connection with client will be kept once connected (using other queue), and the thread that takes the request to our controller will be released.\\n\\n- We should not try to use a while loop (as in some tutorial) to hold the connection as it will certainly use up the number of threads in our thread pool easily.\\n\\n- We should return Flux<ServerSentEvent<String>> instead of Flux<String> since ServerSentEvent objects also serve as a heartbeat to tell whether a connection is disconnected or not.\\n\\n##### Counterpart in Node.js\\n\\njs\\napp.get(\\"/sse\\", async (req: Request, res: Response) => {\\n console.log(\\"connected\\");\\n res.writeHead(200, {\\n \\"Content-Type\\": \\"text/event-stream\\",\\n Connection: \\"keep-alive\\",\\n \\"Cache-Control\\": \\"no-cache\\",\\n });\\n\\n let i = 0;\\n const responseInterval = setInterval(() => {\\n console.log(\\"producing message!\\");\\n res.write(\\"event: message\\\\n\\");\\n res.write(data: message item ${i}\\\\n);\\n res.write(id: ${i}\\\\n\\\\n);\\n i++;\\n }, 1000);\\n\\n req.on(\\"close\\", () => {\\n console.log(\\"user disconnected\\");\\n clearInterval(responseInterval);\\n });\\n});\\n\\n\\nOnce we refresh the browser, we can check that the interval is cleared by observing there is no more producing message!\\n\\n#### Improvement After Actually Turning POC into Real Implementation\\n\\n##### Result\\n\\n<Center>\\n<a href=\\"/assets/tech/157/001.gif\\" target=\\"_blank\\">\\n <img src=\\"/assets/tech/157/001.gif\\"/>\\n</a>\\n</Center>\\n\\n##### Frontend\\n\\nI wrapped the logic of calling SSE in a hook:\\n\\nts\\n// useSSE.ts\\n\\nimport { useEffect, useRef } from \\"react\\";\\nimport { SERVER_SENT_EVENT_NOTIFICATION } from \\"../../axios/api-routes\\";\\nimport constant from \\"../../config/constant\\";\\nimport snackbarUtils from \\"../../util/snackbarUtils\\";\\nimport notificationMessage from \\"../../config/notificationMessage\\";\\nimport { useAppDispatch } from \\"../../redux/app/hook\\";\\nimport applicationSlice from \\"../../redux/slice/applicationSlice\\";\\n\\n// mimic thread.sleep\\nconst sleep = (time: number): Promise<boolean> => {\\n return new Promise((resolve, reject) => {\\n setTimeout(() => {\\n resolve(true);\\n }, time);\\n });\\n};\\n\\n// define actions for different message from backend\\nconst actions = (data: string, dispatch: ReturnType<typeof useAppDispatch>) => {\\n if (data === notificationMessage.FETCH_MAILCHAINS) {\\n snackbarUtils.info(\\"Fetching new mailchains\\");\\n } else if (data === notificationMessage.DISPLAY_CONNECTED) {\\n dispatch(applicationSlice.actions.updatePushNotificationState(true));\\n snackbarUtils.info(\\"Connected for push notification\\");\\n }\\n};\\n\\nexport default () => {\\n const dispatch = useAppDispatch();\\n const reconnectionTries = useRef(0);\\n useEffect(() => {\\n let sse: EventSource | null = null;\\n\\n // wait for 2 seconds, since backend will disconnect users once they refresh,\\n // avoid racing with backend:\\n // refresh -> login -> backend logout due to previous disconnection\\n sleep(2000).then(() => {\\n try {\\n sse = new EventSource(SERVER_SENT_EVENT_NOTIFICATION, {\\n withCredentials: true,\\n });\\n } catch (err) {\\n console.log(err);\\n }\\n\\n if (sse) {\\n sse.addEventListener(\\"message\\", (event) => {\\n const data = event.data as string;\\n console.log(\\"[data received]\\", data);\\n if (data.startsWith(\\"ERROR\\")) {\\n snackbarUtils.error(data);\\n }\\n actions(data, dispatch);\\n });\\n\\n sse.onerror = () => {\\n if (reconnectionTries.current < constant.SSE_MAX_RETRY_COUNT) {\\n reconnectionTries.current++;\\n console.log(\\"err event, retry\\");\\n } else {\\n if (sse) {\\n console.log(\\n ${reconnectionTries.current + 1}th attempt, close connection\\n );\\n sse.close();\\n }\\n }\\n };\\n }\\n });\\n\\n return () => {\\n if (sse) {\\n dispatch(applicationSlice.actions.updatePushNotificationState(false));\\n sse.close();\\n }\\n };\\n }, []);\\n};\\n\\n\\n##### Backend\\n\\nA fake webhook to push notification to all connected users:\\n\\njava\\n@GetMapping(value = \\"/push\\", produces = { MediaType.APPLICATION_JSON_VALUE })\\n@ResponseBody\\npublic Document pushNotification() {\\n var responses = new ArrayList<Document>();\\n Set<String> usersConnected = redisTemplate.opsForSet().members(\\"room\\");\\n for (String userName : usersConnected) {\\n String message = NotificationMessage.FETCH_MAILCHAINS;\\n var note = new Document();\\n note.append(\\"user\\", userName);\\n note.append(\\"message\\", message);\\n responses.add(note);\\n redisTemplate.opsForList().leftPush(userName, NotificationMessage.FETCH_MAILCHAINS);\\n }\\n var res = new Document();\\n res.append(\\"success\\", true);\\n res.append(\\"result\\", responses);\\n return res;\\n}\\n\\n\\nNext we implement the data-streaming request which also handles possible exceptions carefully:\\n\\njava\\n@Data\\nprivate class NotificationDataRef {\\n private String errMessage = null;\\n private String userName = null;\\n\\n public String getUserName() {\\n return \\"notification::\\" + this.userName;\\n }\\n}\\n@GetMapping(value = \\"/notification\\", produces = MediaType.TEXT_EVENT_STREAM_VALUE)\\n@ResponseBody\\npublic Flux<ServerSentEvent<String>> getEvents() throws InterruptedException {\\n final NotificationDataRef ref = new NotificationDataRef();\\n try {\\n var user = userService.getCurrentUser();\\n String userName = user.getString(\\"user_name\\");\\n\\n ref.setUserName(userName);\\n redisTemplate.opsForSet().add(\\"room\\", ref.getUserName());\\n logger.info(String.format(\\"User %s has connected\\", userName));\\n } catch (Exception err) {\\n String errMessage = err.getMessage();\\n ref.setErrMessage(\\"ERROR:\\" + errMessage);\\n logger.info(errMessage);\\n }\\n\\n return Flux.interval(Duration.ofSeconds(1))\\n .map((seq) -> {\\n if (ref.getErrMessage() != null) {\\n return ref.getErrMessage();\\n }\\n if (seq.equals(Long.valueOf(0))) {\\n return NotificationMessage.DISPLAY_CONNECTED;\\n }\\n String message = null;\\n try {\\n message = redisTemplate.opsForList().rightPop(ref.getUserName());\\n // the poped value will only be constant\\n // defined in NotificationMessage class.\\n } catch (Exception err) {\\n message = String.format(\\"ERROR:%s\\", err.getMessage());\\n }\\n return message == null ? \\"\\" : message;\\n })\\n .map(message -> ServerSentEvent.<String>builder()\\n .event(\\"message\\")\\n .data(message)\\n .build())\\n .takeUntil((event) -> {\\n String message = event.data();\\n Boolean errorExists = message.startsWith(\\"ERROR\\");\\n if (errorExists) {\\n logger.info(message);\\n }\\n return errorExists;\\n })\\n .doFinally(signalType -> {\\n redisTemplate.opsForSet().remove(\\"room\\", ref.getUserName());\\n redisTemplate.delete(ref.getUserName());\\n logger.info(String.format(\\"%s has disconnected\\", ref.getUserName()));\\n });\\n}\\n\\n\\n- When backend received a message started with ERROR, it will close the connection, in this way we handle the error gracefully.\\n- When frontend received a message started with ERROR, this message will be logged in the frontend.\\n- Due to closure of the connection from backend, our frontend will try serveral times and close() the connection when retry count reaches its maximum.\\n","title":"Server Sent Event in Java and Node.js Backend","date":"2023-09-03T00:00:00.000Z","id":"blog0157","tag":"java, springboot, express, SSE, nodejs","intro":"An introduction to how to effectively create SSE event mimicing the single-thread event loop adopted by nodejs in springboot, and how do we actually implement it in nodejs as well.","toc":true},{"content":"\\n#### What Approaches do we Have when it Comes to Search Engine?\\n\\nRecently I am responsible for building search functions in frontend. I come up with the following in my mind:\\n\\n- We sends **_everything_** we want from backend to frontend, and we may either use standard regular expression or dedicated library like Fuse.js or lunr.js to query for desired results. This works perfectly fine for static web pages (such as this blog).\\n\\n- We build Elastic Stack, such as Elastic Search and Kibana, which in essense also save results in Document and index the fields for searching the documents.\\n\\nAnd after struggling for tutorials in youtube, I came across:\\n\\n- We use **_Algolia_** by feeding our json files (wich consists of search targets) and setting the field names we want to use as search indexes.\\n\\n#### Code Implementation for Fuse.js\\n\\n##### Search Target\\n\\nFirst we build our blog.json file which serves as a search resource.\\n\\njson\\n[\\n ...\\n {\\n \\"content\\": \\"...\\"\\n \\"title\\": \\"Write Middleware in Redux-Toolkit\\",\\n \\"date\\": \\"2023-06-20T00:00:00.000Z\\",\\n \\"id\\": \\"blog0132\\",\\n \\"tag\\": \\"react\\",\\n \\"intro\\": \\"We list sample usage of ...\\"\\n \\"toc\\": true\\n },\\n ...\\n]\\n\\n\\n##### Build a blog.json which Contains Documents to Search\\n\\nIn my case I use the following script:\\n\\njs\\nimport fs from \\"fs\\";\\nimport matter from \\"gray-matter\\";\\nimport path from \\"path\\";\\n\\nconst mdDirs = [\\"./src/mds/articles/tech\\", \\"./src/mds/articles/math\\"];\\n\\nconst getAllMdFilePaths = (dir: string) => {\\n const mdFiles: string[] = [];\\n\\n const getFiles = (dir: string) => {\\n const paths = fs.readdirSync(dir);\\n paths.forEach((p) => {\\n const newPath = path.join(${dir}/${p});\\n const pathStat = fs.statSync(newPath);\\n if (pathStat.isDirectory()) {\\n getFiles(newPath);\\n } else {\\n if (newPath.endsWith(\\".md\\")) {\\n mdFiles.push(newPath);\\n }\\n }\\n });\\n };\\n\\n getFiles(dir);\\n return mdFiles;\\n};\\n\\nconst writeMdInJson = () => {\\n const targetPaths = \\"./src/mds/blog.json\\";\\n const blogJson: any[] = [];\\n for (const dirpath of mdDirs) {\\n const mdpaths = getAllMdFilePaths(dirpath);\\n mdpaths.forEach((path) => {\\n const mdText = fs.readFileSync(path, { encoding: \\"utf8\\", flag: \\"r\\" });\\n const { data, content } = matter(mdText);\\n const { wip = false } = data;\\n if (!wip) {\\n blogJson.push({ content, ...data });\\n }\\n });\\n }\\n fs.writeFileSync(\\n targetPaths,\\n JSON.stringify(blogJson, null, 0)\\n .replace(/(\\\\\\\\r\\\\\\\\n)/g, \\" \\")\\n .replace(//g, \\"\\")\\n .replace(/\\\\s+/g, \\" \\")\\n );\\n};\\n\\nconst main = () => {\\n writeMdInJson();\\n};\\n\\nmain();\\n\\n\\n##### Search Component\\n\\n###### Fuse.js (Deprecated as the result is not satisfactory)\\n\\nNext in the our search component:\\n\\ntsx\\nimport searchJson from \\"../../../mds/blog.json\\";\\n\\nexport default function SearchComponent() {\\n const fuse = useRef(\\n new Fuse(searchJson, {\\n keys: [\\"content\\", \\"tag\\", \\"tags\\", \\"title\\", \\"intro\\"],\\n threshold: config.fuzzySearchThreshold,\\n })\\n );\\n const [searchResults, setSearchedResults] = useState<\\n { title: string; intro: string; tag: string; tags: string }[]\\n >([]);\\n const [searchText, setSearchText] = useState(\\"\\");\\n const searchBarRef = useRef<HTMLInputElement>(null);\\n\\n const handleSearchChange = debounce((e: ChangeEvent<HTMLInputElement>) => {\\n setSearchText(e.target.value);\\n const searchValue = e.target.value;\\n if (searchValue) {\\n const result = fuse.current.search(searchValue);\\n setSearchedResults(\\n result.map((r) => {\\n const { title, intro, tag, tags } = r.item;\\n return { title, intro, tag: tag || \\"\\", tags: tags || \\"\\" };\\n })\\n );\\n } else {\\n setSearchedResults([]);\\n }\\n }, 300);\\n\\n return (\\n <SearchBar\\n placeholder=\\"Tag, title or content\\"\\n onChange={handleSearchChange}\\n inputRef={searchBarRef}\\n />\\n );\\n}\\n\\n\\n- The Fuse object can be created anywhere and imported into the component.\\n- In my case I simply use useRef as it is going to be aways static and unchanged in the life cycle of the SearchComponent.\\n\\n###### lunr.js, A much more Powerful Version of Fuse.js\\n\\nThe implementation is very similar to Fuse.js:\\n\\njs\\nexport default function SearchComponent() {\\n const [searchResults, setSearchedResults] = useState<\\n { title: string; intro: string; tag: string; tags: string }[]\\n >([]);\\n const [searchText, setSearchText] = useState(\\"\\");\\n const searchBarRef = useRef<HTMLInputElement>(null);\\n const lunrSearch = useRef<lunr.Index | null>(null);\\n const searchMapping = useRef<{\\n [id: string]: {\\n content: string,\\n title: string,\\n intro: string,\\n tag: string,\\n tags: string\\n }\\n }>({});\\n\\n useState(() => {\\n lunrSearch.current = lunr(function () {\\n this.field(\\"tag\\");\\n this.field(\\"tags\\");\\n this.field(\\"title\\");\\n this.field(\\"intro\\");\\n this.field(\\"content\\");\\n\\n console.log(\\"indexing ...\\");\\n\\n (searchJson as { content: string, title: string, date: string, id: string, tag?: string, tags?: string, intro: string, toc: boolean }[]).forEach(\\n (searchTarget, index) => {\\n const id = index.toString();\\n const { intro, tag = \\"\\", tags = \\"\\", title, content } = searchTarget;\\n const searchJson = { intro, tag, tags, title, content };\\n searchMapping.current[id] = searchJson\\n this.add({ ...searchJson, id })\\n }\\n );\\n })})\\n\\n const handleSearchChange = debounce((e: ChangeEvent<HTMLInputElement>) => {\\n setSearchText(e.target.value);\\n const searchValue = e.target.value;\\n if (searchValue) {\\n const result = lunrSearch?.current?.search(searchValue);\\n const displayResult = result?.sort((r1, r2) => r2.score - r1.score).map(r => {\\n const { ref } = r;\\n const doc = searchMapping.current?.[ref];\\n // we dont\' need to return content in the search field\\n return {\\n intro: doc.intro,\\n tag: doc.tag,\\n tags: doc.tags,\\n title: doc.title\\n }\\n }) || [];\\n setSearchedResults(displayResult);\\n }\\n else {\\n setSearchedResults([]);\\n }\\n }, 300);\\n\\n return (\\n <SearchBar\\n placeholder=\\"Tag, title or content\\"\\n onChange={handleSearchChange}\\n inputRef={searchBarRef}\\n />\\n );\\n}\\n\\n\\n#### Code Implementation for Algolia\\n\\n##### Backend Using Java\\n\\n###### Responsibilities of Backend in Using Algolia\\n\\nOur backend will take the following tasks:\\n\\n- Provide ALGOLIA_SEARCH_INDEX\\n\\n ![](/assets/tech/158/001.png)\\n\\n- Provide applicationID\\n- Provide frontend client with searchApiKey\'s with differnent priviledges for searching, for example:\\n - Admin users can search everything\\n - Users of some organization can only search their own related remails\\n- Upload searchable targets (named Record) to Algolia database\\n- Add new search item into algolia when needed (like emails)\\n\\n###### Dependencies\\n\\nAfter registering an account in Algolia and creating an application there, we include the following two dependencies:\\n\\nxml\\n<dependency>\\n <groupId>com.algolia</groupId>\\n <artifactId>algoliasearch-core</artifactId>\\n <version>3.16.5</version>\\n</dependency>\\n<dependency>\\n <groupId>com.algolia</groupId>\\n <artifactId>algoliasearch-java-net</artifactId>\\n <version>3.16.5</version>\\n</dependency>\\n\\n\\n###### Record Object\\n\\n- Algolia requires users define a Record object which at least contains a non-nullable field called objectID.\\n- Luckily we use mongodb in our java backend, we simply use a stringified _id and we use a modelMapper.map() to take a Document object into our desired Record object:\\n\\njava\\npackage com.organization.web.service.dto;\\n\\nimport java.util.List;\\nimport lombok.Data;\\n\\n@Data\\npublic class EmailChainRecord {\\n\\n @Data\\n public static class Supplier {\\n private List<String> material_manu_internal_codes;\\n }\\n\\n @Data\\n public static class NameField {\\n private String name;\\n }\\n\\n @Data\\n public static class EmailField {\\n private String body;\\n private List<String> participant_emails;\\n }\\n\\n @Data\\n public static class SenderInDb {\\n private Integer id;\\n private String user_name;\\n private String first_name;\\n private String last_name;\\n private String email;\\n }\\n\\n @Data\\n public static class Task {\\n private String code;\\n private String name;\\n }\\n\\n @Data\\n public static class Section {\\n private String name;\\n private List<Task> tasks;\\n }\\n\\n @Data\\n public static class ProgramDetail {\\n private String prog_ref_no;\\n private String name;\\n private List<Section> sections;\\n }\\n\\n private String oid;\\n private String objectID;\\n private String title;\\n private String buyer_company_code;\\n private String latest_gmail_snippet;\\n private List<String> sender_emails;\\n private List<SenderInDb> sendersInDb;\\n private NameField buyerCompanyDetail;\\n private NameField projectDetail;\\n private List<ProgramDetail> programmesDetail;\\n private List<EmailField> emails_body;\\n private List<String> participant_emails;\\n}\\n\\n\\n###### SearchIndex Object\\n\\nIn both frontend and backend, the major api calls are all managed by the SearchIndex object:\\n\\njava\\npackage com.organization.web.algolia;\\n\\nimport org.springframework.beans.factory.annotation.Value;\\nimport org.springframework.context.annotation.Bean;\\nimport org.springframework.stereotype.Service;\\n\\nimport com.algolia.search.DefaultSearchClient;\\nimport com.algolia.search.SearchClient;\\nimport com.algolia.search.SearchIndex;\\nimport com.organization.web.controller.err.CustomException;\\nimport com.organization.web.service.dto.EmailChainRecord;\\n\\n@Service\\npublic class Algolia {\\n @Value(\\"${algolia.application.id}\\")\\n private String applicationID;\\n @Value(\\"${algolia.api.key}\\")\\n private String APIKEY;\\n\\n @Bean\\n public SearchClient getSearchClient() throws CustomException {\\n if (this.applicationID == null || this.APIKEY == null) {\\n throw new CustomException(\\"application id and apikey cannot be null for algolia\\");\\n }\\n return DefaultSearchClient.create(this.applicationID, this.APIKEY);\\n }\\n\\n @Bean\\n public SearchIndex<EmailChainRecord> getIndex() throws CustomException {\\n SearchClient client = getSearchClient();\\n var initedIndex = client.initIndex(\\"correspondence\\", EmailChainRecord.class);\\n return initedIndex;\\n }\\n}\\n\\n\\n###### SearchService: All the Utility Functions\\n\\n**Contructor Injection.** To facilitate unit testing, we use autowired constructor injection:\\n\\njava-1\\npackage com.organization.web.service.impl;\\n\\nimport com.algolia.search.SearchClient;\\nimport com.algolia.search.SearchIndex;\\nimport com.algolia.search.models.apikeys.SecuredApiKeyRestriction;\\nimport com.algolia.search.models.indexing.Query;\\nimport com.algolia.search.models.settings.IndexSettings;\\nimport com.mongodb.client.MongoCollection;\\nimport com.mongodb.client.model.Filters;\\nimport com.organization.web.controller.codes.UserRoles;\\nimport com.organization.web.controller.err.CustomException;\\nimport com.organization.web.mongodb.CollectionNames;\\nimport com.organization.web.mongodb.MongoDB;\\nimport com.organization.web.mongodb.MongoDB.JsonPipeline;\\nimport com.organization.web.service.SearchService;\\nimport com.organization.web.service.dto.EmailChainRecord;\\nimport com.organization.web.service.dto.EmailChainRecord.ProgramDetail;\\n\\nimport java.util.ArrayList;\\nimport java.util.Arrays;\\nimport java.util.List;\\nimport java.util.stream.Collectors;\\n\\nimport org.apache.commons.collections4.ListUtils;\\nimport org.bson.Document;\\nimport org.bson.types.ObjectId;\\nimport org.modelmapper.ModelMapper;\\nimport org.springframework.beans.factory.annotation.Autowired;\\nimport org.springframework.stereotype.Service;\\nimport org.springframework.beans.factory.annotation.Value;\\n\\n@Service\\npublic class SearchServiceImpl implements SearchService {\\n\\n @Value(\\"${algolia.public.search.api.key}\\")\\n private String publicSearchAPIKey;\\n\\n private MongoDB mongodb;\\n private ModelMapper modelMapper = new ModelMapper();\\n private SearchIndex<EmailChainRecord> index;\\n private SearchClient searchClient;\\n // A search key that you keep private\\n\\n @Autowired\\n public SearchServiceImpl(\\n MongoDB mongodb,\\n ModelMapper modelMapper,\\n SearchIndex<EmailChainRecord> index,\\n SearchClient searchClient) {\\n this.mongodb = mongodb;\\n this.modelMapper = modelMapper;\\n this.index = index;\\n this.searchClient = searchClient;\\n }\\n\\n public void clearObjects() {\\n this.index.clearObjects();\\n }\\n\\n\\n**Insert Data Into Algolia.**\\n\\njava-59\\n public void insertEmailsIntoAlgolia() {\\n clearObjects();\\n ...\\n var searchDocuments = someCollection\\n .aggregate(somePipeline)\\n .map(u -> {\\n return modelMapper.map(u, EmailChainRecord.class);\\n })\\n .forEach(u -> {\\n // refine data in u for search logic\\n })\\n .into(new ArrayList<>());\\n if (searchDocuments != null) {\\n this.index.saveObjects(searchDocuments).waitTask();\\n }\\n }\\n\\n\\n**Define Attributes that Contributes to the Search.**\\n\\njava-75\\n public void setKeyAndFacetsForQueryAndFilter() {\\n var indexSettings = new IndexSettings();\\n\\n List<String> attributes = Arrays.asList(\\n \\"latest_gmail_snippet\\",\\n \\"sender_emails\\",\\n \\"projectDetail.name\\",\\n \\"searchabletitle\\",\\n \\"title\\",\\n \\"projectDetail.name\\",\\n \\"senderInDb.user_name\\",\\n \\"senderInDb.first_name\\",\\n \\"senderInDb.last_name\\",\\n \\"programmesDetail.name\\",\\n \\"programmesDetail.sections.tasks.code\\",\\n \\"buyerCompanyDetail.name\\",\\n \\"emails_body.body\\",\\n \\"emails_body.participant_emails\\",\\n \\"participant_emails\\");\\n indexSettings.setSearchableAttributes(attributes);\\n\\n\\n**Define Facets (configs to the search keys)**\\n\\njava-95\\n List<String> filterFacets = Arrays.asList(\\n \\"filterOnly(participant_emails)\\",\\n \\"filterOnly(emails_body.participant_emails)\\");\\n\\n\\njava-98\\n List<String> searchFacets = attributes.stream()\\n .map(key -> String.format(\\"searchable(%s)\\", key))\\n .collect(Collectors.toList());\\n\\n\\n**Add the Facets into Index Settings.** ListUtils.union is the same as arr1 + arr2 in python:\\n\\njava-101\\n indexSettings.setAttributesForFaceting(\\n ListUtils.union(searchFacets, filterFacets));\\n\\n this.index.setSettings(indexSettings);\\n }\\n\\n\\n**Impose Restrictions to Search Api Key.**\\n\\njava-107\\n public String createSearchAPIKey(Document user) throws Exception {\\n List<String> roles = user.getList(\\"roles\\", String.class);\\n\\n if (roles.contains(UserRoles.MANAGER) || roles.contains(UserRoles.STAFF)) {\\n return this.publicSearchAPIKey;\\n }\\n\\n String userName = user.getString(\\"user_name\\");\\n SecuredApiKeyRestriction restriction = new SecuredApiKeyRestriction()\\n .setQuery(new Query().setFilters(String.format(\\n \\"participant_emails:%s OR emails_body.participant_emails:%s\\",\\n userName,\\n userName)));\\n\\n String publicKey = this.searchClient.generateSecuredAPIKey(\\n this.publicSearchAPIKey,\\n restriction);\\n\\n return publicKey;\\n }\\n\\n\\n**Save a Record into Algolia.**\\n\\njava-127\\n public void saveObject(ObjectId someId) throws CustomException {\\n // logics to fetch search targets\\n\\n EmailChainRecord record = modelMapper.map(\\n targetMailchain,\\n EmailChainRecord.class);\\n\\n if (record != null) {\\n this.index.partialUpdateObject(record);\\n }\\n }\\n}\\n\\n\\n**Remark.** From [documentation](https://www.algolia.com/doc/api-reference/api-methods/partial-update-objects/?client=java) if a record exists in your database but does not exist in algolia, then:\\n\\nnone\\nIf the objectID is specified but doesn\u2019t exist, Algolia creates a new record\\n\\n\\nThat means an upsert operation is automatic.\\n\\n##### Frontend\\n\\n###### Responsibility of Frontend\\n\\nThe frontend needs to\\n\\n- Get applicationID and searchApiKey from backend\\n- Call the search api to get\\n - target document\\n - searchable facets for search suggestions.\\n\\n###### Frontend Implementation in React\\n\\n- Algolia provides us with an npm package: react-instantsearch.\\n- However, if we use the UI component provided by that library, we will quickly use up our free quota for the api.\\n- It is because the change handler in the provided searchbar is intentionally designed not to have any debounce rule.\\n\\n- Instead we create our own search component (with <input/>) and use debounced onChange handler with the following search<T> function.\\n\\njs\\nexport default class AlgoliaUtil {\\n\\tpublic static instance: AlgoliaUtil | undefined;\\n\\tpublic algoliaEnabled: boolean | undefined;\\n\\tprivate algoliaSearchIndex: string | undefined;\\n\\tprivate searchClient: SearchClient | undefined;\\n\\tprivate searchIndex: SearchIndex | undefined;\\n\\n\\tconstructor(props: { applicationID: string, apiKey: string, initIndex: string, algoliaEnabled: boolean }) {\\n\\t\\tthis.algoliaEnabled = props.algoliaEnabled;\\n\\t\\tthis.algoliaSearchIndex = props.initIndex;\\n\\t\\tthis.searchClient = algoliasearch(\\n\\t\\t\\tprops.applicationID,\\n\\t\\t\\tprops.apiKey,\\n\\t\\t);\\n\\t}\\n\\n\\tpublic static getInstance() {\\n\\t\\tif (!AlgoliaUtil.instance) {\\n\\t\\t\\tthrow new Error(\\"An algolia instance has not been instantiated yet.\\")\\n\\t\\t}\\n\\t\\treturn AlgoliaUtil.instance;\\n\\t}\\n\\n\\tprivate getSearchClient(): SearchClient {\\n\\t\\tif (!this.searchClient) {\\n\\t\\t\\tthrow new Error(\\"Search Client is undefined\\");\\n\\t\\t}\\n\\t\\treturn this.searchClient;\\n\\t}\\n\\n\\tprivate getIndex() {\\n\\t\\tif (!this.searchIndex) {\\n\\t\\t\\tconst searchClient = this.getSearchClient();\\n\\t\\t\\tif (this.algoliaSearchIndex) {\\n\\t\\t\\t\\tthis.searchIndex = searchClient.initIndex(this.algoliaSearchIndex);\\n\\t\\t\\t}\\n\\t\\t}\\n\\t\\treturn this.searchIndex;\\n\\t}\\n\\n\\tpublic search<T>(params: { queryString: string, attributesToRetrieve: Extract<keyof T, string>[] }) {\\n\\t\\tconst { attributesToRetrieve, queryString } = params;\\n\\t\\tconst index = this.getIndex();\\n\\t\\treturn index?.search(queryString, {\\n\\t\\t\\tattributesToRetrieve, facets: constant.FACETS_TO_RECEIVE\\n\\t\\t});\\n\\t}\\n}\\n\\n\\nWe instantiate AlgoliaUtil object when some page is rendered. Sometimes when search feature is not ready yet, and we determine whether algolia is available by setting:\\n\\njs\\nuseEffect(() => {\\n if (dialogOpen) {\\n const enabled = AlgoliaUtil.getInstance().algoliaEnabled;\\n setAlgoliaEnabled(enabled || false);\\n }\\n}, [dialogOpen]);\\n\\n\\n- Here the type T in search<T> is simply the target attribute to retrieve. In our case, we use T = { oid: string }.\\n\\n- Also:\\n js\\n constant.FACETS_TO_RECEIVE = [\\n \\"title\\",\\n \\"latest_gmail_snippet\\",\\n \\"programmesDetail.name\\",\\n \\"emails_body.body\\",\\n \\"projectDetail.name\\"\\n ],\\n \\n are the results that were hit in the past, they are used as search suggestions.\\n","title":"Build a Search Function","date":"2023-07-26T00:00:00.000Z","id":"blog0158","tag":"java, react, algolia, fusejs, searching","intro":"We introduce a service called Algolia which provides an easy search engine integration that helps build quick and accurate search functionality.","toc":true},{"content":"\\njs\\n// useOutsideClicked.ts\\n\\nimport { RefObject, useEffect, useState } from \\"react\\";\\n\\nexport default ({ ref }: { ref: RefObject<Element> }) => {\\n const [outsideClicked, setOutsideClicked] = useState(true);\\n useEffect(() => {\\n function handleClickOutside(event: any) {\\n if (ref.current && !ref.current.contains(event.target)) {\\n setOutsideClicked(true);\\n } else {\\n setOutsideClicked(false);\\n }\\n }\\n document.addEventListener(\\"mousedown\\", handleClickOutside);\\n return () => {\\n document.removeEventListener(\\"mousedown\\", handleClickOutside);\\n };\\n }, []);\\n return { outsideClicked, setOutsideClicked };\\n};\\n\\n\\nNow we can customize the behaviour of our target component by using\\n\\n- the state outsideClicked and\\n- setter setOutsideClicked.\\n\\nA simple use case is a searchbar with dropdown.\\n","title":"Detect Click Outside","date":"2023-07-27T00:00:00.000Z","id":"blog0159","tag":"react","intro":"Record a hook for determining whether click have happened outside of our target dom element.","toc":false},{"content":"\\n#### Effect\\n\\n<Center>\\n<img src=\\"/assets/tech/160/001.png\\">\\n</Center>\\n\\n#### Usage\\n\\nts\\n<ListUpdateSelector\\n defaultSelectionStrings={[]}\\n allSelectionStrings={[\\"selection1\\", \\"selection2\\", \\"selection3\\"]}\\n optionChangeHandler={(selectedValues) => {\\n someRef.current = selectedValues;\\n }}\\n/>\\n\\n\\n#### Code Implementation\\n\\nts\\nimport CheckIcon from \\"@mui/icons-material/Check\\";\\nimport CloseIcon from \\"@mui/icons-material/Close\\";\\nimport { autocompleteClasses } from \\"@mui/material/Autocomplete\\";\\nimport { styled } from \\"@mui/material/styles\\";\\nimport useAutocomplete, {\\n AutocompleteGetTagProps,\\n} from \\"@mui/material/useAutocomplete\\";\\nimport { CSSProperties, useEffect, useRef } from \\"react\\";\\n\\nconst Root = styled(\\"div\\")(\\n ({ theme }) => \\n color: ${\\n theme.palette.mode === \\"dark\\" ? \\"rgba(255,255,255,0.65)\\" : \\"rgba(0,0,0,.85)\\"\\n };\\n font-size: 14px;\\n\\n);\\n\\nconst Label = styled(\\"label\\")\\n padding: 0 0 4px;\\n line-height: 1.5;\\n display: block;\\n;\\n\\nconst InputWrapper = styled(\\"div\\")(\\n ({ theme }) => \\n border-radius: 4px;\\n flex-wrap: wrap;\\n\\n &:hover {\\n border-color: ${theme.palette.mode === \\"dark\\" ? \\"#177ddc\\" : \\"#40a9ff\\"};\\n }\\n\\n & div {\\n display: flex;\\n justify-content: space-between;\\n }\\n\\n & input {\\n background-color: ${theme.palette.mode === \\"dark\\" ? \\"#141414\\" : \\"#fff\\"};\\n color: ${\\n theme.palette.mode === \\"dark\\"\\n ? \\"rgba(255,255,255,0.65)\\"\\n : \\"rgba(0,0,0,.85)\\"\\n };\\n height: 30px;\\n box-sizing: border-box;\\n padding: 4px 6px;\\n width: 1px;\\n border: 1px solid rgba(0, 0, 0, 0.1);\\n border-radius: 2px;\\n min-width: 30px;\\n flex-grow: 1;\\n margin: 0;\\n outline: 0;\\n width: 100%;\\n margin: 0px;\\n margin-top: 4px;\\n }\\n\\n);\\n\\ninterface TagProps extends ReturnType<AutocompleteGetTagProps> {\\n label: string;\\n}\\n\\nfunction Tag(props: TagProps) {\\n const { label, onDelete, ...other } = props;\\n return (\\n <div {...other}>\\n <span>{label}</span>\\n <CloseIcon onClick={onDelete} />\\n </div>\\n );\\n}\\n\\nconst StyledTag = styled(Tag)<TagProps>(\\n ({ theme }) => \\n display: flex;\\n align-items: center;\\n height: 24px;\\n margin: 2px 0px;\\n line-height: 22px;\\n background-color: ${\\n theme.palette.mode === \\"dark\\" ? \\"rgba(255,255,255,0.08)\\" : \\"#fafafa\\"\\n };\\n border: 1px solid ${theme.palette.mode === \\"dark\\" ? \\"#303030\\" : \\"#e8e8e8\\"};\\n border-radius: 2px;\\n box-sizing: content-box;\\n padding: 0 4px 0 10px;\\n outline: 0;\\n overflow: hidden;\\n\\n &:focus {\\n border-color: ${theme.palette.mode === \\"dark\\" ? \\"#177ddc\\" : \\"#40a9ff\\"};\\n background-color: ${theme.palette.mode === \\"dark\\" ? \\"#003b57\\" : \\"#e6f7ff\\"};\\n }\\n\\n & span {\\n overflow: hidden;\\n white-space: nowrap;\\n text-overflow: ellipsis;\\n }\\n\\n & svg {\\n font-size: 12px;\\n cursor: pointer;\\n padding: 4px;\\n }\\n\\n);\\n\\nconst Listbox = styled(\\"ul\\")(\\n ({ theme }) => \\n width: 300px;\\n margin: 2px 0 0;\\n padding: 0;\\n position: absolute;\\n top: calc(100% + 5px);\\n list-style: none;\\n background-color: ${theme.palette.mode === \\"dark\\" ? \\"#141414\\" : \\"#fff\\"};\\n overflow: auto;\\n max-height: 250px;\\n border-radius: 4px;\\n box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15);\\n z-index: 3;\\n\\n\\n & li {\\n padding: 5px 12px;\\n display: flex;\\n\\n & span {\\n flex-grow: 1;\\n }\\n\\n & svg {\\n color: transparent;\\n }\\n }\\n\\n & li[aria-selected=\'true\'] {\\n background-color: ${theme.palette.mode === \\"dark\\" ? \\"#2b2b2b\\" : \\"#fafafa\\"};\\n font-weight: 600;\\n\\n & svg {\\n color: #1890ff;\\n }\\n }\\n\\n & li.${autocompleteClasses.focused} {\\n background-color: ${theme.palette.mode === \\"dark\\" ? \\"#003b57\\" : \\"#e6f7ff\\"};\\n cursor: pointer;\\n\\n & svg {\\n color: currentColor;\\n }\\n }\\n\\n);\\n\\nexport default function ListUpdateSelector({\\n defaultSelectionStrings,\\n allSelectionStrings,\\n optionChangeHandler,\\n style = {},\\n inputStyle = {},\\n}: {\\n defaultSelectionStrings: string[];\\n allSelectionStrings: string[];\\n optionChangeHandler: (option: string[]) => void;\\n style?: CSSProperties;\\n inputStyle?: CSSProperties;\\n}) {\\n const selections = allSelectionStrings;\\n\\n const {\\n getRootProps,\\n getInputLabelProps,\\n getInputProps,\\n getTagProps,\\n getListboxProps,\\n getOptionProps,\\n groupedOptions,\\n value,\\n focused,\\n setAnchorEl,\\n } = useAutocomplete({\\n id: \\"selector-hook\\",\\n defaultValue: defaultSelectionStrings,\\n multiple: true,\\n options: selections,\\n getOptionLabel: (option) => option,\\n });\\n\\n const optionChangeHandlerTakesEffect = useRef(false);\\n\\n useEffect(() => {\\n // prevent handler is called on the first render.\\n if (optionChangeHandlerTakesEffect.current) {\\n optionChangeHandler(value);\\n } else {\\n optionChangeHandlerTakesEffect.current = true;\\n }\\n }, [value]);\\n\\n return (\\n <Root>\\n <div\\n className=\\"user-row-selector\\"\\n style={{ position: \\"relative\\", ...style }}\\n >\\n <div {...getRootProps()}>\\n <InputWrapper ref={setAnchorEl} className={focused ? \\"focused\\" : \\"\\"}>\\n {value.map((option: string, index: number) => (\\n <span title={option}>\\n <StyledTag label={option} {...getTagProps({ index })} />\\n </span>\\n ))}\\n <input {...getInputProps()} style={inputStyle} />\\n </InputWrapper>\\n </div>\\n {groupedOptions.length > 0 ? (\\n <Listbox {...getListboxProps()}>\\n {(groupedOptions as string[]).map((option, index) => (\\n <li {...getOptionProps({ option, index })}>\\n <span>{option}</span>\\n <CheckIcon fontSize=\\"small\\" />\\n </li>\\n ))}\\n </Listbox>\\n ) : null}\\n </div>\\n </Root>\\n );\\n}\\n\\n","title":"Multi-Selections","date":"2023-08-03T00:00:00.000Z","id":"blog0160","tag":"react","intro":"Record a multi selection component.","toc":true},{"content":"\\n#### Simple Cases\\n\\n- json-to-table is a simple library which converts nested json object into a table view. However, the library does not provide type defitions, so we need to declare it on our own:\\n\\n ts\\n // dess.d.ts\\n\\n declare module \\"json-to-table\\" {\\n function jsonToTable(jsonData: object): string[][];\\n export default jsonToTable;\\n }\\n \\n\\n- Sometimes an import is simply a string:\\n\\n ts\\n declare module \\"*.pdf\\" {\\n const src: string;\\n export default src;\\n }\\n \\n\\n- Sometimes a library have export and export default:\\n ts\\n declare module \\"react-images\\" {\\n export var Modal;\\n export var ModalGateway;\\n export default Carousel;\\n }\\n \\n\\nRemember to have\\n\\njson\\n \\"esModuleInterop\\": true,\\n\\n\\nin tsconfig.json for the above to work.\\n\\n\\n#### More Complete Example\\n\\n##### Objective \\n\\nWe try to type the 3rd party library:\\n\\n- https://www.npmjs.com/package/excel4node\\n\\n##### typing/desc.d.ts\\n\\njs\\ndeclare module \\"excel4node\\" {\\n export class Worksheet {\\n column(col: number): Worksheet;\\n setWidth(width: number): Worksheet;\\n cell(row: number, col: number): Worksheet;\\n string(text: string): Worksheet;\\n titleStyle(style: any): Worksheet;\\n style(style: any): Worksheet;\\n link(link: string): Worksheet;\\n number(num: number): Worksheet;\\n addImage(config: any): Worksheet;\\n row(rowNum: number): Worksheet;\\n setHeight(height: number): Worksheet;\\n }\\n export class Workbook {\\n constructor();\\n addWorksheet(path: string | undefined): Worksheet;\\n createStyle(style: any): any;\\n writeToBuffer(): Promise<Buffer>;\\n }\\n}\\n\\n\\n##### The tsconfig.json\\n\\nNote that we must provide typeRoots for typescript to find the custom type-definition file desc.d.ts.\\n\\nAlso we don\'t need to provide ./node_modules/@types/ in typeRoots as it must be included by default.\\n\\n\\njson\\n{\\n \\"compilerOptions\\": {\\n \\"target\\": \\"ES2016\\",\\n \\"lib\\": [\\n \\"es6\\",\\n \\"dom\\"\\n ],\\n \\"outDir\\": \\"dist\\",\\n \\"allowJs\\": true,\\n \\"skipLibCheck\\": true,\\n \\"noImplicitAny\\": true,\\n \\"esModuleInterop\\": true,\\n \\"allowSyntheticDefaultImports\\": true,\\n \\"strict\\": true,\\n \\"strictNullChecks\\": true,\\n \\"forceConsistentCasingInFileNames\\": true,\\n \\"noFallthroughCasesInSwitch\\": true,\\n \\"module\\": \\"CommonJS\\",\\n \\"moduleResolution\\": \\"node\\",\\n \\"resolveJsonModule\\": true,\\n \\"isolatedModules\\": false,\\n \\"noEmit\\": false,\\n \\"typeRoots\\": [\\n \\"./typing/\\",\\n ]\\n },\\n \\"include\\": [\\n \\"src/**/*\\"\\n ],\\n \\"exclude\\": [\\n \\"node_modules\\"\\n ]\\n}\\n\\n","title":"Declare Types for Non-typed 3rd Party Library","date":"2024-02-24T00:00:00.000Z","id":"blog0161","tag":"react","intro":"Record some exmaple how to declare types from library that is untyped.","toc":true},{"content":"\\n<center></center>\\n\\nts\\n// snackbarUtils.ts\\n\\nimport { ProviderContext, VariantType, useSnackbar } from \\"notistack\\";\\n\\nlet useSnackbarRef: ProviderContext;\\n\\nexport const SnackbarUtilsConfigurator = () => {\\n useSnackbarRef = useSnackbar();\\n return null;\\n};\\n\\nconst option: Parameters<typeof useSnackbarRef.enqueueSnackbar>[1] = {\\n preventDuplicate: true,\\n autoHideDuration: 10000,\\n};\\n\\nexport default {\\n success(msg: string) {\\n useSnackbarRef.enqueueSnackbar(msg, { variant: \\"success\\", ...option });\\n },\\n warning(msg: string) {\\n useSnackbarRef.enqueueSnackbar(msg, { variant: \\"warning\\", ...option });\\n },\\n info(msg: string) {\\n useSnackbarRef.enqueueSnackbar(msg, { variant: \\"info\\", ...option });\\n },\\n error(msg: string) {\\n useSnackbarRef.enqueueSnackbar(msg, { variant: \\"error\\", ...option });\\n },\\n toast(msg: string, variant: VariantType = \\"default\\") {\\n useSnackbarRef.enqueueSnackbar(msg, { variant, ...option });\\n },\\n};\\n\\n\\nNext in our file wrapping the <App/> element, we add:\\n\\nts\\nimport { SnackbarProvider } from \'notistack\';\\n\\n ...\\n <SnackbarProvider\\n maxSnack={2}\\n anchorOrigin={{\\n horizontal: \'left\',\\n vertical: \\"bottom\\"\\n }}\\n >\\n <SnackbarUtilsConfigurator />\\n ...\\n <App />\\n </SnackbarProvider>\\n\\n","title":"Snackbar Utils","date":"2023-08-10T00:00:00.000Z","id":"blog0162","tag":"react","intro":"Record a configuraton for snackbar utils.","toc":false},{"content":"\\n<style>\\nimg {\\n width: 100%;\\n}\\n</style>\\n\\n#### Usage and Examples\\n\\nConsider the following path:\\n\\nnone\\nhttp://localhost:3000/#/buyer/order/6347b89b67762f48a700d4be/contract/64d7912d33184f49a6346f20\\n\\n\\nWe print the result in chrome debug console:\\n\\n<Center>\\n<a href=\\"/assets/tech/163/001.png\\" taget=\\"_blank\\">\\n <img src=\\"/assets/tech/163/001.png\\"/>\\n</a>\\n</Center>\\n\\n#### Code Implmentation\\n\\nts\\n// usePathUtils.ts\\n\\nimport { useLocation } from \\"react-router-dom\\";\\n\\nexport default () => {\\n const { pathname } = useLocation();\\n\\n const paramRightAfter = (rightAfterKeyword: string) => {\\n const matchingRegex = new RegExp(\\n (?<=${rightAfterKeyword}).*?(?=($|\\\\/)),\\n \\"g\\"\\n );\\n return pathname.match(matchingRegex)?.[0] || \\"\\";\\n };\\n const getPathUpto = (stopKeyword: string) => {\\n const matchingRegex = new RegExp(^.*${stopKeyword}, \\"g\\");\\n return pathname.match(matchingRegex)?.[0] || \\"\\";\\n };\\n\\n return { paramRightAfter, getPathUpto };\\n};\\n\\n\\nNext for convenience we create another hook so that instead of calling paramRightAfter and putting the desired string as param sporadically, we have a single source of truth:\\n\\nts\\n// useGetPathParams.ts\\n\\nimport usePathUtils from \\"./usePathUtils\\";\\n\\nexport default () => {\\n const { paramRightAfter } = usePathUtils();\\n\\n return {\\n projectOid: paramRightAfter(\\"/order/\\"),\\n programmeOid: paramRightAfter(\\"/contract/\\"),\\n section: paramRightAfter(\\"/order/.*?/\\"),\\n };\\n};\\n\\n\\nBut any change in the path may trigger rerender for components using this hook which may just use projectId but not section. Therefore we may consider **_putting path params in redux_** and let redux control which component to rerender.\\n\\n#### Ultimate Version\\n\\nWe invoke the following hook at the first routing component (where we have useLocation hook):\\n\\njs\\n//useGenPathParams.ts\\n\\nimport { useEffect } from \\"react\\";\\nimport { useLocation } from \\"react-router-dom\\";\\nimport { useAppDispatch } from \\"../redux/app/hook\\";\\nimport applicationSlice from \\"../redux/slice/applicationSlice\\";\\nimport usePathUtils from \\"./usePathUtils\\";\\n\\nexport default () => {\\n const { pathname } = useLocation();\\n const dispatch = useAppDispatch();\\n const { paramRightAfter } = usePathUtils();\\n\\n useEffect(() => {\\n dispatch(\\n applicationSlice.actions.updatePathParams({\\n projectOid: paramRightAfter(\\"/order/\\"),\\n programmeOid: paramRightAfter(\\"/contract/\\"),\\n section: paramRightAfter(\\"/order/.*?/\\"),\\n mailchainOid: paramRightAfter(\\"/mailchain/\\"),\\n })\\n );\\n }, [pathname]);\\n};\\n\\n\\nNext we create a reducer in our slice that stores path params:\\n\\njs\\nupdatePathParams: (state, action: PayloadAction<ApplicationSliceState[\\"pathParams\\"]>) => {\\n state.pathParams = { ...state.pathParams, ...action.payload };\\n},\\n\\n\\nFinally we adjust our existing useGetPathParams:\\n\\njs\\n// useGetPathParams.ts\\n\\nimport { useAppSelector } from \\"../redux/app/hook\\"\\nimport { ApplicationSliceState } from \\"../redux/slice/applicationSlice\\"\\n\\nexport default (paramKey: keyof ApplicationSliceState[\\"pathParams\\"]) => {\\n\\treturn useAppSelector(s => s.application.pathParams?.[paramKey]);\\n}\\n\\n\\nwhich minimizes potential rerender problem while using this hook!\\n","title":"Getting Path Parameters","date":"2023-08-11T00:00:00.000Z","id":"blog0163","tag":"react","intro":"I used to use useRouteMatch in react-router-dom v5, which has been changed completely in v6 into something called useMatch or useMatches. We talk about building the param searching function on our own to get rid of these unhandy \\"black boxes\\".","toc":true},{"content":"\\n#### Repository\\n\\n- https://github.com/machingclee/2023-08-13-serverless-google-login\\n\\n#### Result\\n\\n<Center>\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/NFqS6AXw-jM\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe>\\n</Center>\\n\\n#### Backend\\n\\n##### CORS: Restrict Origins to have Accesses to the API\\n\\nts-1\\nimport express, { Request } from \\"express\\";\\nimport { google } from \\"googleapis\\";\\nimport jwt from \\"jsonwebtoken\\";\\nimport cors from \'cors\';\\nimport errorHandler from \\"./middlewares/errorHandler\\";\\n\\nconst allowlist = [\'http://localhost:3000\']\\n\\nconst corsOptionsDelegate = (req, callback) => {\\n var corsOptions;\\n if (allowlist.indexOf(req.header(\'Origin\')) > -1) {\\n corsOptions = { origin: true } // reflect (enable) the requested origin in the CORS response\\n } else {\\n corsOptions = { origin: false } // disable CORS for this request\\n }\\n callback(null, corsOptions) // callback expects two parameters: error and options\\n}\\n\\n\\nLater we will bring the delegate into use by app.use(cors(corsOptionsDelegate));\\n\\n##### .env-cmdrc, the Environment Variables\\n\\nWe create a .env-cmdrc for environment variables:\\n\\njson\\n{\\n \\"default\\": {\\n \\"PORT\\": 8080\\n },\\n \\"production\\": {\\n \\"GOOGLE_CLIENT_ID\\": \\"XXX.com\\",\\n \\"GOOGLE_CLIENT_SECRET\\": \\"YYY\\",\\n \\"GOOGLE_API_REDIRECT\\": \\"http://localhost:8080/login-google\\",\\n \\"ALLOWED_EMAILS\\": [\\"machingclee@gmail.com\\", \\"james.lee@wonderbricks.com\\"],\\n \\"JWT_SECRET\\": \\"SECRET!\\",\\n \\"FRONTEND_URL\\": \\"http://localhost:3000\\"\\n }\\n}\\n\\n\\nThe script (remember to yarn add env-cmd)\\n\\njson\\n\\"scripts\\": {\\n \\"start\\": \\"env-cmd -f .env-cmdrc -e default,production nodemon --exec ts-node src/app.ts\\",\\n ...\\n}\\n\\n\\nwill set default and production configs into process.env. Then:\\n\\n##### Get Url for Choosing Google Account\\n\\nts-17\\nconst {\\n GOOGLE_CLIENT_ID,\\n GOOGLE_CLIENT_SECRET,\\n ALLOWED_EMAILS,\\n GOOGLE_API_REDIRECT,\\n JWT_SECRET,\\n FRONTEND_URL\\n} = process.env;\\n\\nconst ALLOWED_EMAILS_ = ALLOWED_EMAILS.split(\\",\\");\\n\\nconst app = express();\\napp.use(cors(corsOptionsDelegate));\\n\\nconst oAuth2Client = new google.auth.OAuth2(\\n {\\n clientId: GOOGLE_CLIENT_ID,\\n clientSecret: GOOGLE_CLIENT_SECRET,\\n redirectUri: GOOGLE_API_REDIRECT\\n }\\n);\\n\\nfunction getAuthUrl() {\\n return oAuth2Client.generateAuthUrl({\\n access_type: \\"offline\\",\\n prompt: \\"consent\\",\\n scope: [\\n \\"https://www.googleapis.com/auth/userinfo.email\\",\\n \\"https://www.googleapis.com/auth/userinfo.profile\\"\\n ]\\n });\\n}\\n\\napp.get(\\"/login\\", async (req, res) => {\\n const url = getAuthUrl()\\n res.status(200).json({ url });\\n});\\n\\n\\n##### The O-Auth Credential Setting in Google Cloud for Redirect Url\\n\\n- Here is a tricky part, upon successful request to /login-in, we will get an url (see line 23 for the frontend part below) to redirect frontend user to:\\n\\n ![](/assets/tech/164/002.png)\\n\\n via a window.location.href = url.\\n\\n- After an identity is chosen, user will then be redirected to an url specified here:\\n\\n ![](/assets/tech/164/001.png)\\n\\n ts-54\\n app.get(\\n \\"/login-google\\",\\n async (req: Request<any, any, any, { code: string }>, res, next) => {\\n const { code } = req.query;\\n const { tokens } = await oAuth2Client.getToken(code);\\n oAuth2Client.credentials = tokens;\\n const oauth2 = google.oauth2(\\"v2\\");\\n\\n const res_ = await oauth2.userinfo.v2.me.get({\\n auth: oAuth2Client,\\n });\\n const { email } = res_.data;\\n const hasRight = ALLOWED_EMAILS_.includes(email);\\n if (!hasRight) {\\n return next(\\n Only ${ALLOWED_EMAILS_.join(\\", \\")} has access to this project.\\n );\\n }\\n const token = jwt.sign(\\n { email },\\n JWT_SECRET,\\n { expiresIn: 60 * 60 }\\n );\\n res.redirect(${FRONTEND_URL}/token/${token});\\n }\\n );\\n \\n\\n- We redirect user back to our frontend, and the frontend needs to be able to get the token from the url.\\n- In our case we redirect user to ${FRONTEND_URL}/token/${token}, a query string at the end ?token=${token} is also fine.\\n\\n##### Authenticate the jwt Token\\n\\n- We save the token in local storage or cookie.\\n- On refresh, if that token is found in the frontend, we authenticate the token (line 27 of the frontend part) via:\\n\\nts-80\\napp.get(\\n \\"/authenticate\\",\\n async (req: Request<any, any, any, { token: string }>, res, next) => {\\n const { token } = req.query;\\n try {\\n const decoded = jwt.verify(token, JWT_SECRET);\\n res.json({ success: true });\\n } catch (err) {\\n next(err);\\n }\\n }\\n);\\n\\napp.use(errorHandler);\\n\\nconst PORT = Number(process.env[\\"PORT\\"]);\\n\\napp.listen(PORT, () => {\\n console.log(Server started on http://localhost:${PORT});\\n});\\n\\n\\nHere our errorHandler is as simple as\\n\\nts\\nexport default (err, req, res, next) => {\\n if (err) {\\n res.json({ success: false, errorMessage: err });\\n }\\n};\\n\\n\\nAny error that is caught can be passed to this middleware via next(err) (see line 88 above).\\n\\n#### Frontend\\n\\nOur routing is as simple as\\n\\nts\\n<Routes>\\n <Route path=\\"/\\" element={Login()} />\\n <Route path=\\"/token/*\\" element={Login()} />\\n</Routes>\\n\\n\\nAnd the frontend is simply:\\n\\nts-1\\n// Login.tsx\\n\\nimport { Grid, Button, Container } from \\"@mui/material\\";\\nimport axio from \\"axios\\";\\nimport usePathUtils from \\"../hooks/usePathUtils\\";\\nimport { useEffect, useRef, useState } from \\"react\\";\\nimport { useNavigate } from \\"react-router-dom\\";\\n\\nexport default () => {\\n const { paramRightAfter } = usePathUtils();\\n const token = paramRightAfter(\\"/token/\\");\\n const navigate = useNavigate();\\n const [accessToken, setAccessToken] = useState(\\"\\");\\n const [loginSuccess, setLoginSucess] = useState(false);\\n\\n const tokenFetched = useRef(false);\\n\\n const loginHandler = async () => {\\n const res = await axio.get<{ url: string }>(\\n \\"http://localhost:8080/login\\"\\n );\\n const { url } = res.data;\\n window.location.href = url;\\n }\\n\\n useEffect(() => {\\n if (token && !tokenFetched.current) {\\n setAccessToken(token);\\n tokenFetched.current = true;\\n navigate(\\"/\\");\\n\\n axio.get<{ success: boolean }>(\\n \\"http://localhost:8080/authenticate?token=\\" + token\\n )\\n .then((res) => {\\n const { success } = res.data;\\n setLoginSucess(success);\\n });\\n }\\n }, [token]);\\n\\n return (\\n <Container>\\n <Grid item>\\n <div style={{ marginBottom: 10, marginTop: 10 }}>\\n Login Status: {loginSuccess ? \\"logged in\\" : \\"not logged in \\"}\\n </div>\\n <Button onClick={loginHandler} variant=\\"outlined\\">Login</Button>\\n </Grid>\\n </Container>\\n );\\n};\\n\\n","title":"Google Login","date":"2023-08-13T00:00:00.000Z","id":"blog0164","tag":"express, cors","intro":"A simple backend that perform google authentication. I personally use this to restrict users who can access my project.","toc":true},{"content":"\\n<Center></Center>\\n\\nConsider a file uploader called Uploader:\\n\\nts\\nexport type UploaderHandle = {\\n fileIdArrs: number[],\\n postFilesToServer: () => Promise<number[]>,\\n}\\n\\ntype UploaderProps = {\\n documents: SomeFile[],\\n}\\n\\nconst Uploader = forwardRef<UploaderHandle, UploaderProps>((props, ref) => {\\n const { documents } = props\\n useImperativeHandle(ref, () => ({\\n fileIdArrs,\\n postFilesToServer,\\n }))\\n ...\\n const [fileIdArrs, setfileIdArrs] = useState<number[]>([]);\\n\\n const postFilesToServer = async () => {\\n for (let file of fileList) {\\n await uploadFile(file);\\n ...\\n }\\n }\\n})\\n\\n\\n- We want the method to be triggered by the parent of <Uploader/> instead.\\n- Sometimes it is done for the purpose of code separation (e.g., its parent is already complicated enough).\\n\\nNow consider a parent that makes use of <Uploader/>:\\n\\nts-1\\nexport default function SomeComponent() {\\n const uploaderRef = useRef<UploaderHandle>(null);\\n ...\\n const saveHandler = async () => {\\n const fileIds = await uploaderRef.current?.postFilesToServer();\\n ...\\n }\\n return(\\n ...\\n <Box>\\n <Uploader\\n documents={documents}\\n ref={uploaderRef}\\n />\\n </Box>\\n )\\n}\\n\\n\\nIn line 5 we use the method postFilesToServer which belongs to its child <Uploader/>.\\n","title":"forwardRef and useImperativeHandle","date":"2023-08-14T00:00:00.000Z","id":"blog0165","tag":"react","intro":"A method to pass complicated function in a component to its parent.","toc":false},{"content":"\\n<Center></Center>\\n\\nts\\n// useRerender.ts\\n\\nimport { ReactNode, useState } from \\"react\\";\\n\\nexport default () => {\\n const [rerenderFlag, setRerenderFlag] = useState(true);\\n const rerender = () => {\\n setRerenderFlag(false);\\n setTimeout(() => setRerenderFlag(true), 1);\\n };\\n\\n const Rerender = ({ children }: { children: ReactNode }) => {\\n return <>{rerenderFlag && children}</>;\\n };\\n\\n return { rerender, Rerender };\\n};\\n\\n\\nThe trick is simply executing setRerenderFlag(true) later by letting it execute in callback-queue, such a callback is created by setTimeout and it will be executed once every task in the call stack are cleared.\\n\\nFor example, what would be the result of the following?\\n\\njs\\nsetTimeout(() => {\\n console.log(\\"I come from timeout\\");\\n}, 1);\\n\\nfor (let i = 0; i < 100; i++) {\\n console.log(\\"I come from for loop\\");\\n}\\n\\n\\n![](/assets/tech/166/001.png)\\n\\n<Center></Center>\\n\\nWhy is that? A very clear explanation can be found in:\\n\\n- [[Part 1] \u6240\u4ee5\u8aaa event loop \u5230\u5e95\u662f\u4ec0\u9ebc\u73a9\u610f\u5152\uff1f| Philip Roberts | JSConf EU](https://www.youtube.com/watch?v=8aGhZQkoFbQ)\\n- [[Part 2] Further Adventures of the Event Loop - Erin Zimmer - JSConf EU 2018](https://www.youtube.com/watch?v=u1kqx6AenYw)\\n\\nUnderstanding how v8 works can essentially help investigate how nodejs works, and after that we can appreciate how single-threaded model can handle high concurrency problem.\\n\\nSimilar idea can be brought to other languages like Java, and from that we are lead to the use of WebFlux in the realm of reactive programming.\\n\\nThe word **_single-threaded_** may be \\"misleading\\" as nodejs is not always single-threaded, but the main thread will not be blocked by designating some tasks to other thread like DNS-resolution or file io (which inevitably blocks!).\\n","title":"Hook to Rerender Component By Making use of the Single Threaded Event Driven Model Behind v8","date":"2023-08-15T00:00:00.000Z","id":"blog0166","tag":"react","intro":"A simple component and function that helps rerender a component effectively.","toc":false},{"content":"\\njs\\n// boxShadow.ts\\n\\nexport default {\\n SHADOW_01: \\"rgba(149, 157, 165, 0.2) 0px 8px 24px\\",\\n SHADOW_02: \\"rgba(100, 100, 111, 0.2) 0px 7px 29px 0px\\",\\n SHADOW_03: \\"rgba(0, 0, 0, 0.15) 1.95px 1.95px 2.6px\\",\\n SHADOW_04: \\"rgba(0, 0, 0, 0.35) 0px 5px 15px\\",\\n SHADOW_05: \\"rgba(0, 0, 0, 0.16) 0px 1px 4px\\",\\n SHADOW_06: \\"rgba(0, 0, 0, 0.24) 0px 3px 8px\\",\\n SHADOW_07: \\"rgba(99, 99, 99, 0.2) 0px 2px 8px 0px\\",\\n SHADOW_08:\\n \\"rgba(0, 0, 0, 0.02) 0px 1px 3px 0px, rgba(27, 31, 35, 0.15) 0px 0px 0px 1px\\",\\n SHADOW_09: \\"rgba(0, 0, 0, 0.1) 0px 4px 12px\\",\\n SHADOW_10:\\n \\"rgba(0, 0, 0, 0.25) 0px 54px 55px,rgba(0, 0, 0, 0.12) 0px -12px 30px,rgba(0, 0, 0, 0.12) 0px 4px 6px,rgba(0, 0, 0, 0.17) 0px 12px 13px,rgba(0, 0, 0, 0.09) 0px -3px 5px\\",\\n SHADOW_11:\\n \\"rgba(0, 0, 0, 0.05) 0px 6px 24px 0px,rgba(0, 0, 0, 0.08) 0px 0px 0px 1px\\",\\n SHADOW_12:\\n \\"rgba(0, 0, 0, 0.16) 0px 10px 36px 0px,rgba(0, 0, 0, 0.06) 0px 0px 0px 1px\\",\\n SHADOW_13: \\"rgba(17, 12, 46, 0.15) 0px 48px 100px 0px\\",\\n SHADOW_14:\\n \\"rgba(255, 255, 255, 0.1) 0px 1px 1px 0px inset,rgba(50, 50, 93, 0.25) 0px 50px 100px -20px,rgba(0, 0, 0, 0.3) 0px 30px 60px -30px\\",\\n SHADOW_15:\\n \\"rgba(50, 50, 93, 0.25) 0px 50px 100px -20px,rgba(0, 0, 0, 0.3) 0px 30px 60px -30px\\",\\n SHADOW_16:\\n \\"rgba(50, 50, 93, 0.25) 0px 50px 100px -20px,rgba(0, 0, 0, 0.3) 0px 30px 60px -30px\\",\\n SHADOW_17:\\n \\"rgba(50, 50, 93, 0.25) 0px 13px 27px -5px,rgba(0, 0, 0, 0.3) 0px 8px 16px -8px\\",\\n SHADOW_18:\\n \\"rgba(50, 50, 93, 0.25) 0px 2px 5px -1px,rgba(0, 0, 0, 0.3) 0px 1px 3px -1px\\",\\n SHADOW_19:\\n \\"rgba(50, 50, 93, 0.25) 0px 6px 12px -2px,rgba(0, 0, 0, 0.3) 0px 3px 7px -3px\\",\\n SHADOW_20:\\n \\"rgba(50, 50, 93, 0.25) 0px 13px 27px -5px,rgba(0, 0, 0, 0.3) 0px 8px 16px -8px\\",\\n SHADOW_21:\\n \\"rgba(50, 50, 93, 0.25) 0px 30px 60px -12px,rgba(0, 0, 0, 0.3) 0px 18px 36px -18px\\",\\n SHADOW_22:\\n \\"rgba(50, 50, 93, 0.25) 0px 50px 100px -20px,rgba(0, 0, 0, 0.3) 0px 30px 60px -30px\\",\\n SHADOW_23: \\"rgba(0, 0, 0, 0.12) 0px 1px 3px,rgba(0, 0, 0, 0.24) 0px 1px 2px\\",\\n SHADOW_24: \\"rgba(0, 0, 0, 0.16) 0px 3px 6px, rgba(0, 0, 0, 0.23) 0px 3px 6px\\",\\n SHADOW_25:\\n \\"rgba(0, 0, 0, 0.25) 0px 14px 28px,rgba(0, 0, 0, 0.22) 0px 10px 10px\\",\\n SHADOW_26:\\n \\"rgba(0, 0, 0, 0.3) 0px 19px 38px,rgba(0, 0, 0, 0.22) 0px 15px 12px\\",\\n SHADOW_27:\\n \\"rgba(60, 64, 67, 0.3) 0px 1px 2px 0px,rgba(60, 64, 67, 0.15) 0px 2px 6px 2px\\",\\n SHADOW_28:\\n \\"rgba(60, 64, 67, 0.3) 0px 1px 2px 0px,rgba(60, 64, 67, 0.15) 0px 1px 3px 1px\\",\\n SHADOW_29: \\"rgba(0, 0, 0, 0.05) 0px 0px 0px 1px\\",\\n SHADOW_30: \\"rgba(0, 0, 0, 0.05) 0px 1px 2px 0px\\",\\n SHADOW_31:\\n \\"rgba(0, 0, 0, 0.1) 0px 1px 3px 0px,rgba(0, 0, 0, 0.06) 0px 1px 2px 0px\\",\\n SHADOW_32:\\n \\"rgba(0, 0, 0, 0.1) 0px 4px 6px -1px,rgba(0, 0, 0, 0.06) 0px 2px 4px -1px\\",\\n SHADOW_33:\\n \\"rgba(0, 0, 0, 0.1) 0px 10px 15px -3px,rgba(0, 0, 0, 0.05) 0px 4px 6px -2px\\",\\n SHADOW_34:\\n \\"rgba(0, 0, 0, 0.1) 0px 20px 25px -5px,rgba(0, 0, 0, 0.04) 0px 10px 10px -5px\\",\\n SHADOW_35: \\"rgba(0, 0, 0, 0.25) 0px 25px 50px -12px\\",\\n SHADOW_36:\\n \\"rgba(0, 0, 0, 0.1) 0px 0px 5px 0px,rgba(0, 0, 0, 0.1) 0px 0px 1px 0px\\",\\n SHADOW_37:\\n \\"rgba(0, 0, 0, 0.07) 0px 1px 2px,rgba(0, 0, 0, 0.07) 0px 2px 4px,rgba(0, 0, 0, 0.07) 0px 4px 8px,rgba(0, 0, 0, 0.07) 0px 8px 16px,rgba(0, 0, 0, 0.07) 0px 16px 32px,rgba(0, 0, 0, 0.07) 0px 32px 64px\\",\\n SHADOW_38: \\"rgba(0, 0, 0, 0.2) 0px 18px 50px -10px\\",\\n SHADOW_39: \\"rgba(0, 0, 0, 0.1) 0px 10px 50px\\",\\n SHADOW_40: \\"rgba(0, 0, 0, 0.04) 0px 3px 5px\\",\\n SHADOW_41:\\n \\"rgba(67, 71, 85, 0.27) 0px 0px 0.25em,rgba(90, 125, 188, 0.05) 0px 0.25em 1em\\",\\n SHADOW_42:\\n \\"rgba(14, 30, 37, 0.12) 0px 2px 4px 0px,rgba(14, 30, 37, 0.32) 0px 2px 16px 0px\\",\\n SHADOW_43:\\n \\"rgba(0, 0, 0, 0.2) 0px 12px 28px 0px,rgba(0, 0, 0, 0.1) 0px 2px 4px 0px,rgba(255, 255, 255, 0.05) 0px 0px 0px 1px inset\\",\\n SHADOW_44: \\"rgba(0, 0, 0, 0.15) 0px 5px 15px 0px\\",\\n SHADOW_45:\\n \\"rgba(136, 165, 191, 0.48) 6px 2px 16px 0px,rgba(255, 255, 255, 0.8) -6px -2px 16px 0px\\",\\n SHADOW_46:\\n \\"rgba(17, 17, 26, 0.05) 0px 1px 0px,rgba(17, 17, 26, 0.1) 0px 0px 8px\\",\\n SHADOW_47: \\"rgba(17, 17, 26, 0.1) 0px 0px 16px\\",\\n SHADOW_48:\\n \\"rgba(17, 17, 26, 0.05) 0px 4px 16px,rgba(17, 17, 26, 0.05) 0px 8px 32px\\",\\n SHADOW_49:\\n \\"rgba(17, 17, 26, 0.1) 0px 4px 16px,rgba(17, 17, 26, 0.05) 0px 8px 32px\\",\\n SHADOW_50:\\n \\"rgba(17, 17, 26, 0.1) 0px 1px 0px, rgba(17, 17, 26, 0.1) 0px 8px 24px, rgba(17, 17, 26, 0.1) 0px 16px 48px\\",\\n SHADOW_51:\\n \\"rgba(17, 17, 26, 0.1) 0px 4px 16px,rgba(17, 17, 26, 0.1) 0px 8px 24px,rgba(17, 17, 26, 0.1) 0px 16px 56px\\",\\n SHADOW_52:\\n \\"rgba(17, 17, 26, 0.1) 0px 8px 24px,rgba(17, 17, 26, 0.1) 0px 16px 56px,rgba(17, 17, 26, 0.1) 0px 24px 80px\\",\\n SHADOW_53:\\n \\"rgba(0, 0, 0, 0.15) 0px 15px 25px,rgba(0, 0, 0, 0.05) 0px 5px 10px\\",\\n SHADOW_54: \\"rgba(0, 0, 0, 0.08) 0px 4px 12px\\",\\n SHADOW_55: \\"rgba(0, 0, 0, 0.15) 0px 2px 8px\\",\\n SHADOW_56: \\"rgba(0, 0, 0, 0.1) -4px 9px 25px -6px\\",\\n SHADOW_57: \\"rgba(0, 0, 0, 0.2) 0px 20px 30px\\",\\n SHADOW_58:\\n \\"rgba(0, 0, 0, 0.25) 0px 0.0625em 0.0625em,rgba(0, 0, 0, 0.25) 0px 0.125em 0.5em,rgba(255, 255, 255, 0.1) 0px 0px 0px 1px inset\\",\\n SHADOW_59: \\"rgba(0, 0, 0, 0.09) 0px 3px 12px\\",\\n SHADOW_60:\\n \\"rgba(0, 0, 0, 0.05) 0px 0px 0px 1px,rgb(209, 213, 219) 0px 0px 0px 1px inset\\",\\n SHADOW_61:\\n \\"rgba(9, 30, 66, 0.25) 0px 1px 1px,rgba(9, 30, 66, 0.13) 0px 0px 1px 1px\\",\\n SHADOW_62:\\n \\"rgba(9, 30, 66, 0.25) 0px 4px 8px -2px,rgba(9, 30, 66, 0.08) 0px 0px 0px 1px\\",\\n};\\n\\n","title":"Box Shadow","date":"2023-08-27T00:00:00.000Z","id":"blog0168","tag":"react","intro":"Record a list of box-shadow in a json object.","toc":false},{"content":"\\n#### Life Time\\n\\nThe compiler has 3 rules for the lifetime:\\n\\n- The compiler assigns a lifetime parameter to **_each_** parameter that\'s a reference\\n- If there is exactly **_one_** input lifetime parameter, that lifetime is assigned to **_all_** output lifetime parameters.\\n- If there are **_multiple_** input lifetime parameters, **_but_** one of them is &self or &mut self because this is a method, the lifetime of self is assigned to **_all_** output lifetime parameters\\n\\nNote that\\n\\n- A liftime comes from an input reference is called an **input lifeime**;\\n- that comes from an output reference is called an **output lifetime**.\\n\\nTo sum up, we always expect:\\n\\n$$\\n\\\\text{output lifetime}=\\\\min_\\\\alpha\\\\big\\\\{\\\\text{input_lifeime}_{\\\\!\\\\!\\\\!\\\\!\\\\alpha}\\\\big\\\\}.\\n$$\\n\\n#### Result Type\\n\\nWe treat Result type like a Promise in javascript, in which we have\\n\\n- return Ok(...); = resolve(...); and\\n- return Err(...); = reject(...);.\\n\\nThe generic type parameters of Result following the rule:\\n\\nrust\\nResult<type returned by Ok, type returned by Err>\\n\\n\\nrust\\nstruct Config<\'a> {\\n query: &\'a String,\\n filename: &\'a String,\\n}\\n\\nimpl<\'a> Config<\'a> {\\n fn new(args: &\'a [String]) -> Result<Config, &str> {\\n if args.len() < 3 {\\n return Err(\\"not enough arguments\\");\\n }\\n let query = &args[1];\\n let filename = &args[2];\\n Ok(Config { query, filename })\\n }\\n}\\n\\n\\nThere are two ways to squeeze the Config out of Result enum:\\n\\n##### Extraction Method 1: Squeezing by Unwrap\\n\\nNext in our programme if we unwrap and handle the error gracefully:\\n\\nrust-1\\nlet args: Vec<String> = env::args().collect();\\nlet config = Config::new(&args).unwrap_or_else(|err| {\\n println!(\\"Problem parsing arguments: {}\\", err);\\n process::exit(1);\\n});\\nrun(config);\\n\\n\\nThen from line 6 onwards our config has been converted from Result to Config.\\n\\n> **Take away.** We can squeeze Result<T> to T by executing unwrap() once.\\n\\n##### Extraction Method 2: Assigning by Some\\n\\nAlternatively, it is conventional to write a placeholder null variable and assign value into it when something exists, that pattern in rust is implemented by Option enum and Some object:\\n\\nrust\\nlet mut config: Option<Config> = None;\\n\\nlet result = Config::new(&args);\\nif let Ok(config_) = result {\\n config = Some(config_);\\n};\\n\\nif let Some(config_unwrapped) = config {\\n run(config_unwrapped);\\n};\\n\\n\\nWe didn\'t handle the error and error message. We can combine unwrap_or_else and the assignment = Some(config_) approach depending on the ways of doing things.\\n\\nIf we want multiple nulls checking,\\n\\nrust\\nif let (Some(a_), Some(b_)) = (a, b) {\\n // do something\\n}\\n\\n\\nFor example,\\n\\nrust\\nif let (Some(a), Some(b)) = (Some(7), Some(8)) {\\n println!(\\"Result: {}\\", a * b);\\n}\\n\\n\\nprints 56.\\n\\n#### Throwing Arbitrary Error\\n\\nConsider the following function:\\n\\nrust\\nfn run(config: Config) -> Result<(), Box<dyn Error>> {\\n let query = config.query;\\n let filename = config.filename;\\n let contents = fs::read_to_string(filename)?;\\n println!(\\"{}\\", contents);\\n Ok(())\\n}\\n\\n\\n- fs::read_to_string returns a Result object. If we want to throw an Error and let the function call in the previous stack frame to handle it, we just add a ?.\\n- The Box<dyn Error> in the return type serves the same purpose as Java\'s\\n java\\n public void function someFunction() throws Exception {};\\n \\n\\n#### Handle the Final Execution Error\\n\\nAssume that we have:\\n\\nrust-1\\nfn main() {\\n let args: Vec<String> = env::args().collect();\\n let config = Config::new(&args).unwrap_or_else(|err| {\\n println!(\\"[Problem parsing arguments] {}\\", err);\\n process::exit(1);\\n });\\n\\n\\nThen the following two are equivalent:\\n\\nrust-7\\n run(config).unwrap_or_else(|err| {\\n println!(\\"Application Error: {}\\", err);\\n process::exit(1);\\n });\\n}\\n\\n\\nrust-7\\n if let Err(err) = run(config) {\\n println!(\\"Application Error: {}\\", err);\\n process::exit(1);\\n }\\n}\\n\\n\\n#### Second Visit to the Multi-threading Web Server Example In Rust Book\\n\\nMy energy got exhausted at the first time I go with rust book to the last chapter (you can see how much detail I have recorded [**_here_**](/blog/article/Summarize-Rust-Beginning-Tutorial-by-a-Simplified-Multithreading-Web-Server#Implementation-of-ThreadPool-for-Multithreaded-Web-Server) before the last chapter on web server!).\\n\\nThis time I grabbed and digested detail in a deeper understanding. I try to record the detail in this blog post.\\n\\n##### fn main()\\n\\nWe start off by writing down the general structure of the program in main function, the intersting part lies inside lib.rs, i.e., how we define ThreadPool.\\n\\nrust\\nuse std::io::prelude::*;\\nuse std::net::TcpListener;\\nuse std::net::TcpStream;\\nuse std::time::Duration;\\nuse std::{fs, thread};\\n\\nuse web_server::ThreadPool;\\n\\nfn main() {\\n let listener = TcpListener::bind(\\"127.0.0.1:7878\\").unwrap();\\n\\n let pool = ThreadPool::new(4);\\n\\n for stream in listener.incoming() {\\n let stream = stream.unwrap();\\n pool.execute(|| {\\n handle_connection(stream);\\n });\\n }\\n}\\n\\nfn handle_connection(mut stream: TcpStream) {\\n let get = b\\"GET / HTTP/1.1\\\\r\\\\n\\";\\n let sleep = b\\"GET /sleep HTTP/1.1\\\\r\\\\n\\";\\n\\n let mut buffer = [0; 1024];\\n stream.read(&mut buffer).unwrap();\\n\\n println!(\\"{}\\", buffer.starts_with(get));\\n\\n let (status_line, filename) = if buffer.starts_with(get) {\\n (\\"HTTP/1.1 200 OK\\", \\"hello.html\\")\\n } else if buffer.starts_with(sleep) {\\n println!(\\"{}\\", \\"sleeping...\\");\\n thread::sleep(Duration::from_secs(5));\\n println!(\\"{}\\", \\"awake!\\");\\n (\\"HTTP/1.1 200 OK\\", \\"hello.html\\")\\n } else {\\n (\\"HTTP/1.1 404 NOT FOUND\\", \\"404.html\\")\\n };\\n\\n let contents = fs::read_to_string(filename).unwrap();\\n let response = format!(\\n \\"{}\\\\r\\\\nContent-Length: {}\\\\r\\\\n\\\\r\\\\n{}\\",\\n status_line,\\n &contents.len(),\\n &contents\\n );\\n stream.write(response.as_bytes()).unwrap();\\n stream.flush().unwrap();\\n}\\n\\n\\n##### lib.rs, the web_server::ThreadPool\\n\\nlib.rs is a single module which by default is imported by calling\\n\\nrust\\nproject_name::{what\'s defined as pub in lib.rs}\\n\\n\\nInside our lib.rs we have\\n\\nrust-1\\nuse std::option::Option;\\nuse std::{\\n sync::{\\n mpsc::{self, Receiver},\\n Arc, Mutex,\\n },\\n thread,\\n};\\n\\npub struct ThreadPool {\\n workers: Vec<Worker>,\\n sender: mpsc::Sender<Message>,\\n}\\n\\n\\nI would like to pin the takeways in this program (instead of introducing what\'s the target and what\'s to be done in this example).\\n\\nrust-14\\n// property of a mutable reference is at most mutable reference\\n// we cannot move it out, moving is not a mutation\\nimpl Drop for ThreadPool {\\n fn drop(&mut self) {\\n println!(\\"Terminating all workers\\");\\n for _ in &self.workers {\\n self.sender.send(Message::Terminate).unwrap();\\n }\\n\\n for worker in &mut self.workers {\\n\\n\\n**_1st Takeaway._** We would write line 23 as\\n\\nrust\\nfor worker in self.workers\\n\\n\\ninstead by our first instinct, an error will pop up:\\n\\nnone\\nself.workers moved due to this implicit call to .into_iter()\\ninto_iter takes ownership of the receiver self, which moves self.workers\\n\\n\\n- The property of a **_mutable reference_** is **_at most_** a mutable reference (which we need to specify).\\n- The reason is that .into_iter(self) is implicitly called, which moves our self.workers into a function that generates iterator.\\n- Although self is a **_mutable_** reference, moving its property is not a mutation, a move will _drain the memory out_ by assigning the source property to null_ptr and assign that original pointer to the target that we move into.\\n\\nrust-24\\n println!(\\"Shutting down worker {}\\", worker.id);\\n\\n if let Some(thread) = worker.thread.take() {\\n thread.join().unwrap();\\n }\\n }\\n }\\n}\\n\\n\\n**_2nd Takeaway._** Note that worker is a property of a mutable reference self.workers, hence again worker itself is at most a mutable reference.\\n\\nHowever, we want to call worker.thread.join().unwrap(), the function join has signature join(self), i.e., worker.thread will be moved.\\n\\nThe usual _trick_ in rust is to wrap T into Option<T>, then Option<T>::take() allows moving the Some<T> out by careful unsafe rust implementation.\\n\\nrust-32\\nenum Message {\\n Job(Box<dyn FnOnce() + Send + \'static>),\\n Terminate,\\n}\\n\\n\\n**_3nd Takeway._** In the course of coding this example, instead of implementing enum Message, what we originally implemented is simply\\n\\nrust\\ntype Job = Box<dyn FnOnce() + Send + \'static>\\n\\n\\nand in line 42 has been\\n\\nrust\\nlet (sender, receiver) = mpsc::channel::<Job>();\\n\\n\\nBecause later on we not only want to signal a Job to the threads, we also want to signal a Termination to the threads.\\n\\nIn plain javascript we can naively implement this by sending [\\"job\\", job] and [\\"terminate\\", null] to the workers, i.e., we append some field to distinguish the messages.\\n\\nIn rust approach, we treat that field\'s as enum **_variants_**:\\n\\n- Job(job trait) $\\\\longleftrightarrow$ [\\"job\\", job] (job is a closure)\\n- Terminate $\\\\longleftrightarrow$ [\\"terminate\\", null]\\n\\nand we group the variants in an enum class:\\n\\nrust\\nenum Message {\\n Job(Box<dyn FnOnce() + Send + \'static>),\\n Terminate,\\n}\\n\\n\\nrust-36\\nimpl ThreadPool {\\n pub fn new(size: usize) -> Self {\\n assert!(size > 0);\\n let mut workers = Vec::with_capacity(size);\\n let (sender, receiver) = mpsc::channel::<Message>();\\n\\n let receiver = Arc::new(Mutex::new(receiver));\\n\\n for id in 0..size {\\n workers.push(Worker::new(id, receiver.clone()));\\n }\\n\\n ThreadPool { workers, sender }\\n }\\n\\n pub fn execute<F>(&self, f: F)\\n where\\n F: FnOnce() + Send + \'static,\\n {\\n let job = Message::Job(Box::new(f));\\n self.sender.send(job).unwrap();\\n }\\n}\\n\\nstruct Worker {\\n id: usize,\\n thread: Option<thread::JoinHandle<()>>,\\n}\\n\\nimpl Worker {\\n fn new(id: usize, receiver: Arc<Mutex<Receiver<Message>>>) -> Worker {\\n let thread = thread::spawn(move || loop {\\n let msg = receiver.clone().lock().unwrap().recv().unwrap();\\n match msg {\\n Message::Job(job) => {\\n println!(\\"Worker {} got a job; excuting.\\", id);\\n job();\\n }\\n Message::Terminate => {\\n println!(\\"Terminated!\\");\\n break;\\n }\\n }\\n });\\n Worker {\\n id,\\n thread: Some(thread),\\n }\\n }\\n}\\n\\n\\nFinally:\\n\\n- Arc is a multi-threaded version of Rc for multiple reference to the same wrappered object.\\n- Mutex is to block access from other threads to the wrapped object.\\n","title":"Rust Study Notes","date":"2023-09-04T00:00:00.000Z","id":"blog0169","tag":"rust","intro":"This is a beginner notes.","toc":true},{"content":"\\n#### Implementation\\n\\njs\\n// useScroll.ts\\n\\nimport { useEffect, useRef, useState } from \\"react\\";\\n\\nexport default (props: { up: () => void, down: () => void }) => {\\n const scrollRef = useRef < HTMLDivElement > null;\\n const { up, down } = props;\\n const prevScrollTop = (useRef < number) | (null > null);\\n useEffect(() => {\\n const scrollHandler = () => {\\n if (!scrollRef.current) {\\n return;\\n }\\n if (prevScrollTop.current === null) {\\n prevScrollTop.current = scrollRef.current.scrollTop;\\n } else {\\n const currScrollTop = scrollRef.current.scrollTop;\\n // +ve => move down, -ve => move up\\n const scrollDistance = currScrollTop - prevScrollTop.current;\\n if (scrollDistance > 0) {\\n down();\\n } else if (scrollDistance < 0) {\\n up();\\n }\\n prevScrollTop.current = currScrollTop;\\n }\\n };\\n\\n const scrollAssginmentInterval = setInterval(() => {\\n if (scrollRef.current) {\\n scrollRef.current.addEventListener(\\"scroll\\", scrollHandler);\\n clearInterval(scrollAssginmentInterval);\\n }\\n }, 100);\\n\\n return () => {\\n scrollRef?.current?.removeEventListener(\\"scroll\\", scrollHandler);\\n };\\n }, []);\\n\\n return { scrollRef };\\n};\\n\\n\\n#### Usage\\n\\nHere we want:\\n\\n- A sticky-positioned div disappears when we scroll down;\\n- Show the div again when we scroll up.\\n\\njs\\nexport default () = > {\\n\\tconst [collapseTitle, setCollapseTitle] = useState(false);\\n\\tconst closedRef = useRef<boolean>(false);\\n\\n\\tconst { scrollRef } = useScroll({\\n\\t\\tup: () => {\\n\\t\\t\\tif (closedRef.current) {\\n\\t\\t\\t\\tconsole.log(\\"open\\");\\n\\t\\t\\t\\tsetCollapseTitle(false);\\n\\t\\t\\t\\tclosedRef.current = false;\\n\\t\\t\\t}\\n\\t\\t},\\n\\t\\tdown: () => {\\n\\t\\t\\t\\tif (!closedRef.current) {\\n\\t\\t\\t\\tconsole.log(\\"closed\\");\\n\\t\\t\\t\\tsetCollapseTitle(true);\\n\\t\\t\\t\\tclosedRef.current = true;\\n\\t\\t\\t}\\n\\t\\t}\\n\\t});\\n\\n return (\\n <div\\n id=\\"mails-container\\"\\n ref={scrollRef}\\n style={{ overflowY: \\"scroll\\" }}>\\n ...\\n </div>\\n )\\n}\\n\\n\\n- Here we have used a storage closedRef to make sure the scrolling event is just dispatched once.\\n- We can remove it if we wish to trigger the event every time the user scrolls.\\n","title":"Scroll Up and Down Events in React","date":"2023-09-02T00:00:00.000Z","id":"blog0170","tag":"react","intro":"Record a hook to listen to scrolling-up and scrolling-down events, and an easy way to inject our callback to this events using this hook.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 100%\\n }\\n</style>\\n\\n#### Streams\\n\\n##### Simple File-Upload UI\\n\\nWe start off by having a simple input UI:\\n\\njs\\nimport axios from \\"axios\\";\\nimport { useState } from \\"react\\";\\n\\nexport default () => {\\n const [files, setFiles] = useState<FileList | null>(null);\\n\\n const uploadSelectedHandler = (e: React.ChangeEvent<HTMLInputElement>) => {\\n if (e.target.files) {\\n setFiles(e.target.files);\\n }\\n };\\n const uploadClickedHanlder = async () => {\\n if ((files?.length || 0) > 0) {\\n uplaodToServer({ oid: \\"someid\\", file: files![0] });\\n }\\n };\\n\\n const uplaodToServer =\\n async ({ oid, file }: { oid: string; file: File }) => {\\n const formData = new FormData();\\n formData.append(\\"file_01\\", file);\\n formData.append(\\"oid\\", oid);\\n const response = await axios.post(\\n \\"http://localhost:8080/stream\\",\\n formData\\n );\\n\\n return response.data;\\n }\\n\\n return (\\n <div>\\n <input type=\\"file\\" accept=\\"image/*\\" onChange={uploadSelectedHandler} />\\n <button onClick={uploadClickedHanlder}> Upload </button>\\n </div>\\n )\\n}\\n\\n\\nWe have:\\n\\n![](/assets/tech/171/001.png)\\n\\n<center></center>\\n\\n##### Files from FormData\\n\\n###### Simple Text File Upload\\n\\nAs is always we upload files by FormData. We will study the stream of data by using this route:\\n\\njs\\napp.post(\\"/stream\\", async (req, res) => {\\n const outputStream = createWriteStream(\\"server_received.txt\\");\\n req.pipe(outputStream);\\n});\\n\\n\\nLet\'s upload a text file with the following content:\\n\\nnone\\nIt is a long established fact that a reader\\nwill be distracted by the readable\\ncontent of a page when looking at its layout.\\nThe point of using Lorem Ipsum is that it has a more-or-less\\nnormal distribution of letters.\\n\\n\\nAnd our server_received.txt becomes:\\n\\nnone\\n------WebKitFormBoundaryQeBMs9N4limtK0VP\\nContent-Disposition: form-data; name=\\"file_01\\"; filename=\\"sometext.txt\\"\\nContent-Type: text/plain\\n\\nIt is a long established fact that a reader\\nwill be distracted by the readable\\ncontent of a page when looking at its layout.\\nThe point of using Lorem Ipsum is that it has a more-or-less\\nnormal distribution of letters.\\n------WebKitFormBoundaryQeBMs9N4limtK0VP\\nContent-Disposition: form-data; name=\\"oid\\"\\n\\nsomeid\\n------WebKitFormBoundaryQeBMs9N4limtK0VP--\\n\\n\\n- Things become complicated even we simply upload a text file.\\n- The entries of our FormData are separated by FormBoundary.\\n- Therefore we need to **_parse the output stream_** in order to get file-specfic content in each of separated streamed data.\\n\\n###### Multiparty Package\\n\\nLet\'s\\n\\ntext\\nyarn add multiparty @types/multiparty\\n\\n\\nin this regard. And let\'s modify /stream route to parse incoming stream:\\n\\njs\\napp.post(\\"/stream\\", async (req, res) => {\\n const form = new multiparty.Form();\\n const chunks: number[] = [];\\n form.parse(req);\\n let counter = 0;\\n form.on(\\"part\\", (inputStream) => {\\n try {\\n counter += 1;\\n console.log(\\"InputStream #: \\", counter);\\n\\n const outputStream = createWriteStream(inputStream.filename);\\n inputStream.pipe(outputStream);\\n } catch (e) {\\n res.json({ success: false, erroreMessage: JSON.stringify(e) });\\n }\\n });\\n});\\n\\n\\n- Now we can **_upload arbitary file_** and find that an **_identical_** file appears in our backend.\\n- Recall that our form data have two keys, file_01 and oid.\\n- We experirmentally add a counter to see whether multiparty also parses data without filename, and the result is positive as the output becomes:\\n none\\n InputStream #: 1\\n InputStream #: 2\\n \\n- Infact inputStream.filename becomes undefined for the second entry in the FormData.\\n- The programme ended sliently even the input of createWriteStream is undefined.\\n\\nSimple enough right? Let\'s dig deeper by studying what happens in\\n\\njs\\ninputStream.pipe(outputStream);\\n\\n\\n##### What Happens in inputStream.pipe(outputStream)?\\n\\nFor this, we remove the pipe line, we read chunk and write chunk on our own:\\n\\n###### Version 1 of pipe (Demonstrative Purpose, Not Recommended)\\n\\nWe accumulate all the bytes and then write it into a file:\\n\\njs\\napp.post(\\"/stream\\", async (req, res) => {\\n const form = new multiparty.Form();\\n form.parse(req);\\n form.on(\\"part\\", (inputStream) => {\\n try {\\n const chunks: number[] = [];\\n const outputStream = createWriteStream(inputStream.filename);\\n inputStream.on(\\"readable\\", () => {\\n let chunk: Buffer;\\n while ((chunk = inputStream.read()) != null) {\\n const chunk_ = Array.from(chunk);\\n chunks.push(...chunk_);\\n }\\n });\\n\\n inputStream.on(\\"close\\", () => {\\n const finalBuffer = Buffer.from(chunks);\\n outputStream.write(finalBuffer);\\n });\\n } catch (e) {\\n res.json({ success: false, erroreMessage: JSON.stringify(e) });\\n }\\n });\\n});\\n\\n\\nFor output of raw chunk, byte chunk and buffered chunk\\n\\nnone\\nraw chunk : <Buffer ca 4c 98 7f ac c8 98 ed 4f 77 f6 41 ba df b4 9c 3a d5 9b 66 93 92 c7 25 29 23 dd c9 66 51 e2 50 7b 9c be 25 66 a0 39 4a df 1a 63 6c 8f 33 89 e2 cd 9d ... 51798 more bytes>\\nbyte chunk: [\\n 202, 76, 152, 127, 172, 200, 152, 237, 79, 119, 246, 65,\\n 186, 223, 180, 156, 58, 213, 155, 102, 147, 146, 199, 37,\\n 41, 35, 221, 201, 102, 81, 226, 80, 123, 156, 190, 37,\\n 102, 160, 57, 74, 223, 26, 99, 108, 143, 51, 137, 226,\\n 205, 157, 9, 22, 113, 210, 168, 152, 52, 222, 147, 50,\\n 209, 155, 106, 147, 129, 163, 207, 100, 95, 218, 116, 63,\\n 213, 161, 164, 57, 85, 80, 66, 186, 81, 191, 110, 32,\\n 123, 197, 152, 183, 102, 98, 108, 89, 138, 246, 198, 152,\\n 143, 109, 236, 71,\\n ... 51748 more items\\n]\\nfinalBuffer: <Buffer 89 50 4e 47 0d 0a 1a 0a 00 00 00 0d 49 48 44 52 00 00 0d 70 00 00 05 a0 08 02 00 00 00 c2 10 1d 93 00 00 00 06 62 4b 47 44 00 00 00 00 00 00 f9 43 bb ... 9553650 more bytes>\\n\\n\\n###### Version 2 of pipe\\n\\nWhenever we read a chunk, we write it into outputStream:\\n\\njs\\napp.post(\\"/stream\\", async (req, res) => {\\n const form = new multiparty.Form();\\n form.parse(req);\\n form.on(\\"part\\", (inputStream) => {\\n try {\\n const outputStream = createWriteStream(inputStream.filename);\\n inputStream.on(\\"readable\\", () => {\\n let chunk: Buffer;\\n while ((chunk = inputStream.read()) != null) {\\n outputStream.write(chunk);\\n }\\n });\\n } catch (e) {\\n res.json({ success: false, erroreMessage: JSON.stringify(e) });\\n }\\n });\\n});\\n\\n\\nHere if we console.log(chunk), we have\\n\\ntext\\n<Buffer 81 a5 f4 37 66 e1 3a d1 72 9b 23 68 ... 65486 more bytes>\\n<Buffer f1 a7 eb 57 4e df be 8a b7 2e 29 dc ... 36888 more bytes>\\n<Buffer a6 1d aa 2e c0 3c 1e 76 c4 ae 75 c5 ... 28548 more bytes>\\n<Buffer 8c 37 a3 ec 93 b1 ec a3 21 c6 f1 30 ... 37611 more bytes>\\n<Buffer e9 4e 0d e9 8e 82 34 e1 7b 2b 2b bc ... 27825 more bytes>\\n<Buffer 24 f6 fe 13 d9 0a 57 e2 16 ba 62 e7 ... 65486 more bytes>\\n<Buffer fe 66 ac 0a 72 ef de eb 7f 05 70 b6 ... 65486 more bytes>\\n\\n\\nWe can observe that our chunks never exceed $2^8 \\\\times 2^8 = 2^{16} = 65536$ bytes.\\n\\n###### Summary of Version 1 and Version 2\\n\\n- **_Version 2_** is exactly what inputStream.pipe(outputStream) does for us. Therefore we have no hassle of worrying memory overflow problem for data streaming.\\n\\n- Not only that, pipe method also handles **back-pressure** problem which we haven\'t implemented anything to handle yet:\\n\\n###### Back-Pressure\\n\\n- If we look that the return of WriteStream.write:\\n <center></center>\\n\\n ![](/assets/tech/171/002.png)\\n <center></center>\\n\\n It is in fact a boolean.\\n\\n- When it returns false, which means that the buffer of size 65kb is not large enough to receive the incoming chunk immediately.\\n- The reason is mostly because of that the writing speed is slower than the data-pulling speed.\\n- We need to\\n - inputStream.pause() when outputStream.write returns false;\\n - inputStream.resume() when our buffer in the outputStream gets **_drained of_** data;\\n - The **drainded event** can be subscribed by\\n js\\n outputStream.on(\\"drain\\", () => { ... });\\n \\n\\n#### Download Zip of Files with Stream Manipulation\\n\\n##### Node.js\\n\\n###### Backend Implementation\\n\\nWe take AWS S3 bucket as an example. We will:\\n\\n- Use npm package aws-sdk to get ReadStream of our object through bucketName and objectKey.\\n- Use npm package archiver to pipe the ReadStream into a ZipStream, data will then be piped into our final WriteStream, i.e., res.\\n\\nLet\'s create another route called /download for downloading zip of multiple files:\\n\\njs\\napp.get(\\"/download\\", async (req, res) => {\\n const bucketName = \\"jaems-cicd\\";\\n const objectKey1 = \\"assets/fonts/FreightTextProMedium-Italic.woff2\\";\\n const objectKey2 = \\"assets/fonts/FreightTextProMedium-Italic.woff\\";\\n\\n const stream1 = await awsS3Util.getFileStream({\\n bucketName,\\n objectKey: objectKey1,\\n });\\n const stream2 = await awsS3Util.getFileStream({\\n bucketName,\\n objectKey: objectKey2,\\n });\\n\\n const zipStream = streamUtil.getZipStream();\\n\\n if (stream1) {\\n zipStream.append(stream1, { name: \\"FreightTextProMedium-Italic.woff2\\" });\\n }\\n if (stream2) {\\n zipStream.append(stream2, { name: \\"FreightTextProMedium-Italic.woff\\" });\\n }\\n zipStream.finalize();\\n zipStream.pipe(res);\\n\\n res.setHeader(\\"Access-Control-Expose-Headers\\", \\"Content-Disposition\\");\\n res.setHeader(\\n \\"Content-Disposition\\",\\n \'attachment; filename=\\"zip-file-example.zip\\"\'\\n );\\n});\\n\\n\\n- Here streamUtil.getZipStream and awsS3Util.getFileStream are defined below.\\n- ZipStream is a kind of middleware between streams, we call it a Duplex stream in node.js, which is both a ReadStream and a WriteStream.\\n- Our name field can be some/path/file.ext inside zipStream.append. archiver will mkdir -p for us.\\n- Note that by default Content-Disposition is not among the auto-allowed headers, we need to specify it explicitly.\\n- It would be better to enclose the filename by \\"\'s.\\n- Modern browser will try to parse the filename without quotes, but enclosing by \\"\'s makes frontend more easy to grab the filename using regex, as we shall see later.\\n\\n- streamUtil.getZipStream\\n\\n js\\n // streamUtil.ts\\n\\n import archiver from \\"archiver\\";\\n\\n const getZipStream = () => {\\n const archive = archiver(\\"zip\\", {\\n zlib: { level: 9 },\\n });\\n return archive;\\n };\\n\\n export default {\\n getZipStream,\\n };\\n \\n\\n- awsS3Util.getFileStream\\n\\n js\\n // awsS3Util.ts\\n import AWS from \\"aws-sdk\\";\\n\\n const S3 = new AWS.S3();\\n\\n async function getFileStream(props: {\\n bucketName: string,\\n objectKey: string,\\n }) {\\n const { bucketName, objectKey } = props;\\n return S3.getObject({\\n Bucket: bucketName,\\n Key: objectKey,\\n }).createReadStream();\\n }\\n\\n export default {\\n getFileStream,\\n };\\n \\n\\n###### Frontend to Handle the Stream\\n\\nWe extract filename from header in line 14.\\n\\njs-1\\nimport Button from \\"@mui/material/Button/Button\\";\\nimport Box from \\"@mui/material/Box\\";\\nimport axios from \\"axios\\";\\nconst fileNameRegex = /(?<=filename\\\\=\\\\\\").*?(?=\\\\\\")/g\\n\\n\\nexport default () => {\\n const downloadZip = async () => {\\n const res = await axios.get(\\"http://localhost:8080/download\\", { responseType: \\"blob\\" });\\n const blob = new Blob([res.data], { type: \\"application/zip\\" });\\n const url = window.URL.createObjectURL(blob);\\n const link = document.createElement(\\"a\\");\\n const contentDisposition = res.headers?.[\\"content-disposition\\"] as string;\\n const fileName = contentDisposition.match(fileNameRegex)?.[0] || \\"\\";\\n link.download = fileName;\\n link.href = url;\\n link.click();\\n window.URL.revokeObjectURL(url);\\n }\\n\\n return (\\n <Box sx={\\n {\\n \\"& .MuiButton-root\\": {\\n textTransform: \\"none\\"\\n }\\n }\\n }\\n >\\n <div>\\n <Button onClick={downloadZip}> Download Zip File </Button>\\n </div>\\n </Box >\\n )\\n}\\n\\n\\nNote that since it is a GET request, downloadZip can be alternatively defined by\\n\\njs\\nconst downloadZip = async () => {\\n const link = document.createElement(\\"a\\");\\n link.href = \\"http://localhost:8080/download\\";\\n link.click();\\n};\\n\\n\\nand we still get the same result (the filename can be correctly obtained).\\n\\nThe former approach (line 8) works equally well for POST request (e.g., we may want more complicated query data in the body).\\n\\n##### Springboot\\n\\n- Idea in node.js can be translated to spring directly.\\n- We create a middleware before piping data into outputStream.\\n\\njava\\n@Data\\n@Accessors(chain = true)\\npublic static class FullPathAndInputStream {\\n private String fullPath;\\n private InputStream inputStream;\\n}\\npublic static StreamingResponseBody inputStreamsIntoZip(List<FullPathAndInputStream> inputs) {\\n StreamingResponseBody responseBody = outputStream -> {\\n try (ZipOutputStream zipOutStream = new ZipOutputStream(outputStream)) {\\n for (var fullPathAndInputStream : inputs) {\\n String fullPath = fullPathAndInputStream.getFullPath().replace(\\"\\\\\\\\\\", \\"/\\");\\n InputStream inputStream = fullPathAndInputStream.getInputStream();\\n\\n ZipEntry zipEntry = new ZipEntry(fullPath);\\n zipEntry.setTime(System.currentTimeMillis());\\n zipOutStream.putNextEntry(zipEntry);\\n StreamUtils.copy(inputStream, zipOutStream);\\n zipOutStream.closeEntry();\\n inputStream.close();\\n }\\n }\\n };\\n return responseBody;\\n}\\n\\n\\nHere InputStream can be obtained from S3ObjectInputStream:\\n\\njava\\npublic S3ObjectInputStream getFileStream(String bucketName, String key) {\\n try {\\n S3Object obj = s3.getObject(bucketName, key);\\n S3ObjectInputStream s3inputStream = obj.getObjectContent();\\n return s3inputStream;\\n } catch (AmazonServiceException e) {\\n logger.error(e);\\n }\\n return null;\\n}\\n\\n\\n- In spring\'s controller we can return ResponseEntity and pass StreamingResponseBody object into its body argument.\\n- Frontend code that handles the response is the same as the previous section.\\n\\n#### Reference\\n\\n- **Web Dev Jounry**, Discussion on Streaming:\\n - [Node JS - HTTP Streaming](https://www.youtube.com/watch?v=CiGnubZC5cs)\\n - [Node JS - Streams Intro](https://www.youtube.com/watch?v=qU8PmZOOnac)\\n - [Node JS - Readable Streams](https://www.youtube.com/watch?v=_pqv06ySvuk)\\n - [Node JS - Writable Streams & Backpressure](https://www.youtube.com/watch?v=FS2OWxS5P_E&t=605s)\\n - [Node JS - Pipe, Duplex, & Transform Streams](https://www.youtube.com/watch?v=rQXaDH__Suk)\\n","title":"Handle Streams in File-Responding Request","date":"2023-09-03T00:00:00.000Z","id":"blog0171","tag":"express, java","intro":"We implement file downloading feature in a get/post request in both express and spring, we handle them in memory and therefore no disk i/o is needed.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 100%\\n }\\n</style>\\n\\njs\\nimport express from \\"express\\";\\nimport cors from \'cors\';\\n\\nconst allowlist = [\'http://localhost:3000\']\\n\\nconst corsOptionsDelegate = (req, callback) => {\\n var corsOptions;\\n\\n if (allowlist.indexOf(req.header(\'Origin\')) > -1) {\\n // reflect (enable) the requested origin in the CORS response\\n corsOptions = { origin: true };\\n } else {\\n // disable CORS for this request\\n corsOptions = { origin: false }; \\n }\\n // callback expects two parameters: (error, options)\\n callback(null, corsOptions) \\n}\\n\\napp.use(cors(corsOptionsDelegate));\\n","title":"Restrict CORS to Limited Origins","date":"2023-09-03T00:00:00.000Z","id":"blog0172","tag":"express","intro":"Record how to implement CORS in express to allow certain origins to get access instead of allowing all origins","toc":false},{"content":"\\n<style>\\n img {\\n max-width: 100%\\n }\\n</style>\\n\\n#### Routing Schema as a Type Variable\\n\\nLet\'s define a routing schema by defining, sequentially, which value is possible to appear:\\n\\njs\\nexport type NavigationRouteSchema = {\\n\\tvalue: \\"/buyer\\" | \\"/supplier\\",\\n\\tnext: {\\n\\t\\tvalue: \\"order\\",\\n\\t\\tnext: {\\n\\t\\t\\tvalue: Oid\\n\\t\\t\\tnext: {\\n\\t\\t\\t\\tvalue: \\"\\" | \\"requirements\\" | \\"quotation\\" | \\"sampling\\" | \\"freight\\"\\n\\t\\t\\t} |\\n\\t\\t\\t{\\n\\t\\t\\t\\tvalue: \\"contract\\",\\n\\t\\t\\t\\tnext: {\\n\\t\\t\\t\\t\\tvalue: Oid\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t} |\\n\\t{\\n\\t\\tvalue: \\"projects\\",\\n\\t} |\\n\\t{\\n\\t\\tvalue: \\"correspondence-dashboard\\",\\n\\t\\tnext:\\n\\t\\t{\\n\\t\\t\\tvalue: \\"mailchains\\",\\n\\t\\t\\tnext: {\\n\\t\\t\\t\\tvalue: \\"mailchainOid\\",\\n\\t\\t\\t\\tnext: {\\n\\t\\t\\t\\t\\tvalue: Oid\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t} |\\n\\t\\t{\\n\\t\\t\\tvalue: \\"unlinked-emails\\",\\n\\t\\t\\tnext: {\\n\\t\\t\\t\\tvalue: \\"emailOid\\",\\n\\t\\t\\t\\tnext: {\\n\\t\\t\\t\\t\\tvalue: Oid\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n};\\n\\n\\nFor example,\\n\\nnone\\n/buyer/order/abcdsfds123123/contract/6dsf456sd6f4s\\n\\n\\nis among the possible choices.\\n\\nLet\'s define the following type:\\n\\n#### Define Custom Type of Tuples from the Schema\\n\\njs\\ntype RouteBreakdown<T> =\\n\\tT extends { value: infer U, next: infer V } ? [U, ...RouteBreakdown<V>] :\\n\\tT extends { value: infer U } ? [U] :\\n\\tnever\\n\\nexport type ClientNavigation = RouteBreakdown<NavigationRouteSchema>;\\n\\n\\nThen by hovering ClientNavigation:\\n\\nnone\\ntype ClientNavigation = [\\"/buyer\\" | \\"/supplier\\", \\"order\\", string, \\"\\" | \\"requirements\\" |\\n\\"quotation\\" | \\"sampling\\" | \\"freight\\"] | [\\"/buyer\\" | \\"/supplier\\", \\"order\\", string, \\"contract\\",\\nstring] | [...] | [...] | [...]\\n\\n\\nwhich consists of all possible tuples of values in our NavigationRouteSchema with correct sequential order.\\n\\n#### Construct Navigation Route with ZERO Chance of Making Mistake\\n\\nNow we define\\n\\njs\\nconst getNavigationRoute = (...args: ClientNavigation) => {\\n return args.join(\\"/\\");\\n};\\n\\n\\nwhich simply assembles the values into a correct URL for navigation.\\n\\n- We are 100% confident that our path is correct without hard-coding. Why?\\n- First all value can be auto-completed:\\n\\n ![](/assets/tech/173/002.png)\\n\\n- Any route that is not among our **_tuple of string-types_** will trigger an error:\\n\\n ![](/assets/tech/173/003.png)\\n\\n- Apart from auto-complete suggestions, there will also be a pop-up indicating the type at the current positional argument:\\n\\n ![](/assets/tech/173/004.png)\\n\\n- From now on any **_change of routes_** requirement becomes extremely trivial!\\n","title":"Routing Schema for Frontend Project","date":"2023-09-05T00:00:00.000Z","id":"blog0173","tag":"react","intro":"In the browser our route consist of many information, like messageId, emailId, etc, which enables our web page to select correct data based on the route. \\nIn this post we discuss how to effectively construct such url with type safty under a RouteSchema.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 100%\\n }\\n</style>\\n\\n#### Component Study\\n\\nLet\'s study the following forms:\\n\\n<Center>\\n<a href=\\"/assets/tech/174/003.png\\">\\n<img src=\\"/assets/tech/174/003.png\\"/>\\n</a>\\n</Center>\\n\\n<p></p>\\n\\n<center></center>\\n\\n- We can obseve that there are many repititions.\\n- Working any single part is very easy, we just nest the CSS and the <div>\'s.\\n- The problem becomes complicated when it comes to reusing what we have done.\\n\\n#### Grabbing Child Compoenent of Desired Component Name\\n\\nBefore proceeding, let\'s discuss how a react component can identify certain special children component, extract them, and place it in our desired order:\\n\\njs\\nimport React, { ReactNode } from \\"react\\";\\n\\nfunction grabChildrenByType(children: ReactNode, type: string) {\\n\\tconst mappedChildren = React.Children.map(children, (child) => {\\n\\t\\tconst child_ = child as (React.ReactElement<any> & { type?: { name?: string } });\\n\\t\\tif (React.isValidElement(child_)\\n\\t\\t\\t&& (child_?.type?.name || \\"\\") === type\\n\\t\\t) {\\n\\t\\t\\treturn child;\\n\\t\\t} else {\\n\\t\\t\\treturn null;\\n\\t\\t}\\n\\t}) as JSX.Element[];\\n\\treturn mappedChildren;\\n}\\n\\nexport default { grabChildrenByType }\\n\\n\\nBasically each children should have .type.name.\\n\\nNote that if a component is generated by a function with trait () => () => JSX.Element, then this compoonent will have empty string as its .type.name.\\n\\n#### Divide the Component into Reusable ones\\n\\nIn general there are only two sub-components:\\n\\n- ![](/assets/tech/174/004.png)\\n\\nLet\'s denote DC as DialogComponent. The above component can be described by\\n\\njs\\n<DC.RadioRow>\\n <DC.RadioRowButton>\\n <RadioButton />\\n </DC.RadioRowButton>\\n\\n <DC.RadioRowTitle>Create New Buyer Company</DC.RadioRowTitle>\\n</DC.RadioRow>\\n\\n\\nwhere each of the dirty parts is defined below.\\n\\njs\\nexport class DC {\\n\\tpublic static RadioRowButton({ children, ...props }: {\\n\\t\\tchildren: ReactNode\\n\\t} & HTMLAttributes<HTMLDivElement>) { return <div {...props}>{children}</div> }\\n\\tpublic static RadioRowTitle({ children, ...props }: {\\n\\t\\tchildren: ReactNode\\n\\t} & HTMLAttributes<HTMLDivElement>) { return <div {...props}>{children}</div> }\\n\\n\\tpublic static RadioRow({ children, ...props }: { children: ReactNode } & HTMLAttributes<HTMLDivElement>) {\\n\\t\\tconst radioBlock = childUtil.grabChildrenByType(children, DC.RadioRowButton.name);\\n\\t\\tconst titleBlock = childUtil.grabChildrenByType(children, DC.RadioRowTitle.name);\\n\\t\\treturn (\\n\\t\\t\\t<div style={{ display: \\"flex\\" }}>\\n\\t\\t\\t\\t<div style={{ display: \\"flex\\", justifyContent: \\"flex-end\\", }}>\\n\\t\\t\\t\\t\\t{radioBlock}\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t<div style={{ flex: 1 }}>\\n\\t\\t\\t\\t\\t<div style={{ paddingTop: 4 }}>{titleBlock}</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</div>\\n\\t\\t)\\n\\t}\\n}\\n\\n\\nNote that RadioRowButton and RadioRowTitle are simply children => children and identitcal. They serve as a building block of our custom rendering.\\n\\n- ![](/assets/tech/174/005.png)\\n\\nThis can be expressed by\\n\\njs\\n<DC.InputRow>\\n <DC.InputRowLeft>Choose Company</DC.InputRowLeft>\\n <DC.InputRowRight>\\n <GeneralDropdown fullList={buyerCompany} initialValue={null} />\\n </DC.InputRowRight>\\n</DC.InputRow>\\n\\n\\nWe hide all the dirty work by defining:\\n\\njs\\nexport class DC {\\n\\tpublic static InputRowLeft({ children, ...props }: { children: ReactNode } & HTMLAttributes<HTMLDivElement>) { return <div {...props}>{children}</div> }\\n\\tpublic static InputRowRight({ children, ...props }: { children: ReactNode } & HTMLAttributes<HTMLDivElement>) { return <div {...props}>{children}</div> }\\n\\tpublic static InputRow({ children, ...props }: { children: ReactNode } & HTMLAttributes<HTMLDivElement>) {\\n\\t\\tconst left = childUtil.grabChildrenByType(children, DC.InputRowLeft.name);\\n\\t\\tconst right = childUtil.grabChildrenByType(children, DC.InputRowRight.name);\\n\\t\\treturn (\\n\\t\\t\\t<tr>\\n\\t\\t\\t\\t<td style={{ verticalAlign: \\"middle\\" }}>{left}</td>\\n\\t\\t\\t\\t<td style={{ verticalAlign: \\"middle\\" }}>{right}</td>\\n\\t\\t\\t</tr>\\n\\t\\t)\\n\\t}\\n\\n\\nFinally we want our component to be able to capture <InputRow /> and <RadioRow /> and inject the components in desired position.\\n\\njs\\npublic static Body({ children, ...props }: { children: ReactNode } & HTMLAttributes<HTMLDivElement>) {\\n\\tconst inputs = childUtil.grabChildrenByType(children, DC.InputRow.name);\\n\\tconst radioRows = childUtil.grabChildrenByType(children, DC.RadioRow.name);\\n\\treturn (\\n\\t\\t<div {...props}>\\n\\t\\t\\t{radioRows}\\n\\t\\t\\t<Spacer height={5} />\\n\\t\\t\\t<Table sx={\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\\"& td\\": {\\n\\t\\t\\t\\t\\t\\tpaddingTop: \\"5px\\"\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t\\"& td:nth-child(1)\\": {\\n\\t\\t\\t\\t\\t\\twidth: 160,\\n\\t\\t\\t\\t\\t\\tverticalAlign: \\"middle\\",\\n\\t\\t\\t\\t\\t\\tpaddingLeft: \\"34px\\"\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t\\"& td:nth-child(2)\\": { verticalAlign: \\"middle\\" }\\n\\t\\t\\t\\t}\\n\\t\\t\\t\\t}>\\n\\t\\t\\t\\t{inputs}\\n\\t\\t\\t</Table>\\n\\t\\t</div>\\n\\t)\\n}\\n\\n\\n#### Final Result\\n\\nIf we put all the single pieces into the component we want, it will look very long:\\n\\njs\\nexport type Step1FormData = {\\n\\toption?: string,\\n\\tcompany?: string,\\n\\temailDomain?: string\\n}\\n\\nexport default ({ step1FormData }: { step1FormData: React.MutableRefObject<Step1FormData> }) => {\\n\\tconst dispatch = useAppDispatch();\\n\\tconst { RadioButton, selectedValue, setSelectedValue } = useWbRadioButtonGroup<string>();\\n\\tconst updateField = (value: Partial<Step1FormData>) => {\\n\\t\\tstep1FormData.current = { ...step1FormData.current, ...value };\\n\\t}\\n\\tconst [buyerCompany, setBuyerCompany] = useState<BuyerCompany[]>([]);\\n\\n\\tuseEffect(() => {\\n\\t\\tdispatch(mailThunkAction.getBuyerCompanies())\\n\\t\\t\\t.unwrap()\\n\\t\\t\\t.then(data => { setBuyerCompany(data); })\\n\\t\\t\\t.catch(err => { snackbarUtils.error(JSON.stringify(err)) });\\n\\t}, []);\\n\\n\\n\\treturn (\\n\\t\\t<>\\n\\t\\t\\t<DC.Body>\\n\\t\\t\\t\\t<DC.RadioRow>\\n\\t\\t\\t\\t\\t<DC.RadioRowButton>\\n\\t\\t\\t\\t\\t\\t<RadioButton\\n\\t\\t\\t\\t\\t\\t\\tvalue={options.step1.CREATE_NEW_COMPANY}\\n\\t\\t\\t\\t\\t\\t\\tstyle={{ padding: 0, paddingRight: 10 }}\\n\\t\\t\\t\\t\\t\\t\\tonClick={() => { updateField({ option: options.step1.CREATE_NEW_COMPANY }) }}\\n\\t\\t\\t\\t\\t\\t/>\\n\\t\\t\\t\\t\\t</DC.RadioRowButton>\\n\\t\\t\\t\\t\\t<DC.RadioRowTitle>\\n\\t\\t\\t\\t\\t\\tCreate New Buyer Company\\n\\t\\t\\t\\t\\t</DC.RadioRowTitle>\\n\\t\\t\\t\\t</DC.RadioRow>\\n\\n\\t\\t\\t\\t<DC.InputRow>\\n\\t\\t\\t\\t\\t<DC.InputRowLeft>\\n\\t\\t\\t\\t\\t\\tChoose Company\\n\\t\\t\\t\\t\\t</DC.InputRowLeft>\\n\\t\\t\\t\\t\\t<DC.InputRowRight>\\n\\t\\t\\t\\t\\t\\t<GeneralDropdown fullList={buyerCompany} initialValue={null} />\\n\\t\\t\\t\\t\\t</DC.InputRowRight>\\n\\t\\t\\t\\t</DC.InputRow>\\n\\n\\t\\t\\t\\t<DC.InputRow>\\n\\t\\t\\t\\t\\t<DC.InputRowLeft>\\n\\t\\t\\t\\t\\t\\tEmail Domain\\n\\t\\t\\t\\t\\t</DC.InputRowLeft>\\n\\t\\t\\t\\t\\t<DC.InputRowRight>\\n\\t\\t\\t\\t\\t\\t<div style={{ display: \\"flex\\", alignItems: \\"center\\" }}>\\n\\t\\t\\t\\t\\t\\t\\t<div style={{ display: \\"inline-block\\", marginRight: 10 }}>@</div>\\n\\t\\t\\t\\t\\t\\t\\t<WbInput onChange={e => { updateField({ emailDomain: e.target.value }) }} />\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</DC.InputRowRight>\\n\\t\\t\\t\\t</DC.InputRow>\\n\\t\\t\\t</DC.Body>\\n\\t\\t\\t<Spacer height={20} />\\n\\n\\t\\t\\t<DC.Body>\\n\\t\\t\\t\\t<DC.RadioRow>\\n\\t\\t\\t\\t\\t<DC.RadioRowButton>\\n\\t\\t\\t\\t\\t\\t<RadioButton\\n\\t\\t\\t\\t\\t\\t\\tvalue={options.step1.USE_EXISTING_COMPANY}\\n\\t\\t\\t\\t\\t\\t\\tstyle={{ padding: 0, paddingRight: 10 }}\\n\\t\\t\\t\\t\\t\\t\\tonClick={() => { updateField({ option: options.step1.USE_EXISTING_COMPANY }) }}\\n\\t\\t\\t\\t\\t\\t/>\\n\\t\\t\\t\\t\\t</DC.RadioRowButton>\\n\\t\\t\\t\\t\\t<DC.RadioRowTitle>\\n\\t\\t\\t\\t\\t\\tUse Existing Company\\n\\t\\t\\t\\t\\t</DC.RadioRowTitle>\\n\\t\\t\\t\\t</DC.RadioRow>\\n\\n\\t\\t\\t\\t<DC.InputRow>\\n\\t\\t\\t\\t\\t<DC.InputRowLeft>\\n\\t\\t\\t\\t\\t\\tChoose Company\\n\\t\\t\\t\\t\\t</DC.InputRowLeft>\\n\\t\\t\\t\\t\\t<DC.InputRowRight>\\n\\t\\t\\t\\t\\t\\t<WbInput />\\n\\t\\t\\t\\t\\t</DC.InputRowRight>\\n\\t\\t\\t\\t</DC.InputRow>\\n\\n\\t\\t\\t\\t<DC.InputRow>\\n\\t\\t\\t\\t\\t<DC.InputRowLeft>\\n\\t\\t\\t\\t\\t\\tEmail Domain\\n\\t\\t\\t\\t\\t</DC.InputRowLeft>\\n\\t\\t\\t\\t\\t<DC.InputRowRight>\\n\\t\\t\\t\\t\\t\\t<div style={{ display: \\"flex\\", alignItems: \\"center\\" }}>\\n\\t\\t\\t\\t\\t\\t\\t<div style={{ display: \\"inline-block\\", marginRight: 10 }}>@</div>\\n\\t\\t\\t\\t\\t\\t\\t<WbInput onChange={e => { updateField({ emailDomain: e.target.value }) }} />\\n\\t\\t\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t\\t</DC.InputRowRight>\\n\\t\\t\\t\\t</DC.InputRow>\\n\\t\\t\\t</DC.Body>\\n\\t\\t</>\\n\\t)\\n}\\n\\n\\n#### Simplification of the Final Result\\n\\nThe result so far is not complicated but **_cumbersome_**, if we look back there are room for simplification.\\n\\nWe use the same concept but define only **_two_** child components:\\n\\n- For the biggest components (the whole form in our case) we devide it by grabbing relatively smaller component.\\n- For relatively smaller components we divide it by injecting dynamic components by props.\\n\\njs\\nexport class DC {\\n\\tpublic static InputRow({ left, right }: {\\n\\t\\tleft: ReactNode,\\n\\t\\tright: ReactNode,\\n\\t}) {\\n\\t\\treturn (\\n\\t\\t\\t<tr>\\n\\t\\t\\t\\t<td style={{ verticalAlign: \\"middle\\" }}>{left}</td>\\n\\t\\t\\t\\t<td style={{ verticalAlign: \\"middle\\" }}>{right}</td>\\n\\t\\t\\t</tr>\\n\\t\\t)\\n\\n\\t}\\n\\tpublic static RadioRow({ left, right }: {\\n\\t\\tleft: ReactNode,\\n\\t\\tright: ReactNode,\\n\\t}) {\\n\\t\\treturn (\\n\\t\\t\\t<div style={{ display: \\"flex\\" }}>\\n\\t\\t\\t\\t<div id=\\"raio-column\\" style={{ display: \\"flex\\", justifyContent: \\"flex-end\\", }}>\\n\\t\\t\\t\\t\\t{left}\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t\\t<div id=\\"input-form\\" style={{ flex: 1 }}>\\n\\t\\t\\t\\t\\t<div style={{ paddingTop: 4 }}>{right}</div>\\n\\t\\t\\t\\t</div>\\n\\t\\t\\t</div>\\n\\t\\t)\\n\\t}\\n\\n\\tpublic static Body({ children, disabled = true, ...props }: {\\n\\t\\tchildren: ReactNode,\\n\\t\\tdisabled?: boolean\\n\\t} & HTMLAttributes<HTMLDivElement>) {\\n\\t\\tconst inputs = childUtil.grabChildrenByType(children, DC.InputRow.name);\\n\\t\\tconst radioRows = childUtil.grabChildrenByType(children, DC.RadioRow.name);\\n\\t\\tconst disabledStyle = { opacity: 0.5, pointerEvents: \\"none\\" };\\n\\t\\treturn (\\n\\t\\t\\t<div\\n\\t\\t\\t\\t{...props}\\n\\t\\t\\t>\\n\\t\\t\\t\\t{radioRows}\\n\\t\\t\\t\\t<Spacer height={5} />\\n\\t\\t\\t\\t<Table sx={{\\n\\t\\t\\t\\t\\t\\"& td\\": {\\n\\t\\t\\t\\t\\t\\tpaddingTop: \\"5px\\",\\n\\t\\t\\t\\t\\t\\t...(disabled ? disabledStyle : {})\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t\\"& td:nth-child(1)\\": {\\n\\t\\t\\t\\t\\t\\twidth: 160,\\n\\t\\t\\t\\t\\t\\tverticalAlign: \\"middle\\",\\n\\t\\t\\t\\t\\t\\tpaddingLeft: \\"34px\\"\\n\\t\\t\\t\\t\\t},\\n\\t\\t\\t\\t\\t\\"& td:nth-child(2)\\": {\\n\\t\\t\\t\\t\\t\\tverticalAlign: \\"middle\\"\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}}>\\n\\t\\t\\t\\t\\t{inputs}\\n\\t\\t\\t\\t</Table>\\n\\t\\t\\t</div>\\n\\t\\t)\\n\\t}\\n}\\n\\n\\nWhich becomes\\n\\njs\\n<DC.Body>\\n\\t<DC.RadioRow\\n\\t\\tleft={\\n\\t\\t\\t<RadioButton\\n\\t\\t\\t\\tvalue={options.step1.CREATE_NEW_COMPANY}\\n\\t\\t\\t\\tstyle={{ padding: 0, paddingRight: 10 }}\\n\\t\\t\\t\\tonClick={() => { updateField({ option: options.step1.CREATE_NEW_COMPANY }) }}\\n\\t\\t\\t/>\\n\\t\\t}\\n\\t\\tright={<span>Create New Buyer Company</span>}\\n\\t/>\\n\\n\\t<DC.InputRow\\n\\t\\tleft={<span>Company Name</span>}\\n\\t\\tright={<WbInput onChange={e => updateField({ newCompName: e.target.value })} />}\\n\\t/>\\n\\n\\t<DC.InputRow\\n\\t\\tleft={<span>Email Domain</span>}\\n\\t\\tright={\\n\\t\\t\\t<div style={{ display: \\"flex\\", alignItems: \\"center\\" }}>\\n\\t\\t\\t\\t<div style={{ display: \\"inline-block\\", marginRight: 10 }}>@</div>\\n\\t\\t\\t\\t<WbInput onChange={e => { updateField({ emailDomain: e.target.value }) }} />\\n\\t\\t\\t</div>\\n\\t\\t}\\n\\t/>\\n</DC.Body >\\n<Spacer height={20} />\\n\\n<DC.Body disabled={selectedOption !== options.step1.USE_EXISTING_COMPANY}>\\n\\t<DC.RadioRow\\n\\t\\tleft={\\n\\t\\t\\t<RadioButton\\n\\t\\t\\t\\tvalue={options.step1.USE_EXISTING_COMPANY}\\n\\t\\t\\t\\tstyle={{ padding: 0, paddingRight: 10 }}\\n\\t\\t\\t\\tonClick={() => { updateField({ option: options.step1.USE_EXISTING_COMPANY }) }}\\n\\t\\t\\t/>\\n\\t\\t}\\n\\t\\tright={<span>Use Existing Company</span>}\\n\\t/>\\n\\n\\t<DC.InputRow\\n\\t\\tleft={<span>Choose Company</span>}\\n\\t\\tright={<GeneralDropdown\\n\\t\\t\\tfullList={buyerCompany}\\n\\t\\t\\tinitialValue={null}\\n\\t\\t\\trefUpdateHandler={(option) => {\\n\\t\\t\\t\\tupdateField({ existingBuyerCompany: option!, compCodeForStep2: option?.code || \\"\\" });\\n\\t\\t\\t}}\\n\\t\\t/>}\\n\\t/>\\n\\t<DC.InputRow\\n\\t\\tleft={<span>Email Domain</span>}\\n\\t\\tright={\\n\\t\\t\\t<div style={{ display: \\"flex\\", alignItems: \\"center\\" }}>\\n\\t\\t\\t\\t<div style={{ display: \\"inline-block\\", marginRight: 10 }}>@</div>\\n\\t\\t\\t\\t<WbInput onChange={e => { updateField({ emailDomain: e.target.value }) }} />\\n\\t\\t\\t</div>\\n\\t\\t}\\n\\t/>\\n</DC.Body>\\n\\n\\n#### Summary\\n\\n- Our **_child-grabbing_** strategy helps locate **_specific component_** in **_specific location_**.\\n- We can design elements injection of DC.Body by\\n\\n js\\n <DC.Body\\n \\tradios={<><DC.Radiorow {...} /></>}\\n \\tinputs={<><DC.InputRow {...} /></>}\\n />\\n \\n\\n which is much flexible than:\\n\\n- js\\n <DC.Body> \\t\\t\\t\\t // only child of type RadioRow or InputRow is allowed,\\n \\t<DC.RadioRow {...} />\\t\\t// look of output is easy to expect\\n \\t<DC.InputRow {...} />\\n </DC.Body>\\n \\n\\n- No absolute true/false, they are all matter of taste (by the way I love the latter one).\\n","title":"Custom Rendering of Custom Child Component","date":"2023-09-06T00:00:00.000Z","id":"blog0174","tag":"react","intro":"We dicuss how to customize the way of rendering custom children component to produce highly reusable code.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 100%\\n }\\n</style>\\n\\n#### Repository\\n\\n- https://github.com/machingclee/2023-09-10-Elliptic-Curve-in-Rust/tree/main/ECC/src/modules\\n\\n#### Prefered Headers to Ignore Annoying Warnings:\\n\\nrust\\n#![allow(unused)]\\n#![allow(non_camel_case_types)]\\n#![allow(non_snake_case)]\\n\\n\\n#### Utility Struct to Perform Arithmetic on $\\\\mathbb Z/ p \\\\mathbb Z$ for BigUint\\n\\nrust\\nstruct Fp<\'a> {\\n p: &\'a BigUint,\\n}\\n\\nimpl<\'a> Fp<\'a> {\\n fn power(&self, u: &BigUint, i: u32) -> BigUint {\\n u.modpow(&BigUint::from(i), &self.p)\\n }\\n\\n fn add(&self, u: &BigUint, v: &BigUint) -> BigUint {\\n (u + v).modpow(&BigUint::from(1u32), self.p)\\n }\\n fn mul(&self, u: &BigUint, v: &BigUint) -> BigUint {\\n (u * v).modpow(&BigUint::from(1u32), self.p)\\n }\\n fn add_inverse(&self, u: &BigUint) -> BigUint {\\n assert!(\\n u < &self.p,\\n \\"{}\\",\\n format!(\\"{} >= {} should not happen\\", u, &self.p)\\n );\\n self.p - u\\n }\\n fn mul_inverse(&self, u: &BigUint) -> BigUint {\\n if self.p < &BigUint::from(2u32) {\\n BigUint::from(1u32)\\n } else {\\n let two = BigUint::from(2u32);\\n let power = self.add(&self.p, &self.add_inverse(&two));\\n u.modpow(&power, &self.p)\\n }\\n }\\n}\\n\\n\\n#### Definition of Addition and Double on Elliptic Curve\\n\\nrust\\nuse num_bigint::BigUint;\\n\\n#[derive(PartialEq, Clone, Debug)]\\nenum Point {\\n Coor(BigUint, BigUint),\\n Identity,\\n}\\n\\nstruct EllipticCurve {\\n a: BigUint,\\n b: BigUint,\\n p: BigUint,\\n}\\n\\nimpl EllipticCurve {\\n fn double(&self, h: &Point) -> Point {\\n let fp = Fp { p: &self.p };\\n let h_on_curve = self.is_on_curve(h);\\n assert!(h_on_curve, \\"point h is not on the curve\\");\\n // s = (3*x^2 + a)/(2*y)\\n // x_ = s^2 - 2*x\\n // y_ = s*(x - x_) - y\\n match h {\\n Point::Identity => Point::Identity,\\n Point::Coor(x, y) => {\\n let three_times_xsq = fp.mul(&BigUint::from(3u32), &fp.power(x, 2));\\n let two_times_y = fp.mul(&BigUint::from(2u32), &y);\\n let inverse_two_times_y = fp.mul_inverse(&two_times_y);\\n let s = fp.mul(&fp.add(&three_times_xsq, &self.a), &inverse_two_times_y);\\n let x_ = fp.add(\\n &fp.power(&s, 2),\\n &fp.add_inverse(&fp.mul(&BigUint::from(2u32), x)),\\n );\\n let s_times_x_minus_x_ = fp.mul(&s, &fp.add(x, &fp.add_inverse(&x_)));\\n let y_ = fp.add(&s_times_x_minus_x_, &fp.add_inverse(y));\\n Point::Coor(x_, y_)\\n }\\n }\\n }\\n\\n fn add(&self, h: &Point, k: &Point) -> Point {\\n let fp = Fp { p: &self.p };\\n let h_on_curve = self.is_on_curve(h);\\n let k_on_curve = self.is_on_curve(k);\\n assert!(*h != *k, \\"two points should not be the same\\");\\n assert!(h_on_curve, \\"point h is not on the curve\\");\\n assert!(k_on_curve, \\"point k is not on the curve\\");\\n match (h, k) {\\n (Point::Identity, _) => k.clone(),\\n (_, Point::Identity) => h.clone(),\\n (Point::Coor(x1, y1), Point::Coor(x2, y2)) => {\\n // s = (y2-y1)/(x2-x1)\\n // x3 = s^2 - x1 - x2\\n // y3 = s*(x1-x3) - y1\\n\\n let y1_plus_y2 = fp.add(y1, y2);\\n if x1 == x2 && y1_plus_y2 == BigUint::from(0u32) {\\n return Point::Identity;\\n }\\n\\n let s = fp.mul(\\n &(y2 + &fp.add_inverse(y1)),\\n &fp.mul_inverse(&(x2 + &fp.add_inverse(x1))),\\n );\\n let x3 = fp.add(\\n &fp.add(&fp.power(&s, 2), &fp.add_inverse(x1)),\\n &fp.add_inverse(x2),\\n );\\n\\n let y3 = fp.add(\\n &fp.mul(&s, &(x1 + &fp.add_inverse(&x3))),\\n &fp.add_inverse(y1),\\n );\\n Point::Coor(x3, y3)\\n }\\n }\\n }\\n\\n fn scalar_mul(&self, c: &Point, d: &BigUint) -> Point {\\n let mut t = c.clone();\\n for i in (0..(d.bits() - 1)).rev() {\\n t = self.double(&t);\\n if d.bit(i) {\\n t = self.add(&t, c);\\n }\\n }\\n t\\n }\\n\\n fn is_on_curve(&self, point: &Point) -> bool {\\n let fp = Fp { p: &self.p };\\n if let Point::Coor(x, y) = point {\\n let y2 = fp.power(y, 2);\\n let x3 = fp.power(x, 3);\\n let ax = fp.mul(&self.a, x);\\n y2 == fp.add(&x3, &fp.add(&ax, &self.b))\\n } else {\\n true\\n }\\n }\\n}\\n\\n\\n#### Operator Overloading\\n\\n##### New Struct Field\\n\\nWe wish to convert BigUint into our own struct Field and define all the usual operation on $\\\\mathbb Z/p \\\\mathbb Z$, i.e., among the Field objects.\\n\\nThat is, we wish to overload the operators:\\n\\ntext\\n+ - * /\\n\\n\\nwithout the utility struct Fp.\\n\\nrust\\n#[derive(PartialEq, Clone, Debug)]\\npub struct Field<\'a> {\\n value: BigUint,\\n p: &\'a BigUint,\\n}\\n\\n\\n##### New Definition of Point and EllipticCurve base on Field\\n\\nrust\\npub enum Point<\'a> {\\n Coor(Field<\'a>, Field<\'a>),\\n Identity,\\n}\\n\\npub struct EllipticCurve<\'a> {\\n a: Field<\'a>,\\n b: Field<\'a>,\\n}\\n\\n\\n##### Operator Overloadings on Field\\n\\n###### Implementations\\n\\nrust\\npub struct Field<\'a> {\\n pub value: BigUint,\\n pub p: &\'a BigUint,\\n}\\nimpl<\'a> Field<\'a> {\\n pub fn new(i: u32, p: &\'a BigUint) -> Self {\\n Field {\\n value: BigUint::from(i),\\n p,\\n }\\n }\\n}\\n\\nimpl<\'a> Add<&Field<\'a>> for &Field<\'a> {\\n type Output = Field<\'a>;\\n\\n fn add(self, rhs: &Field) -> Self::Output {\\n let value = (&self.value + &rhs.value).modpow(&BigUint::from(1u32), self.p);\\n Field { value, p: self.p }\\n }\\n}\\n\\nimpl<\'a> Sub<&Field<\'a>> for &Field<\'a> {\\n type Output = Field<\'a>;\\n\\n fn sub(self, rhs: &Field) -> Self::Output {\\n let value: BigUint;\\n let a = &self.value;\\n let b = &rhs.value;\\n if a > b {\\n value = a - b;\\n } else {\\n value = (self.p + a) - b;\\n }\\n Field { value, p: &self.p }\\n }\\n}\\n\\nimpl<\'a> Mul<BigUint> for &Field<\'a> {\\n type Output = Field<\'a>;\\n fn mul(self, rhs: BigUint) -> Self::Output {\\n let a = &self.value;\\n let value = (a * &rhs).modpow(&BigUint::from(1u32), &self.p);\\n return Field { value, p: self.p };\\n }\\n}\\n\\nimpl<\'a> Mul<&Field<\'a>> for &Field<\'a> {\\n type Output = Field<\'a>;\\n\\n fn mul(self, rhs: &Field) -> Self::Output {\\n let value = (&self.value * &rhs.value).modpow(&BigUint::from(1u32), self.p);\\n Field { value, p: self.p }\\n }\\n}\\n\\nimpl<\'a> Div<&Field<\'a>> for &Field<\'a> {\\n type Output = Field<\'a>;\\n\\n fn div(self, rhs: &Field) -> Self::Output {\\n let left = &self.value;\\n let right = &rhs.value;\\n let p_minus_2 = (self.p - BigUint::from(2u32)).modpow(&BigUint::from(1u32), self.p);\\n\\n let multiplicative_inverse_right = right.modpow(&p_minus_2, &self.p);\\n let value = (left * &multiplicative_inverse_right).modpow(&BigUint::from(1u32), self.p);\\n Field { value, p: &self.p }\\n }\\n}\\n\\n\\n###### Rewrite of EllipticCurve::double With Field in Place of BigUint\\n\\nrust\\nimpl<\'a> EllipticCurve<\'a> {\\n\\t\\tpub fn double(&self, h: &Point<\'a>) -> Point {\\n let h_on_curve = self.is_on_curve(h);\\n assert!(h_on_curve, \\"point h is not on the curve\\");\\n // s = (3*x^2 + a)/(2*y)\\n // x_ = s^2 - 2*x\\n // y_ = s*(x - x_) - y\\n match h {\\n Point::Identity => Point::Identity,\\n Point::Coor(xp, yp) => {\\n if yp.value == BigUint::from(0u32) {\\n return Point::Identity;\\n }\\n let two_times_yp = yp * BigUint::from(2u32);\\n let s = xp * xp;\\n let s = &s * BigUint::from(3u32);\\n let s = &s + &self.a;\\n let s = &s / &two_times_yp;\\n\\n let two_times_x = xp * BigUint::from(2u32);\\n let new_x = &s * &s;\\n let new_x = &new_x - &two_times_x;\\n\\n let new_y = xp - &new_x;\\n let new_y = &s * &new_y;\\n let new_y = &new_y - yp;\\n\\n Point::Coor(new_x, new_y)\\n }\\n }\\n }\\n}\\n\\n\\n###### Rewrite of EllipticCurve::add With Field in Place of BigUint\\n\\nrust\\nimpl<\'a> EllipticCurve<\'a> {\\n\\t\\tpub fn add(&self, h: &Point<\'a>, k: &Point<\'a>) -> Point {\\n let h_on_curve = self.is_on_curve(h);\\n let k_on_curve = self.is_on_curve(k);\\n assert!(*h != *k, \\"two points should not be the same\\");\\n assert!(h_on_curve, \\"point h is not on the curve\\");\\n assert!(k_on_curve, \\"point k is not on the curve\\");\\n match (h, k) {\\n (Point::Identity, _) => k.to_owned(),\\n (_, Point::Identity) => h.to_owned(),\\n (Point::Coor(x1p, y1p), Point::Coor(x2p, y2p)) => {\\n if x1p == x2p && (y1p + y2p).value == BigUint::from(0u32) {\\n return Point::Identity;\\n }\\n // s = (y2-y1)/(x2-x1)\\n // x3 = s^2 - x1 - x2\\n // y3 = s*(x1-x3) - y1\\n\\n let s = y2p - y1p;\\n let x2_minus_x1 = x2p - x1p;\\n let s = &s / &x2_minus_x1;\\n let s_square = &s * &s;\\n\\n let x3p = &s_square - &x1p;\\n let x3p = &x3p - &x2p;\\n\\n let y3p = &s * &(x1p - &x3p);\\n let y3p = &y3p - &y1p;\\n\\n Point::Coor(x3p, y3p)\\n }\\n }\\n }\\n}\\n\\n\\n###### Rewrite of EllipticCurve::is_on_curve With Field in Place of BigUint\\n\\nrust\\nimpl<\'a> EllipticCurve<\'a> {\\n\\t\\tfn is_on_curve(&self, point: &Point) -> bool {\\n\\t\\t\\t\\tif let Point::Coor(x, y) = point {\\n\\t\\t\\t\\t\\t\\tlet y2 = y * y;\\n\\t\\t\\t\\t\\t\\tlet x3 = x * x;\\n\\t\\t\\t\\t\\t\\tlet x3 = &x3 * x;\\n\\t\\t\\t\\t\\t\\tlet ax = x * &self.a;\\n\\t\\t\\t\\t\\t\\ty2 == &(&x3 + &ax) + &self.b\\n\\t\\t\\t\\t} else {\\n\\t\\t\\t\\t\\t\\ttrue\\n\\t\\t\\t\\t}\\n\\t\\t}\\n}\\n\\n\\n###### EllipticCurve::scalar_mul --- the Double and Add Algorithm under Field\\n\\nrust\\nimpl<\'a> EllipticCurve<\'a> {\\n pub fn scalar_mul(&\'a self, q: &Point<\'a>, k: &Field<\'a>) -> Point<\'a> {\\n let mut t = q.clone();\\n for i in (0..(k.value.bits() - 1)).rev() {\\n t = self.double(&t);\\n if k.value.bit(i) {\\n t = self.add(&t, q);\\n }\\n }\\n t\\n }\\n}\\n\\n\\n#### Test Cases\\n\\nThe following 3 cases can pass successfully:\\n\\ntext\\nrunning 4 tests\\ntest test::test_ec_point_add_identity ... ok\\ntest test::test_ec_point_addition ... ok\\ntest test::test_double ... ok\\ntest test::test_scalar_mul ... ok\\n\\n\\nrust\\n#[cfg(test)]\\nmod test {\\n use super::*;\\n #[test]\\n fn test_ec_point_addition() {\\n let p = BigUint::from(17u32);\\n let ec = EllipticCurve {\\n a: Field::new(2, &p),\\n b: Field::new(2, &p),\\n };\\n\\n // (6, 3) + (5, 1) = (10, 6);\\n let p1 = Point::Coor(Field::new(6, &p), Field::new(3, &p));\\n let p2 = Point::Coor(Field::new(5, &p), Field::new(1, &p));\\n let r = Point::Coor(Field::new(10, &p), Field::new(6, &p));\\n\\n let res = ec.add(&p1, &p2);\\n assert_eq!(r, res);\\n }\\n #[test]\\n fn test_ec_point_add_identity() {\\n let p = BigUint::from(17u32);\\n let ec = EllipticCurve {\\n a: Field::new(2, &p),\\n b: Field::new(2, &p),\\n };\\n\\n // (6, 3) + (5, 1) = (10, 6);\\n let p1 = Point::Coor(Field::new(6, &p), Field::new(3, &p));\\n let p2 = Point::Identity;\\n let expect = Point::Coor(Field::new(6, &p), Field::new(3, &p));\\n\\n let result = ec.add(&p1, &p2);\\n assert_eq!(expect, result);\\n }\\n\\n #[test]\\n fn test_scalar_mul() {\\n let p = BigUint::from(17u32);\\n let ec = EllipticCurve {\\n a: Field::new(2, &p),\\n b: Field::new(2, &p),\\n };\\n let q = Point::Coor(Field::new(5, &p), Field::new(1, &p));\\n let k = BigUint::from(16u32);\\n let result = ec.scalar_mul(&q, &k);\\n\\n let expected = Point::Coor(Field::new(10, &p), Field::new(11, &p));\\n assert_eq!(result, expected);\\n }\\n\\n #[test]\\n fn test_double() {\\n let p = BigUint::from(17u32);\\n let ec = EllipticCurve {\\n a: Field::new(2, &p),\\n b: Field::new(2, &p),\\n };\\n\\n let p = Point::Coor(Field::new(6, &p), Field::new(3, &p));\\n let double = ec.double(&p);\\n let p_on_curve = ec.is_on_curve(&double);\\n assert!(p_on_curve);\\n }\\n}\\n\\n\\n#### Specific case: The Secp256k1\\n\\nConfiguration of the values: n(order), p, a, b can be found [in this wiki page](https://en.bitcoin.it/wiki/Secp256k1).\\n\\nWe test our algorithm by the following test case:\\n\\nrust\\nfn test_secp256k1() {\\n\\t\\tlet p = BigUint::parse_bytes(\\n\\t\\t\\t\\tb\\"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F\\",\\n\\t\\t\\t\\t16,\\n\\t\\t)\\n\\t\\t.expect(\\"Parsing fail for p\\");\\n\\n\\t\\tlet a = BigUint::parse_bytes(\\n\\t\\t\\t\\tb\\"0000000000000000000000000000000000000000000000000000000000000000\\",\\n\\t\\t\\t\\t16,\\n\\t\\t)\\n\\t\\t.expect(\\"Parsing fail for a\\");\\n\\n\\t\\tlet b = BigUint::parse_bytes(\\n\\t\\t\\t\\tb\\"0000000000000000000000000000000000000000000000000000000000000007\\",\\n\\t\\t\\t\\t16,\\n\\t\\t)\\n\\t\\t.expect(\\"Parsing fail for b\\");\\n\\n\\t\\tlet n = BigUint::parse_bytes(\\n\\t\\t\\t\\tb\\"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141\\",\\n\\t\\t\\t\\t16,\\n\\t\\t)\\n\\t\\t.expect(\\"Parsing fail for n\\");\\n\\n\\t\\tlet x = BigUint::parse_bytes(\\n\\t\\t\\t\\tb\\"79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798\\",\\n\\t\\t\\t\\t16,\\n\\t\\t)\\n\\t\\t.expect(\\"Parsing fail for x\\");\\n\\n\\t\\tlet y = BigUint::parse_bytes(\\n\\t\\t\\t\\tb\\"483ADA7726A3C4655DA4FBFC0E1108A8FD17B448A68554199C47D08FFB10D4B8\\",\\n\\t\\t\\t\\t16,\\n\\t\\t)\\n\\t\\t.expect(\\"Parsing fail for y\\");\\n\\n\\t\\tlet point = Point::Coor(Field { value: x, p: &p }, Field { value: y, p: &p });\\n\\n\\t\\tlet ec = EllipticCurve {\\n\\t\\t\\t\\ta: Field { value: a, p: &p },\\n\\t\\t\\t\\tb: Field { value: b, p: &p },\\n\\t\\t};\\n\\n\\t\\tlet result = ec.scalar_mul(&point, &n);\\n\\n\\t\\tassert_eq!(Point::Identity, result);\\n}\\n\\n\\n#### The Elliptic Curve Digital Signature Algorithm (ECDSA)\\n\\n##### Statement of the Theorem and Proof\\n\\nNote that the group of **_all points_** on an elliptic curve is not always cyclic. There are some necessary conditions for that group to be cyclic [in this post](https://math.stackexchange.com/questions/2323595/under-what-conditions-do-all-the-points-on-an-elliptic-curve-form-a-cyclic-group).\\n\\n> **Theorem.** Let $G$ be a cyclic subgroup of points on an elliptic curve\\n>\\n> $$\\n> C: y^2 = x^3+ax+b\\\\quad\\\\text{over }\\\\,\\\\mathbb Z_p\\n> $$\\n>\\n> and $g\\\\in \\\\mathbb Z_p\\\\times \\\\mathbb Z_p$ a given generator of $G$. For a fixed $k_\\\\text{pri}\\\\in \\\\mathbb Z_p$, define $K_\\\\text{pub} = k_\\\\text{pri} g$, then for every $k,z\\\\in \\\\mathbb Z_{|G|}$, there holds\\n>\\n> $$\\n>\\n> \\\\begin{aligned}\\n> &\\\\qquad \\\\,\\\\,\\\\,\\\\,\\\\begin{cases}\\n> R := \\\\pi_x(kg), \\\\\\\\\\n> S := k^{-1}\\\\big(z +\\\\pi_x(kg) k_\\\\text{pri}\\\\big )\\n> \\\\end{cases}\\\\\\\\\\n> &\\\\implies\\\\pi_x\\\\bigg(\\\\big[S^{-1}z\\\\big]g + \\\\big[S^{-1}R\\\\big]K_\\\\text{pub}\\\\bigg)\\\\equiv \\\\pi_x(kg)\\\\pmod{|G|}.\\n> \\\\end{aligned}\\n>\\n>\\n> $$\\n>\\n> Where $\\\\pi_x$ denotes the canonical projection to the first coordinate.\\n>\\n> In other words, if $(R,S)$ defined above is given to the target receiver, then **_necessarily_**\\n>\\n> $$\\n>\\n> \\\\pi_x\\\\bigg(\\\\big[S^{-1}z\\\\big]g + \\\\big[S^{-1}R\\\\big]K_\\\\text{pub}\\\\bigg)\\\\equiv R \\\\pmod{|G|}.\\n>\\n>\\n> $$\\n>\\n> - This **necessary** condition is defined to be the **_valid_** condition of a message, where $z=h(m)$ for some hash $h:\\\\texttt{&str}\\\\to\\\\texttt{u32}$ and string $m: \\\\texttt{&str}$ the message.\\n> - The tuple $(R,S)$ is called the **_signature_** of the message.\\n\\nNote that here $K_\\\\text{pub}$ is called a **_public key_**, $k_\\\\text{pri}$ a **_private key_** and $k$ a **_random number_**.\\n\\n<proof>\\n\\n**_Proof._** The proof is a direct transformation from the definition over field $\\\\mathbb Z_p$:\\n\\n$$\\n\\\\begin{aligned}\\nS&= k^{-1}(z + \\\\pi_x(kg)k_\\\\text{pri})\\\\\\\\\\nk &\\\\equiv S^{-1}(z + \\\\underbrace{\\\\pi_x(kg)}_{=:R}k_\\\\text{pri}) \\\\pmod{|G|}\\\\\\\\\\n kg &\\\\equiv [S^{-1}z]g + [S^{-1}R]k_\\\\text{pri}g \\\\pmod{|G|}\\\\\\\\\\n kg &\\\\equiv [S^{-1}z]g + [S^{-1}R]K_\\\\text{pub} \\\\pmod{|G|}\\\\\\\\\\nR &\\\\equiv \\\\pi_x\\\\big( [S^{-1}z]g + [S^{-1}R]K_\\\\text{pub}\\\\big ). \\\\pmod{|G|}\\n\\\\end{aligned}\\n$$\\n\\nThe $(\\\\Longleftarrow)$ direction of the last line is incorrect because $\\\\pi_x(k_1g) = \\\\pi_x(k_2g)$ **_cannot_** imply $k_1=k_2$.\\n\\nFor example, recall that $g$ is a generator of $G$, if $k_1 g=(x_0, y_0)$, then since $G$ is cyclic, there must be a unique $k_2\\\\neq k_1$ in $\\\\mathbb Z_{|G|}$ such that $k_2g = ({\\\\boldsymbol x_{\\\\boldsymbol 0}}, -y_0) \\\\in G$.\\n\\n</proof>\\n\\nIt has to be careful that $k\\\\cdot g$ is calculated on $\\\\mathbb Z_p$ but the message validation above are operated on $\\\\mathbb Z_{|G|}$.\\n\\n\x3c!-- ##### How to use This Theorem? --\x3e\\n\\n##### Coding\\n\\nrust\\n#![allow(unused)]\\n#![allow(non_snake_case)]\\n\\nuse core::panic;\\n\\nuse crate::modules::elliptic_curve::{EllipticCurve, Field, Point};\\nuse crate::modules::field_utils::Futil;\\nuse num_bigint::{BigUint, RandBigInt};\\nuse rand::{self, Rng};\\nuse sha256::{digest, try_digest};\\n\\n#[derive(PartialEq, Clone, Debug)]\\npub struct ECDSA<\'a> {\\n pub elliptic_curve: EllipticCurve<\'a>,\\n pub generator: Point<\'a>,\\n pub order: BigUint,\\n}\\n\\nimpl<\'a> ECDSA<\'a> {\\n pub fn generate_key_pair(&\'a self) -> (BigUint, Point<\'a>) {\\n let priv_key = self.generate_private_key();\\n let pub_key = self.generate_public_key(&priv_key);\\n return (priv_key, pub_key);\\n }\\n\\n pub fn generate_private_key(&self) -> BigUint {\\n self.generate_random_positive_number_less_than(&self.order)\\n }\\n\\n pub fn generate_random_positive_number_less_than(&self, max: &BigUint) -> BigUint {\\n let mut rng = rand::thread_rng();\\n rng.gen_biguint_range(&BigUint::from(1u32), &max)\\n }\\n\\n pub fn generate_public_key(&\'a self, priv_key: &BigUint) -> Point<\'a> {\\n self.elliptic_curve.scalar_mul(&self.generator, priv_key)\\n }\\n\\n pub fn sign(&\'a self, hash: &BigUint, priv_key: &BigUint, k_random: &BigUint) -> (BigUint, BigUint) {\\n assert!(hash < &self.order, \\"Hash is bigger than the order of Elliptic Curve\\");\\n assert!(\\n priv_key < &self.order,\\n \\"Private key has value bigger than the order of Elliptic Curve\\"\\n );\\n assert!(k_random < &self.order, \\"k_random has value bigger than the order of Elliptic Curve\\");\\n let g = &self.generator;\\n let z = hash;\\n let k_pri = priv_key;\\n let kg = self.elliptic_curve.scalar_mul(g, &k_random);\\n\\n match kg {\\n Point::Identity => panic!(\\"Public key should not be an identity.\\"),\\n Point::Coor(kg_x, _) => {\\n let R = kg_x.value;\\n\\n let S = Futil::mul(&R, &priv_key, &self.order);\\n let S = Futil::add(&z, &S, &self.order);\\n let S = Futil::mul(&Futil::mul_inverse(k_random, &self.order), &S, &self.order);\\n (R, S)\\n }\\n }\\n }\\n\\n pub fn verify(&self, hash: &BigUint, pub_key: &Point, signature: &(BigUint, BigUint)) -> bool {\\n assert!(hash < &self.order, \\"Hash is bigger than the order of the elliptic curve\\");\\n let (R, S) = signature;\\n let z = hash;\\n let P = self.elliptic_curve.add(\\n // &self.elliptic_curve.scalar_mul(&self.generator, (z / S)),\\n &self\\n .elliptic_curve\\n .scalar_mul(&self.generator, &Futil::mul(&z, &Futil::mul_inverse(S, &self.order), &self.order)),\\n &self\\n .elliptic_curve\\n .scalar_mul(pub_key, &Futil::mul(R, &Futil::mul_inverse(S, &self.order), &self.order)),\\n );\\n\\n if let Point::Coor(X, Y) = &P {\\n (&X.value - R).modpow(&BigUint::from(1u32), &self.order) == BigUint::from(0u32)\\n } else {\\n false\\n }\\n }\\n\\n pub fn generate_hash_less_than(&self, message: &str, max: &BigUint) -> BigUint {\\n let digested = digest(message);\\n let bytes = hex::decode(&digested).expect(\\"Cannot convert to Vec<u8>\\");\\n let one = BigUint::from(1u32);\\n let hash = BigUint::from_bytes_be(&bytes).modpow(&one, &(max - &one));\\n let hash = hash + one;\\n hash\\n }\\n}\\n\\n\\n##### Tests\\n\\nrust\\n#[cfg(test)]\\nmod test {\\n use crate::modules::curves::{Curve, CurveConfig};\\n\\n use super::*;\\n #[test]\\n fn test_sign_verify() {\\n let p = BigUint::from(17u32);\\n let ec = EllipticCurve {\\n a: Field::new(2, &p),\\n b: Field::new(2, &p),\\n };\\n let gp_order = BigUint::from(19u32);\\n let g = Point::Coor(Field::new(5, &p), Field::new(1, &p));\\n\\n let ecdsa = ECDSA {\\n elliptic_curve: ec,\\n generator: g,\\n order: gp_order,\\n };\\n\\n let priv_key = BigUint::from(7u32);\\n let pub_key = ecdsa.generate_public_key(&priv_key);\\n\\n let hash = Field::new(10, &p);\\n let k_random = BigUint::from(18u32);\\n\\n let message = \\"Bob -> 1BTC -> Alice\\";\\n let hash_ = ecdsa.generate_hash_less_than(message, &ecdsa.order);\\n let hash = BigUint::from(hash_);\\n let signature = ecdsa.sign(&hash, &priv_key, &k_random);\\n let verify_result = ecdsa.verify(&hash, &pub_key, &signature);\\n assert!(verify_result);\\n }\\n\\n #[test]\\n fn test_sign_tempered_verify() {\\n let p = BigUint::from(17u32);\\n let ec = EllipticCurve {\\n a: Field::new(2, &p),\\n b: Field::new(2, &p),\\n };\\n let gp_order = BigUint::from(19u32);\\n let g = Point::Coor(Field::new(5, &p), Field::new(1, &p));\\n\\n let ecdsa = ECDSA {\\n elliptic_curve: ec,\\n generator: g,\\n order: gp_order,\\n };\\n\\n let priv_key = BigUint::from(7u32);\\n let pub_key = ecdsa.generate_public_key(&priv_key);\\n\\n let hash = Field::new(10, &p);\\n let k_random = BigUint::from(18u32);\\n\\n let message = \\"Bob -> 1BTC -> Alice\\";\\n let hash_ = ecdsa.generate_hash_less_than(message, &ecdsa.order);\\n let hash = BigUint::from(hash_);\\n let signature = ecdsa.sign(&hash, &priv_key, &k_random);\\n let (R, S) = signature;\\n let R = R + BigUint::from(1u32);\\n let R = Futil::power(&R, 1, &ecdsa.order);\\n\\n let tempered_signature = (R, S);\\n let verify_result = ecdsa.verify(&hash, &pub_key, &tempered_signature);\\n assert!(!verify_result);\\n }\\n\\n #[test]\\n fn test_secp256_sign_verify_tempered<\'a>() {\\n let CurveConfig { a, b, generator, order, p } = Curve::get_Secp256k1_config();\\n let (x, y) = generator;\\n let generator = Point::Coor(Field { value: x, p: &p }, Field { value: y, p: &p });\\n let ec = Curve::get_elliptic_cuve(&p, &a, &b);\\n let ecdsa = Curve::get_ecdsa(&ec, &generator, &order);\\n\\n // modifiied from the order n\\n let priv_key = BigUint::parse_bytes(b\\"FFFFF0000FFF0F0F0F0F0F0F0F0F0F0EBAAEDCE6AF48A03BBFD25E8CD0364141\\", 16)\\n .expect(\\"Cannot parse into interger\\");\\n let pub_key = ecdsa.generate_public_key(&priv_key);\\n\\n let hash = Field::new(10, &p);\\n let k_random = ecdsa.generate_random_positive_number_less_than(&order);\\n\\n let message = \\"Bob -> 1BTC -> Alice\\";\\n let hash_ = ecdsa.generate_hash_less_than(message, &ecdsa.order);\\n let hash = BigUint::from(hash_);\\n let signature = ecdsa.sign(&hash, &priv_key, &k_random);\\n\\n // let verify_result = ecdsa.verify(&hash, &pub_key, &signature);\\n // println!(\\"hash: {}\\", hash);\\n // assert!(verify_result);\\n\\n let (R, S) = signature;\\n let R = R + BigUint::from(1u32);\\n let R = Futil::power(&R, 1, &ecdsa.order);\\n\\n let tempered_signature = (R, S);\\n let verify_result = ecdsa.verify(&hash, &pub_key, &tempered_signature);\\n assert!(!verify_result);\\n }\\n}\\n\\n\\n#### Reference\\n\\n- Udemy course: [Elliptic Curve Cryptography in Rust](https://www.udemy.com/course/elliptic-curve-cryptography-in-rust/)\\n- [\u4e00\u6587\u8bfb\u61c2 ECDSA \u7b97\u6cd5\u5982\u4f55\u4fdd\u62a4\u6570\u636e](https://zhuanlan.zhihu.com/p/97953640?fbclid=IwAR0MMbQbGXVTAcErHNlHWRU1lUnpzqHTGdmB7rUxiD-xfFgguh_czX-gm50)\\n","title":"Elliptic Curve and Operator Overloading","date":"2023-09-09T00:00:00.000Z","id":"blog0175","tag":"rust","intro":"Let\'s define operator overloading on finite field Z/pZ for prime p.","toc":true},{"content":"\\nCreate a rustfmt.toml at the project root level, then use\\n\\ntext\\nedition = \\"2018\\"\\ncolor = \\"Auto\\"\\nunstable_features = true\\n\\n# Width\\nmax_width = 140\\n\\n# Layout\\nindent_style = \\"Block\\"\\nbrace_style = \\"PreferSameLine\\"\\ncontrol_brace_style = \\"AlwaysSameLine\\"\\nimports_indent = \\"Block\\"\\nfn_args_layout = \\"Compressed\\"\\nmatch_arm_blocks = true\\n\\n# Delimiter\\ntrailing_comma = \\"Vertical\\"\\ntrailing_semicolon = true\\nmatch_block_trailing_comma = false\\nbinop_separator = \\"Front\\"\\noverflow_delimited_expr = true\\n\\n# Line\\nnewline_style = \\"Unix\\"\\nfn_single_line = true\\nstruct_lit_single_line = true\\nwhere_single_line = false\\nempty_item_single_line = true\\nwrap_comments = true\\nblank_lines_lower_bound = 0\\nblank_lines_upper_bound = 1\\n\\n# Space\\ntab_spaces = 4\\nhard_tabs = false\\nspace_after_colon = true\\nspace_before_colon = false\\nspaces_around_ranges = false\\nspace_around_attr_eq = true\\ncombine_control_expr = true\\ntype_punctuation_density = \\"Wide\\"\\nstruct_field_align_threshold = 0\\nenum_discrim_align_threshold = 0\\n\\n# Simplification\\nuse_field_init_shorthand = true\\nuse_try_shorthand = true\\nremove_nested_parens = true\\nmerge_derives = true\\nmerge_imports = true\\nnormalize_comments = false\\nnormalize_doc_attributes = true\\ncondense_wildcard_suffixes = true\\n\\n# Sort\\nreorder_imports = true\\nreorder_modules = true\\nreorder_impl_items = true\\n\\n# Force\\nforce_explicit_abi = true\\nforce_multiline_blocks = false\\n\\n# Format\\ndisable_all_formatting = false\\nformat_code_in_doc_comments = true\\nformat_macro_matchers = true\\nformat_macro_bodies = true\\nformat_strings = true\\n\\n# Error\\nerror_on_line_overflow = false\\nerror_on_unformatted = false\\nhide_parse_errors = false\\n\\n","title":"Rust Formatter Configruation","date":"2023-09-10T00:00:00.000Z","id":"blog0176","tag":"rust","intro":"Record a configuration for auto-formattting.","toc":false},{"content":"\\n<center></center>\\n\\n- **Step 1.** yarn add env-cmd\\n- **Step 2.** Create a .env-cmdrc in root level:\\n text\\n {\\n \\"default\\": {\\n \\"PORT\\": 8080\\n },\\n \\"production\\": {\\n \\"GOOGLE_CLIENT_ID\\": \\"XXX.com\\",\\n \\"GOOGLE_CLIENT_SECRET\\": \\"YYY\\",\\n \\"GOOGLE_API_REDIRECT\\": \\"http://localhost:8080/login-google\\",\\n \\"ALLOWED_EMAILS\\": [\\"machingclee@gmail.com\\", \\"james.lee@wonderbricks.com\\"],\\n \\"JWT_SECRET\\": \\"SECRET!\\",\\n \\"FRONTEND_URL\\": \\"http://localhost:3000\\"\\n }\\n }\\n \\n- **Step 3.** We prefix any command by\\n text\\n env-cmd -f .env-cmdrc -e default,production\\n \\n For example:\\n text\\n env-cmd -f .env-cmdrc -e default,production nodemon --exec ts-node src/app.ts\\n \\n","title":"Environment Variable by env-cmdrc","date":"2023-09-11T00:00:00.000Z","id":"blog0177","tag":"nodejs","intro":"Package for combining env files conveniently.","toc":false},{"content":"\\n#### Client Side\\n\\n##### List of Listeners\\n\\njs\\nexport default function WebSocket() {\\n const socket = useRef<ReturnType<typeof io> | null>(null);\\n const addMessage = (msg: Message) => {\\n setMessagesFromServer(msgs => {\\n const msgs_ = lodash.cloneDeep(msgs);\\n msgs_.push(msg);\\n return msgs_;\\n })\\n }\\n\\n const getSocketConnection = () => {\\n if (socket.current) {\\n return;\\n }\\n\\n socket.current = io(SOCKET_SERVER, { withCredentials: true });\\n\\n socket.current.on(\\"connect\\", () => {\\n addMessage({ sender: \\"System\\", msg: \\"Connected\\" });\\n })\\n\\n socket.current.on(MSG_TO_CLIENTS, (data: Message) => {\\n addMessage(data);\\n })\\n socket.current.on(\\"disconnect\\", (reason) => {\\n // note that if a user is disconnected actively by server (like no token is found)\\n // the reason will be \\"io server disconnect\\"\\n addMessage({ sender: \\"disconnect\\", msg: JSON.stringify(reason) });\\n })\\n socket.current.on(\\"connect_error\\", (err) => {\\n addMessage({ sender: \\"connect_error\\", msg: JSON.stringify(err) });\\n // addMessage({ sender: \\"connect_error\\", msg: \\"Closing connection ...\\" });\\n // socket.current?.disconnect();\\n // socket.current = null;\\n });\\n socket.current.io.on(\\"reconnect_attempt\\", (data) => {\\n addMessage({ sender: \\"reconnect_attempt\\", msg: String(JSON.stringify(data)) });\\n });\\n socket.current.io.on(\\"reconnect_error\\", (error) => {\\n addMessage({ sender: \\"reconnect_error\\", msg: String(JSON.stringify(error)) });\\n });\\n socket.current.io.on(\\"reconnect_failed\\", () => {\\n addMessage({ sender: \\"reconnect_failed\\", msg: \\"\\" });\\n });\\n }\\n}\\n\\n\\n##### Error Observation by Adruptly Closing the Server\\n\\nWe can make use of the observations below to determine:\\n\\n- When should we close the socket connection and\\n\\n- What error message to display to the client.\\n\\n![](/assets/tech/178/001.png)\\n\\n#### Server Side\\n\\n##### Entrypoint: app.ts\\n\\njs\\nimport getMongoConnection from \\"./db/getMongoConnection\\";\\nimport expressService from \\"./service/expressService\\";\\n\\nexpressService.initExpressApp([\\n async () => {\\n await getMongoConnection();\\n },\\n]);\\n\\n\\n##### Servics\\n\\n###### expressService.ts\\n\\nHere we try to split configuration into separate files, with configSocketio the only exception since it does not have api like app.ws for us the configure to Express object directly.\\n\\njs\\nimport express from \\"express\\";\\nimport \\"express-async-errors\\";\\nimport http from \\"http\\";\\nimport configCors from \\"../config/configCors\\";\\nimport configParsers from \\"../config/configParsers\\";\\nimport configRouting from \\"../config/configRouting\\";\\nimport { IORef } from \\"../dto/types\\";\\nimport configSocketio from \\"../config/configSocketio\\";\\nimport configErrorHandler from \\"../config/configErrorHandler\\";\\nconst { PORT } = process.env;\\n\\nconst app = express();\\nconst ioRef: IORef = { current: undefined };\\n\\napp.set(\\"trust proxy\\", 1);\\nconfigCors(app);\\nconfigParsers(app);\\nconfigRouting(app);\\nconfigErrorHandler(app);\\n\\nconst initExpressApp = (callbacks: (() => Promise<void> | void)[]) => {\\n const httpServer = http.createServer(app);\\n configSocketio(httpServer, ioRef);\\n const port = PORT || \\"8080\\";\\n httpServer.listen(parseInt(port), async () => {\\n for (const cb of callbacks) {\\n await cb();\\n }\\n console.log(App running on port ${port});\\n });\\n return app;\\n};\\n\\nconst getSocketIo = () => {\\n return ioRef;\\n};\\n\\nexport default {\\n initExpressApp,\\n getSocketIo,\\n};\\n\\n\\n###### socketService.ts\\n\\n- Since each user should have at most one socket connecting to the server, we use a Map object socketStore to store the correspondence between userOid and socket.\\n\\n- We will plug the socket object into req: Express.Request in the middleware socketioMiddleware below.\\n\\n- Then the controllers in chatRouter can get access to user\'s socket (as well as the server side io object).\\n\\njs\\nimport { Socket } from \\"socket.io\\";\\nimport expressService from \\"./expressService\\"\\nimport { DefaultEventsMap } from \\"socket.io/dist/typed-events\\";\\n\\n\\nconst socketStore = new Map<string, Socket<DefaultEventsMap, DefaultEventsMap, DefaultEventsMap, any>>();\\n\\nconst saveSocket = (uuid: string, socket: Socket<DefaultEventsMap, DefaultEventsMap, DefaultEventsMap, any>) => {\\n socketStore.set(uuid, socket);\\n};\\n\\nconst getSocket = (userOid: string) => {\\n console.log(\\"token\\", userOid);\\n return socketStore.get(userOid) || undefined;\\n}\\n\\nconst deleteSocket = (uuid: string) => {\\n socketStore.delete(uuid);\\n}\\n\\n\\nconst getIo = () => {\\n const ioRef = expressService.getSocketIo();\\n return ioRef.current;\\n}\\n\\nexport default {\\n getIo,\\n saveSocket,\\n getSocket,\\n deleteSocket\\n}\\n\\n\\n##### Mongo Connection\\n\\n###### getMongoConnection.ts\\n\\njs\\nimport mongoose from \\"mongoose\\";\\n\\nlet connCache: typeof mongoose;\\n\\nconst { DB_URL } = process.env;\\n\\nconst getMongoConnection = async () => {\\n console.log(\\"Connecting to mongo ...\\");\\n if (!DB_URL) {\\n throw new Error(\\"DB_URL cannot be found.\\");\\n }\\n if (!connCache) {\\n console.log(\\"Mongo connected.\\");\\n connCache = await mongoose.connect(DB_URL);\\n }\\n return connCache;\\n};\\n\\nexport default getMongoConnection;\\n\\n\\n###### Models (aka Collections)\\n\\n- **_Users._**\\n\\n js\\n import mongoose, { InferSchemaType, Schema } from \\"mongoose\\";\\n\\n export const userSchema = new Schema(\\n {\\n name: { type: String, required: true, index: true },\\n email: { type: String, required: true },\\n passwordHash: { type: String, required: true },\\n },\\n {\\n timestamps: { createdAt: \\"created_at\\", updatedAt: \\"updated_at\\" },\\n }\\n );\\n\\n export type User = InferSchemaType<typeof userSchema>;\\n export const UserModel = mongoose.model(\\"User\\", userSchema);\\n \\n\\n- **_ChatRoom._**\\n\\n js\\n import mongoose, { InferSchemaType, Schema } from \\"mongoose\\";\\n\\n const chatSessionSchema = new Schema(\\n {\\n code: { type: String, required: true, index: true },\\n hostUserOid: { type: String, required: true },\\n active: { type: Boolean, require: true },\\n members: {\\n type: [{ userOid: { type: String, required: true } }],\\n default: [],\\n },\\n },\\n {\\n timestamps: { createdAt: \\"createdAt\\", updatedAt: \\"updatedAt\\" },\\n }\\n );\\n\\n export type ChatSession = InferSchemaType<typeof chatSessionSchema>;\\n export const ChatSessionModel = mongoose.model(\\n \\"ChatSession\\",\\n chatSessionSchema\\n );\\n \\n\\n##### Utils for JWT Authentication\\n\\n###### tokenUtils.ts\\n\\njs\\nimport jwt from \\"jsonwebtoken\\";\\nimport { User, UserModel } from \\"../db/models/User\\";\\nimport { TokenInfo } from \\"../dto/types\\";\\nimport { Types } from \\"mongoose\\";\\nconst { JWT_SECRET = \\"\\", JWS_SALT_ROUNDS = \\"\\", JWT_EXPIRE_IN = \\"\\" } = process.env;\\n\\nconst getTokenFromUser = (user: User & { _id: Types.ObjectId }): Promise<string> => {\\n return new Promise((resolve, reject) => {\\n try {\\n if (!JWT_SECRET) {\\n reject(new Error(\\"jwt secret not found\\"));\\n }\\n const data: TokenInfo = {\\n userOid: user._id.toString(),\\n email: user.email,\\n name: user.name\\n };\\n\\n const token = jwt.sign(\\n data,\\n JWT_SECRET || \\"\\",\\n { expiresIn: JWT_EXPIRE_IN }\\n );\\n resolve(token);\\n } catch (err) {\\n reject(err);\\n }\\n });\\n}\\n\\nconst getUserFromToken = async (token: string) => {\\n const decoded = await verifyToken(token) as TokenInfo;\\n const userInToken: TokenInfo = decoded;\\n return userInToken\\n}\\n\\nconst verifyToken = (token: string) => {\\n return new Promise((resolve, reject) => {\\n try {\\n const decoded = jwt.verify(token, JWT_SECRET);\\n resolve(decoded);\\n } catch (err) {\\n reject(err);\\n }\\n })\\n\\n}\\n\\n\\nexport default {\\n getTokenFromUser,\\n verifyToken,\\n getUserFromToken,\\n}\\n\\n\\n###### hashUtil.ts\\n\\njs\\nimport bcrypt from \\"bcrypt\\";\\nconst { JWS_SALT_ROUNDS = \\"0\\" } = process.env;\\n\\nconst passwordIntoHash = (password: string) => {\\n return new Promise((resolve, reject) => {\\n bcrypt.hash(password, parseInt(JWS_SALT_ROUNDS), (err, hash) => {\\n if (err) {\\n return reject(err);\\n }\\n resolve(hash);\\n });\\n });\\n};\\n\\nconst comparePasswordWithHash = (password: string, hash: string) => {\\n return new Promise((resolve, reject) => {\\n bcrypt.compare(password, hash, (err, result) => {\\n if (err) {\\n return reject(err);\\n }\\n resolve(result);\\n });\\n });\\n};\\n\\nexport default {\\n passwordIntoHash,\\n comparePasswordWithHash,\\n};\\n\\n\\n##### Middlewares\\n\\n###### errorMiddleware.ts\\n\\njs\\nimport { NextFunction, Request, Response } from \\"express\\";\\n\\nexport default (\\n err: Error,\\n req: Request,\\n res: Response,\\n next: NextFunction\\n) => {\\n if (err) {\\n res.json({ success: false, errorMessage: err?.message });\\n }\\n};\\n\\n\\n###### socketioMiddleware.ts <----------- inject userSocket and io into req here!\\n\\njs\\nimport { NextFunction, Request, Response } from \\"express\\";\\nimport socketService from \\"../service/socketService\\";\\n\\nexport default (req: Request, res: Response, next: NextFunction) => {\\n try {\\n const userOid = req.user?.userOid!;\\n const socket = socketService.getSocket(userOid);\\n req.userSocket = socket;\\n const io = socketService.getIo();\\n req.io = io;\\n next();\\n } catch (err) {\\n next(err);\\n }\\n}\\n\\n\\n###### jwtAuthMiddleware.ts <------------ We parse token into req.user here!\\n\\njs\\nimport { NextFunction, Request, Response } from \\"express\\";\\nimport authUtil from \\"../util/tokenUtil\\";\\nimport { User } from \\"../db/models/User\\";\\nimport { Token } from \\"typescript\\";\\nimport { TokenInfo } from \\"../dto/types\\";\\n\\nexport default async (req: Request, res: Response, next: NextFunction) => {\\n try {\\n const token = req.cookies?.[\\"token\\"];\\n const user = await authUtil.getUserFromToken(token);\\n req.user = user;\\n next();\\n } catch (err) {\\n next(err);\\n }\\n};\\n\\n\\n##### Routers\\n\\n###### authRouter.ts <--------------------- We set cookie here!\\n\\njs\\nimport express from \\"express\\";\\nimport { UserModel } from \\"../db/models/User\\";\\nimport authUtil from \\"../util/tokenUtil\\";\\nimport hashUtil from \\"../util/hashUtil\\";\\nimport tokenUtil from \\"../util/tokenUtil\\";\\n\\nconst authRouter = express.Router();\\n\\nauthRouter.post(\\"/login\\", async (req, res, next) => {\\n try {\\n const { email, password } = req.body as { email: string, password: string };\\n const user = await UserModel.findOne({ email }).exec();\\n if (user) {\\n const { passwordHash } = user;\\n const valid = await hashUtil.comparePasswordWithHash(password, passwordHash);\\n if (valid) {\\n const token = await tokenUtil.getTokenFromUser(user);\\n res.cookie(\\"token\\", token, {\\n httpOnly: true,\\n sameSite: \\"none\\",\\n path: \\"/\\",\\n secure: true\\n });\\n res.json({ success: true });\\n } else {\\n throw new Error(\\"Username or password is incorrect\\");\\n }\\n }\\n } catch (err) {\\n next(err);\\n }\\n});\\n\\nauthRouter.get(\\"/refresh\\", (req, res) => {\\n res.json({ hello: \\"world\\" });\\n});\\n\\n\\nexport default authRouter;\\n\\n\\n###### userRouter\\n\\njs\\nimport express, { NextFunction } from \\"express\\";\\nimport { UserModel } from \\"../db/models/User\\";\\nimport hashUtil from \\"../util/hashUtil\\";\\nimport tokenUtil from \\"../util/tokenUtil\\";\\n\\nconst userRouter = express.Router();\\n\\nuserRouter.post(\\"/create\\", async (req, res, next: NextFunction) => {\\n try {\\n const { email, name, password } = req.body as { email: string, name: string, password: string };\\n if (!(email && name && password)) {\\n next(new Error(\\"Email, name or password is null\\"));\\n }\\n\\n const dbUser = await UserModel.findOne({ email }).exec();\\n if (dbUser) {\\n next(new Error(\\"User already exists\\"));\\n } else {\\n const passwordHash = await hashUtil.passwordIntoHash(password);\\n const userDoc = await new UserModel({ email, name, passwordHash });\\n const user = await userDoc.save();\\n\\n const token = await tokenUtil.getTokenFromUser(user);\\n // expiration is controlled by jwtMiddleware.\\n res.cookie(\\"token\\", token);\\n res.json({\\n success: true,\\n message: User ${name} has been created successfully.\\n });\\n }\\n } catch (err) {\\n next(err);\\n }\\n});\\n\\nexport default userRouter;\\n\\n\\n###### chatRouter\\n\\njs\\nimport express from \\"express\\";\\nimport chatController from \\"../controller/chatController\\";\\n\\nconst chatRouter = express.Router();\\n\\nchatRouter.get(\\"/rooms\\", chatController.getRooms);\\nchatRouter.get(\\"/create-room\\", chatController.createRoom);\\nchatRouter.get(\\"/join-room/:roomCode\\", chatController.joinRoom);\\nchatRouter.post(\\"/message\\", chatController.sendMessageFromClient);\\n\\nexport default chatRouter;\\n\\n\\nwith\\n\\njs\\nimport { NextFunction, Request, Response } from \\"express\\"\\nimport { Room, RoomModel } from \\"../db/models/Room\\";\\nimport codeUtil from \\"../util/codeUtil\\";\\nimport chatService from \\"../service/chatService\\";\\n\\n\\nconst getRooms = async (req: Request, res: Response) => {\\n const results = await RoomModel.find({}).exec();\\n const rooms = results.map(r => r.toObject());\\n res.json({\\n success: true,\\n result: { rooms }\\n })\\n};\\n\\nconst createRoom = async (req: Request, res: Response) => {\\n const { roomName } = req.body as { roomName: string };\\n const code = codeUtil.generateCode();\\n const roomProps: Room = {\\n active: true,\\n code,\\n hostUserOid: req.user?.userOid || \\"\\",\\n members: [{ userOid: req.user?.userOid || \\"\\" }],\\n name: roomName\\n }\\n\\n const room = new RoomModel(roomProps).save();\\n res.json({\\n success: true,\\n result: { newRoom: room }\\n });\\n};\\n\\nconst sendMessageFromClient = async (req: Request, res: Response) => {\\n const { roomCode, msg } = req.body as { roomCode: string, msg: string };\\n const room = await chatService.findRoomDocByCode(roomCode);\\n const roomName = room?.name || \\"\\"\\n\\n req.io?.to(roomCode).emit(...chatService.createMsgToClients({ sender: ${req.user?.name} (from ${roomName || \\"unknown\\"}) || \\"\\", msg: msg }));\\n}\\n\\nconst joinRoom = async (req: Request, res: Response) => {\\n // disconnect all existing rooms\\n const userConnectedRooms = req.userSocket?.rooms;\\n if (userConnectedRooms) {\\n userConnectedRooms.forEach(async (roomCode) => {\\n try {\\n const room = await chatService.getRoombyOid(roomCode);\\n if (room) {\\n req.io?.to(roomCode).emit(...chatService.createMsgToClients({\\n sender: \\"server\\",\\n msg: ${req.user?.name || \\"\\"} leaved room: ${room.name}\\n }));\\n }\\n }\\n catch (err) {\\n }\\n req.userSocket?.leave(roomCode);\\n })\\n }\\n\\n // start to join new room\\n const { roomCode } = req.params;\\n const room = await chatService.findRoomDocByCode(roomCode);\\n if (room) {\\n req.userSocket?.join(roomCode!);\\n } else {\\n throw new Error(No room of code ${roomCode} exists);\\n }\\n\\n // history purpose\\n const existingUser = room?.members.find(m => m.userOid === req.user?.userOid);\\n if (!existingUser) {\\n room?.members.push({ userOid: req.user!.userOid });\\n await room?.save();\\n }\\n\\n req.io?.to(roomCode).emit(...chatService.createMsgToClients({\\n sender: \\"server\\",\\n msg: ${req.user?.name} just connected to room ${room.name}\\n }));\\n res.json({ success: true });\\n}\\n\\nexport default {\\n getRooms,\\n joinRoom,\\n sendMessageFromClient,\\n createRoom\\n}\\n\\n\\n##### Configuration Files\\n\\n###### configCors.ts\\n\\njs\\nimport { Express } from \\"express\\";\\nimport cors, { CorsOptions } from \\"cors\\";\\nconst { ALLOWED_ORIGINS } = process.env;\\n\\nexport default (app: Express) => {\\n const allowlist = ALLOWED_ORIGINS?.split(\\",\\") || [];\\n const corsOptionsDelegate = (req: any, callback: any) => {\\n let corsOptions: CorsOptions;\\n\\n if (allowlist.indexOf(req.header(\\"Origin\\")) > -1) {\\n corsOptions = { origin: true, credentials: true };\\n } else {\\n corsOptions = { origin: false };\\n }\\n // callback expects two parameters: (error, options)\\n callback(null, corsOptions);\\n };\\n\\n app.use(cors(corsOptionsDelegate));\\n};\\n\\n\\n###### configErrorHandler.ts\\n\\njs\\nimport errorMiddleware from \\"../middleware/errorMiddleware\\";\\nimport { Express } from \\"express\\";\\n\\nexport default (app: Express) => {\\n app.use(errorMiddleware);\\n};\\n\\n\\n###### configParsers.ts\\n\\njs\\nimport bodyParser from \\"body-parser\\";\\nimport cookieParser from \\"cookie-parser\\";\\nimport { Express } from \\"express\\";\\n\\nexport default (app: Express) => {\\n app.use(cookieParser());\\n app.use(bodyParser.json());\\n};\\n\\n\\n###### configRouting.ts <----------- jwtAuthMiddleware and socketioMiddleware!\\n\\njs\\nimport jwtAuthMiddleware from \\"../middleware/jwtAuthMiddleware\\";\\nimport socketioMiddleware from \\"../middleware/socketioMiddleware\\";\\nimport authRouter from \\"../router/authRouter\\";\\nimport chatRouter from \\"../router/chatRouter\\";\\nimport userRouter from \\"../router/userRouter\\";\\nimport { Express } from \\"express\\";\\n\\nexport default (app: Express) => {\\n app.use(\\"/chat\\", jwtAuthMiddleware, socketioMiddleware, chatRouter);\\n app.use(\\"/user\\", userRouter);\\n app.use(\\"/auth\\", authRouter);\\n};\\n\\n\\n###### configSocketio.ts: <---------------- We parse token here!\\n\\njs\\nimport { Server } from \\"socket.io\\";\\nimport http from \\"http\\";\\nimport { IORef } from \\"../dto/types\\";\\nconst { ALLOWED_ORIGINS } = process.env;\\nconst allowlist = ALLOWED_ORIGINS?.split(\\",\\") || [];\\nimport cookie from \\"cookie\\";\\nimport userSerevice from \\"../service/userSerevice\\";\\nimport { serialize, parse } from \\"cookie\\";\\nimport { v4 as uuidv4 } from \\"uuid\\";\\nimport socketService from \\"../service/socketService\\";\\nimport chatService from \\"../service/chatService\\";\\n\\nexport const MSG_TO_CLIENTS = \\"MSG_TO_CLIENTS\\";\\n\\nexport default (httpServer: http.Server, ioRef: IORef) => {\\n ioRef.current = new Server(httpServer, {\\n cookie: true,\\n allowUpgrades: true,\\n cors: {\\n origin: allowlist,\\n methods: [\\"GET\\", \\"POST\\"],\\n credentials: true,\\n },\\n });\\n const io = ioRef.current;\\n\\n io.on(\\"connect\\", async (socket) => {\\n try {\\n const cookieString = cookie.parse(socket.request.headers.cookie || \\"\\");\\n\\n const token = cookieString?.[\\"token\\"];\\n if (!token) {\\n console.log(socket was dropped because no token is found.);\\n return socket.disconnect();\\n }\\n let usernameInDb = \\"\\";\\n try {\\n const user = await userSerevice.getUserFromToken(token);\\n if (user) {\\n usernameInDb = ${user.name};\\n console.log(${usernameInDb} has connected);\\n socketService.saveSocket(user.userOid, socket);\\n socket.emit(\\n ...chatService.createMsgToClients({\\n sender: \\"server\\",\\n msg: \\"Connected.\\",\\n })\\n );\\n }\\n socket.on(\\"disconnect\\", (reason) => {\\n socketService.deleteSocket(user.userOid);\\n console.log(socketService.deleteSocket(${user.userOid}););\\n console.log(${usernameInDb} has disconnected because: ${reason});\\n });\\n } catch (err) {\\n socket.disconnect();\\n }\\n } catch (error) {\\n io.close();\\n }\\n });\\n};\\n\\n\\n##### The desc.d.ts for req.user\\n\\nSince by default there is no req.user for Express.Request object, we need to declare it in desc.d.ts:\\n\\njs\\nexport type TokenInfo = {\\n userOid: string,\\n email: string,\\n name: string\\n}\\n\\ndeclare global {\\n namespace Express {\\n interface Request {\\n user?: TokenInfo\\n }\\n }\\n}\\n\\n\\nAnd this special desc.d.ts needs to be included as files in tsconfig.json:\\n\\njson\\n{\\n \\"compilerOptions\\": {\\n ...\\n },\\n ...\\n \\"files\\": [\\n \\"src/desc.d.ts\\"\\n ]\\n}\\n\\n","title":"Setup of Express with Socket.io with JWT Authentication Using Cookie","date":"2023-09-16T00:00:00.000Z","id":"blog0178","tag":"nodejs, express, jwt, socketio","intro":"Basic reivew of API provided by socket.io-client and socket.io in nodejs.","toc":true},{"content":"\\n#### Installation\\n\\ntext\\nyarn add jest typescript ts-jest @types/jest\\n\\n\\ntext\\nyarn ts-jest config:init\\n\\n\\nAt package.json, add\\n\\njson\\n{\\n \\"script\\": {\\n \\"test\\": \\"env-cmd -f .env-cmdrc -e default,dev jest --coverage\\"\\n }\\n}\\n\\n\\nThis will debug all the file that bare the name as suffix: .test.ts.\\n\\n#### Debug a Single Jest File\\n\\nLet\'s define the following in .vscode/launch.json\\n\\njs\\n{\\n \\"version\\": \\"0.2.0\\",\\n \\"configurations\\": [\\n ...,\\n {\\n \\"type\\": \\"node-terminal\\",\\n \\"request\\": \\"launch\\",\\n \\"name\\": \\"Jest: Current File\\",\\n \\"command\\": \\"yarn env-cmd -f .env-cmdrc -e default,dev jest ${fileBasenameNoExtension} --config jest.config.js --coverage\\",\\n \\"cwd\\": \\"${workspaceRoot}\\",\\n }\\n ]\\n}\\n\\n\\nNow go to the file we want to debug, press F5.\\n\\n#### Examples of Test Files\\n\\n##### Expect an Output is as Expected\\n\\nLet\'s consider testing an API to create a document:\\n\\njs\\nimport getMongoConnection from \\"../src/db/getMongoConnection\\";\\nimport { NameSpaceModel } from \\"../src/db/models/NameSpace\\";\\n\\ntest(\\"Create Namespaces\\", async () => {\\n await getMongoConnection();\\n const namespaceDoc = await new NameSpaceModel({\\n name: \\"wonderbricks\\",\\n path: \\"/wonderbricks\\",\\n }).save();\\n\\n expect(\\n namespaceDoc.name === \\"wonderbricks\\" &&\\n namespaceDoc.path === \\"/wonderbricks\\"\\n ).toBe(true);\\n});\\n\\n\\nAnd we get\\n\\ntext\\n$ C:\\\\Users\\\\user\\\\Repos\\\\wonderbricks\\\\2023-09-12-serverless-chat-billie\\\\node_modules\\\\.bin\\\\env-cmd -f .env-cmdrc -e default,dev jest new-namspace.test --config jest.config.js --coverage\\nDebugger attached.\\nDebugger attached.\\n console.log\\n Connecting to mongo ...\\n\\n at log (src/db/getMongoConnection.ts:8:13)\\n\\n console.log\\n Mongo connected.\\n\\n at log (src/db/getMongoConnection.ts:13:17)\\n\\n PASS test/new-namspace.test.ts (5.286 s)\\n \u221a Create Unique Namespaces (2858 ms)\\n\\n------------------------|---------|----------|---------|---------|-------------------\\nFile | % Stmts | % Branch | % Funcs | % Lines | Uncovered Line #s\\n------------------------|---------|----------|---------|---------|-------------------\\nAll files | 93.33 | 33.33 | 100 | 92.85 |\\n db | 91.66 | 33.33 | 100 | 90.9 |\\n getMongoConnection.ts | 91.66 | 33.33 | 100 | 90.9 | 10\\n db/models | 100 | 100 | 100 | 100 |\\n NameSpace.ts | 100 | 100 | 100 | 100 |\\n------------------------|---------|----------|---------|---------|-------------------\\nTest Suites: 1 passed, 1 total\\nTests: 1 passed, 1 total\\nSnapshots: 0 total\\nTime: 5.449 s, estimated 8 s\\nRan all test suites matching /new-namspace.test/i.\\n\\n\\n##### Expect an Async Function not to Throw Error\\n\\njs\\ntest(\\"Create Unique Namespaces\\", async () => {\\n const createNamespace = async () => {\\n await getMongoConnection();\\n await new NameSpaceModel({\\n name: \\"jamesCompany\\",\\n path: \\"/jamesCompany\\",\\n }).save();\\n };\\n await expect(createNamespace()).resolves.not.toThrowError();\\n});\\n\\n","title":"Jest Fundamentals in TS","date":"2023-09-15T00:00:00.000Z","id":"blog0179","tag":"nodejs, test","intro":"Testings not only justify our functions are working, it also demonstrates how our function is used. We will be setting up tests in typescript.","toc":true},{"content":"\\nrust\\nuse std::{net::SocketAddr, sync::Arc};\\n\\nuse tokio::{\\n io::{AsyncBufReadExt, AsyncWriteExt, BufReader},\\n net::TcpListener,\\n sync::broadcast,\\n};\\n\\n#[tokio::main]\\nasync fn main() {\\n let listener = TcpListener::bind(\\"localhost:8080\\").await.unwrap();\\n let (tx, _rx) = broadcast::channel::<(String, SocketAddr)>(10);\\n let tx_ptr = Arc::new(tx);\\n loop {\\n let (mut socket, addr) = listener.accept().await.unwrap();\\n let tx_ptr = tx_ptr.clone();\\n\\n tokio::spawn(async move {\\n let (reader, mut writer) = socket.split();\\n let tx_ptr = tx_ptr.clone();\\n\\n let mut reader = BufReader::new(reader);\\n let mut line = String::new();\\n let mut rx = tx_ptr.subscribe();\\n loop {\\n tokio::select! {\\n result = reader.read_line(&mut line) => {\\n if result.unwrap() == 0 {\\n break;\\n }\\n tx_ptr.send((line.clone(), addr)).unwrap();\\n line.clear();\\n }\\n result = (&mut rx).recv() => {\\n let (msg, other_addr) = result.unwrap();\\n if !addr.eq(&other_addr) {\\n writer.write_all(msg.as_bytes()).await.unwrap();\\n }\\n }\\n };\\n }\\n });\\n }\\n}\\n\\n\\n","title":"Simple Chat Server in Rust via Telnet","date":"2023-09-19T00:00:00.000Z","id":"blog0180","tag":"rust","intro":"Study of Tokio by building a chat server.","toc":false},{"content":"\\n<style>\\n img {\\n max-width: 100%\\n }\\n</style>\\n\\n- Architecture:\\n\\n [![](/assets/tech/181/001.png)](/assets/tech/181/001.png)\\n\\n- Each of the u_k\'s connects to web-socket server as normal.\\n\\n- For each of ws_k\'s, they still rely on room\'s for publishing messages within a group of sockets.\\n\\n- Same room can repeatedly appears in each of the socket servers.\\n\\n - For example, room_1 can appear in both ws_1 and ws_2.\\n - u_1 join room_1 via ws_1 and u_2 join room_1 via ws_2.\\n\\n- **_Fun part._** Now each of the ws_k servers registers publish and subscribe listeners to redis server.\\n\\n Note that the following code can be executed right before app.listen().\\n\\n js-1\\n // ws_k\\n // pubsubWithIo.ts\\n // to be run right before app.listen();\\n\\n import redis from \\"redis\\";\\n\\n const subscriber = redis.createClient({\\n port: 6379,\\n host: redis_url,\\n });\\n\\n export const publisher = redis.createClient({\\n port: 6379,\\n host: redis_url,\\n });\\n \\n\\n js-16\\n const io = getIoSingletonFromSomewhere();\\n\\n subscriber.on(\\"subscribe\\", function (channel, count) {\\n // do something, or even omit this listener\\n });\\n\\n export type Message = {\\n namespace: string,\\n roomCode: string,\\n msg:{ sender:string, text: string }\\n }\\n subscriber.on(\\"message\\", function (_channel: string, message: Message) {\\n try {\\n const { namespace, roomCode, msg } = message;\\n io.of(namespace).to(roomCode).emit(\\"MSG_TO_CLIENTS\\", msg);\\n } catch (err) {\\n consol.log(err);\\n }\\n });\\n\\n subscriber.subscribe(\\"livechat\\");\\n \\n\\n Finally we also have a publish event, this is supposed to be wrapped inside a post request:\\n\\n js\\n // ws_k\\n\\n import { publisher, Message } from some/where/pubsubWithIo;\\n ...\\n router.post(\\"/some-outer-route\\", async (req, res) => {\\n const msg = req.body as Message;\\n publisher.publish(\\"livechat\\", msg);\\n res.json({ success: true });\\n });\\n \\n\\n- Now we easily scaled up the a chat server!\\n\\n#### Referenece\\n\\n- [Scaling Websockets with Redis, HAProxy and Node JS - High-availability Group Chat Application](https://www.youtube.com/watch?v=gzIcGhJC8hA&t=920s)\\n- [Understanding Redis Pub/Sub (Getting Started)](https://www.youtube.com/watch?v=KIFA_fFzSbo&t=449s)\\n","title":"Scaling Websocket Chat Sever by Redis","date":"2023-09-20T00:00:00.000Z","id":"blog0181","tag":"redis, socketio, nodejs","intro":"We study how to scale up the chat services horizontally by Redis.","toc":false},{"content":"\\n#### The Pipelines\\n\\n##### createLeftJoin\\n\\njs\\nconst createLeftJoin = (params: {\\n from: string,\\n localField: { stringToOid?: boolean, fieldName: string },\\n foreignField: string,\\n leftjoinPipeline: PipelineStage.Lookup[\\"$lookup\\"][\\"pipeline\\"],\\n as: string,\\n}) => {\\n const {\\n as: newFieldName,\\n foreignField,\\n from,\\n localField,\\n leftjoinPipeline,\\n } = params;\\n const pipeline: PipelineStage[] = [\\n {\\n $addFields: {\\n tmp_localfield: \\"$\\" + localField.fieldName,\\n },\\n },\\n {\\n $unwind: {\\n path: \\"$tmp_localfield\\",\\n preserveNullAndEmptyArrays: true,\\n },\\n },\\n {\\n $lookup: {\\n let: { tmp: \\"$tmp_localfield\\" },\\n from: from,\\n pipeline: [\\n {\\n $match: {\\n $expr: {\\n $eq: [\\n \\"$\\" + foreignField,\\n localField.stringToOid ? { $toObjectId: \\"$$tmp\\" } : \\"$$tmp\\",\\n ],\\n },\\n },\\n },\\n ...(leftjoinPipeline || []),\\n ],\\n as: \\"tmp_singleElementList\\",\\n },\\n },\\n {\\n $unwind: {\\n path: \\"$tmp_singleElementList\\",\\n preserveNullAndEmptyArrays: true,\\n },\\n },\\n {\\n $group: {\\n _id: \\"$_id\\",\\n originalData: { $first: \\"$$ROOT\\" },\\n pushList: { $push: \\"$tmp_singleElementList\\" },\\n },\\n },\\n {\\n $replaceRoot: {\\n newRoot: {\\n $mergeObjects: [\\"$originalData\\", { [newFieldName]: \\"$pushList\\" }],\\n },\\n },\\n },\\n {\\n $unset: \\"tmp_singleElementList\\",\\n },\\n {\\n $unset: \\"tmp_localfield\\",\\n },\\n ];\\n return pipeline;\\n};\\n\\n\\n##### createSqueeze\\n\\njs\\nconst createSqueeze = (fieldName: string): PipelineStage[] => {\\n return [\\n {\\n $unwind: {\\n path: \\"$\\" + fieldName,\\n preserveNullAndEmptyArrays: true,\\n },\\n },\\n ];\\n};\\n\\n\\n#### Usage\\n\\njs\\nconst results = await RoomModel.aggregate([\\n { $match: {} },\\n {\\n $project: { code: 1, name: 1, hostUserOid: 1, active: 1, createdAt: 1 },\\n },\\n ...mongoUtil.createLeftJoin({\\n from: UserModel.collection.name,\\n localField: { stringToOid: true, fieldName: \\"hostUserOid\\" },\\n foreignField: \\"_id\\",\\n leftjoinPipeline: [{ $project: { _id: 0, name: 1, email: 1 } }],\\n as: \\"hostUser\\",\\n }),\\n ...mongoUtil.createSqueeze(\\"hostUser\\"),\\n]).exec();\\n\\n\\nNote that we also cover the case when our localField is an **_stringified_** ObjectId, we provide a stringToOid option to convert the string into ObjectId which inherits an index for searching.\\n","title":"Create a left-join in Mongoose","date":"2023-09-21T00:00:00.000Z","id":"blog0182","tag":"mongo","intro":"We create an aggregation pipeline which acts like a left-join.","toc":true},{"content":"\\n#### Frontend: React Native\\n\\nSince we are dealing with audio, there is **_no_** UI element needed and hence we will have a bunch of configurations without elements like <audio /> and object of AudioTrack.\\n\\njs\\nimport { useRef, useState } from \\"react\\";\\nimport { useAppDispatch, useAppSelector } from \\"../redux/app/hooks\\";\\nimport {\\n ChannelProfileType,\\n ClientRoleType,\\n IRtcEngine,\\n RtcConnection,\\n UserOfflineReasonType,\\n createAgoraRtcEngine,\\n} from \\"react-native-agora\\";\\nimport { PermissionsAndroid } from \\"react-native\\";\\nimport appSlice from \\"../redux/slices/appSlice\\";\\nimport msgUtil from \\"../util/msgUtil\\";\\nimport chatSlice, { ChatSliceState } from \\"../redux/slices/chatSlice\\";\\n\\nconst appId = process.env.EXPO_PUBLIC_ALGORA_APPID!;\\n// also get host userid in the joinroom function\\n\\nexport default () => {\\n const dispatch = useAppDispatch();\\n const channelId = useAppSelector(s => s.chat.selectedRoomOid);\\n const userId = useAppSelector(s => s.auth.userId);\\n const engine = useRef<{ instance: IRtcEngine | null }>({ instance: null });\\n const {\\n speakerphoneEnabled,\\n joinSucceeded,\\n localMuted,\\n peerIds,\\n rtcToken\\n } = useAppSelector(s => s.chat.AgoraRtcConnection);\\n\\n const updateAgora = (update: Partial<ChatSliceState[\\"AgoraRtcConnection\\"]>) => {\\n dispatch(chatSlice.actions.updateAgoraRtcConnection(update));\\n }\\n\\n const requestAudioPermission = async () => {\\n const granted = await PermissionsAndroid.requestMultiple([\\n PermissionsAndroid.PERMISSIONS.RECORD_AUDIO\\n ]);\\n if (granted[\\"android.permission.RECORD_AUDIO\\"]\\n !== PermissionsAndroid.RESULTS.GRANTED\\n ) {\\n dispatch(appSlice.actions.updateAppDialog({\\n open: true,\\n title: \\"Permission Error\\",\\n desc: \\"Microphone access has not been granted\\",\\n ok: { label: \\"OK\\" }\\n }));\\n };\\n }\\n\\n const initRTC = async () => {\\n const client = createAgoraRtcEngine();\\n engine.current.instance = client;\\n\\n client.enableAudio();\\n client.setEnableSpeakerphone(true);\\n client.muteLocalAudioStream(false);\\n // enableSpeakerphone is initialized to ture in useState\\n client.setDefaultAudioRouteToSpeakerphone(true);\\n client.muteLocalAudioStream(true);\\n\\n\\n client.initialize({\\n appId,\\n channelProfile: ChannelProfileType.ChannelProfileLiveBroadcasting\\n });\\n\\n client.addListener(\\n \\"onJoinChannelSuccess\\",\\n (channel, elapsed) => {\\n try {\\n console.log(\\"[onJoinChannelSuccess] AgoraRTC Join Success\\", channel, elapsed);\\n msgUtil.success(\\"AgoraRTC Join Success\\", ${channel}, ${elapsed})\\n updateAgora({\\n joinSucceeded: true,\\n peerIds: [channel.localUid!, ...peerIds]\\n })\\n } catch (err) {\\n console.log(\\"[onJoinChannelSuccess]\\" + JSON.stringify(err));\\n }\\n }\\n );\\n\\n client.addListener(\\n \\"onUserJoined\\",\\n (_, remoteUid, __) => {\\n try {\\n console.log([onUserJoined] new remote uid: ${remoteUid})\\n if (peerIds.indexOf(remoteUid) > -1) {\\n return;\\n }\\n // setPeerIds(ids => [...ids, remoteUid]);\\n } catch (err) {\\n console.log(\\"[onUserJoined]\\" + JSON.stringify(err));\\n }\\n }\\n );\\n\\n\\n client.addListener(\\n \\"onUserOffline\\",\\n (_, remoteUid, __) => {\\n try {\\n console.log([onUserOffline] ${remoteUid} offlined)\\n updateAgora({ peerIds: peerIds.filter(id => id !== remoteUid) })\\n // if (remoteUid === hostUserId) {\\n // leaveChannel();\\n // }\\n } catch (err) {\\n console.log(\\"[onUserOffline]\\" + JSON.stringify(err));\\n }\\n }\\n );\\n\\n };\\n\\n const joinChannel = () => {\\n console.log(\\"join channel\\")\\n engine.current.instance?.joinChannel(\\n rtcToken, channelId, userId,\\n { clientRoleType: ClientRoleType.ClientRoleBroadcaster }\\n );\\n };\\n\\n const leaveChannel = () => {\\n console.log(\\"leave channel\\")\\n engine.current.instance?.leaveChannel();\\n updateAgora({ peerIds: [], joinSucceeded: false });\\n };\\n\\n const toggleMute = () => {\\n engine.current.instance?.muteLocalAudioStream(!localMuted);\\n updateAgora({ localMuted: !localMuted });\\n };\\n\\n const toggleSpeakerphoneEnabled = () => {\\n engine.current.instance?.setEnableSpeakerphone(!speakerphoneEnabled);\\n updateAgora({ speakerphoneEnabled: !speakerphoneEnabled });\\n }\\n\\n const destroyAgora = () => {\\n engine.current.instance?.removeAllListeners();\\n engine.current.instance = null;\\n }\\n\\n return {\\n requestAudioPermission,\\n initRTC,\\n joinChannel,\\n leaveChannel,\\n toggleMute,\\n toggleSpeakerphoneEnabled,\\n destroyAgora,\\n peerIds,\\n localMuted,\\n speakerphoneEnabled,\\n joinSucceeded\\n }\\n}\\n\\n\\nHere state needed for UI representation (muted, unmuted, connection success, etc) are all bound to redux store.\\n\\nNote that in joinChannel we need to provide rtcToken, channelId and userId.\\n\\n- rtcToken will be generated by our token server in the next section.\\n- channelId will be a roomOid that we create in a database.\\n- userId will be a serialized columns of integer id in our database.\\n\\n#### Backend: Token Server\\n\\ntext\\nyarn add react-native-agora\\n\\n\\nOur token is generated by sending POST request to:\\n\\njs\\nchatRouter.post(\\"/voice-token\\", (req, res) => {\\n const { roomOid } = req.body as { roomOid: string };\\n const userId = req.user?.userId!;\\n const rtcToken = chatService.generateRtcToken(roomOid, userId);\\n res.json({\\n success: true,\\n result: { rtcToken }\\n })\\n})\\n\\n\\nHere:\\n\\njs\\nconst generateRtcToken = (channelId: string, uid: number) => {\\n const appID = process.env.AGORARTC_APP_ID!;\\n const appCertificate = process.env.AGORARTC_APP_CERT!;\\n const role = RtcRole.PUBLISHER;\\n const expirationTimeInSeconds = 3600\\n const currentTimestamp = Math.floor(Date.now() / 1000)\\n const privilegeExpiredTs = currentTimestamp + expirationTimeInSeconds\\n\\n // Build token with uid\\n const token = RtcTokenBuilder.buildTokenWithUid(appID, appCertificate, channelId, uid, role, privilegeExpiredTs, 600);\\n return token;\\n}\\n\\n\\n- appId, appCertificate can be obtained from https://console.agora.io/projects.\\n- The first person calling this api with this channelId will be the host of this channel.\\n- Once the host leaves, the channel will be closed.\\n\\n#### Reference\\n\\n- [Agora.io Official Repo in Nodejs](https://github.com/AgoraIO/Tools/blob/master/DynamicKey/AgoraDynamicKey/nodejs/sample.js)\\n- [Voice Calling in React Native using Agora - Aryan Agarwal](https://www.youtube.com/watch?v=OBW96M_fafk)\\n- [AgoraIO Official Example in Class Component](https://github.com/AgoraIO-Extensions/react-native-agora/tree/main/example/src/examples/basic)\\n","title":"Voice Chat on Mobile Using AgoraRTC","date":"2023-09-23T00:00:00.000Z","id":"blog0183","tag":"react-native","intro":"We create a voice chat application on mobile using third party API called AgoraRTC.","toc":true},{"content":"\\n#### Create a Counter Collection to Store all Serialized id Fields\\n\\nWe start from creating a Counter collection:\\n\\njs\\nimport mongoose, { Document, InferSchemaType, Schema } from \\"mongoose\\";\\n\\nconst counterSchema = new Schema({\\n _id: { type: String, required: true },\\n seq: { type: Number, default: 0 }\\n});\\nexport const CounterModel = mongoose.model(\'Counter\', counterSchema);\\n\\nexport async function getNextSequence(name: string): Promise<number> {\\n const result = await CounterModel.findOneAndUpdate({\\n _id: name\\n }, {\\n $inc: { seq: 1 }\\n }, {\\n upsert: true\\n });\\n if (!result) {\\n throw new Error(\\"Error in counter model\\");\\n }\\n const seq = result.seq;\\n return seq\\n}\\n\\n\\n- In Counter collection each document will have _id as the desired name (like userId).\\n- On save() a new target document, we will query for Counter collection, add the seq value by 1, and then inject that seq value into our target document.\\n\\n#### Hook to save() Method of a Collection\\n\\nWe create a hook to save() method and perform the seq injection by:\\n\\njs\\nimport mongoose, { InferSchemaType, Schema } from \\"mongoose\\";\\nimport { getNextSequence } from \\"./Counter\\";\\n\\nexport const userSchema = new Schema(\\n {\\n id: { type: Number, required: true, unique: true, min: 1 },\\n name: { type: String, required: true, index: true },\\n email: { type: String, required: true },\\n passwordHash: { type: String, required: true },\\n },\\n {\\n timestamps: { createdAt: \'created_at\', updatedAt: \'updated_at\' },\\n versionKey: false,\\n }\\n);\\n\\nuserSchema.pre(\\"save\\", function (next) {\\n const doc = this;\\n getNextSequence(\\"userId\\").then(nextId => {\\n doc.id = nextId;\\n next();\\n }).catch((err) => {\\n console.log(getNextSequence failed: ${err});\\n })\\n})\\n\\nexport type User = InferSchemaType<typeof userSchema>;\\nexport const UserModel = mongoose.model(\'User\', userSchema);\\n\\n","title":"Auto-Incremented Id for Mongo Collection","date":"2023-09-24T00:00:00.000Z","id":"blog0184","tag":"mongo","intro":"We create a special collection and a hook to the save method of a collection to obtain an auto-incremented id field!","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### Result\\n\\n<center>\\n <video controls width=\\"400\\">\\n <source src=\\"/assets/tech/185/001.mp4\\" type=\\"video/mp4\\">\\n Sorry, your browser doesn\'t support embedded videos.\\n </video>\\n</center>\\n\\n#### Code Implementation\\n\\njs\\nimport { View, StyleSheet, Dimensions, TouchableOpacity } from \'react-native\'\\nimport React, { ReactNode, forwardRef, useEffect, useImperativeHandle, useRef, useState } from \'react\'\\nimport { PanGestureHandler, PanGestureHandlerGestureEvent } from \'react-native-gesture-handler\';\\nimport Animated, {\\n cancelAnimation,\\n useAnimatedGestureHandler,\\n useAnimatedStyle,\\n useDerivedValue,\\n useSharedValue,\\n withDecay,\\n withTiming\\n} from \'react-native-reanimated\';\\nimport panGesTranslationUtil from \'../util/panGesTranslationUtil\';\\nconst { width } = Dimensions.get(\\"window\\");\\n\\nexport type TrinityProps = {\\n remainingWidth?: number,\\n backgroundColor?: string,\\n left?: ReactNode,\\n middle: ReactNode,\\n right?: ReactNode\\n}\\n\\nclass Trinity {\\n public turnLeft: () => void = () => { };\\n public turnCenter: () => void = () => { };\\n public turnRight: () => void = () => { };\\n public enableScroll: () => void = () => { };\\n public disableScroll: () => void = () => { };\\n\\n public instance = (props: TrinityProps) => {\\n const { left, middle, right, remainingWidth = 40, backgroundColor = \\"#EBEBEB\\" } = props;\\n const translateX = useSharedValue(0);\\n const enabledScrollSharedValue = useSharedValue(1);\\n const disableScroll = () => {\\n enabledScrollSharedValue.value = 0\\n }\\n const enableScroll = () => {\\n enabledScrollSharedValue.value = 1;\\n }\\n const turnLeft = () => {\\n translateX.value = withTiming(width);\\n }\\n const turnCenter = () => {\\n translateX.value = withTiming(0);\\n }\\n const turnRight = () => {\\n translateX.value = withTiming(-width);\\n }\\n\\n this.turnLeft = turnLeft;\\n this.turnCenter = turnCenter;\\n this.turnRight = turnRight;\\n this.enableScroll = enableScroll;\\n this.disableScroll = disableScroll;\\n\\n const returnCenterRange = width * 4 / 5\\n\\n const clampedX = useDerivedValue(() => {\\n if (!right) {\\n return panGesTranslationUtil.clampX(translateX, 0, width - remainingWidth);\\n }\\n else if (!left) {\\n return panGesTranslationUtil.clampX(translateX, width - remainingWidth, 0);\\n } else {\\n return panGesTranslationUtil.clampX(translateX, width - remainingWidth, width - remainingWidth);\\n }\\n })\\n\\n const maxTranslateX = width - remainingWidth;\\n const leftOpacity = useDerivedValue(() => {\\n const scale = clampedX.value / maxTranslateX;\\n if (clampedX.value > 0) {\\n return Math.abs(scale)\\n } else {\\n return 0\\n }\\n })\\n\\n const rightOpacity = useDerivedValue(() => {\\n const scale = clampedX.value / maxTranslateX;\\n if (clampedX.value < 0) {\\n return Math.abs(scale)\\n } else {\\n return 0\\n }\\n })\\n\\n\\n const leftpageRStyle = useAnimatedStyle(() => {\\n if (clampedX.value > 0) {\\n return {\\n zIndex: 1,\\n opacity: leftOpacity.value\\n };\\n } else {\\n return {\\n zIndex: 0,\\n opacity: 0,\\n };\\n }\\n })\\n\\n const rightpageRStyle = useAnimatedStyle(() => {\\n if (clampedX.value > 0) {\\n return {\\n zIndex: 0,\\n opacity: 0\\n };\\n } else {\\n return {\\n zIndex: 1,\\n opacity: rightOpacity.value\\n };\\n }\\n })\\n\\n const panGestureHandler = useAnimatedGestureHandler<PanGestureHandlerGestureEvent, {\\n translateX: number,\\n }>({\\n onStart: (event, context) => {\\n context.translateX = clampedX.value;\\n cancelAnimation(translateX);\\n },\\n onActive: (event, context) => {\\n if (enabledScrollSharedValue.value === 0) {\\n return;\\n }\\n translateX.value = context.translateX + event.translationX;\\n },\\n onEnd: (event, context) => {\\n if (enabledScrollSharedValue.value === 0) {\\n return;\\n }\\n translateX.value = withDecay({ velocity: event.velocityX });\\n if (context.translateX === 0) {\\n if (event.translationX > 0) {\\n translateX.value = withTiming(width);\\n }\\n else if (event.translationX < 0) {\\n translateX.value = withTiming(-width);\\n }\\n } else {\\n if (Math.abs(event.translationX) < returnCenterRange && (context.translateX * event.translationX < 0)) {\\n translateX.value = withTiming(0);\\n }\\n }\\n }\\n });\\n\\n const middleRstyle = useAnimatedStyle(() => {\\n return {\\n transform: [{ translateX: clampedX.value }]\\n }\\n });\\n\\n const middleBorderRadius = useDerivedValue(() => {\\n return 10 * Math.abs(clampedX.value / width);\\n })\\n const middleINnerRstyle = useAnimatedStyle(() => {\\n return {\\n borderRadius: middleBorderRadius.value\\n }\\n })\\n\\n const backdropRStyle = useAnimatedStyle(() => {\\n const opacity = Math.abs(1 - ((width - remainingWidth) - clampedX.value) / (width - remainingWidth));\\n return {\\n backgroundColor: rgba(0,0,0,0.2),\\n opacity: opacity,\\n zIndex: opacity === 0 ? -1 : 5\\n }\\n })\\n\\n const backDrop = (\\n <Animated.View\\n style={[\\n { ...StyleSheet.absoluteFillObject },\\n backdropRStyle\\n ]}>\\n <TouchableOpacity style={{ ...StyleSheet.absoluteFillObject }} onPress={turnCenter} />\\n </Animated.View>)\\n\\n return (\\n\\n <View style={{ flex: 1, backgroundColor: backgroundColor }}>\\n\\n <PanGestureHandler\\n onGestureEvent={panGestureHandler}\\n activeOffsetX={[-10, 10]}\\n >\\n <Animated.View style={[{ ...StyleSheet.absoluteFillObject }]}>\\n <Animated.View style={[{\\n ...StyleSheet.absoluteFillObject, flex: 1,\\n flexDirection: \\"row\\",\\n zIndex: 2,\\n }, middleRstyle]}>\\n <Animated.View style={[middleINnerRstyle, {\\n width,\\n overflow: \\"hidden\\"\\n }]}>\\n {middle}\\n {backDrop}\\n </Animated.View>\\n\\n </Animated.View>\\n\\n\\n {left && <Animated.View style={[\\n {\\n ...StyleSheet.absoluteFillObject,\\n flex: 0,\\n zIndex: 1,\\n width,\\n },\\n leftpageRStyle\\n ]}>\\n <View style={{\\n width: width - remainingWidth,\\n overflow: \\"hidden\\"\\n }}>\\n {left}\\n </View>\\n </Animated.View>}\\n\\n {right && <Animated.View style={[\\n {\\n ...StyleSheet.absoluteFillObject,\\n flex: 0,\\n zIndex: 1,\\n width,\\n alignItems: \\"flex-end\\"\\n },\\n rightpageRStyle\\n ]}>\\n <View style={{ width: width - remainingWidth }}>\\n {right}\\n </View>\\n </Animated.View>\\n }\\n </Animated.View>\\n </PanGestureHandler>\\n </View >\\n )\\n };\\n}\\n\\nexport default Trinity\\n\\n\\n#### Usage\\n\\n##### Two Pages to Swipe\\n\\njs\\nexport const roomTrinity = new Trinity();\\n\\nconst Room = () => {\\n return (\\n <roomTrinity.instance\\n remainingWidth={width - 150}\\n middle={<MultiChat />}\\n right={<Namelist />}\\n />\\n )\\n}\\n\\n\\n##### Three Pages to Swipe, with Tabs Navigator at the Middle\\n\\nHere we use expo-router:\\n\\njs\\nexport const tradesTrinity = new Trinity();\\n\\nexport default () => {\\n const accessToken = useAppSelector(s => s.auth.accessToken);\\n const router = useRouter();\\n\\n useEffect(() => {\\n tradesTrinity.turnCenter();\\n }, []);\\n\\n useEffect(() => {\\n if (!accessToken) {\\n router.push(\\"/login\\")\\n }\\n }, [accessToken]);\\n\\n return (\\n <SafeAreaView>\\n <View style={{ width: \\"100%\\", height: \\"100%\\" }}>\\n <tradesTrinity.instance\\n remainingWidth={40}\\n left={<LeftScreens />}\\n right={<RightScreen />}\\n middle={(\\n <Tabs\\n initialRouteName=\\"issue\\"\\n screenOptions={{\\n headerShown: false,\\n tabBarStyle: { backgroundColor: \\"#292929\\" }\\n }}>\\n <Tabs.Screen name=\\"issue\\"\\n options={{\\n tabBarShowLabel: false,\\n tabBarActiveTintColor: \\"white\\",\\n tabBarIcon: (props) => <Ionicons name=\\"document-text\\" size={24} color=\\"#666666\\" style={{ color: props.color }} />\\n }} />\\n <Tabs.Screen name=\\"(chat)\\" options={{\\n tabBarShowLabel: false,\\n tabBarActiveTintColor: \\"white\\",\\n tabBarIcon: (props) => <Ionicons name=\\"chatbubbles\\" size={24} color=\\"#666666\\" style={{ color: props.color }} />\\n }} />\\n <Tabs.Screen name=\\"mail\\" options={{\\n tabBarShowLabel: false,\\n tabBarActiveTintColor: \\"white\\",\\n tabBarIcon: (props) => <Ionicons name=\\"mail-sharp\\" size={24} color=\\"#666666\\" style={{ color: props.color }} />\\n }} />\\n <Tabs.Screen name=\\"voiceMemo\\" options={{\\n tabBarShowLabel: false,\\n tabBarActiveTintColor: \\"white\\",\\n\\n tabBarIcon: (props) => <FontAwesome name=\\"plus-circle\\" size={24} color=\\"#666666\\" style={{ color: props.color }} />\\n }} />\\n </Tabs>\\n )}\\n />\\n </View>\\n </SafeAreaView>\\n )\\n}\\n\\n\\n","title":"Clone a Swipable Page Inspired from Discord Mobile App","date":"2023-09-26T00:00:00.000Z","id":"blog0185","tag":"react-native","intro":"We can create our custom component by creating custom behaviour via customizing animtation!","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px\\n }\\n</style>\\n\\n#### Commonly Used Commands\\n\\n- yarn add eas-cli\\n- yarn eas device:create\\n- json\\n \\"scripts\\": {\\n \\"start:dev\\": \\"env-cmd -f .env-cmdrc -e default,dev expo start --dev-client\\",\\n \\"android\\": \\"expo run:android\\",\\n \\"ios\\": \\"expo run:ios\\",\\n \\"prebuild\\": \\"npx expo prebuild\\",\\n \\"doctor\\": \\"npx expo-doctor\\",\\n \\"easios\\": \\"eas build --profile development --platform ios\\",\\n \\"easan\\": \\"eas build --profile development --platform android\\"\\n },\\n \\n- repeated use of expo doctor can resolve many building issues at the early stage.\\n\\n#### Development Build\\n\\n- After yarn easios or yarn easan, a development build will be compiled on cloud.\\n\\n [![](/assets/tech/186/001.png)](/assets/tech/186/001.png)\\n\\n- This **development build** must be linked with expo dev-client via our yarn start:dev command above.\\n- Our flag --profile development in eas build will use the config in eas.json\\n json\\n {\\n \\"cli\\": {\\n \\"version\\": \\">= 5.2.0\\"\\n },\\n \\"build\\": {\\n \\"development\\": {\\n \\"distribution\\": \\"internal\\",\\n \\"android\\": {\\n \\"gradleCommand\\": \\":app:assembleDebug\\"\\n },\\n \\"ios\\": {\\n \\"buildConfiguration\\": \\"Debug\\"\\n }\\n },\\n \\"preview\\": {\\n \\"distribution\\": \\"internal\\"\\n },\\n \\"production\\": {}\\n },\\n \\"submit\\": {\\n \\"production\\": {}\\n }\\n }\\n \\n Under this flag, the **_environment variable_** in **_development build_** is not determined at compile time, we can change the environment variables when we spawn a dev-client.\\n- If you wish, you can embed fixed environment variable in eas build by providing the variables in eas.json:\\n json\\n {\\n \\"build\\": {\\n \\"production\\": {\\n \\"env\\": {\\n \\"EXPO_PUBLIC_API_URL\\": \\"https://api.production.com\\"\\n }\\n },\\n \\"test\\": {\\n \\"env\\": {\\n \\"EXPO_PUBLIC_API_URL\\": \\"https://api.test.com\\"\\n }\\n }\\n }\\n }\\n \\n- After eas build is finished, there will be a QR code for android and ios to download the development build that contains all the native code we need.\\n\\n- We will keep developing the project in expo dev-client, but this time, we will link to our development build with a new QR code:\\n\\n [![](/assets/tech/186/002.png)](/assets/tech/186/002.png)\\n\\n instead of expo-go. Just scan it, we get the same developer experience as before even on Windows machine.\\n\\n#### Prebuild\\n\\n- A **_prebuild_** is actually the stuff that is generated in eas build on cloud if we have never prebuild the project.\\n\\n- For example, if we look at [react-native-twilio-video-webrtc](https://www.npmjs.com/package/react-native-twilio-video-webrtc), since this package has native code in both android and ios, **in the past** we need to\\n - Start from react-cli / eject from expo.\\n - Inject native code, and keep going without expo.\\n- This workflow is nowadays called a **_bare workflow_**.\\n\\n- Now look at **Usage with Expo** section of the npm page, the process becomes simply adding\\n\\n json\\n {\\n \\"name\\": \\"my app\\",\\n \\"plugins\\": [\\n [\\n \\"react-native-twilio-video-webrtc\\",\\n {\\n \\"cameraPermission\\": \\"Allow $(PRODUCT_NAME) to access your camera\\",\\n \\"microphonePermission\\": \\"Allow $(PRODUCT_NAME) to access your microphone\\"\\n }\\n ]\\n ]\\n }\\n \\n\\n in app.json. This will automatically insert\\n\\n text\\n <key>NSCameraUsageDescription</key>\\n <string>Your message to user when the camera is accessed for the first time</string>\\n <key>NSMicrophoneUsageDescription</key>\\n <string>Your message to user when the microphone is accessed for the first time</string>\\n \\n\\n into Info.plist of ios and insert similar requirements into AndroidManifest.xml of android.\\n\\n- For native code, package developer will write expo-modules with [Expo Modules API](https://docs.expo.dev/modules/overview/) that helps bridge native code and javascript.\\n\\n- **_Problem: What if they havn\'t?_**\\n\\n- Then we will need to **_prebuild the project_**, this will generate ios/ and android/ folders containing the native code of the project. We then follow the guidelines to modify the native code on our own.\\n\\n- After modification is done, we can continue to run eas build on the cloud, this time since ios/ and android/ exist, the cloud service will determine to use our own prebuild file to compile the application.\\n\\n- A barework workflow doesn\'t mean you need to start react-native-cli (in the old days we need to physically connect our ios device to Mac in order to continue).\\n\\n- After eas build --profile development and after we have downloaded the built mobile app from cloud, we can expo start --dev-client again.\\n","title":"Expo-CLI for Development Build","date":"2023-09-30T00:00:00.000Z","id":"blog0186","tag":"react-native, expo","intro":"Record detail of expo-cli workflow in development build.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### Result\\n\\n<center>\\n <video controls width=\\"400\\">\\n <source src=\\"/assets/tech/187/001.MP4\\" type=\\"video/mp4\\">\\n Sorry, your browser doesn\'t support embedded videos.\\n </video>\\n</center>\\n\\n#### Reason to Make One\\n\\n- With react-reanimated making a toast notification is extremely simple.\\n- Therefore there is no reason to rely on notification packages in npm, at the same time, we have full control how the notification looks and behaves.\\n\\n- Our notications get no boilerplate feeling.\\n\\n#### Code Implementation\\n\\n- Our strategy is to initialize ToastProvider (the default export) as quick as possible, so as to initialize the function addMessage which (whose reference) is eventually stored inside of toastProviderStore.\\n\\n- We are thereby able to addMessage() anywhere using the reference stored in this toastProviderStore.\\n\\n- We make a utility function to play around with this store at the end.\\n\\njs\\nimport { View, Text, Dimensions, StyleSheet, ScrollView } from \\"react-native\\"\\nimport { BlurView } from \\"@react-native-community/blur\\";\\nimport Animated, {\\n FadeIn,\\n FadeOut,\\n cancelAnimation,\\n runOnJS,\\n runOnUI,\\n useAnimatedGestureHandler,\\n useAnimatedScrollHandler,\\n useAnimatedStyle,\\n useSharedValue,\\n withDelay,\\n withSequence,\\n withSpring,\\n withTiming\\n} from \\"react-native-reanimated\\";\\nimport { PanGestureHandler, PanGestureHandlerGestureEvent } from \\"react-native-gesture-handler\\";\\nimport { ToastMessage } from \\"../dto/dto\\";\\nimport Spacer from \\"./Spacer\\";\\nimport { useEffect, useState } from \\"react\\";\\nimport { Ionicons } from \'@expo/vector-icons\';\\nimport uuid from \\"react-native-uuid\\";\\nimport lodash from \\"lodash\\";\\n\\nconst { width, height } = Dimensions.get(\\"window\\");\\nconst toastWidth = width - 20;\\n\\nexport const toastProviderStore: {\\n addMessage: ((props: { type: ToastMessage[\\"type\\"], msg: string }) => void) | null,\\n} = {\\n addMessage: null\\n}\\n\\nconst Toast = ({ index, message, remainingIds, deleteMessage }: {\\n index: number,\\n message: ToastMessage,\\n remainingIds: string[],\\n deleteMessage: (props: { uuid: string }) => void;\\n}) => {\\n const deleteToast = () => {\\n setTimeout(() => deleteMessage({ uuid: message.uuid }), 500);\\n };\\n\\n const translateX = useSharedValue(0);\\n const maxHeight = useSharedValue(100);\\n\\n const rstyle = useAnimatedStyle(() => {\\n return {\\n maxHeight: maxHeight.value,\\n transform: [{ translateX: translateX.value }]\\n }\\n })\\n\\n const containerRstyle = useAnimatedStyle(() => {\\n return {\\n maxHeight: maxHeight.value,\\n }\\n });\\n\\n const suicide = () => {\\n \\"worklet\\";\\n let sign = -1;\\n const magnitute = Math.abs(translateX.value);\\n if (magnitute > 0) {\\n sign = translateX.value / magnitute;\\n }\\n translateX.value = withTiming(width * sign);\\n maxHeight.value = withTiming(0);\\n\\n runOnJS(deleteToast)();\\n }\\n\\n const panGesture = useAnimatedGestureHandler<PanGestureHandlerGestureEvent>({\\n onActive: (e) => {\\n translateX.value = e.translationX;\\n },\\n onEnd: () => {\\n if (Math.abs(translateX.value) > 20) {\\n suicide();\\n } else {\\n translateX.value = withTiming(0);\\n }\\n }\\n })\\n\\n const toastStyle = (() => {\\n if (message.type === \\"success\\") {\\n return styles.successToast\\n } else if (message.type === \\"info\\") {\\n return styles.infoToast\\n } else {\\n return styles.errorToast\\n }\\n })()\\n\\n const toastIcon = (() => {\\n if (message.type === \\"success\\") {\\n return <Ionicons name=\\"checkmark-circle\\" size={24} color={toastStyle.color} />\\n } else if (message.type === \\"info\\") {\\n return <Ionicons name=\\"md-information-circle-sharp\\" size={24} color=\\"white\\" />\\n } else {\\n return <Ionicons name=\\"alert-circle\\" size={24} color=\\"white\\" />\\n }\\n })()\\n\\n useEffect(() => {\\n if (!remainingIds.includes(message.uuid)) {\\n setTimeout(() => { suicide(); }, 500)\\n }\\n }, [remainingIds])\\n\\n const extrStyle = index === 0 ? { marginTop: 20 } : {}\\n\\n return (\\n <Animated.View\\n entering={FadeIn}\\n exiting={FadeOut}\\n style={[extrStyle, containerRstyle]}\\n >\\n <PanGestureHandler\\n activeOffsetX={[-20, 20]}\\n onGestureEvent={panGesture}\\n >\\n <Animated.View style={[\\n {\\n marginTop: 10,\\n width: toastWidth,\\n paddingVertical: 10,\\n paddingHorizontal: 20,\\n paddingLeft: 10,\\n borderWidth: 1,\\n borderRadius: 10,\\n overflow: \\"hidden\\",\\n ...toastStyle\\n },\\n rstyle\\n ]}>\\n <BlurView\\n style={styles.absolute}\\n blurType=\\"light\\"\\n blurAmount={10}\\n reducedTransparencyFallbackColor=\\"rgba(247,223,192)\\"\\n />\\n <View style={{ flexDirection: \\"row\\", alignItems: \\"center\\" }}>\\n <View style={{ width: 30 }}>{toastIcon}</View>\\n <View style={{ flex: 1 }}><Text style={{ color: toastStyle.color }}>{message.text}</Text></View>\\n </View>\\n </Animated.View>\\n </PanGestureHandler>\\n </Animated.View>\\n )\\n}\\n\\nexport default () => {\\n const [toastMessages, setToastMessages] = useState<ToastMessage[]>([]);\\n const remainingIds = toastMessages.map(m => m.uuid);\\n\\n const addMessage = ({ type, msg }: { type: ToastMessage[\\"type\\"], msg: string }) => {\\n const toast: ToastMessage = { uuid: uuid.v4() as string, type, text: msg };\\n\\n\\n setToastMessages(msgs => {\\n if (msgs.length >= 4) {\\n return [...msgs.slice(msgs.length - 4, msgs.length), toast]\\n } else {\\n return [...msgs, toast];\\n }\\n });\\n };\\n\\n if (!toastProviderStore.addMessage) {\\n toastProviderStore.addMessage = addMessage;\\n }\\n\\n const deleteMessage = ({ uuid }: { uuid: string }) => {\\n setToastMessages(msgs => {\\n const newMsgs = lodash.cloneDeep(msgs);\\n return newMsgs.filter(m => m.uuid !== uuid);\\n });\\n }\\n\\n\\n const containerTranslateY = useSharedValue(0);\\n\\n const scrollRegionHeight = useSharedValue(0);\\n\\n const scrollSpacerRstyle = useAnimatedStyle(() => {\\n return {\\n height: scrollRegionHeight.value\\n }\\n })\\n\\n const toastContainerStyle = useAnimatedStyle(() => {\\n return {\\n transform: [{\\n translateY: containerTranslateY.value,\\n }]\\n }\\n })\\n\\n const killMessages = () => {\\n scrollRegionHeight.value = withDelay(200, withTiming(0));\\n setTimeout(() => { setToastMessages([]) }, 390);\\n containerTranslateY.value = withSequence(\\n withDelay(200, withTiming(-height)),\\n withDelay(400, withTiming(0))\\n );\\n };\\n\\n\\n const scrollHandler = useAnimatedScrollHandler({\\n onBeginDrag: (event, ctx) => {\\n scrollRegionHeight.value = 200;\\n },\\n onEndDrag: (event) => {\\n if (Math.abs(event.contentOffset.y) > 20) {\\n runOnJS(killMessages)();\\n } else {\\n scrollRegionHeight.value = withDelay(300, withTiming(0));\\n }\\n\\n }\\n })\\n\\n return (\\n <Animated.View style={[\\n {\\n alignItems: \\"center\\",\\n position: \\"absolute\\",\\n zIndex: 1,\\n width,\\n maxHeight: height\\n },\\n toastContainerStyle\\n ]}\\n >\\n <Animated.ScrollView\\n showsHorizontalScrollIndicator={false}\\n showsVerticalScrollIndicator={false}\\n scrollEventThrottle={16}\\n onScroll={scrollHandler}\\n >\\n {toastMessages.map((msg, index) => {\\n return (\\n <Toast index={index} message={msg} key={msg.uuid} deleteMessage={deleteMessage} remainingIds={remainingIds} />\\n )\\n })}\\n <Animated.View style={[scrollSpacerRstyle]} />\\n </Animated.ScrollView>\\n </Animated.View >\\n )\\n}\\n\\nconst styles = StyleSheet.create({\\n container: {\\n justifyContent: \\"center\\",\\n alignItems: \\"center\\"\\n },\\n\\n successToast: {\\n color: \\"#499F1F\\",\\n borderColor: \\"rgba(48,200,97,0.4)\\",\\n backgroundColor: \\"rgba(247,223,192, 0.5)\\",\\n },\\n errorToast: {\\n color: \\"white\\",\\n borderColor: \\"#DD7C72\\",\\n backgroundColor: \\"rgba(204,25,6,0.6)\\",\\n },\\n infoToast: {\\n color: \\"white\\",\\n borderColor: \\"#4489DC\\",\\n backgroundColor: \\"rgba(153,39,172,0.75)\\",\\n },\\n absolute: {\\n position: \\"absolute\\",\\n top: 0,\\n left: 0,\\n bottom: 0,\\n right: 0\\n }\\n});\\n\\n\\n#### Usage\\n\\nIn _layout.tsx at the root project level (or App.tsx without expo-router) we add our ToastProvider to initialize addMessage function:\\n\\njs\\nfunction RootLayoutNav() {\\n const colorScheme = useColorScheme();\\n\\n return (\\n <ThemeProvider value={colorScheme === \\"dark\\" ? DarkTheme : DefaultTheme}>\\n <Provider store={store}>\\n <PersistGate persistor={persistor}>\\n <ConfigApiClient store={store}>...</ConfigApiClient>\\n <ToastProvider />\\n </PersistGate>\\n </Provider>\\n <FlashMessage position=\\"top\\" hideOnPress={true} />\\n </ThemeProvider>\\n );\\n}\\n\\n\\nSince we simply use useState in our Toast\'s, our ToastProvider does not necessarily lie inside Provider.\\n\\nNext we create a utility function to send different kinds of messages:\\n\\njs\\n// toastUtil.ts\\n\\nimport { toastProviderStore } from \\"../components/ToastProvider\\";\\n\\nconst success = (msg: string) => {\\n toastProviderStore?.addMessage?.({ type: \\"success\\", msg });\\n};\\n\\nconst info = (msg: string) => {\\n toastProviderStore?.addMessage?.({ type: \\"info\\", msg });\\n};\\n\\nconst error = (msg: string) => {\\n toastProviderStore?.addMessage?.({ type: \\"error\\", msg });\\n};\\n\\nexport default {\\n success,\\n info,\\n error,\\n};\\n\\n","title":"Custom Toast Messages","date":"2023-10-01T00:00:00.000Z","id":"blog0187","tag":"react-native","intro":"We build our own toast meassages.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### Result\\n\\n<center>\\n <video controls width=\\"400\\">\\n <source src=\\"/assets/tech/188/001.MP4\\" type=\\"video/mp4\\">\\n Sorry, your browser doesn\'t support embedded videos.\\n </video>\\n</center>\\n\\n#### Code Implementation\\n\\nAs usual code implementation is a mess of ad-hoc detail, but the usage is very simple!\\n\\njs\\nimport { ReactNode, forwardRef, useImperativeHandle } from \\"react\\";\\nimport Animated, { useAnimatedGestureHandler, useAnimatedStyle, useDerivedValue, useSharedValue, withSpring, withTiming } from \\"react-native-reanimated\\";\\nimport { useHeaderHeight, } from \'@react-navigation/elements\';\\nimport Constants from \'expo-constants\';\\nimport { PanGestureHandler, PanGestureHandlerGestureEvent } from \\"react-native-gesture-handler\\";\\nimport { Dimensions, StyleSheet, TouchableOpacity, View } from \\"react-native\\";\\n\\nconst { height } = Dimensions.get(\\"window\\");\\n\\nexport type BottomSheetHandle = {\\n show: () => void;\\n close: () => void;\\n}\\n\\nexport type BottomSheetProps = {\\n component: ReactNode,\\n maxHeight?: number\\n}\\n\\nconst BottomSheet = forwardRef<BottomSheetHandle, BottomSheetProps>((props, ref) => {\\n const bottomSheetShiftY = useSharedValue(0);\\n const headerHeight = useHeaderHeight();\\n const statusbarHeight = Constants.statusBarHeight;\\n const topHeaderHeight = headerHeight + statusbarHeight\\n const screenHeight = height - topHeaderHeight;\\n const { maxHeight = screenHeight * 9 / 10 } = props;\\n\\n const show = () => {\\n bottomSheetShiftY.value = withSpring(-maxHeight / 2)\\n }\\n const close = () => {\\n bottomSheetShiftY.value = withTiming(0);\\n }\\n useImperativeHandle(ref, () => ({\\n show,\\n close,\\n }))\\n\\n const bottomSheetClampedY = useDerivedValue(() => {\\n return Math.max(-maxHeight, Math.min(screenHeight, bottomSheetShiftY.value));\\n })\\n\\n\\n const sheetRstyle = useAnimatedStyle(() => {\\n return {\\n transform: [{ translateY: bottomSheetClampedY.value }]\\n }\\n })\\n\\n const backdropRstyle = useAnimatedStyle(() => {\\n return {\\n opacity: - bottomSheetClampedY.value / screenHeight\\n }\\n })\\n\\n const panHandler = useAnimatedGestureHandler<PanGestureHandlerGestureEvent, { translationY: number }>({\\n onStart: (event, context) => {\\n context.translationY = bottomSheetShiftY.value;\\n },\\n onActive: (event, context) => {\\n\\n bottomSheetShiftY.value = context.translationY + event.translationY;\\n },\\n onEnd: (event) => {\\n console.log(event.absoluteY, maxHeight);\\n\\n if (event.absoluteY > maxHeight * 1.5) {\\n bottomSheetShiftY.value = withTiming(0);\\n } else {\\n bottomSheetShiftY.value = withTiming(-maxHeight);\\n }\\n }\\n });\\n\\n const bottomsheetRstyle = useAnimatedStyle(() => {\\n if (bottomSheetShiftY.value < 0) {\\n return { height: screenHeight };\\n } else {\\n return { height: 0 };\\n }\\n });\\n\\n return (\\n <Animated.View style={[StyleSheet.absoluteFillObject, { zIndex: 2 }, bottomsheetRstyle]}>\\n <PanGestureHandler onGestureEvent={panHandler}>\\n <Animated.View style={StyleSheet.absoluteFillObject}>\\n <Animated.View style={[\\n StyleSheet.absoluteFillObject, { backgroundColor: \\"rgba(0,0,0,0.6)\\" },\\n backdropRstyle,\\n ]} />\\n <TouchableOpacity\\n onPress={() => { bottomSheetShiftY.value = withTiming(0) }}\\n style={[StyleSheet.absoluteFillObject, { width: \\"100%\\", height: \\"100%\\" }]}\\n />\\n <Animated.View style={[\\n {\\n position: \\"absolute\\",\\n width: \\"100%\\",\\n height: \\"100%\\",\\n backgroundColor: \\"white\\",\\n top: screenHeight,\\n borderRadius: 10,\\n overflow: \\"hidden\\"\\n },\\n sheetRstyle\\n ]}>\\n <Animated.View style={[\\n { width: \\"100%\\", flexDirection: \\"row\\", justifyContent: \\"center\\", paddingTop: 15 },\\n ]}>\\n <View style={[\\n { width: 50, height: 4, backgroundColor: \\"rgba(0,0,0,0.3)\\", borderRadius: 2 },\\n ]} />\\n </Animated.View>\\n\\n <View style={{ marginTop: 20 }}>\\n {props.component}\\n </View>\\n </Animated.View>\\n\\n </Animated.View>\\n </PanGestureHandler>\\n </Animated.View>\\n )\\n})\\n\\nexport default BottomSheet;\\n\\n\\n#### Usage\\n\\njs\\nconst playground = () => {\\n const ref = useRef < BottomSheetHandle > null;\\n const showBottomSheet = () => {\\n ref.current?.show();\\n };\\n return (\\n <View\\n style={{\\n width: \\"100%\\",\\n height: \\"100%\\",\\n justifyContent: \\"center\\",\\n alignItems: \\"center\\",\\n }}\\n >\\n <TouchableOpacity onLongPress={showBottomSheet} delayLongPress={1000}>\\n <Text style={{ padding: 20, borderWidth: 1 }}>Test</Text>\\n </TouchableOpacity>\\n <BottomSheet\\n component={\\n <>\\n <Text>This is my nice Test</Text>\\n </>\\n }\\n ref={ref}\\n maxHeight={400}\\n />\\n </View>\\n );\\n};\\n\\n","title":"Custom BottomSheet","date":"2023-10-03T00:00:00.000Z","id":"blog0188","tag":"react-native","intro":"We build our own bottom sheet.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### What is Task? Task Definition? Service?\\n\\nIn short:\\n\\n- **Task Definition.** Define which image to use, define how much resources (vCPU, memory, etc) should be allocated to the task.\\n\\n- **Task.** It is the most basic building blocks in Fargate, they are **_instances_** of Task Definition.\\n\\n- **Service.** It is a system that **_ensures_** _X_ amount of tasks are up and running.\\n\\nWhen we have containerized an application, we can readily deploy it on cloud using ECS Fargate!\\n\\nFargate is designed to work with load balancer. Make sure to have one before proceeding.\\n\\n#### Procedures to Create an ECS Fargate Service/Task\\n\\n##### Create a Task Definition\\n\\n- We fill in the highlighted fields and leave the rest as default.\\n\\n [![](/assets/tech/189/001.png)](/assets/tech/189/001.png)\\n\\n- A new definition revision will be created\\n\\n [![](/assets/tech/189/002.png)](/assets/tech/189/002.png)\\n\\n- Since we will repeatedly create **_new docker image_** as an update, we can reuse our old revision by **_simply changing the image URI_**:\\n\\n 1. Create new revision\\n\\n [![](/assets/tech/189/003.png)](/assets/tech/189/003.png)\\n\\n 2. Use latest docker image\\n\\n [![](/assets/tech/189/004.png)](/assets/tech/189/004.png)\\n\\n##### Create Target Group and Associte it with a Load Balancer\\n\\n- We can only use Target Group of type IP Address\\n\\n [![](/assets/tech/189/image.png)](/assets/tech/189/image.png)\\n\\n- Target group acts like a forward proxy, we just need HTTP (without SSL):\\n\\n [![](/assets/tech/189/image-22.png)](/assets/tech/189/image-22.png)\\n\\n- Make sure we have created a route for health-check, in my case I use /test which simply responses {\\"success\\": true}.\\n\\n [![](/assets/tech/189/image-2.png)](/assets/tech/189/image-2.png)\\n\\n- Click Next.\\n\\n [![](/assets/tech/189/image-3.png)](/assets/tech/189/image-3.png)\\n\\n- Fill in the destination port\\n\\n [![](/assets/tech/189/image-4.png)](/assets/tech/189/image-4.png)\\n\\n since we have not created a task/service yet, we can leave everything unchanged and click **_create target group_**.\\n\\n- Associate this target group with our load balancer by creating a new listener:\\n\\n [![](/assets/tech/189/image-5.png)](/assets/tech/189/image-5.png)\\n\\n- Choose a certificate:\\n\\n [![](/assets/tech/189/image-6.png)](/assets/tech/189/image-6.png)\\n\\n- and click Add:\\n\\n [![](/assets/tech/189/image-7.png)](/assets/tech/189/image-7.png)\\n\\n##### Back to ECS\'s Task Definition: Create a Service\\n\\n- We can start our deployment by running a task or creating a service using this task definition.\\n\\n [![](/assets/tech/189/image-8.png)](/assets/tech/189/image-8.png)\\n\\n- Why there are two options?\\n\\n - **Create service.** With this option we can set how many tasks are up and running, we can also set min and max number of tasks to handle sudden changes of traffic.\\n\\n - **Run task.** However, not every task is readily scalable.\\n\\n For example, if our web server is also a socket.io chat server, we need to scale it by subscribing and publishing to a redis client (see [here](/blog/article/Scaling-Websocket-Chat-Sever-by-Redis)) and change the mechanism of \\"client send message\\" in backend to adapt this change.\\n\\n In such cases, we only want 1 task to be kept running.\\n\\n Back to task definition, check our desired revision, we first proceed by \\"Create service\\".\\n\\n- Choose cluster (which groups our services), choose Launch type and choose FARGATE (default)\\n\\n [![](/assets/tech/189/image-9.png)](/assets/tech/189/image-9.png)\\n\\n- Input a service name, then configure deployment options (leave it unchanged)\\n\\n [![](/assets/tech/189/image-10.png)](/assets/tech/189/image-10.png)\\n\\n- We use an existing security group, later we will allow load balancer to access our service by adding a new inbound rule.\\n\\n [![](/assets/tech/189/image-11.png)](/assets/tech/189/image-11.png)\\n\\n- Choose our load balancer, and then skip to **Target Group**, choose the target group that we have asscoiated with the load balancer, the _Listener_ fields will be filled up automatically.\\n\\n [![](/assets/tech/189/image-12.png)](/assets/tech/189/image-12.png)\\n\\n- Click Create.\\n\\n [![](/assets/tech/189/image-13.png)](/assets/tech/189/image-13.png)\\n\\n##### Let Load Balancer Access our Service\\n\\n- In clusters dashboard, click the service name\\n\\n [![](/assets/tech/189/image-15.png)](/assets/tech/189/image-15.png)\\n\\n- Go to Networking tab\\n\\n [![](/assets/tech/189/image-16.png)](/assets/tech/189/image-16.png)\\n\\n- Now the networking is governed by the security group list here, click it\\n\\n [![](/assets/tech/189/image-17.png)](/assets/tech/189/image-17.png)\\n\\n- Edit inbound rules and add the security group of our load balancer into the whitelist.\\n\\n [![](/assets/tech/189/image-19.png)](/assets/tech/189/image-19.png)\\n\\n- We can find the name of security group of the load balancer here\\n\\n [![](/assets/tech/189/image-18.png)](/assets/tech/189/image-18.png)\\n\\n- After that our deployment is complete.\\n\\n [![](/assets/tech/189/image-20.png)](/assets/tech/189/image-20.png)\\n\\n##### Verifying it is Working\\n\\n- We have created a /test route, let\'s check it:\\n\\n [![](/assets/tech/189/image-23.png)](/assets/tech/189/image-23.png)\\n\\n##### Run a Task Instead of Running a Service\\n\\n- Recall that our deployment setting is:\\n\\n [![](/assets/tech/189/image-25.png)](/assets/tech/189/image-25.png)\\n\\n- If we want only one instance (task) to be deployed, we might want to change the **Max running tasks %** from 200 to 101, which in my case results in buggy behaviour.\\n\\n- To make sure there are only one task, we can choose to **_run task_** instead of **_create service_**.\\n\\n [![](/assets/tech/189/image-26.png)](/assets/tech/189/image-26.png)\\n\\n- Same setting as before:\\n\\n [![](/assets/tech/189/image-27.png)](/assets/tech/189/image-27.png)\\n\\n- Next we leave everything unchanged, click create.\\n\\n [![](/assets/tech/189/image-28.png)](/assets/tech/189/image-28.png)\\n\\n- This time we will bind our task to Target Group through **_private IP_**.\\n\\n- Click Tasks tab and click the running task:\\n\\n [![](/assets/tech/189/image-29.png)](/assets/tech/189/image-29.png)\\n\\n- Copy the private IP\\n\\n [![](/assets/tech/189/image-30.png)](/assets/tech/189/image-30.png)\\n\\n- Choose our old target group that associated with our load balancer, register a new target:\\n\\n [![](/assets/tech/189/image-31.png)](/assets/tech/189/image-31.png)\\n\\n- Choose an availability zone that does not show warning, then click **_Include as pending below_**.\\n\\n- Finally click **_Register pending targets_**.\\n\\n [![](/assets/tech/189/image-32.png)](/assets/tech/189/image-32.png)\\n\\n- Let\'s wait for health check:\\n\\n [![](/assets/tech/189/image-33.png)](/assets/tech/189/image-33.png)\\n\\n- and we are done:\\n\\n [![](/assets/tech/189/image-35.png)](/assets/tech/189/image-35.png)\\n\\n- Same result:\\n\\n [![](/assets/tech/189/image-34.png)](/assets/tech/189/image-34.png)\\n\\n- Cheers!\\n","title":"AWS Fargate: Let\'s Create Service and Run Task!","date":"2023-10-07T00:00:00.000Z","id":"blog0189","tag":"aws","intro":"There are two ways of deploying tasks in ECS fargate, by either \\"create service\\" or \\"run task\\", let\'s get hands on experience with both methods.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### File Upload\\n\\n##### The Visual Result\\n\\n<center>\\n <video controls width=\\"400\\">\\n <source src=\\"/assets/tech/190/001.mp4\\" type=\\"video/mp4\\">\\n Sorry, your browser doesn\'t support embedded videos.\\n </video>\\n</center>\\n\\n<p></p>\\n\\n<center></center>\\n\\n##### Behind the Scene\\n\\nThe following pieces have taken place:\\n\\n- An audio file is base64-encoded in frontend\\n- We send this base64-encoded string as formData\\n- We have nice package in backend to receive formData as inputStreams\\n- The input stream of audio file is processed by the following pipelines:\\n - base64-encoded to uint8\\n - m4a file to mp3 file\\n - The stream is piped into a Duplex stream (to adapt to api design of ffmpeg)\\n - That Duplex stream is piped into azure\'s uploadStream() method\\n\\n##### Fontend: Upload an Audio Using FormData and Base64 Encoded String\\n\\njs\\nimport * as FileSystem from \\"expo-file-system\\";\\n\\nconst uploadFile = async (audioFileUri: string) => {\\n const base64EncodedFile = await FileSystem.readAsStringAsync(audioFileUri, {\\n encoding: FileSystem.EncodingType.Base64,\\n });\\n\\n dispatch(\\n chatThunkAction.uploadVoice({\\n roomOid: selectedRoom._id,\\n base64EncodedFile: { current: base64EncodedFile },\\n })\\n )\\n .unwrap()\\n .finally(() => {\\n FileSystem.deleteAsync(audioFileUri).catch(() => {\\n msgUtil.error(Cannot delete file: ${audioFileUri});\\n });\\n });\\n};\\n\\n\\nHere the thunk action chatThunkAction.uploadVoice() is defined as follows:\\n\\njs\\nchatThunkAction =\\n{\\n uploadVoice: createAsyncThunk(\\"chatSlice/upload-voice\\", async (props: {\\n roomOid: string,\\n base64EncodedFile: { current: string }\\n }, api) => {\\n const { base64EncodedFile, roomOid } = props;\\n const formData = new FormData();\\n formData.append(\\"file\\", base64EncodedFile.current);\\n const res = await apiClient\\n .post<WBResponse<undefined>>(\\n apiRoutes.POST_UPLOAD_VOICE(roomOid),\\n formData,\\n { headers: { \\"Content-Type\\": \\"multipart/form-data\\" } }\\n );\\n return processRes(res, api);\\n }),\\n ...\\n}\\n\\n\\nWe also pass an object to avoid copying the base64 encoded string (which is huge).\\n\\n- Now we have changed the file-upload procedures into a standard form-data approach that we have learnt from web developement.\\n\\n- As a full-stack developer in nodejs we love to handle incoming file stream by multiparty!\\n\\n##### Backend: Process the String Stream: Base64 to Uint8, From m4a To mp3, Pass Resulting Stream to azureClient.uploadStream()\\n\\nWe import a duplex to transform the base64-string-stream into a bytes-stream:\\n\\njs\\n... // other dependencies\\nimport { Base64Decode } from \\"base64-stream\\";\\nimport multiparty from \'multiparty\';\\n\\nconst voiceUpload = async (req: Request, res: Response) => {\\n const { roomOid } = req.query as { roomOid: string }\\n const form = new multiparty.Form();\\n const msgDoc = await MessageModel.create({\\n roomOid,\\n userOid: req.user?.userOid,\\n type: \\"Voice\\"\\n });\\n form.parse(req);\\n form.on(\\"part\\", async (inputStream) => {\\n const uint8Stream = inputStream.pipe(new Base64Decode());\\n const bufferStream = new PassThrough();\\n const ffmpeg = ffmpegUtil.getFfmpeg();\\n ffmpeg(uint8Stream)\\n .inputFormat(\\"m4a\\")\\n .audioCodec(\'libmp3lame\')\\n .audioChannels(1)\\n .audioBitrate(128)\\n .format(\'mp3\')\\n .pipe(bufferStream)\\n\\n const res_ = await client.getBlockBlobClient(filename).uploadStream(bufferStream);\\n })\\n\\n\\n##### Summary for Backend\\n\\nSince every step is merely processing stream, our data processing (from data conversion to file uploading to azure) is memory efficient as we never wait for the whole stream to complete before moving to the next step.\\n\\nApart from handling data conversion in stream, we also discussed zip stream in the past! [Check this out](/blog/article/Handle-Streams-in-File-Responding-Request)!\\n\\n#### File Download\\n\\n##### Example of Redesigned Image Component for API That Returns a Stream\\n\\nWhen <Image source={{ uri: imageUrl }}/> fails, it is possible that the API returns a stream (chunks), then you may try the following:\\n\\njs\\nexport default (props: { imageUri: string }) => {\\n const { imageUri } = props;\\n const [base64, setBase64] = useState(\\"\\");\\n const [id, setId] = useState(\\"\\");\\n\\nuseEffect(() => {\\n RNFetchBlob\\n .fetch(\'GET\', imageUri)\\n .then((res) => {\\n setBase64(\\"data:image/jpeg;base64,\\" + res.base64());\\n setId(uuid.v4() as string);\\n })\\n .catch((err) => {\\n msgUtil.error(err);\\n })\\n }, []);\\n\\n if (!base64) {\\n return null;\\n }\\n\\n return (\\n <Image\\n source={{ uri: base64 }}\\n key={id}\\n style={{\\n width: 180,\\n height: 300,\\n marginTop: 10,\\n borderRadius: 4\\n }}\\n />\\n )\\n}\\n\\n\\n#### More on Base64 Encoding\\n\\n- [Why do we use Base64?](https://stackoverflow.com/questions/3538021/why-do-we-use-base64)\\n","title":"File Upload and Download Using Stream and FormData in React-Native","date":"2023-10-05T00:00:00.000Z","id":"blog0190","tag":"react-native","intro":"File streaming is a very basic technique to effectively transmit files from frontend to backend and, of course, from within backends as well.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### Code Implementation\\n\\nThe following hook is simply a rewrite from the [official documentation](https://github.com/hyochan/react-native-audio-recorder-player#usage) written in class component:\\n\\njs\\nimport { useRef, useState } from \\"react\\";\\nimport AudioRecorderPlayer from \\"react-native-audio-recorder-player\\";\\n\\nexport default () => {\\n const audioRecorderRef =\\n useRef < AudioRecorderPlayer > new AudioRecorderPlayer();\\n const [recordSecs, setRecordSecs] = useState(0);\\n const [recordTime, setRecordTime] = useState(\\"\\");\\n const [currentPositionSec, setCurrentPositionSec] = useState(0);\\n const [currentDurationSec, setCurrentDurationSec] = useState(0);\\n const [playTime, setPlayTime] = useState(\\"\\");\\n const [duration, setDuration] = useState(\\"\\");\\n const [audioFileUri, setAudioFileUri] = useState(\\"\\");\\n\\n const onStartRecord = async (path: string) => {\\n let result = \\"\\";\\n if (path) {\\n result = await audioRecorderRef.current.startRecorder(path);\\n } else {\\n result = await audioRecorderRef.current.startRecorder();\\n }\\n setAudioFileUri(result);\\n audioRecorderRef.current.addRecordBackListener((e) => {\\n setRecordSecs(e.currentPosition);\\n setRecordTime(\\n audioRecorderRef.current.mmssss(Math.floor(e.currentPosition))\\n );\\n return;\\n });\\n console.log(\\"my defined filepath\\", path);\\n console.log(\\"result filepath\\", result);\\n };\\n\\n const onStopRecord = async () => {\\n await audioRecorderRef.current.stopRecorder();\\n audioRecorderRef.current.removeRecordBackListener();\\n setRecordSecs(0);\\n };\\n\\n const onStartPlay = async () => {\\n console.log(\\"onStartPlay\\");\\n const msg = await audioRecorderRef.current.startPlayer(audioFileUri);\\n console.log(msg);\\n audioRecorderRef.current.addPlayBackListener((e) => {\\n setCurrentPositionSec(e.currentPosition);\\n setCurrentDurationSec(e.duration);\\n setPlayTime(\\n audioRecorderRef.current.mmssss(Math.floor(e.currentPosition))\\n );\\n setDuration(audioRecorderRef.current.mmssss(Math.floor(e.duration)));\\n return;\\n });\\n };\\n\\n const onPausePlay = async () => {\\n await audioRecorderRef.current.pausePlayer();\\n };\\n\\n const onStopPlay = async () => {\\n console.log(\\"onStopPlay\\");\\n audioRecorderRef.current.stopPlayer();\\n audioRecorderRef.current.removePlayBackListener();\\n };\\n\\n return {\\n audioFileUri,\\n onStartRecord,\\n onStopRecord,\\n onStartPlay,\\n onPausePlay,\\n onStopPlay,\\n };\\n};\\n\\n\\n#### Usage\\n\\njs\\n// component used to record audio\\n\\nexport default () => {\\n const {\\n audioFileUri,\\n onPausePlay,\\n onStartPlay,\\n onStartRecord,\\n onStopPlay,\\n onStopRecord,\\n } = useAudioRecording();\\n\\n\\n const startRecording = () => {\\n await onStartRecord(\\"\\");\\n setIsRecording(true);\\n };\\n\\n const stopRecording =()=>{\\n await onStopRecord();\\n setIsRecording(false);\\n await uploadAudio();\\n }\\n\\n const uploadAudio = async () => {\\n console.log(\\"uploading\\", audioFileUri)\\n const base64EncodedFile = await FileSystem.readAsStringAsync(\\n audioFileUri,\\n { encoding: FileSystem.EncodingType.Base64 }\\n );\\n dispatch(chatThunkAction.uploadAudioFile(\\n {\\n roomOid: selectedRoom._id,\\n base64EncodedFile: { current: base64EncodedFile }\\n }\\n )).unwrap().finally(() => {\\n FileSystem.deleteAsync(audioFileUri)\\n .catch(() => {\\n msgUtil.error(Cannot delete file: ${audioFileUri})\\n });\\n })\\n }\\n\\n return (\\n ...\\n )\\n};\\n\\n","title":"Audio Recording in React-Native","date":"2023-10-07T00:00:00.000Z","id":"blog0191","tag":"react-native","intro":"An introduction to react-native-audio-recorder-player","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### How do I come up with this Implementation?\\n\\nI find an inspiring article\\n\\n> [VisionCamera - Zooming](https://react-native-vision-camera.com/docs/guides/zooming)\\n\\n<center></center>\\n\\nbut I don\'t need its full list of extra features....\\n\\nHowever, it does shed some light on how to let the animated-state take effect to the actual react component. Combined we what I learn in\\n\\n> [The basics of PinchGestureHandler with React Native Reanimated 2](https://www.youtube.com/watch?v=R7vyLItMQJw&list=PLjHsmVtnAr9TWoMAh-3QMiP7bPUqPFuFZ&index=5)\\n\\n<center></center>\\n\\nI reproduce the pin-to-zoom function successfully.\\n\\n#### Code Implementation\\n\\njs\\nimport { Camera, CameraProps, CameraType, ImageType } from \'expo-camera\';\\nimport { useEffect, useRef, useState } from \'react\';\\nimport { useAppDispatch, useAppSelector } from \'../../redux/app/hooks\';\\nimport appSlice from \'../../redux/slices/appSlice\';\\nimport { dialogColor } from \'../../components/WbDialog\';\\nimport { useRouter } from \'expo-router\';\\nimport { View } from \'react-native\';\\nimport { MaterialCommunityIcons } from \'@expo/vector-icons\';\\nimport BlurBackgorund from \'../../components/BlurBackgorund\';\\nimport {\\n PinchGestureHandler,\\n PinchGestureHandlerGestureEvent,\\n TouchableOpacity\\n} from \'react-native-gesture-handler\';\\nimport { chatThunkAction } from \'../../redux/slices/chatSlice\';\\nimport msgUtil from \'../../util/msgUtil\';\\nimport Animated, {\\n useAnimatedGestureHandler,\\n useAnimatedProps,\\n useSharedValue,\\n withSpring\\n} from \'react-native-reanimated\';\\n\\nconst AnimatedCamera = Animated.createAnimatedComponent(Camera);\\n\\nexport default () => {\\n const [type, setType] = useState(CameraType.back);\\n const [permission, requestPermission] = Camera.useCameraPermissions();\\n const roomOid = useAppSelector(s => s.chat.selectedRoom._id);\\n const router = useRouter();\\n const cameraRef = useRef<Camera>(null);\\n const dispatch = useAppDispatch();\\n\\n const takePhoto = async () => {\\n const res = await cameraRef.current?.takePictureAsync({ base64: true, quality: 0.5, imageType: ImageType.jpg });\\n if (res) {\\n if (!res.base64) {\\n return msgUtil.error(\\"Image data is invalid\\");\\n }\\n const base64String = res.base64;\\n dispatch(chatThunkAction.uploadImageFile({ base64EncodedFile: { current: base64String }, roomOid }));\\n router.push(\\"/(billie)/room\\");\\n }\\n }\\n\\n\\n const zoom = useSharedValue(0);\\n\\n const animatedProps = useAnimatedProps<Partial<CameraProps>>(\\n () => ({ zoom: zoom.value }),\\n [zoom]\\n )\\n\\n const pinchHandler = useAnimatedGestureHandler<PinchGestureHandlerGestureEvent, { zoom: number }>({\\n onStart: (event, context) => {\\n context.zoom = zoom.value;\\n },\\n onActive: (event, context) => {\\n console.log(event.scale / 30);\\n if (context.zoom == 0) {\\n zoom.value = event.scale / 300;\\n } else {\\n zoom.value = context.zoom * (1 + event.scale / 2);\\n }\\n },\\n onEnd: (event, context) => {\\n context.zoom = zoom.value;\\n if (event.scale < 0.8) {\\n zoom.value = withSpring(0);\\n }\\n }\\n })\\n\\n useEffect(() => {\\n requestPermission().then((res) => {\\n if (!res.granted) {\\n dispatch(appSlice.actions.updateAppDialog({\\n open: true,\\n desc: \\"No camera right has been granted.\\",\\n ok: {\\n color: dialogColor.BLUE,\\n label: \\"OK\\",\\n action: () => {\\n router.push(\\"/room\\")\\n }\\n }\\n }))\\n }\\n })\\n }, []);\\n\\n return (\\n <View style={{ width: \\"100%\\", height: \\"100%\\" }}>\\n <PinchGestureHandler onGestureEvent={pinchHandler}>\\n <AnimatedCamera\\n animatedProps={animatedProps}\\n style={[{ width: \\"100%\\", height: \\"100%\\", position: \\"relative\\" }]}\\n type={type}\\n ref={cameraRef}\\n >\\n <View style={{\\n position: \\"absolute\\",\\n bottom: 0,\\n backgroundColor: \\"rgba(0,0,0,0.8)\\",\\n height: 100,\\n width: \\"100%\\",\\n justifyContent: \\"center\\",\\n alignItems: \\"center\\"\\n }}>\\n <BlurBackgorund />\\n <TouchableOpacity onPress={takePhoto}>\\n <MaterialCommunityIcons name=\\"circle-slice-8\\" size={80} color=\\"white\\" />\\n </TouchableOpacity>\\n </View>\\n </AnimatedCamera>\\n </PinchGestureHandler>\\n </View >\\n )\\n}\\n\\n\\n#### Reference\\n\\n- [VisionCamera-Zooming](https://react-native-vision-camera.com/docs/guides/zooming)\\n- [The basics of PinchGestureHandler with React Native Reanimated 2](https://www.youtube.com/watch?v=R7vyLItMQJw&list=PLjHsmVtnAr9TWoMAh-3QMiP7bPUqPFuFZ&index=5)\\n","title":"Pin to Zoom Camera","date":"2023-10-09T00:00:00.000Z","id":"blog0192","tag":"react-native","intro":"Pin to zoom is very common but it is not an out-of-the-box feature for camera API in iOS. Let\'s bring it back on our own.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### A Complete EC2 with Security Group\\n\\nhcl\\nprovider \\"aws\\" {\\n region = \\"ap-northeast-1\\"\\n}\\n\\nresource \\"aws_instance\\" \\"web\\" {\\n ami = \\"ami-08a706ba5ea257141\\"\\n instance_type = \\"t2.micro\\"\\n vpc_security_group_ids = [aws_security_group.web.id]\\n user_data = file(\\"user_data.sh\\")\\n\\n tags = {\\n Name = \\"WebServer Built by Terraform\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\nresource \\"aws_default_vpc\\" \\"default\\" {}\\n\\nresource \\"aws_security_group\\" \\"web\\" {\\n name = \\"Webserver James\\"\\n description = \\"Security Group for Webserver James\\"\\n vpc_id = aws_default_vpc.default.id\\n\\n ingress {\\n description = \\"Allow port HTTP\\"\\n from_port = 80\\n to_port = 80\\n protocol = \\"tcp\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n\\n ingress {\\n description = \\"Allow port HTTPS\\"\\n from_port = 443\\n to_port = 443\\n protocol = \\"tcp\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n\\n egress {\\n from_port = 0\\n to_port = 0\\n protocol = \\"-1\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n\\n tags = {\\n Name = \\"WebServer Built by Terraform\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\n\\nHere user_data.sh is:\\n\\nbash\\n#!/bin/bash\\nyum -y update\\nyum -y install httpd\\necho \\"<h2> Hello World!</h2> <h3>by external file!</h3>\\" > /var/www/html/index.html\\nservice httpd start\\nchkconfig httpd on\\n\\n\\nIn resource \\"aws_instance\\" \\"web\\" {} we can replace user_data = file(user_data.sh) by\\n\\nhcl\\nuser_data = templatefile(\\"user_data.sh.tpl\\", {\\n first_name = \\"James\\"\\n last_name = \\"Lee\\"\\n names = [\\"John\\", \\"Angel\\", \\"David\\", \\"Victor\\", \\"Frank\\", \\"Melissa\\", \\"Kitana\\"]\\n})\\n\\n\\nwhere\\n\\nhcl\\n// user_data.sh.tpl\\n\\n#!/bin/bash\\nyum -y update\\nyum -y install httpd\\n\\ncat <<EOF > /var/www/html/index.html\\n<html>\\n<h2>Built by Power of <font color=\\"red\\">Terraform</font></h2><br>\\n\\nServer Owner is: ${first_name} ${last_name}<br>\\n\\n%{ for name in names ~}\\nHello to ${name} from ${first_name}<br>\\n%{ endfor ~}\\n\\n</html>\\nEOF\\n\\nservice httpd start\\nchkconfig httpd on\\n\\n\\n#### Dynamic Properties\\n\\nExample of a security group:\\n\\nhcl\\nresource \\"aws_security_group\\" \\"web\\" {\\n name = \\"Webserver James\\"\\n description = \\"Security Group for Webserver James\\"\\n vpc_id = aws_default_vpc.default.id\\n\\n ingress {\\n description = \\"Allow port HTTP\\"\\n from_port = 80\\n to_port = 80\\n protocol = \\"tcp\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n\\n dynamic \\"ingress\\" {\\n for_each = [\\"80\\", \\"8080\\", \\"443\\", \\"1000\\", \\"8443\\"]\\n content {\\n description = \\"Allow port HTTP\\"\\n from_port = ingress.value\\n to_port = ingress.value\\n protocol = \\"tcp\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n }\\n\\n ingress {\\n description = \\"Allow port SSH\\"\\n from_port = 22\\n to_port = 22\\n protocol = \\"tcp\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n\\n egress {\\n from_port = 0\\n to_port = 0\\n protocol = \\"-1\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n\\n\\n tags = {\\n Name = \\"WebServer Built by Terraform\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\n\\n#### Elastic IP\\n\\nhcl\\nresource \\"aws_eip\\" \\"web\\" {\\n instance = aws_instance.web.id\\n tags = {\\n Name = \\"WebServer Built by Terraform\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\nresource \\"aws_instance\\" \\"web\\" {\\n ami = \\"ami-08a706ba5ea257141\\"\\n instance_type = \\"t2.micro\\"\\n vpc_security_group_ids = [aws_security_group.web.id]\\n user_data = file(\\"user_data.sh\\")\\n\\n tags = {\\n Name = \\"WebServer Built by Terraform\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\n\\n#### Life Cycle: Create Before Destroy\\n\\nSince we have attached an elastic IP to an aws instance.\\n\\nIf we specify create_before_destroy = true, then a new instance will be created first, and our elastic IP will be transferred to new instace, this results in almost zero down time of our web server.\\n\\nhcl\\nresource \\"aws_instance\\" \\"web\\" {\\n ami = \\"ami-08a706ba5ea257141\\"\\n instance_type = \\"t2.micro\\"\\n vpc_security_group_ids = [aws_security_group.web.id]\\n user_data = file(\\"user_data.sh\\")\\n\\n tags = {\\n Name = \\"WebServer Built by Terraform\\"\\n Owner = \\"James Lee\\"\\n }\\n\\n lifecycle {\\n create_before_destroy = true\\n }\\n}\\n\\n\\n#### Implicit and Explicit Dependencies\\n\\nhcl\\nresource \\"aws_instance\\" \\"my_web_server\\" {\\n ami = \\"ami-08a706ba5ea257141\\"\\n instance_type = \\"t2.micro\\"\\n vpc_security_group_ids = [aws_security_group.general.id]\\n tags = {\\n Name = \\"Server-Web\\"\\n Owner = \\"James Lee\\"\\n }\\n depends_on = [aws_instance.my_db_server, aws_instance.my_app_server]\\n}\\n\\nresource \\"aws_instance\\" \\"my_app_server\\" {\\n ami = \\"ami-08a706ba5ea257141\\"\\n instance_type = \\"t2.micro\\"\\n vpc_security_group_ids = [aws_security_group.general.id]\\n tags = {\\n Name = \\"Server-App\\"\\n Owner = \\"James Lee\\"\\n }\\n depends_on = [aws_instance.my_db_server]\\n}\\n\\nresource \\"aws_instance\\" \\"my_db_server\\" {\\n ami = \\"ami-08a706ba5ea257141\\"\\n instance_type = \\"t2.micro\\"\\n vpc_security_group_ids = [aws_security_group.general.id]\\n tags = {\\n Name = \\"Server-Db\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\nresource \\"aws_security_group\\" \\"general\\" {\\n dynamic \\"ingress\\" {\\n for_each = [\\"80\\", \\"443\\", \\"22\\", \\"3389\\"]\\n content {\\n from_port = ingress.value\\n to_port = ingress.value\\n protocol = \\"tcp\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n }\\n\\n egress {\\n from_port = 0\\n to_port = 0\\n protocol = \\"-1\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n\\n tags = {\\n Name = \\"My Security Group\\"\\n }\\n}\\n\\n\\n#### Random Password in SSM Parameter Store\\n\\nhcl\\nresource \\"random_password\\" \\"main\\" {\\n length = 20\\n special = true\\n override_special = \\"#!()_\\"\\n}\\n\\nresource \\"aws_ssm_parameter\\" \\"rds_password\\" {\\n name = \\"/prod/prod-my-sql-rds/password\\"\\n type = \\"SecureString\\"\\n description = \\"Master Password\\"\\n value = random_password.main.result\\n\\n tags = {\\n environment = \\"production\\"\\n }\\n}\\n\\n\\nPassword retrival:\\n\\nhcl\\ndata \\"aws_ssm_parameter\\" \\"rds_password\\" {\\n name = \\"/prod/prod-my-sql-rds/password\\"\\n}\\n\\n\\nAnd we use data.aws_ssm_parameter.rds_password.value as the value in attributes.\\n\\n#### Random Password in Secrets Managers\\n\\nhcl\\nresource \\"random_password\\" \\"main\\" {\\n length = 20\\n special = true\\n override_special = \\"#!()_\\"\\n}\\n\\nresource \\"aws_secretsmanager_secret\\" \\"rds_password\\" {\\n name = \\"/prod/rds/password\\"\\n description = \\"Password for my RDS database\\"\\n recovery_window_in_days = 0 // completely delete once set up\\n}\\n\\nresource \\"aws_secretsmanager_secret_version\\" \\"rds_password\\" {\\n secret_id = aws_secretsmanager_secret.rds_password.id\\n secret_string = random_password.main.result\\n}\\n\\ndata \\"aws_secretsmanager_secret_version\\" \\"rds_password\\" {\\n secret_id = aws_secretsmanager_secret.rds_password.id\\n depends_on = [aws_secretsmanager_secret_version.rds_password]\\n}\\n\\noutput \\"random_password_rds\\" {\\n value = aws_secretsmanager_secret_version.rds_password.secret_string\\n sensitive = true\\n}\\n\\n\\n#### JSON as Environment Variable Stored in Secrets Managers\\n\\nWe additionally add a database for more data to store.\\n\\nhcl\\nresource \\"aws_db_instance\\" \\"prod\\" {\\n allocated_storage = 10\\n db_name = \\"mydb\\"\\n engine = \\"mysql\\"\\n engine_version = \\"5.7\\"\\n instance_class = \\"db.t3.micro\\"\\n username = \\"admin\\"\\n password = random_password.main.result\\n parameter_group_name = \\"default.mysql5.7\\"\\n skip_final_snapshot = true\\n}\\n\\n\\nresource \\"random_password\\" \\"main\\" {\\n length = 20\\n special = true\\n override_special = \\"#!()_\\"\\n}\\n\\nresource \\"aws_secretsmanager_secret\\" \\"rds\\" {\\n name = \\"/prod/rds/all\\"\\n description = \\"Everything about the rds database\\"\\n recovery_window_in_days = 0 // completely delete once set up\\n}\\n\\nresource \\"aws_secretsmanager_secret_version\\" \\"rds\\" {\\n secret_id = aws_secretsmanager_secret.rds.id\\n secret_string = jsonencode({\\n rds_address = aws_db_instance.prod.address\\n rds_port = aws_db_instance.prod.port\\n rds_username = aws_db_instance.prod.username\\n rds_password = random_password.main.result\\n })\\n}\\n\\ndata \\"aws_secretsmanager_secret_version\\" \\"rds\\" {\\n secret_id = aws_secretsmanager_secret.rds.id\\n depends_on = [aws_secretsmanager_secret_version.rds]\\n}\\n\\noutput \\"rds_all\\" {\\n value = jsondecode(aws_secretsmanager_secret_version.rds.secret_string)\\n sensitive = true\\n}\\n\\n\\n#### Create VPC and Subnets\\n\\nhcl\\nprovider \\"aws\\" {}\\n\\nresource \\"aws_vpc\\" \\"prod\\" {\\n cidr_block = \\"10.0.0.0/16\\"\\n tags = {\\n Name = \\"PROD\\"\\n }\\n}\\n\\nresource \\"aws_subnet\\" \\"subnet1\\" {\\n vpc_id = data.aws_vpc.prod.id\\n cidr_block = \\"10.0.1.0/24\\"\\n availability_zone = data.aws_availability_zones.working.names[0]\\n tags = {\\n Name = \\"Subnet1\\"\\n }\\n}\\n\\nresource \\"aws_subnet\\" \\"subnet2\\" {\\n vpc_id = data.aws_vpc.prod.id\\n cidr_block = \\"10.0.2.0/24\\"\\n availability_zone = data.aws_availability_zones.working.names[1]\\n tags = {\\n Name = \\"Subnet2\\",\\n Info = \\"AZ: ${data.aws_availability_zones.working.names[1]} in Region: ${data.aws_region.current.description}\\"\\n\\n }\\n}\\n\\ndata \\"aws_region\\" \\"current\\" {}\\ndata \\"aws_caller_identity\\" \\"current\\" {}\\ndata \\"aws_availability_zones\\" \\"working\\" {}\\ndata \\"aws_vpc\\" \\"prod\\" {\\n depends_on = [aws_vpc.prod]\\n tags = {\\n Name = \\"PROD\\"\\n }\\n}\\n\\n\\noutput \\"region_name\\" {\\n value = data.aws_region.current.name\\n}\\n\\noutput \\"region_description\\" {\\n value = data.aws_region.current.description\\n}\\n\\noutput \\"account_id\\" {\\n value = data.aws_caller_identity.current.account_id\\n}\\n\\noutput \\"availability_zones\\" {\\n value = data.aws_availability_zones.working.names\\n}\\n\\n\\n#### Get the AMI-id of Ubuntu/Amazon Linux by Filtering\\n\\nhcl\\nprovider \\"aws\\" {}\\n\\ndata \\"aws_ami\\" \\"latest_ubuntu20\\" {\\n owners = [\\"099720109477\\"]\\n most_recent = true\\n filter {\\n name = \\"name\\"\\n values = [\\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\\"]\\n }\\n}\\n\\ndata \\"aws_ami\\" \\"latest_amazonlinux\\" {\\n owners = [\\"137112412989\\"]\\n most_recent = true\\n filter {\\n name = \\"name\\"\\n values = [\\"al2023-ami-2023.*-kernel-6.1-x86_64\\"]\\n }\\n}\\n\\n\\noutput \\"latest_ubuntu20_ami_id\\" {\\n value = data.aws_ami.latest_ubuntu20.id\\n}\\n\\noutput \\"latest_amazonlinux_ami_id\\" {\\n value = data.aws_ami.latest_amazonlinux.id\\n}\\n\\n\\noutput:\\n\\ntext\\nlatest_amazonlinux_ami_id = \\"ami-0fd8f5842685ca887\\"\\nlatest_ubuntu20_ami_id = \\"ami-09a81b370b76de6a2\\"\\n\\n","title":"AWS Resources Instanciation in Terraform","date":"2023-10-10T00:00:00.000Z","id":"blog0193","tag":"aws, cloud, terraform","intro":"A study of Terraform basic building blocks.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### Remote State\\n\\nWe create a versioned and Encryped S3 bucket and start our main.tf by\\n\\nhcl\\nterraform {\\n backend \\"s3\\" {\\n bucket = \\"machingclee-terraform-remote-state\\"\\n key = \\"dev/network/terraform.tfstate\\"\\n region = \\"ap-northeast-1\\"\\n }\\n}\\n\\n\\nNote that if we are working with web layer, then change the key to\\n\\ntext\\n\\"dev/web/terraform.tfstate\\"\\n\\n\\n#### Network-Layer: Let\'s Create our own vpc and Public Subnet\\n\\n##### main.tf\\n\\nFirst:\\n\\n- [AWS best practice to not use the default VPC for workflows](https://aquasecurity.github.io/tfsec/v1.8.0/checks/aws/vpc/no-default-vpc/)\\n\\nThe following code actually repeat what is done in [this video](https://www.youtube.com/watch?v=TUTqYEZZUdc).\\n\\n![](/assets/tech/194/image.png)\\n\\nhcl\\nprovider \\"aws\\" {}\\n\\nterraform {\\n backend \\"s3\\" {\\n bucket = \\"machingclee-terraform-remote-state\\"\\n key = \\"dev/network/terraform.tfstate\\"\\n region = \\"ap-northeast-1\\"\\n }\\n}\\n\\nresource \\"aws_vpc\\" \\"main\\" {\\n cidr_block = var.vpc_cidr\\n tags = {\\n Name = \\"${var.env}-vpc\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\nresource \\"aws_internet_gateway\\" \\"main\\" {\\n vpc_id = aws_vpc.main.id\\n tags = {\\n Name = \\"${var.env}-igw\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\nresource \\"aws_subnet\\" \\"public_subnets\\" {\\n count = length(var.public_subnet_cidrs)\\n vpc_id = aws_vpc.main.id\\n cidr_block = element(var.public_subnet_cidrs, count.index)\\n map_public_ip_on_launch = true\\n tags = {\\n Name = \\"${var.env}-public-${count.index + 1}\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\nresource \\"aws_route_table\\" \\"public_subnet\\" {\\n vpc_id = aws_vpc.main.id\\n route {\\n cidr_block = \\"0.0.0.0/0\\"\\n gateway_id = aws_internet_gateway.main.id\\n }\\n tags = {\\n Name = \\"${var.env}-route-public-subnets\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\nresource \\"aws_route_table_association\\" \\"public_routes\\" {\\n count = length(aws_subnet.public_subnets[*].id)\\n route_table_id = aws_route_table.public_subnet.id\\n subnet_id = element(aws_subnet.public_subnets[*].id, count.index)\\n}\\n\\n\\n##### outputs.tf\\n\\nThe following outputs will be reusable by terraform file of web-layer.\\n\\nhcl\\noutput \\"vpc_id\\" {\\n value = aws_vpc.main.id\\n}\\n\\noutput \\"vpc_cidr\\" {\\n value = aws_vpc.main.cidr_block\\n}\\n\\noutput \\"public_subnsets_ids\\" {\\n value = aws_subnet.public_subnets[*].id\\n}\\n\\n\\n#### Web-Layer: Deploy an EC2 Instance on this Public Subnet\\n\\n##### main.tf\\n\\nhcl\\nprovider \\"aws\\" {}\\n\\nlocals {\\n network_outputs = data.terraform_remote_state.network.outputs\\n}\\n\\nterraform {\\n backend \\"s3\\" {\\n bucket = \\"machingclee-terraform-remote-state\\"\\n key = \\"dev/web/terraform.tfstate\\"\\n region = \\"ap-northeast-1\\"\\n }\\n}\\n\\n# fetch network remote\\ndata \\"terraform_remote_state\\" \\"network\\" {\\n backend = \\"s3\\"\\n config = {\\n bucket = \\"machingclee-terraform-remote-state\\"\\n key = \\"dev/network/terraform.tfstate\\"\\n region = \\"ap-northeast-1\\"\\n }\\n}\\n\\nresource \\"aws_instance\\" \\"web\\" {\\n ami = \\"ami-08a706ba5ea257141\\"\\n instance_type = \\"t2.micro\\"\\n subnet_id = local.network_outputs.public_subnsets_ids[0]\\n vpc_security_group_ids = [aws_security_group.web.id]\\n user_data = file(\\"user_data.sh\\")\\n\\n tags = {\\n Name = \\"WebServer Built by Terraform\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\nresource \\"aws_security_group\\" \\"web\\" {\\n name = \\"Webserver James\\"\\n description = \\"Security Group for Webserver James\\"\\n vpc_id = local.network_outputs.vpc_id\\n\\n ingress {\\n description = \\"Allow port HTTP\\"\\n from_port = 80\\n to_port = 80\\n protocol = \\"tcp\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n\\n ingress {\\n description = \\"Allow port HTTPS\\"\\n from_port = 443\\n to_port = 443\\n protocol = \\"tcp\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n\\n\\n ingress {\\n description = \\"Allow port HTTPS\\"\\n from_port = 22\\n to_port = 22\\n protocol = \\"tcp\\"\\n cidr_blocks = [local.network_outputs.vpc_cidr]\\n }\\n\\n egress {\\n from_port = 0\\n to_port = 0\\n protocol = \\"-1\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n\\n tags = {\\n Name = \\"WebServer Built by Terraform\\"\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\n","title":"Remote State for Whole Development Team; Create and Deploy on a Public Subnet of Custom VPC","date":"2023-10-15T00:00:00.000Z","id":"blog0194","tag":"aws, cloud, terraform","intro":"Let\'s split our code in multiple terraform files.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### Dynamic Field by Ternary Operator\\n\\n- (var.env == \\"prod\\") ? true : false\\n\\n- whether to create an attribute block:\\n hcl\\n dynamic \\"ebs_block_device\\" {\\n for_each = var.env == \\"prod\\" ? [true] : []\\n content {\\n device_name = \\"/dev/sdb\\"\\n volume_size = 40\\n encrypted = true\\n }\\n }\\n \\n\\nNote that lookup has the signature lookup(map, key, default_value), it is an ordinary get method of a Map object.\\n\\n- **Full Example.**\\n\\n hcl\\n resource \\"aws_instance\\" \\"my_server\\" {\\n ami = var.ami_id_per_region[data.aws_region.current.name]\\n instance_type = lookup(var.server_size, var.env, var.server_size[\\"my_default\\"])\\n vpc_security_group_ids = [aws_security_group.my_server.id]\\n\\n root_block_device {\\n volume_size = 10\\n encrypted = (var.env == \\"prod\\") ? true : false\\n }\\n\\n dynamic \\"ebs_block_device\\" {\\n for_each = var.env == \\"prod\\" ? [true] : []\\n content {\\n device_name = \\"/dev/sdb\\"\\n volume_size = 40\\n encrypted = true\\n }\\n }\\n\\n volume_tags = { Name = \\"Disk-${var.env}\\" }\\n tags = { Name = \\"Server-${var.env}\\" }\\n }\\n \\n\\n#### Conditionally Create a Resource by Count\\n\\nNote that the block below can be equivalently created by looping a set:\\n\\nhcl\\nresource \\"aws_instance\\" \\"bastion_server\\" {\\n count = var.create_bastion == true ? 1 : 0\\n ami = \\"ami-0e472933a1395e172\\"\\n instance_type = \\"t3.micro\\"\\n tags = {\\n Name = \\"Bastion Server\\"\\n Owner = \\"Denis Astahov\\"\\n }\\n}\\n\\n\\n#### Create Multiple Instances by Looping a Set\\n\\nhcl\\nresource \\"aws_instance\\" \\"my_server\\" {\\n for_each = toset([\\"Dev\\", \\"Staging\\", \\"Prod\\"])\\n ami = \\"ami-0e472933a1395e172\\"\\n instance_type = \\"t3.micro\\"\\n tags = {\\n Name = \\"Server-${each.value}\\"\\n Owner = \\"Denis Astahov\\"\\n }\\n}\\n\\n\\n#### Create Multiple Instances by Looping a Map of Maps\\n\\nhcl\\n// variables.tf\\nvariable \\"servers_settings\\" {\\n type = map(any)\\n default = {\\n web = {\\n ami = \\"ami-0e472933a1395e172\\"\\n instance_size = \\"t3.small\\"\\n root_disksize = 20\\n encrypted = true\\n }\\n app = {\\n ami = \\"ami-07dd19a7900a1f049\\"\\n instance_size = \\"t3.micro\\"\\n root_disksize = 10\\n encrypted = false\\n }\\n }\\n}\\n\\n\\nhcl\\n// main.tf\\nresource \\"aws_instance\\" \\"server\\" {\\n for_each = var.servers_settings\\n ami = each.value[\\"ami\\"]\\n instance_type = each.value[\\"instance_size\\"]\\n\\n root_block_device {\\n volume_size = each.value[\\"root_disksize\\"]\\n encrypted = each.value[\\"encrypted\\"]\\n }\\n\\n volume_tags = {\\n Name = \\"Disk-${each.key}\\"\\n }\\n tags = {\\n Name = \\"Server-${each.key}\\"\\n Owner = \\"Denis Astahov\\"\\n }\\n}\\n\\n","title":"Conditions and Loops","date":"2023-10-14T00:00:00.000Z","id":"blog0195","tag":"aws, cloud, terraform","intro":"Standard technique to create multiple resources by conditions and loops.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### The variables.tf\\n\\nhcl\\nvariable \\"aws_region\\" {\\n description = \\"Region where you want to provision\\"\\n type = string //number, bool\\n default = \\"ap-northeast-1\\"\\n}\\n\\nvariable \\"port_list\\" {\\n description = \\"List of port to open for your webserver\\"\\n type = list(any)\\n default = [\\"80\\", \\"443\\"]\\n}\\n\\nvariable \\"instance_type\\" {\\n description = \\"EC2 Instance size to provision\\"\\n type = string\\n default = \\"t3.micro\\"\\n}\\n\\nvariable \\"tags\\" {\\n description = \\"Tags to apply to resources\\"\\n type = map(any)\\n default = {\\n Owner = \\"James Lee\\"\\n Environment = \\"Prod\\"\\n Project = \\"Pheonix\\"\\n }\\n}\\n\\noutput \\"latest_ubuntu20_ami_id\\" {\\n value = data.aws_ami.latest_ubuntu20.id\\n}\\n\\noutput \\"latest_amazonlinux_ami_id\\" {\\n value = data.aws_ami.latest_amazonlinux.id\\n}\\n\\n\\n#### The main.tf Using variables.tf\\n\\nhcl\\nprovider \\"aws\\" { region = var.aws_region }\\n\\nresource \\"aws_security_group\\" \\"web\\" {\\n name = \\"${var.tags[\\"Environment\\"]} WebServer-SG\\"\\n description = \\"Security Group for WebServer\\"\\n dynamic \\"ingress\\" {\\n for_each = var.port_list\\n content {\\n from_port = ingress.value\\n to_port = ingress.value\\n protocol = \\"tcp\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n }\\n egress {\\n description = \\"Allow All Ports\\"\\n from_port = 0\\n to_port = 0\\n protocol = \\"-1\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n tags = merge(var.tags, { Name = \\"${var.tags[\\"Environment\\"]} WebServer SG by Terraform\\" })\\n}\\n\\nresource \\"aws_instance\\" \\"web\\" {\\n ami = data.aws_ami.latest_amazonlinux.id\\n instance_type = var.instance_type\\n vpc_security_group_ids = [aws_security_group.web.id]\\n tags = merge(var.tags, { Name = \\"${var.tags[\\"Environment\\"]} WebServer Built by Terraform\\" })\\n}\\n\\nresource \\"aws_eip\\" \\"web\\" {\\n instance = aws_instance.web.id\\n tags = merge(var.tags, { Name = \\"${var.tags[\\"Environment\\"]} Elastic IP by Terraform\\" })\\n}\\n\\ndata \\"aws_ami\\" \\"latest_ubuntu20\\" {\\n owners = [\\"099720109477\\"]\\n most_recent = true\\n filter {\\n name = \\"name\\"\\n values = [\\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\\"]\\n }\\n}\\n\\ndata \\"aws_ami\\" \\"latest_amazonlinux\\" {\\n owners = [\\"137112412989\\"]\\n most_recent = true\\n filter {\\n name = \\"name\\"\\n values = [\\"al2023-ami-2023.*-kernel-6.1-x86_64\\"]\\n }\\n}\\n\\n\\n#### The \\\\*.tfvars\\n\\nThis provides the default values to variables.tf in a separate file:\\n\\n##### New variables.tf\\n\\nhcl\\nvariable \\"aws_region\\" {\\n description = \\"Region where you want to provision\\"\\n type = string //number, bool\\n}\\n\\nvariable \\"port_list\\" {\\n description = \\"List of port to open for your webserver\\"\\n type = list(any)\\n}\\n\\nvariable \\"instance_type\\" {\\n description = \\"EC2 Instance size to provision\\"\\n type = string\\n}\\n\\nvariable \\"tags\\" {\\n description = \\"Tags to apply to resources\\"\\n type = map(any)\\n}\\n\\n\\n##### terraform.tfvars\\n\\nhcl\\naws_region = \\"ap-northeast-1\\"\\nport_list = [\\"80\\", \\"443\\"]\\ninstance_type = \\"t3.micro\\"\\ntags = {\\n Owner = \\"James Lee\\"\\n Environment = \\"Prod\\"\\n Project = \\"Pheonix\\"\\n}\\n\\n\\n##### prod.tfvars\\n\\nLet\'s rename terraform.tfvars to prod.tfvars and terraform apply again, the *.tfvars does not take effect. In that case, we try:\\n\\ntext\\nterraform apply -var-file=prod.tfvars\\n\\n\\n#### Local Variables\\n\\nIn main.tf we can write\\n\\nhcl\\nlocals {\\n X = 1\\n Y = 2\\n}\\n\\nlocals {\\n amazonlinux_ami = data.aws_ami.latest_amazonlinux.id\\n ubuntu_ami = data.aws_ami.latest_ubuntu20.id\\n Z = \\"${local.X} and ${local.Y}\\"\\n some_tags = {\\n Owner = \\"James Lee\\"\\n }\\n}\\n\\nresource \\"aws_instance\\" \\"web\\" {\\n ami = local.amazonlinux_ami\\n instance_type = var.instance_type\\n vpc_security_group_ids = [aws_security_group.web.id]\\n tags = merge(var.tags, local.some_tags, { Name = \\"${var.tags[\\"Environment\\"]} WebServer Built by Terraform\\" })\\n}\\n\\ndata \\"aws_ami\\" \\"latest_ubuntu20\\" {\\n owners = [\\"099720109477\\"]\\n most_recent = true\\n filter {\\n name = \\"name\\"\\n values = [\\"ubuntu/images/hvm-ssd/ubuntu-jammy-22.04-amd64-server-*\\"]\\n }\\n}\\n\\ndata \\"aws_ami\\" \\"latest_amazonlinux\\" {\\n owners = [\\"137112412989\\"]\\n most_recent = true\\n filter {\\n name = \\"name\\"\\n values = [\\"al2023-ami-2023.*-kernel-6.1-x86_64\\"]\\n }\\n}\\n\\n...\\n\\n\\n#### Remote Execution\\n\\nAfter an EC2 instance is created, we arrange shell scripts to be executed as subsequent tasks (for example, we may want to install gitlab runner to perform CICD task).\\n\\nhcl\\nprovider \\"aws\\" {\\n region = \\"ca-central-1\\"\\n}\\n\\nresource \\"aws_default_vpc\\" \\"default\\" {} # This need to be added since AWS Provider v4.29+ to get VPC id\\n\\nresource \\"aws_instance\\" \\"myserver\\" {\\n ami = \\"ami-0c9bfc21ac5bf10eb\\"\\n instance_type = \\"t3.nano\\"\\n vpc_security_group_ids = [aws_security_group.web.id]\\n key_name = \\"denis-key-ca-central-1\\"\\n tags = {\\n Name = \\"My EC2 with remote-exec\\"\\n Owner = \\"Denis Astahov\\"\\n }\\n\\n provisioner \\"remote-exec\\" {\\n inline = [\\n \\"mkdir /home/ec2-user/terraform\\",\\n \\"cd /home/ec2-user/terraform\\",\\n \\"touch hello.txt\\",\\n \\"echo \'Terraform was here...\' > terraform.txt\\"\\n ]\\n connection {\\n type = \\"ssh\\"\\n user = \\"ec2-user\\"\\n host = self.public_ip //Same as: aws_instance.myserver.public_ip\\n private_key = file(\\"denis-key-ca-central-1.pem\\")\\n }\\n }\\n}\\n\\n\\nresource \\"aws_security_group\\" \\"web\\" {\\n name = \\"My-SecurityGroup\\"\\n vpc_id = aws_default_vpc.default.id # This need to be added since AWS Provider v4.29+ to set VPC id\\n ingress {\\n from_port = 22\\n to_port = 22\\n protocol = \\"tcp\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n egress {\\n description = \\"Allow ALL ports\\"\\n from_port = 0\\n to_port = 0\\n protocol = \\"-1\\"\\n cidr_blocks = [\\"0.0.0.0/0\\"]\\n }\\n tags = {\\n Name = \\"SG by Terraform\\"\\n Owner = \\"Denis Astahov\\"\\n }\\n}\\n\\n","title":"Variables and Remote Execution","date":"2023-10-13T00:00:00.000Z","id":"blog0196","tag":"aws, cloud, terraform","intro":"We discuss how to manipulate variables (dev, prod, etc...) and plan a small section on remote execution of shell scripts once an instance is launched.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### Define a Custom Modules\\n\\n- Let\'s exclude provider in main.tf\\n\\n- Create a main.tf that creates public subnet, private subnet and corresponding internget-gateway and NAT.\\n\\nhcl\\n// main.tf\\n\\ndata \\"aws_availability_zones\\" \\"available\\" {}\\n\\nresource \\"aws_vpc\\" \\"main\\" {\\n cidr_block = var.vpc_cidr\\n tags = merge(var.tags, { Name = \\"${var.env}-vpc\\" })\\n}\\n\\n\\nresource \\"aws_internet_gateway\\" \\"main\\" {\\n vpc_id = aws_vpc.main.id\\n tags = merge(var.tags, { Name = \\"${var.env}-igw\\" })\\n}\\n\\nresource \\"aws_subnet\\" \\"public_subnets\\" {\\n count = length(var.public_subnet_cidrs)\\n vpc_id = aws_vpc.main.id\\n cidr_block = element(var.public_subnet_cidrs, count.index)\\n availability_zone = data.aws_availability_zones.available.names[count.index]\\n map_public_ip_on_launch = true\\n tags = merge(var.tags, { Name = \\"${var.env}-public-${count.index + 1}\\" })\\n}\\n\\n// create routable + create route for the gateway + attach gateway to the vpc\\nresource \\"aws_route_table\\" \\"public_subnets\\" {\\n vpc_id = aws_vpc.main.id\\n route {\\n cidr_block = \\"0.0.0.0/0\\"\\n gateway_id = aws_internet_gateway.main.id\\n }\\n tags = merge(var.tags, { Name = \\"${var.env}-route-public-subnets\\" })\\n}\\n\\n// config target subnet to use our route table\\nresource \\"aws_route_table_association\\" \\"public_routes\\" {\\n count = length(aws_subnet.public_subnets[*].id)\\n route_table_id = aws_route_table.public_subnets.id\\n subnet_id = aws_subnet.public_subnets[count.index].id\\n}\\n\\nresource \\"aws_eip\\" \\"nat\\" {\\n count = length(var.private_subnet_cidrs)\\n domain = \\"vpc\\"\\n tags = merge(var.tags, { Name = \\"${var.env}-nat-gw-${count.index + 1}\\" })\\n}\\n\\n\\nresource \\"aws_nat_gateway\\" \\"nat\\" {\\n count = length(var.private_subnet_cidrs)\\n allocation_id = aws_eip.nat[count.index].id\\n subnet_id = aws_subnet.public_subnets[count.index].id\\n tags = merge(var.tags, { Name = \\"${var.env}-nat-gw-${count.index + 1}\\" })\\n}\\n\\n# ===== Private Subnets and Routing =====\\n\\nresource \\"aws_subnet\\" \\"private_subnets\\" {\\n count = length(var.private_subnet_cidrs)\\n vpc_id = aws_vpc.main.id\\n cidr_block = var.private_subnet_cidrs[count.index]\\n availability_zone = data.aws_availability_zones.available.names[count.index]\\n tags = merge(var.tags, { Name = \\"${var.env}-private-${count.index + 1}\\" })\\n}\\n\\n\\nresource \\"aws_route_table\\" \\"private_subnets\\" {\\n count = length(var.private_subnet_cidrs)\\n vpc_id = aws_vpc.main.id\\n route {\\n cidr_block = \\"0.0.0.0/0\\"\\n nat_gateway_id = aws_nat_gateway.nat[count.index].id\\n }\\n tags = merge(var.tags, { Name = \\"${var.env}-route-private-subnet-${count.index + 1}\\" })\\n}\\n\\n\\nresource \\"aws_route_table_association\\" \\"private_routes\\" {\\n count = length(aws_subnet.private_subnets[*].id)\\n route_table_id = aws_route_table.private_subnets[count.index].id\\n subnet_id = aws_subnet.private_subnets[count.index].id\\n}\\n\\n\\nhcl\\n// variables.tf\\n\\nvariable \\"env\\" {\\n default = \\"dev\\"\\n}\\n\\nvariable \\"vpc_cidr\\" {\\n default = \\"10.0.0.0/16\\"\\n}\\n\\nvariable \\"public_subnet_cidrs\\" {\\n default = [\\n \\"10.0.1.0/24\\",\\n \\"10.0.2.0/24\\",\\n ]\\n}\\n\\nvariable \\"private_subnet_cidrs\\" {\\n default = [\\n \\"10.0.11.0/24\\",\\n \\"10.0.22.0/24\\",\\n ]\\n}\\n\\nvariable \\"tags\\" {\\n default = {\\n Owner = \\"James Lee\\"\\n Project = \\"Terraform Experiment\\"\\n }\\n}\\n\\n\\n\\nhcl\\n// outputs.tf\\n\\noutput \\"vpc_id\\" {\\n value = aws_vpc.main.id\\n}\\n\\noutput \\"vpc_cidr\\" {\\n value = aws_vpc.main.cidr_block\\n}\\n\\noutput \\"public_subnet_ids\\" {\\n value = aws_subnet.public_subnets[*].id\\n}\\n\\noutput \\"private_subnet_ids\\" {\\n value = aws_subnet.private_subnets[*].id\\n}\\n\\n\\n#### Apply the Modules\\n\\nLet move the 3 files into module/network/ directory, then create a new main.tf in the root directory:\\n\\nhcl\\n// main.tf\\nprovider \\"aws\\" {}\\n\\nmodule \\"my_vpc_default\\" {\\n source = \\"../modules/aws_network\\"\\n}\\n\\nmodule \\"my_vpc_staging\\" {\\n source = \\"../modules/aws_network\\"\\n env = \\"staging\\"\\n vpc_cidr = \\"10.100.0.0/16\\"\\n}\\n\\n\\nHere we have demonstrated how to apply custom variable (the keys are the ones defined in variables.tf).\\n\\nApart from the inputs (the var\'s), we also have output:\\n\\nhcl\\n// outputs.tf\\n\\noutput \\"my_vpc_id\\" {\\n value = module.my_vpc_default.vpc_id\\n}\\n\\noutput \\"my_vpc_cidr\\" {\\n value = module.my_vpc_default.vpc_cidr\\n}\\n\\noutput \\"my_public_subnet_ids\\" {\\n value = module.my_vpc_default.public_subnet_ids\\n}\\n\\noutput \\"my_private_subnet_ids\\" {\\n value = module.my_vpc_default.private_subnet_ids\\n}\\n\\n\\n#### Predefine Modules in the Internet\\n\\nModules make terraform code reusable, we can search in\\n\\n- [AWS Modules from Terraform Registry](https://registry.terraform.io/search/modules?namespace=terraform-aws-modules)\\n\\nfor useful modules that have been made by others.\\n","title":"Terraform Modules","date":"2023-10-16T00:00:00.000Z","id":"blog0197","tag":"aws, cloud, terraform","intro":"How to make a modules and finally, we can apply pre-made modules from others!","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### The react-native-background-timer\\n\\nWe install by:\\n\\ntext\\nyarn add react-native-background-timer @types/react-native-background-timer\\n\\n\\n#### Use Case: Check Idling\\n\\nIn my app I have a third party voice-conferencing service called AgoraRTC which **_is not free_**. I want to kick user off the service if he/she is not actively using our app. Our criterion is: our app is in background state for 1 minute.\\n\\n##### AppState Investigation\\n\\nWe can invertigate the changes in appState by using\\n\\njs\\nuseEffect(() => {\\n const subscription = AppState.addEventListener(\\"change\\", (nextAppState) => {\\n console.log(nextAppState);\\n });\\n});\\n\\n\\nBehaviour:\\n\\n- **Switch to Another app.** appState: active > inactive > background\\n- **Lock Screen.** appState: active > inactive > ...\\n\\nBy using setInterval we can check \\"lock screen\\" up to inactive state, and our interval stop working once we turn our screen off.\\n\\n**_Alternatively_**, by using\\n\\njs\\nimport BackgroundTimer from \\"react-native-background-timer\\";\\n\\nuseEffect(() => {\\n BackgroundTimer.setInterval(() => {\\n console.log(appStateRef.current);\\n });\\n});\\n\\nuseEffect(() => {\\n const subscription = AppState.addEventListener(\\"change\\", (nextAppState) => {\\n appStateRef.current = nextAppState;\\n });\\n});\\n\\n\\nour interval can continue running in the background, and we come to the conclusion that\\n\\n- **Lock Screen.** appState: active > inactive > background\\n\\nWe therefore come up with the following setTimeout to trigger inactiveAction (which kicks users out of the service).\\n\\n##### Finalize to a Hook\\n\\njs\\nimport { useEffect, useRef } from \'react\';\\nimport { AppState, AppStateStatus } from \'react-native\';\\nimport BackgroundTimer from \'react-native-background-timer\';\\n\\nexport default ({\\n inactiveAction = () => { },\\n inactiveTimeout = 1000 * 60\\n}: {\\n inactiveAction?: () => void,\\n inactiveTimeout?: number\\n}) => {\\n const appStateRef = useRef<AppStateStatus | null>(null);\\n const inactiveTimeoutRef = useRef<ReturnType<typeof BackgroundTimer.setTimeout> | null>(null);\\n const timeoutIsSet = useRef(false);\\n\\n const startInactiveTimeout = () => {\\n inactiveTimeoutRef.current = BackgroundTimer.setTimeout(() => {\\n if (appStateRef.current === \\"inactive\\" || appStateRef.current === \\"background\\") {\\n inactiveAction();\\n if (inactiveTimeoutRef.current) {\\n timeoutIsSet.current = false;\\n BackgroundTimer.clearTimeout(inactiveTimeoutRef.current);\\n }\\n }\\n }, inactiveTimeout);\\n }\\n\\n useEffect(() => {\\n const subscription = AppState.addEventListener(\\"change\\", nextAppState => {\\n appStateRef.current = nextAppState;\\n\\n if (nextAppState === \\"inactive\\" && !timeoutIsSet.current) {\\n startInactiveTimeout();\\n timeoutIsSet.current = true;\\n }\\n\\n if (nextAppState === \\"active\\") {\\n if (inactiveTimeoutRef.current) {\\n timeoutIsSet.current = false;\\n BackgroundTimer.clearInterval(inactiveTimeoutRef.current);\\n }\\n }\\n });\\n\\n return () => {\\n subscription.remove();\\n }\\n }, []);\\n\\n useEffect(() => {\\n return () => {\\n BackgroundTimer.stopBackgroundTimer();\\n }\\n }, []);\\n}\\n\\n\\n##### Usage\\n\\nInside a component:\\n\\njs\\nconst completeQuitRoom = () => {\\n removeAgora();\\n};\\n\\nuseAppInactiveChecker({\\n inactiveAction: completeQuitRoom,\\n inactiveTimeout: 1000 * 60,\\n});\\n\\n","title":"Long Running Task in Background for Mobile App","date":"2023-10-17T00:00:00.000Z","id":"blog0198","tag":"react-native","intro":"We discuss How to keep running tasks even when our app is in background state (including screen being locked.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### Convention\\n\\n- All column and row indexes are designed to **_start with 1_**.\\n- If we see cell_pos: Tuple[int, int], that means (col_index, row_index).\\n\\n#### An ExcelCellUtil Class\\n\\n##### config.py File in the Same Directory\\n\\npython\\nIMG_SEPARATION = 40\\nCOLS_ALPHABET = \\"-ABCDEFGHIJKLMNOPQRSTUVWXYZ\\"\\n\\n\\n##### Imports\\n\\npython\\nfrom openpyxl import Workbook\\nfrom openpyxl.worksheet.worksheet import Worksheet\\nfrom PIL import Image\\nfrom openpyxl.drawing.image import Image as XLImage\\nfrom src import config\\nimport requests\\nfrom openpyxl.utils.units import pixels_to_EMU\\nfrom openpyxl.drawing.xdr import XDRPositiveSize2D\\nfrom openpyxl.drawing.spreadsheet_drawing import AnchorMarker, OneCellAnchor\\nfrom openpyxl.styles import PatternFill, Alignment, Font\\nfrom typing import TypedDict, Tuple, List, Optional, cast, Union, Literal\\nfrom uuid import uuid4\\nfrom src.wb_scripts.llm_excel_report.dto import Issue, ReportGenerationDetail, Summary\\nfrom src.wb_scripts.llm_excel_report.config import COLS_ALPHABET, IMG_SEPARATION\\nfrom datetime import datetime, tzinfo\\nimport os\\nimport math\\n\\n\\nclass ExcelCellUtil:\\n\\n\\n##### Set Fonts\\n\\npython\\n def set_default_font(self, ws: Worksheet, default_font=font_calibri):\\n for row in ws.iter_rows():\\n for cell in row:\\n cell.font = font_calibri\\n\\n\\n##### Auto-Height a row to fit Cell Contents\\n\\npython\\n def set_fit_text(self, ws, from_row: int, to_row: int):\\n row_range = range(from_row, to_row)\\n\\n for i, row in enumerate(ws.iter_rows()):\\n if i not in row_range:\\n continue\\n ws.row_dimensions[i].height = None\\n\\n\\n##### Set a offset of an Image Relative to the Upper-left Corner of a Cell\\n\\npython\\n def offset_img(self, xl_img: XLImage, cell_pos: Tuple[int, int], x_offset: int = 10, y_offset: int = 10):\\n \\"\\"\\"\\n both col and row starts from 1\\n \\"\\"\\"\\n col = cell_pos[0]\\n row = cell_pos[1]\\n p2e = pixels_to_EMU\\n h, w = xl_img.height, xl_img.width\\n size = XDRPositiveSize2D(p2e(w), p2e(h))\\n marker = AnchorMarker(\\n col=col-1,\\n row=row-1,\\n colOff=pixels_to_EMU(x_offset),\\n rowOff=pixels_to_EMU(y_offset)\\n )\\n xl_img.anchor = OneCellAnchor(_from=marker, ext=size)\\n\\n\\n- This approch is **_not I/O efficient_**, but this python script is going to be deployed on **_Lambda Service_**, efficiency is less of importance to getting the job done.\\n\\n##### Fill Cell Color\\n\\npython\\n def fill_cell_color(self, ws: Worksheet, cell_pos: Tuple[int, int], color: str):\\n ws.cell(column=cell_pos[0], row=cell_pos[1]).fill = PatternFill(\\n \\"solid\\",\\n start_color=color\\n )\\n\\n\\n##### Merge Cell\\n\\npython\\n def merge_cell(self, ws, start: Tuple[int, int], end: Tuple[int, int]):\\n start_x = start[0]\\n start_y = start[1]\\n end_x = end[0]\\n end_y = end[1]\\n ws.merge_cells(start_row=start_y,\\n start_column=start_x,\\n end_row=end_y,\\n end_column=end_x)\\n\\n\\n##### Insert Text into a Cell\\n\\npython\\n XAlignment = Literal[\\"center\\",\\"centerContinuous\\",\\"general\\",\\"distributed\\",\\"left\\",\\"right\\",\\"fill\\",\\"justify\\"]\\n YAlignment = Literal[\\"bottom\\", \\"center\\", \\"top\\", \\"distributed\\", \\"justify\\"]\\n\\n def insert_text(self, ws: Worksheet, pos: Tuple[int, int], text: str, horizontal: XAlignment=\\"left\\", vertical: YAlignment=\\"top\\" ):\\n col = pos[0]\\n row = pos[1]\\n ws.cell(column=col, row=row).alignment = Alignment(wrap_text=True, horizontal=horizontal,vertical=vertical)\\n ws.cell(column=col, row=row).value = text\\n\\n\\n##### Resize an Image\\n\\npython\\n def resize_image(self, img: Image.Image, new_width: int):\\n new_height = img.height * new_width/img.width\\n return img.resize((int(new_width), int(new_height)))\\n\\n\\n- Note that we use PIL.Image.Image, we will need to save this image to disk and load from openpyxl.\\n- This approach is not unnecessary, because we need a **_true resize_** (pillow to reduce file size) and a **_faked resize_** (openpyxl for high resolution) when user zooms in.\\n\\n##### Calculate the max Height among a set of Images\\n\\npython\\n def max_height_from_imgs(self, imgs: List[Image.Image]):\\n return max(map(lambda x: x.height, imgs))\\n\\n\\nThis is to get ready for inserting images into a single cell.\\n\\n##### Control the Width and Height of a Cell\\n\\npython\\n def set_cell_width_height(self, ws: Worksheet, cell_pos: Tuple[int, int], width: Optional[float] = None, height: Optional[float] = None):\\n col = cell_pos[0]\\n row = cell_pos[1]\\n if width is not None:\\n ws.column_dimensions[COLS_ALPHABET[col]].width = width / 7\\n if height is not None:\\n ws.row_dimensions[row].height = height * 3 / 4\\n\\n\\n##### Insert Multiple Images into a Cell\\n\\npython\\n def insert_imgs(self,\\n ws: Worksheet,\\n cell_pos: Tuple[int, int],\\n image_width: int = 1,\\n img_separation: int = IMG_SEPARATION,\\n top_y_offset: int = 20,\\n imgs: List[Image.Image] = []):\\n new_imgs = []\\n for img in imgs:\\n img = self.resize_image(img, new_width=image_width)\\n new_imgs.append(img)\\n\\n imgs = new_imgs\\n\\n xl_imgs = []\\n img_tmp_paths = []\\n\\n for img in imgs:\\n img_tmp_path = \\"/tmp/excels/\\" + str(uuid4()) + \\".png\\"\\n img_tmp_paths.append(img_tmp_path)\\n img.save(img_tmp_path)\\n xl_img = XLImage(img_tmp_path)\\n new_width = 100\\n new_height = xl_img.height * (100/xl_img.width)\\n xl_img.width = new_width\\n xl_img.height = new_height\\n xl_imgs.append(xl_img)\\n\\n max_imgs_per_row = config.MAX_IMGS_PER_ROW\\n n_rows = math.floor(len(imgs)/max_imgs_per_row) + 1\\n\\n max_img_height = self.max_height_from_imgs(xl_imgs)\\n\\n self.set_cell_width_height(\\n ws,\\n cell_pos,\\n config.MAX_IMGS_PER_ROW * (100 + img_separation) + img_separation,\\n (max_img_height + img_separation) * n_rows\\n )\\n\\n for index, xl_img in enumerate(xl_imgs):\\n row_index = math.floor(index / max_imgs_per_row)\\n self.offset_img(xl_img,\\n cell_pos,\\n x_offset=img_separation + (index % config.MAX_IMGS_PER_ROW) * (100 + img_separation),\\n y_offset=top_y_offset + (max_img_height + img_separation) * row_index)\\n\\n if not os.path.exists(\\"/tmp/excels/\\"):\\n os.makedirs(\\"/tmp/excels/\\")\\n\\n ws.add_image(xl_img)\\n\\n return img_tmp_paths\\n\\n\\nWe also provide the temporary file paths in order to delete them after the file generation is completed.\\n\\n##### Insert Single Image\\n\\npython\\n def insert_img(self,\\n ws: Worksheet,\\n cell_pos: Tuple[int, int],\\n image_width: int,\\n offsets: Tuple[int, int],\\n img: Image):\\n img = self.resize_image(img, image_width)\\n\\n if not os.path.exists(\\"/tmp/excels\\"):\\n os.makedirs(\\"/tmp/excels\\")\\n\\n img_tmp_path = \\"/tmp/excels/\\" + str(uuid4()) + \\".png\\"\\n img.save(img_tmp_path)\\n xl_img = XLImage(img_tmp_path)\\n\\n self.offset_img(\\n xl_img, cell_pos, x_offset=offsets[0], y_offset=offsets[1]\\n )\\n if not os.path.exists(\\"/tmp/excels/\\"):\\n os.makedirs(\\"/tmp/excels/\\")\\n\\n ws.add_image(xl_img)\\n\\n return img_tmp_path\\n\\n\\n##### Align text to center\\n\\npython\\n def align_center(self, ws, cell_pos: Tuple[int, int]):\\n ws.cell(column=cell_pos[0], row=cell_pos[1]).alignment = Alignment(\\n horizontal=\\"center\\", vertical=\\"center\\")\\n\\n\\n##### Remarks\\n\\nset_default_font and set_fit_text (auto-height) should be executed right before saving the workbook. New data insertation may make the font-assertion and auto-height fail.\\n\\n#### Reason why we need PIL.Image instead of openpyxl.drawing.image.Image\\n\\n##### Strategy to Handle Images in Our ExcelCellUtil\\n\\nWhy we need both? Because normal users (especially to those who have machine learning background using pytorch) are more familiar with APIs in Pillow. We can manipulate images, save that image to disk, and load the image by the api of XLImage.\\n\\n##### Load Image from Internet by PIL.Image\\n\\nopenpyxl does not provide api to load images from file_uri over the internet (i.e., it is designed to load local disk file only). To overcome this limitation, we use PIL.Image (the result will be of type PIL.Image.Image):\\n\\npython\\nimg: PIL.Image.Image = PIL.Image.open(requests.get(img_url, stream=True).raw)\\n\\n\\nThis is why we design the signature to use PIL.Image instead of openpyxl\'s one:\\n\\npython\\nExcelCellUtil.insert_imgs(ws: Worksheet,\\n cell_pos: Tuple[int, int],\\n image_width: int = 1,\\n img_separation: int = IMG_SEPARATION,\\n top_y_offset: int = 20,\\n imgs: List[Image.Image] = [])\\n\\n\\n#### Usage\\n\\npython\\nwb = Workbook()\\nws = cast(Worksheet, wb.active)\\n\\nutil = ExcelCellUtil()\\ntmp_img_paths = []\\n\\nutil.merge_cell(ws, (1, 1), (2, 7))\\nutil.insert_text(ws, (3, 1), \\"Session Name\\")\\n...\\nwb.save(\\"some_where.xlsx\\")\\n\\n","title":"Generate Excel by Openpyxl","date":"2023-10-20T00:00:00.000Z","id":"blog0199","tag":"python, openpyxl","intro":"We implement a reusable utility class that handle most of the jobs in creating excel programmatically.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### The Package\\n\\nThis package is already in typescript, let\'s\\n\\ntext\\nyarn add @gorhom/bottom-sheet\\n\\n\\n#### Wrap our Stack Element in Root Level \\\\_layout.tsx\\n\\njs\\nimport { BottomSheetModalProvider } from \'@gorhom/bottom-sheet\';\\n\\nfunction RootLayoutNav() {\\n ...\\n return (\\n <BottomSheetModalProvider>\\n ...\\n </BottomSheetModalProvider>\\n )\\n}\\n\\n\\n#### Our own Wrapper\\n\\njs\\nimport { BottomSheetBackdrop, BottomSheetBackdropProps, BottomSheetModal } from \\"@gorhom/bottom-sheet\\"\\nimport { ReactNode, useCallback, useMemo, useRef, useState } from \\"react\\"\\n\\nexport type WbBottomSheetProps = {\\n index?: number\\n}\\n\\nclass WbBottomSheet {\\n public static close = () => { }\\n public static open = () => { }\\n public static setContent = (content: ReactNode) => { };\\n\\n public static instance = (props: WbBottomSheetProps) => {\\n const [content, setContent] = useState<ReactNode>(null);\\n const { index = 1, } = props;\\n const snapPoints = useMemo(() => [\'50%\', \'75%\', \\"100%\\"], []);\\n const modalref = useRef<BottomSheetModal>(null);\\n const open = () => {\\n modalref.current?.present();\\n }\\n const close = () => {\\n modalref.current?.dismiss();\\n }\\n this.open = open;\\n this.close = close;\\n this.setContent = setContent\\n\\n const renderBackdrop = useCallback(\\n (props: BottomSheetBackdropProps) => (\\n <>\\n <BottomSheetBackdrop\\n {...props}\\n disappearsOnIndex={-1}\\n appearsOnIndex={1}\\n />\\n </>\\n ), []\\n );\\n\\n return (\\n <BottomSheetModal\\n backgroundStyle={{ backgroundColor: \\"rgb(255,255,255)\\", }}\\n containerStyle={{ backgroundColor: \\"transparent\\", borderRadius: 20 }}\\n handleStyle={{ backgroundColor: \\"transparent\\" }}\\n style={{ borderRadius: 20 }}\\n enablePanDownToClose={true}\\n backdropComponent={renderBackdrop}\\n ref={modalref}\\n index={index}\\n snapPoints={snapPoints}\\n >\\n {content}\\n </BottomSheetModal>\\n )\\n }\\n}\\n\\nexport default WbBottomSheet\\n\\n","title":"@gorhom/bottom-sheet","date":"2023-10-21T00:00:00.000Z","id":"blog0200","tag":"react-native","intro":"Though we are able to create a bottom-sheet on our own, we may as well use existing stable ones from others!","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### Stream-Reponse Result:\\n\\n<center>\\n <video controls width=\\"400\\">\\n <source src=\\"/assets/tech/201/001.mov\\" type=\\"video/mp4\\">\\n Sorry, your browser doesn\'t support embedded videos.\\n </video>\\n</center>\\n\\n#### Usages\\n\\n##### Frontend\\n\\nIn my app when I leave a room, I will send an SSE event to the backend, trigger excel file geneation of the room and get the status from event stream:\\n\\njs\\nconst leave = () => {\\n dispatch(appSlice.actions.closeAppDialog());\\n setTimeout(() => {\\n router.replace(\\"/(billie)/\\");\\n\\n SSE.createSSE({\\n eventSource: apiRoutes.GET_SSE_EXCEL_STATUS(roomOid),\\n token: token,\\n subscriptions: [\\n {\\n key: \\"EXCEL_STATUS\\",\\n action: (data: string) => {\\n dispatch(\\n chatSlice.actions.setRoomExcelStatusOnLeave({\\n roomOid,\\n status: data,\\n })\\n );\\n },\\n },\\n ],\\n endEvent: {\\n key: \\"EXCEL_STATUS_END\\",\\n }});\\n }, 300);\\n};\\n\\n\\nWhen the backend sends an endEventKey, we will close our sse instance.\\n\\n##### Backend\\n\\nSuppose that I have an SSE GET route that has the following controller:\\n\\njs\\nexport const getExcel = async (req: Request, res: Response) => {\\n const excelStatusChannel = new SSEChannel({\\n eventEmitter: chatService.Cache.eventEmitter,\\n channelKey: EXCEL_STATUS_${req.user?.userOid || \\"\\"},\\n SSEMsgKey: \\"EXCEL_STATUS\\",\\n SSEEndKey: \\"EXCEL_STATUS_END\\",\\n res: res\\n });\\n const ssePublisher = excelStatusChannel.getSSEPublisher();\\n\\n const { roomOid } = req.params;\\n if (ssePublisher) {\\n await chatService.generateSaveAndSendExcelReport(roomOid, ssePublisher);\\n }\\n}\\n\\n\\n- Note that we also pass a WriteStream res into SSE channel so that\\n- later our ssePublisher can write a stream-response to end the channel (see ssePublisher.closeChannel(), which executes killChannel() from SSEChannel).\\n\\nNext the final function call is:\\n\\njs\\nconst generateSaveAndSendExcelReport = async (\\n roomOid: string,\\n ssePublisher: SSEChannelPublisher\\n) => {\\n await requestAndSaveLLMSummaryFromRoomOid(roomOid, ssePublisher);\\n const { excelUrl, room } = await dispatchExcelGenerationTaskToFlask(\\n roomOid,\\n ssePublisher\\n );\\n ssePublisher.emit(\\"Finished\\");\\n ssePublisher.closeChannel();\\n\\n await sendEmail({ room, excelUrl });\\n};\\n\\n\\nEach of requestAndSaveLLMSummaryFromRoomOid and dispatchExcelGenerationTaskToFlask has a setInterval to publish messages to frontend by using ssePublisher.emit(\\"something\\").\\n\\n#### Code Implementation\\n\\n- Here we assume access token is passed by header.\\n- In case the reader uses cookie to pass token, we just need to modify the function call of the constructor of Eventsource to use withCredential: true as an option.\\n\\n##### On SSE Request Header\\n\\nDefault EventSource in react and react-native does not provide any option to pass headers via the new EventSource() constructor. We need additional package to replace the native one.\\n\\n- For react, we use [eventsource](https://www.npmjs.com/package/eventsource)\\n- For react-native we use [react-native-event-source](https://www.npmjs.com/package/react-native-event-source)\\n\\n##### Custom SSE Class:\\n\\njs\\n// util/SSE.ts\\n\\nconst SSE_MAX_RETRY_COUNT = 5\\nimport Eventsource from \\"react-native-event-source\\";\\n\\nexport const sseStore: { current: SSE | null } = { current: null };\\n\\ntype SSEProps = {\\n eventSource: string,\\n subscriptions: { key: string, action: (data: string) => void }[],\\n endEvent: { key: string } | null,\\n token: string,\\n}\\n\\nclass SSE {\\n private reconnectionTries: number = 0\\n private eventSource: SSEProps[\\"eventSource\\"] = \\"\\"\\n private subscriptions: SSEProps[\\"subscriptions\\"] = []\\n private endEvent: SSEProps[\\"endEvent\\"] = null\\n private sse: Eventsource | null = null;\\n private token: string = \\"\\";\\n\\n constructor(params: SSEProps) {\\n const { endEvent, eventSource, subscriptions, token } = params;\\n this.eventSource = eventSource;\\n this.subscriptions = subscriptions;\\n this.endEvent = endEvent\\n this.token = token;\\n }\\n\\n public close = () => {\\n this.sse?.close();\\n }\\n\\n public subscribe = () => {\\n try {\\n this.sse = new Eventsource(this.eventSource, {\\n headers: {\\n \\"Authorization\\": \\"Bearer \\" + this.token,\\n }\\n });\\n } catch (err) {\\n console.log(err);\\n }\\n if (!this.sse) {\\n return;\\n }\\n\\n // listen to all subscriptions\\n this.subscriptions.forEach((event) => {\\n const { action, key } = event;\\n this.sse!.addEventListener(key, (event) => {\\n const data = event.data as string;\\n action(data);\\n });\\n })\\n\\n // listen to the only Kill Subscription Event Key\\n if (this.endEvent) {\\n this.sse.addEventListener(this.endEvent.key, () => {\\n this.sse!.close();\\n });\\n }\\n\\n // handle connection error if any\\n this.sse.addEventListener(\\"error\\", () => {\\n if (this.reconnectionTries < SSE_MAX_RETRY_COUNT) {\\n this.reconnectionTries++;\\n console.log(\\"err event, retry\\");\\n } else {\\n if (this.sse) {\\n console.log(\\n ${this.reconnectionTries + 1}th attempt, close connection\\n );\\n this.sse.close();\\n }\\n }\\n })\\n }\\n}\\n\\nconst createSSE = (props: SSEProps) => {\\n if (sseStore.current) {\\n sseStore.current.close();\\n }\\n sseStore.current = new SSE(props);\\n sseStore.current.subscribe();\\n};\\n\\nconst closeSSE = () => {\\n sseStore.current?.close();\\n sseStore.current = null;\\n}\\n\\nexport default {\\n createSSE,\\n closeSSE\\n};\\n\\n\\n##### Code Implementation: SSEChannel class and SSEChannelPublisher class\\n\\nLet\'s fix a cached EventEmitter instance. Let\'s identify each **_event emission key_** as a **_channel_**.\\n\\njs\\n// util/SSEChannel.ts\\n\\nimport { Response } from \\"express\\";\\nimport { EventEmitter, Writable } from \\"stream\\";\\nimport logger from \\"./logger\\";\\nimport chatService from \\"../service/chatService\\";\\n\\n\\nexport class SSEChannelPublisher {\\n private channelKey = \\"\\";\\n private killChannel = () => { };\\n\\n constructor(props: {\\n channelKey: string,\\n killChannel: () => void\\n }) {\\n this.channelKey = props.channelKey;\\n this.killChannel = props.killChannel;\\n }\\n\\n public closeChannel = () => {\\n this.killChannel();\\n chatService.Cache.eventEmitter.removeAllListeners(this.channelKey);\\n }\\n\\n public emit = (data: string) => {\\n chatService.Cache.eventEmitter.emit(this.channelKey, data)\\n }\\n}\\n\\n\\ntype ChannelProps = {\\n res: Response,\\n eventEmitter: EventEmitter,\\n channelKey: string\\n SSEMsgKey: string\\n SSEEndKey: string\\n}\\n\\n/**\\n * SSE specific Pub/Sub, the behaviour of our subscription is fixed, we just create publisher.\\n */\\nclass SSEChannel {\\n private channelOption: ChannelProps | null = null;\\n private channelEmitter: EventEmitter | null = null;\\n private eventId: number = 0;\\n\\n constructor(props: ChannelProps) {\\n this.channelOption = props;\\n this.channelEmitter = this.channelOption.eventEmitter;\\n this.listen();\\n }\\n\\n private killChannel = () => {\\n logger.info(\\"Killing the channel ...\\");\\n this.writeMessage({ message: \\"\\", SSEMsgKey: this.channelOption?.SSEEndKey || \\"\\" });\\n this.channelEmitter?.removeAllListeners(this.channelOption?.channelKey || \\"\\");\\n }\\n\\n public getSSEMsgKey = () => {\\n return this.channelOption?.SSEMsgKey || \\"\\";\\n }\\n\\n public getSSEPublisher = () => {\\n return new SSEChannelPublisher({\\n channelKey: this.channelOption?.channelKey || \\"\\",\\n killChannel: this.killChannel\\n });\\n }\\n\\n private writeMessage = (props: { SSEMsgKey: string, message: string }) => {\\n const res = this.channelOption?.res;\\n if (!res) {\\n return;\\n }\\n this.eventId++;\\n const { SSEMsgKey: frontendKey, message } = props;\\n res.write(event: ${frontendKey}\\\\n);\\n res.write(data: ${message}\\\\n);\\n res.write(id: ${this.eventId}\\\\n\\\\n);\\n }\\n\\n public listen = () => {\\n const res = this.channelOption?.res;\\n if (!res) {\\n return;\\n }\\n res.writeHead(200, {\\n \\"Content-Type\\": \\"text/event-stream\\",\\n \\"Connection\\": \\"keep-alive\\",\\n \\"Cache-Control\\": \\"no-cache\\",\\n });\\n\\n const { channelKey, SSEMsgKey } = this.channelOption!;\\n this.channelEmitter!.removeAllListeners(channelKey);\\n\\n this.channelEmitter!.on(\\n channelKey,\\n (message: string) => {\\n logger.info(Pushing Status ${message} to frontend);\\n this.writeMessage({ SSEMsgKey, message });\\n }\\n );\\n }\\n}\\n\\nexport default SSEChannel;\\n\\n\\n\\n\\n","title":"Push Data to Frontend by SSE via Event-Driven Approach with NO Short Polling","date":"2023-10-22T00:00:00.000Z","id":"blog0201","tag":"react, react-native, nodejs, express, SSE","intro":"In the past we have discussed SSE by kind of short polling in the backend (keep looping to see whether a key has message to pop out in redis queue). This time we send messages to frontend by listening subscriptions on EventEmitter, an approach very native to languages in which channel is implemented, like go and rust.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### PostgreSQL\\n\\nyml\\nversion: \\"2\\"\\nservices:\\n db:\\n image: postgres\\n volumes:\\n - ./data/db:/var/lib/postgresql/data\\n ports:\\n - \\"5432:5432\\"\\n environment:\\n POSTGRES_USER: pguser\\n POSTGRES_PASSWORD: pguser\\n POSTGRES_DB: pgdb\\n\\n\\n#### MySql\\n\\nyml\\nversion: \\"3\\"\\nservices:\\n mysql:\\n restart: always\\n image: mysql:5.7.18\\n container_name: mysql-lable\\n volumes:\\n - ./data/db/mysql/mydir:/mydir\\n - ./data/db/mysql/datadir:/var/lib/mysql\\n - ./data/db/mysql/conf/my.cnf:/etc/my.cnf\\n - ./data/db/mysql/source:/docker-entrypoint-initdb.d\\n environment:\\n - \\"MYSQL_ROOT_PASSWORD=root\\"\\n - \\"MYSQL_DATABASE=issue\\"\\n ports:\\n - 3306:3306\\n\\n","title":"Simple Postgresql and MySQL Server from Docker-Compose","date":"2023-10-29T00:00:00.000Z","id":"blog0202","tag":"docker","intro":"Record a few lines to init a postgresql/mysql db locally.","toc":false},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### Installations\\n\\n- text\\n go install github.com/pressly/goose/v3/cmd/goose@latest\\n \\n- - **_For mac and linux users_**, run\\n text\\n go install github.com/sqlc-dev/sqlc/cmd/sqlc@latest\\n \\n and from now on sqlc generate will be available in your shell.\\n - **_For windows user_**, the sqlc package above wouldn\'t work, instead we run\\n text\\n docker run --rm -v \\"%cd%:/src\\" -w /src sqlc/sqlc generate\\n \\n in cmd.\\n\\n#### Workflow for Database Migrations in Postgresql\\n\\nIn case the reader use mySQL, the only difference will be the sqlc.yml.\\n\\n##### sql/scehma\\n\\n###### sql/schema/001_users.sql\\n\\nsql\\n-- +goose Up\\n\\nCREATE TABLE users (\\n id UUID PRIMARY KEY,\\n created_at TIMESTAMP NOT NULL,\\n updated_at TIMESTAMP NOT NULL,\\n name TEXT NOT NULL\\n);\\n\\n-- +goose Down\\nDROP TABLE users;\\n\\n\\n###### sql/schema/002_users_apikey.sql\\n\\nsql\\n-- +goose Up\\nALTER TABLE users ADD COLUMN api_key VARCHAR(64) UNIQUE NOT NULL DEFAULT (\\n encode(sha256(random()::text::bytea), \'hex\')\\n);\\n\\n-- +goose Down\\nALTER TABLE users DROP COLUMN api_key;\\n\\n\\n###### sql/schema/003_feeds.sql\\n\\nsql\\n-- +goose Up\\n\\nCREATE TABLE feeds (\\n id UUID PRIMARY KEY,\\n created_at TIMESTAMP NOT NULL,\\n updated_at TIMESTAMP NOT NULL,\\n name TEXT NOT NULL,\\n url TEXT UNIQUE NOT NULL,\\n user_id UUID REFERENCES users(id) ON DELETE CASCADE\\n);\\n\\n-- +goose Down\\nDROP TABLE users;\\n\\n\\n###### sql/schema/004_feeds_uuid_nonull.sql\\n\\nsql\\n-- +goose Up\\nALTER TABLE feeds ALTER COLUMN user_id SET NOT NULL;\\n\\n-- +goose Down\\nALTER TABLE feeds ALTER COLUMN user_id DROP NOT NULL;\\n\\n\\n##### Apply Changes in DB Migrations by Goose\\n\\n- After each migration is inserted, we run sh db_migrate_up.sh, where\\n\\n shell\\n # db_migrate_up.sh\\n DB_URL=postgresql://pguser:pguser@127.0.0.1:5432/rssagg\\n\\n cd sql/schema\\n goose postgres $DB_URL up\\n read -p \\"Press any key to leave ...\\"\\n \\n\\n##### Create sqlc.yaml (Only do it Once)\\n\\n- Create a sqlc.yaml at the root project level:\\n\\n yml\\n version: \\"2\\"\\n sql:\\n - schema: \\"sql/schema\\"\\n queries: \\"sql/queries\\"\\n engine: \\"postgresql\\"\\n gen:\\n go:\\n out: \\"internal/database\\"\\n \\n\\n- For mySQL the yaml fiile is more or less the same, which can be copied form documentation.\\n - https://docs.sqlc.dev/en/stable/tutorials/getting-started-mysql.html\\n\\n##### sql/queries\\n\\n###### sql/queries/users.sql\\n\\nsql\\n-- name: CreateUser :one\\nINSERT INTO users(id, created_at, updated_at, name, api_key)\\nVALUES ($1, $2, $3, $4,\\n encode(sha256(random()::text::bytea), \'hex\')\\n)\\nRETURNING *;\\n\\n-- name: GetUserByAPIKey :one\\nSELECT * FROM users WHERE api_key = $1;\\n\\n\\n###### sql/quries/feeds.sql\\n\\nsql\\n-- name: CreateFeed :one\\nINSERT INTO feeds (id, created_at, updated_at, name, url, user_id)\\nVALUES ($1, $2, $3, $4, $5, $6)\\nRETURNING *;\\n\\n\\n##### Execute sqlc generate\\n\\n- After table migration is done, we create correponding schema as struct, queries as functions in go:\\n text\\n sqlc generate\\n \\n or in windows cmd prompt (or create a .bat file):\\n text\\n docker run --rm -v \\"%cd%:/src\\" -w /src sqlc/sqlc generate\\n pause\\n \\n\\n#### Outputs from Sqlc Generate\\n\\n##### internal/database/db.go\\n\\ngo\\n// Code generated by sqlc. DO NOT EDIT.\\n// versions:\\n// sqlc v1.22.0\\n\\npackage database\\n\\nimport (\\n\\t\\"context\\"\\n\\t\\"database/sql\\"\\n)\\n\\ntype DBTX interface {\\n\\tExecContext(context.Context, string, ...interface{}) (sql.Result, error)\\n\\tPrepareContext(context.Context, string) (*sql.Stmt, error)\\n\\tQueryContext(context.Context, string, ...interface{}) (*sql.Rows, error)\\n\\tQueryRowContext(context.Context, string, ...interface{}) *sql.Row\\n}\\n\\nfunc New(db DBTX) *Queries {\\n\\treturn &Queries{db: db}\\n}\\n\\ntype Queries struct {\\n\\tdb DBTX\\n}\\n\\nfunc (q *Queries) WithTx(tx *sql.Tx) *Queries {\\n\\treturn &Queries{\\n\\t\\tdb: tx,\\n\\t}\\n}\\n\\n\\n##### internal/database/models.go\\n\\ngo\\n// Code generated by sqlc. DO NOT EDIT.\\n// versions:\\n// sqlc v1.22.0\\n\\npackage database\\n\\nimport (\\n\\t\\"time\\"\\n\\n\\t\\"github.com/google/uuid\\"\\n)\\n\\ntype Feed struct {\\n\\tID uuid.UUID\\n\\tCreatedAt time.Time\\n\\tUpdatedAt time.Time\\n\\tName string\\n\\tUrl string\\n\\tUserID uuid.UUID\\n}\\n\\ntype User struct {\\n\\tID uuid.UUID\\n\\tCreatedAt time.Time\\n\\tUpdatedAt time.Time\\n\\tName string\\n\\tApiKey string\\n}\\n\\n\\n##### internal/database/users.sql.go\\n\\ngo\\n// Code generated by sqlc. DO NOT EDIT.\\n// versions:\\n// sqlc v1.22.0\\n// source: users.sql\\n\\npackage database\\n\\nimport (\\n\\t\\"context\\"\\n\\t\\"time\\"\\n\\n\\t\\"github.com/google/uuid\\"\\n)\\n\\nconst createUser = -- name: CreateUser :one\\nINSERT INTO users(id, created_at, updated_at, name, api_key)\\nVALUES ($1, $2, $3, $4,\\n encode(sha256(random()::text::bytea), \'hex\')\\n)\\nRETURNING id, created_at, updated_at, name, api_key\\n\\n\\ntype CreateUserParams struct {\\n\\tID uuid.UUID\\n\\tCreatedAt time.Time\\n\\tUpdatedAt time.Time\\n\\tName string\\n}\\n\\nfunc (q *Queries) CreateUser(ctx context.Context, arg CreateUserParams) (User, error) {\\n\\trow := q.db.QueryRowContext(ctx, createUser,\\n\\t\\targ.ID,\\n\\t\\targ.CreatedAt,\\n\\t\\targ.UpdatedAt,\\n\\t\\targ.Name,\\n\\t)\\n\\tvar i User\\n\\terr := row.Scan(\\n\\t\\t&i.ID,\\n\\t\\t&i.CreatedAt,\\n\\t\\t&i.UpdatedAt,\\n\\t\\t&i.Name,\\n\\t\\t&i.ApiKey,\\n\\t)\\n\\treturn i, err\\n}\\n\\nconst getUserByAPIKey = -- name: GetUserByAPIKey :one\\nSELECT id, created_at, updated_at, name, api_key FROM users WHERE api_key = $1\\n\\n\\nfunc (q *Queries) GetUserByAPIKey(ctx context.Context, apiKey string) (User, error) {\\n\\trow := q.db.QueryRowContext(ctx, getUserByAPIKey, apiKey)\\n\\tvar i User\\n\\terr := row.Scan(\\n\\t\\t&i.ID,\\n\\t\\t&i.CreatedAt,\\n\\t\\t&i.UpdatedAt,\\n\\t\\t&i.Name,\\n\\t\\t&i.ApiKey,\\n\\t)\\n\\treturn i, err\\n}\\n\\n\\n##### internal/database/feeds.sql.go\\n\\ngo\\n// Code generated by sqlc. DO NOT EDIT.\\n// versions:\\n// sqlc v1.22.0\\n// source: feeds.sql\\n\\npackage database\\n\\nimport (\\n\\t\\"context\\"\\n\\t\\"time\\"\\n\\n\\t\\"github.com/google/uuid\\"\\n)\\n\\nconst createFeed = -- name: CreateFeed :one\\nINSERT INTO feeds (id, created_at, updated_at, name, url, user_id)\\nVALUES ($1, $2, $3, $4, $5, $6)\\nRETURNING id, created_at, updated_at, name, url, user_id\\n\\n\\ntype CreateFeedParams struct {\\n\\tID uuid.UUID\\n\\tCreatedAt time.Time\\n\\tUpdatedAt time.Time\\n\\tName string\\n\\tUrl string\\n\\tUserID uuid.UUID\\n}\\n\\nfunc (q *Queries) CreateFeed(ctx context.Context, arg CreateFeedParams) (Feed, error) {\\n\\trow := q.db.QueryRowContext(ctx, createFeed,\\n\\t\\targ.ID,\\n\\t\\targ.CreatedAt,\\n\\t\\targ.UpdatedAt,\\n\\t\\targ.Name,\\n\\t\\targ.Url,\\n\\t\\targ.UserID,\\n\\t)\\n\\tvar i Feed\\n\\terr := row.Scan(\\n\\t\\t&i.ID,\\n\\t\\t&i.CreatedAt,\\n\\t\\t&i.UpdatedAt,\\n\\t\\t&i.Name,\\n\\t\\t&i.Url,\\n\\t\\t&i.UserID,\\n\\t)\\n\\treturn i, err\\n}\\n\\n\\n#### Reference\\n\\n- [Go Programming \u2013 Golang Course with Bonus Projects](https://www.youtube.com/watch?v=un6ZyFkqFKo)\\n","title":"Goose and Sqlc for Database Migration and Query Function Generation","date":"2023-10-28T00:00:00.000Z","id":"blog0203","tag":"go, sql, postgresql","intro":"We record a workflow of using goose and sqlc to work with changes of database schema.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n#### Alter Table\\n\\n- sql\\n ALTER TABLE people RENAME TO users;\\n ALTER TABLE users RENAME COLUMN handle TO username;\\n ALTER TABLE users ADD COLUMN password TEXT;\\n ALTER TABLE transactions ADD COLUMN was_successful BOOLEAN;\\n ALTER TABLE transactions ADD COLUMN transaction_type TEXT;\\n \\n\\n#### Examples of Constraints\\n\\n- sql\\n CREATE TABLE users(\\n \\tid INTEGER PRIMARY KEY,\\n \\tname TEXT NOT NULL,\\n \\tage INTEGER NOT NULL,\\n \\tcountry_code TEXT NOT NULL,\\n \\tusername TEXT UNIQUE,\\n \\tpassword TEXT NOT NULL,\\n \\tis_admin BOOLEAN\\n );\\n\\n CREATE TABLE employees(\\n \\tid INTEGER PRIMARY KEY,\\n \\tname TEXT NOT NULL,\\n \\tdepartment_id INTEGER,\\n \\tCONSTRAINT fk_departments -- can be whatever we want, just a name\\n \\tFOREIGN KEY (department_id)\\n \\tREFERENCES departments(id)\\n );\\n \\n\\n#### Data Insertion\\n\\n- sql\\n INSERT INTO users\\n \\t(name, age, contry_code, username, password, is_admin)\\n \\tVALUES\\n \\t(\\"David\\", 34, \\"US\\" \\"DavidDev\\", \\"insertPractice\\", false);\\n \\n\\n#### Filtering\\n\\n- sql\\n SELECT username FROM users WHERE is_admin=true;\\n \\n\\n#### Updating\\n\\n- sql\\n UPDATE emplyees\\n SET job_title = \\"Backend Engineer\\", salary = 150000\\n WHERE id = 251;\\n\\n UPDATE users\\n SET is_admin = true\\n WHERE username = wagslane;\\n \\n\\n#### IIF (analog of ternary in javascript)\\n\\n- sql\\n SELECT *,\\n IIF(was_successful=true,\\t\'No action required.\',\\t\'Perform an audit\') AS audit\\n FROM transactions;\\n \\n\\n#### BETWEEN\\n\\n- sql\\n SELECT name, age FROM users WHERE age BETWEEN 18 and 30;\\n SELECT name, age FROM users WHERE age NOT BETWEEN 18 and 30;\\n \\n\\n#### DISTINCT\\n\\n- sql\\n SELECT DISTINCT previous_company FROM employees;\\n \\n\\n#### IN\\n\\n- sql\\n SELECT name, age, country_code FROM users\\n WHERE country_code IN (\'US\', \'CA\', \'MX\');\\n \\n\\n#### LIKE\\n\\n- % match zero or more characters\\n- _ only matches a _single_ character\\n\\n- sql\\n SELECT * FROM products\\n WHERE product_name LIKE \'%banana%\'; -- (\'%banana\' or \'banana%\', etc)\\n \\n\\n- sql\\n SELECT * FROM products\\n WHERE product_name LIKE \'_oot\'; -- (or \'__oot\' for exactly two characters)\\n \\n\\n#### ORDER BY\\n\\n- sql\\n SELECT * FROM transactions\\n WHERE amount BETWEEN 10 AND 80\\n ORDER BY amount DESC; -- or ASC\\n \\n\\n#### Aggregations\\n\\n##### count\\n\\n- sql\\n SELECT album_id, count(id) as count\\n FROM songs\\n GROUP BY album_id;\\n \\n\\n##### sum\\n\\n- sql\\n SELECT sum(salary)\\n FROM employees;\\n \\n\\n##### max\\n\\n- sql\\n SELECT max(amount), user_id\\n FROM transactions\\n WHERE user_id=4;\\n \\n\\n##### min\\n\\n- sql\\n SELECT min(amount), user_id\\n FROM transactions\\n WHERE user_id=4;\\n \\n\\n##### GROUP BY\\n\\n- sql\\n SELECT user_id sum(amount) AS balance\\n FROM transactions\\n GROUP BY user_id;\\n \\n\\n##### avg\\n\\n- sql\\n SELECT avg(age) FROM users WHERE country_code=\\"US\\";\\n \\n\\n#### HAVING\\n\\n- A HAVING clause operates on rows after an aggregation has taken place due to GROUP BY clause.\\n- A WHERE clause operates on rows before an any aggragation.\\n\\n- sql\\n SELECT sender_id, sum(amount) AS balance\\n FROM transactions\\n WHERE sender_id!=Null AND note note LIKE \\"%lunch%\\"\\n GROUP BY sender_id\\n HAVING balance > 20\\n ORDER BY balance ASC;\\n \\n\\n#### SUBQUERIES\\n\\nBoth valid, though of different purposes.\\n\\n- sql\\n SELECT * FROM transactions\\n WHERE user_id in (\\n \\tSELECT id FROM users\\n \\tWHERE name LIKE \\"David\\"\\n );\\n \\n\\n- sql\\n SELECT * FROM transactions\\n WHERE user_id = (\\n \\tSELECT id FROM users\\n \\tWHERE name=\\"David\\"\\n \\tLIMIT = 1 -- in case there are multiple Davids\\n );\\n \\n\\n#### JOIN\\n\\n- sql\\n SELECT * FROM users\\n INNER JOIN countries ON users.country_code = countries.countr_code;\\n \\n\\n- sql\\n SELECT users.name, users.age, countries.name AS country_name\\n FROM users\\n LEFT JOIN countries ON users.country_code = countries.country_code;\\n ORDER BY country_name ASC;\\n \\n\\n or equivalently (in general **_aliasing namespaces is not a good idea_**):\\n\\n sql\\n SELECT u.name, u.age, c.name AS country_name\\n FROM users u\\n LEFT JOIN countries c ON u.country_code = c.country_code;\\n ORDER BY country_name ASC;\\n \\n\\n- sql\\n SELECT users.name, sum(transactions.amount) as sum, count(transactions.id) as count\\n FROM users\\n LEFT JOIN transactions on users.id = transactions.user_id\\n GROUP BY users.id\\n ORDER BY sum DESC;\\n \\n\\n#### Create Index\\n\\n- Syntax:\\n\\n sql\\n CREATE INDEX index_name on table_name (column_name);\\n \\n\\nExamples:\\n\\n- sql\\n CREATE INDEX email_idx on users (email);\\n \\n\\n- sql\\n CREATE INDEX first_name_last_name_age_index\\n ON USERS (first_name, last_name, age)\\n \\n","title":"PostgreSQL Revisit","date":"2023-11-04T00:00:00.000Z","id":"blog0205","tag":"sql","intro":"Record the standard qureis in SQL.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n\\n#### Configuration\\n\\nSince I am used to deploying applications in Fargate, a CloudWatch logging service is inherited and therefore I don\'t ***transport*** and ***rotate*** logging into a txt or log file.\\n\\n\\n\\n\\njs\\nimport { Express } from \\"express\\";\\nimport winston from \\"winston\\";\\nimport expressWinston from \\"express-winston\\";\\nimport Dayjs from \\"../util/hkDayjs\\";\\n\\nexpressWinston.responseWhitelist.push(\'body\');\\n\\n// caution, the \\"whitelist\\" is shown here just show that we can do that \\n// it is better to log request body only for specific routers (just add a middleware)\\n// otherwise the credentials (like login password) will also be logged\\nexpressWinston.requestWhitelist.push(\'body\'); \\n\\nconst dayjs = Dayjs.get(); // always hk timezone\\n\\nexport const format = winston.format.printf(info => {\\n const currTimeDayjs = dayjs(new Date()).tz(\\"Asia/Hong_Kong\\");\\n const date = currTimeDayjs.format(\\"YYYY-MM-DD\\");\\n const time = currTimeDayjs.format(\\"h:mma\\");\\n const { req, res, responseTime } = info.meta;\\n const statusCode = res[\\"statusCode\\"];\\n const responseBody = JSON.stringify(res[\\"body\\"]);\\n if (req?.headers?.authorization) {\\n req.headers.authorization = undefined;\\n }\\n const req_ = JSON.stringify({ req });\\n return ------------------------------------------------------ + \\"\\\\n\\" +\\n [${date}][HKT: ${time}][${info.level}] ${info.message} ${statusCode} in ${responseTime}ms\\\\n[Request]\\\\t${req_}\\\\n[ResponseBody]\\\\t${responseBody};\\n});\\n\\nexport default (app: Express) => {\\n app.use(expressWinston.logger({\\n ignoreRoute: (req, res): boolean => {\\n // ignore the get request emitted from health check from target group in aws.\\n if (req.originalUrl === \\"/test\\" || req.url === \\"/test\\") {\\n return true;\\n }\\n return false\\n },\\n transports: [\\n new winston.transports.Console({\\n format: format\\n })\\n ],\\n format: winston.format.combine(\\n winston.format.json()\\n )\\n }));\\n}\\n\\n\\n","title":"Custom Winston Logger in Nodejs","date":"2023-11-03T00:00:00.000Z","id":"blog0206","tag":"nodejs, express","intro":"Record my custom configuration of express logger","toc":false},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n<Center></Center>\\n\\nFor example, a $lookup usually involves a linkage with another collection through objectId in pipeline argument.\\n\\njs\\n{\\n $lookup: {\\n let: { hostUserOid: \\"$hostUserOid\\" },\\n from: UserModel.collection.name,\\n pipeline: [\\n {\\n $match: {\\n $expr: { $eq: [\\"$_id\\", { $toObjectId: \\"$$hostUserOid\\" }] }\\n }\\n }\\n ],\\n as: \\"hostUser\\"\\n }\\n},\\n\\n\\nDifferent mongo engine may use different key to convert a string to an ObjectId, in mongodb-java-driver it uses $oid instead of $toObjectId. Aggregation can have slight difference.\\n","title":"Mongoose way to Convert String into Objectid in Aggregation","date":"2023-11-02T00:00:00.000Z","id":"blog0207","tag":"mongoose","intro":"Just a syntax record","toc":false},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n<Center></Center>\\n\\n#### tsconfig.json\\n\\njs\\n{\\n \\"compilerOptions\\": {\\n \\"target\\": \\"ES2016\\",\\n \\"lib\\": [\\n \\"dom\\",\\n \\"dom.iterable\\",\\n \\"esnext\\"\\n ],\\n \\"outDir\\": \\"./dist\\",\\n \\"allowJs\\": true,\\n \\"skipLibCheck\\": true,\\n \\"noImplicitAny\\": true,\\n \\"esModuleInterop\\": true,\\n \\"allowSyntheticDefaultImports\\": true,\\n \\"strict\\": true,\\n \\"strictNullChecks\\": true,\\n \\"forceConsistentCasingInFileNames\\": true,\\n \\"noFallthroughCasesInSwitch\\": true,\\n \\"module\\": \\"CommonJS\\",\\n \\"moduleResolution\\": \\"node\\",\\n \\"resolveJsonModule\\": true,\\n \\"isolatedModules\\": true,\\n \\"noEmit\\": false,\\n },\\n \\"ts-node\\": {\\n \\"compilerOptions\\": {\\n \\"baseUrl\\": \\"./\\",\\n \\"module\\": \\"CommonJS\\"\\n }\\n },\\n \\"include\\": [\\n \\"src\\"\\n ],\\n \\"exclude\\": [\\n \\"**/*.md\\",\\n \\"node_modules\\"\\n ],\\n \\"files\\": [\\n \\"src/desc.d.ts\\"\\n ]\\n}\\n\\n\\n#### package.json (The Build Script)\\n\\njs\\n{\\n \\"scripts\\": {\\n \\"start:dev\\": \\"env-cmd -f .env-cmdrc -e default,dev nodemon --exec ts-node src/app.ts\\",\\n \\"debug\\": \\"env-cmd -f .env-cmdrc -e default,dev ts-node src/app.ts\\",\\n \\"test\\": \\"env-cmd -f .env-cmdrc -e default,dev jest --coverage\\",\\n \\"build\\": \\"tsc\\",\\n \\"start:prod\\": \\"npm run build && env-cmd -f .env-cmdrc -e default,prod node dist/src/app.js\\"\\n },\\n}\\n\\n\\n#### More About the Build Script\\n\\n- The use of npm run build does not mean we need a package named tsc.\\n\\n- In fact, the tsc script comes from typescript package.\\n- npm install tsc will make the build script fail.\\n- Our tsconfig.json have instructed tsc to build the plain js code in ./dist.\\n- As we have included src, our js code will be generated in ./dist/src.\\n\\n#### Dockerfile\\n\\ndocker\\nFROM node:16.20.2-alpine3.18\\n\\nRUN mkdir -p /home/app\\nCOPY . /home/app\\n\\nWORKDIR /home/app\\n\\nRUN npm install\\n\\nEXPOSE 9090\\n\\nCMD [\\"npm\\", \\"run\\", \\"start:prod\\"]\\n\\n\\n#### .dockerignore\\n\\ntext\\nnode_modules\\nnpm-debug.log\\n.vscode\\ncoverage\\nyarn.lock\\nyarn-error.log\\nREADME.md\\ndockerfile.dev\\n.gitignore\\n\\n","title":"Docker Containerization of Nodejs with Typescript","date":"2023-11-03T00:00:00.000Z","id":"blog0208","tag":"docker, nodejs","intro":"Containerization of vanilla JS project is simple, with typescript we need little more steps.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n<Center></Center>\\n\\n#### Result\\n\\n\\n<center>\\n <video controls width=\\"300\\">\\n <source src=\\"/assets/tech/211/01.MP4\\" type=\\"video/mp4\\">\\n Sorry, your browser doesn\'t support embedded videos.\\n </video>\\n</center>\\n\\n\\n#### Usage\\n\\n\\nSince we are going to animate the height when we delete the item, we need to provide it explicitly.\\n\\njs\\n<Swipe\\n ref={swipeRef}\\n height={SELECTION_DEAULT_HEIGHT}\\n containerStyle={{\\n height: SELECTION_DEAULT_HEIGHT,\\n borderColor: \\"rgba(0,0,0,0.2)\\",\\n }}\\n middle={{ component: roomSelection() }}\\n right={{ component: sendReportAndEditButton(), width: 165 }}\\n/>\\n\\n\\n#### Code Implmentation\\n\\nThis is a component that I made in the past:\\n\\njs\\nimport { ReactNode, forwardRef, useImperativeHandle } from \\"react\\";\\nimport { View } from \\"react-native\\";\\nimport { PanGestureHandler, PanGestureHandlerGestureEvent } from \\"react-native-gesture-handler\\";\\nimport Animated, {\\n useAnimatedGestureHandler,\\n useAnimatedStyle,\\n useDerivedValue,\\n useSharedValue,\\n withDelay,\\n withTiming\\n} from \\"react-native-reanimated\\";\\nimport { ViewProps } from \\"react-native-svg/lib/typescript/fabric/utils\\";\\n\\nexport type SwipeHandle = {\\n deleteItemAnimation: () => void,\\n returnCenter: () => void,\\n}\\n\\ntype SwipeProps = {\\n height: number,\\n left?: { component: ReactNode, width: number }\\n middle: { component: ReactNode },\\n right?: { component: ReactNode, width: number }\\n containerStyle?: ViewProps[\\"style\\"]\\n}\\n\\nconst Swipe = forwardRef<SwipeHandle, SwipeProps>((props, ref) => {\\n const { height, left, middle, right, containerStyle } = props;\\n const translateX = useSharedValue(0);\\n const containerOpacity = useSharedValue(1);\\n const containerHeight = useSharedValue(height || 0);\\n\\n const deleteItemAnimation = () => {\\n translateX.value = withTiming(0, { duration: 100 });\\n containerOpacity.value = withTiming(0);\\n containerHeight.value = withDelay(0, withTiming(0));\\n }\\n\\n const returnCenter = () => {\\n translateX.value = withTiming(0);\\n }\\n\\n useImperativeHandle(ref, () => ({\\n deleteItemAnimation,\\n returnCenter\\n }))\\n\\n const containerRstyle = useAnimatedStyle(() => {\\n return {\\n opacity: containerOpacity.value,\\n height: containerHeight.value,\\n transform: [{ translateX: translateX.value }]\\n }\\n })\\n const enclosedButtonOpacity = useDerivedValue(() => {\\n if (translateX.value < 0) {\\n return Math.abs(translateX.value / (right?.width || 1));\\n } else {\\n return Math.abs(translateX.value / (left?.width || 1));\\n }\\n })\\n\\n const hiddenRightButtonRStyles = useAnimatedStyle(() => {\\n return {\\n opacity: enclosedButtonOpacity.value\\n }\\n })\\n\\n const panGeatureEvent = useAnimatedGestureHandler<PanGestureHandlerGestureEvent, { translateX: number }>({\\n onStart: (e, ctx) => {\\n ctx.translateX = translateX.value;\\n },\\n onActive: (e, ctx) => {\\n translateX.value = Math.max(Math.min(ctx.translateX + e.translationX, left?.width || 0), -(right?.width || 0));\\n },\\n onEnd: (e, ctx) => {\\n if (e.translationX * ctx.translateX < 0) {\\n translateX.value = withTiming(0);\\n }\\n else if (e.translationX > 0) {\\n translateX.value = withTiming(left?.width || 0);\\n }\\n else if (translateX.value < 0) {\\n translateX.value = withTiming(-(right?.width || 0));\\n }\\n else {\\n translateX.value = withTiming(0);\\n }\\n }\\n });\\n\\n const rightHiddenButton = () => {\\n return (\\n <Animated.View style={[{\\n position: \\"absolute\\",\\n top: 0,\\n right: 0,\\n height: \\"100%\\",\\n justifyContent: \\"center\\",\\n flexDirection: \\"row\\",\\n }, hiddenRightButtonRStyles]}>\\n {right?.component}\\n </Animated.View>\\n )\\n }\\n\\n const leftHiddenButton = () => {\\n return (\\n <Animated.View style={[{\\n position: \\"absolute\\",\\n top: 0,\\n left: 0,\\n height: \\"100%\\",\\n justifyContent: \\"center\\"\\n }, hiddenRightButtonRStyles]}>\\n {left?.component}\\n </Animated.View>\\n )\\n }\\n\\n return (\\n <View style={[{ position: \\"relative\\", }, containerStyle || {}]} >\\n {leftHiddenButton()}\\n {rightHiddenButton()}\\n <PanGestureHandler onGestureEvent={panGeatureEvent} activeOffsetX={[-10, 10]}>\\n <Animated.View style={[\\n {\\n width: \\"100%\\",\\n flexDirection: \\"row\\",\\n alignItems: \\"center\\",\\n justifyContent: \\"center\\",\\n },\\n containerRstyle]\\n }>\\n {middle.component}\\n </Animated.View>\\n </PanGestureHandler>\\n </View>\\n )\\n});\\n\\nexport default Swipe;\\n\\n","title":"Make a Custom Swipable Item","date":"2023-11-05T00:00:00.000Z","id":"blog0210","tag":"react-native","intro":"Record a pan guesture component that can swipe an item to show hidden buttons"},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n<Center></Center>\\n\\n#### Repository\\n\\n- https://github.com/machingclee/2023-11-04-go-gin/tree/main\\n\\n#### Models\\n\\nWe model a bank account system by\\n\\n- **Transfer.** Record the cash flow\\n- **Entry.** Record the individual account cash flow\\n- **Account.** Record the entity in a bank system\\n\\nIn go they are defined by the following struct:\\n\\ngo\\ntype Account struct {\\n\\tID int64 json:\\"id\\"\\n\\tOwner string json:\\"owner\\"\\n\\tBalance int64 json:\\"balance\\"\\n\\tCurrency string json:\\"currency\\"\\n\\tCreatedAt time.Time json:\\"created_at\\"\\n}\\n\\ntype Entry struct {\\n\\tID int64 json:\\"id\\"\\n\\tAccountID int64 json:\\"account_id\\"\\n\\t// can be +ve or -ve\\n\\tAmount int64 json:\\"amount\\"\\n\\tCreatedAt time.Time json:\\"created_at\\"\\n}\\n\\ntype Transfer struct {\\n\\tID int64 json:\\"id\\"\\n\\tFromAccountID int64 json:\\"from_account_id\\"\\n\\tToAccountID int64 json:\\"to_account_id\\"\\n\\t// must be positive\\n\\tAmount int64 json:\\"amount\\"\\n\\tCreatedAt time.Time json:\\"created_at\\"\\n}\\n\\n\\nThese are generate by sqlc with the following schema (database migration by goose or any sort of similar tool):\\n\\nsql\\n-- +goose Up\\n\\nCREATE TABLE \\"accounts\\" (\\n \\"id\\" bigserial PRIMARY KEY,\\n \\"owner\\" varchar NOT NULL,\\n \\"balance\\" bigint NOT NULL,\\n \\"currency\\" varchar NOT NULL,\\n \\"created_at\\" timestamptz NOT NULL DEFAULT (now())\\n);\\n\\nCREATE TABLE \\"entries\\" (\\n \\"id\\" bigserial PRIMARY KEY,\\n \\"account_id\\" bigint NOT NULL,\\n \\"amount\\" bigint NOT NULL,\\n \\"created_at\\" timestamptz NOT NULL DEFAULT (now())\\n);\\n\\nCREATE TABLE \\"transfers\\" (\\n \\"id\\" bigserial PRIMARY KEY,\\n \\"from_account_id\\" bigint NOT NULL,\\n \\"to_account_id\\" bigint NOT NULL,\\n \\"amount\\" bigint NOT NULL,\\n \\"created_at\\" timestamptz NOT NULL DEFAULT (now())\\n);\\n\\nCREATE INDEX ON \\"accounts\\" (\\"owner\\");\\n\\nCREATE INDEX ON \\"entries\\" (\\"account_id\\");\\n\\nCREATE INDEX ON \\"transfers\\" (\\"from_account_id\\");\\n\\nCREATE INDEX ON \\"transfers\\" (\\"to_account_id\\");\\n\\nCREATE INDEX ON \\"transfers\\" (\\"from_account_id\\", \\"to_account_id\\");\\n\\nCOMMENT ON COLUMN \\"entries\\".\\"amount\\" IS \'can be +ve or -ve\';\\n\\nCOMMENT ON COLUMN \\"transfers\\".\\"amount\\" IS \'must be positive\';\\n\\nALTER TABLE \\"entries\\" ADD FOREIGN KEY (\\"account_id\\") REFERENCES \\"accounts\\" (\\"id\\");\\n\\nALTER TABLE \\"transfers\\" ADD FOREIGN KEY (\\"from_account_id\\") REFERENCES \\"accounts\\" (\\"id\\");\\n\\nALTER TABLE \\"transfers\\" ADD FOREIGN KEY (\\"to_account_id\\") REFERENCES \\"accounts\\" (\\"id\\");\\n\\n\\n-- +goose Down\\nDROP TABLE accounts, entries, transfers;\\n\\n\\n#### Transaction\\n\\n##### Scenario\\n\\n- We will be performing a transaction repeatedly in multiple goroutines.\\n\\n- In each transaction (goroutine), we simply transfer a fixed quantity of 10 from a fixed account1 to another fixed account2.\\n\\n- We will be encountering deadlock and try to fix it.\\n\\n##### Define Store and Transaction Execution Wrapper\\n\\ngo\\ntype Store struct {\\n\\t*Queries\\n\\tdb *sql.DB\\n}\\n\\nfunc NewStore(db *sql.DB) *Store {\\n\\treturn &Store{\\n\\t\\tdb: db,\\n\\t\\tQueries: New(db),\\n\\t}\\n}\\n\\nfunc (store *Store) execTx(ctx context.Context, fn func(*Queries) error) error {\\n\\ttx, err := store.db.BeginTx(ctx, nil)\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tq := New(tx)\\n\\terr = fn(q)\\n\\tif err != nil {\\n\\t\\tif rbErr := tx.Rollback(); rbErr != nil {\\n\\t\\t\\treturn fmt.Errorf(\\"tx err: %v, rb err: %v\\", err, rbErr)\\n\\t\\t}\\n\\t\\treturn err\\n\\t}\\n\\treturn tx.Commit()\\n}\\n\\n\\nwhere Queries implements a DBTX interface generated by sqlc.\\n\\n##### TransferTxResult\\n\\nThe following struct is merely a record purpose:\\n\\ngo\\ntype TransferTxResult struct {\\n\\tTransfer Transfer\\n\\tFromAccount Account\\n\\tToAccount Account\\n\\tFromEntry Entry\\n\\tToEntry Entry\\n}\\n\\n\\nWe will investiage the results by storing the consequence of a transaction into this struct.\\n\\n##### Implement a Transaction\\n\\ngo\\nfunc (store *Store) TransferTx(ctx context.Context, arg TransferTxParams) (TransferTxResult, error) {\\n\\tvar result TransferTxResult\\n\\terr := store.execTx(ctx, func(q *Queries) error {\\n\\t\\tvar err error\\n\\t\\tresult.Transfer, err = q.CreateTransfer(ctx, CreateTransferParams{\\n\\t\\t\\tFromAccountID: arg.FromAccountID,\\n\\t\\t\\tToAccountID: arg.ToAccountID,\\n\\t\\t\\tAmount: arg.Amount,\\n\\t\\t})\\n\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\n\\t\\tresult.FromEntry, err = q.CreateEntry(ctx, CreateEntryParams{\\n\\t\\t\\tAccountID: arg.FromAccountID,\\n\\t\\t\\tAmount: -arg.Amount,\\n\\t\\t})\\n\\n\\t\\tresult.ToEntry, err = q.CreateEntry(ctx, CreateEntryParams{\\n\\t\\t\\tAccountID: arg.ToAccountID,\\n\\t\\t\\tAmount: arg.Amount,\\n\\t\\t})\\n\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\n\\nand here comes the fun part:\\n\\ngo\\n\\n\\t\\taccount1, err := q.GetAccount(ctx, arg.FromAccountID)\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t\\tresult.FromAccount, err = q.UpdateAccount(ctx, UpdateAccountParams{\\n\\t\\t\\tID: arg.FromAccountID,\\n\\t\\t\\tBalance: account1.Balance - arg.Amount,\\n\\t\\t})\\n\\n\\t\\taccount2, err := q.GetAccount(ctx, arg.ToAccountID)\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t\\tresult.ToAccount, err = q.UpdateAccount(ctx, UpdateAccountParams{\\n\\t\\t\\tID: arg.ToAccountID,\\n\\t\\t\\tBalance: account2.Balance + arg.Amount,\\n\\t\\t})\\n\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\n\\t\\treturn nil\\n\\t})\\n\\treturn result, err\\n}\\n\\n\\n##### Run Transactions Concurrently\\n\\ngo\\nfunc TestTransferTx(t *testing.T) {\\n\\tstore := NewStore(testDB)\\n\\n\\taccount1 := createRandomAccount(t)\\n\\taccount2 := createRandomAccount(t)\\n\\n\\tn := 5\\n\\tamount := int64(10)\\n\\n\\terrChan := make(chan error)\\n\\tresultChan := make(chan TransferTxResult)\\n\\n\\tfor i := 0; i < n; i++ {\\n\\t\\tgo func() {\\n\\t\\t\\tresult, err := store.TransferTx(context.Background(), TransferTxParams{\\n\\t\\t\\t\\tFromAccountID: account1.ID,\\n\\t\\t\\t\\tToAccountID: account2.ID,\\n\\t\\t\\t\\tAmount: amount,\\n\\t\\t\\t})\\n\\t\\t\\terrChan <- err\\n\\t\\t\\tresultChan <- result\\n\\t\\t}()\\n\\t}\\n\\n\\tfor i := 0; i < n; i++ {\\n\\t\\terr := <-errChan\\n\\t\\trequire.NoError(t, err)\\n\\n\\t\\tresult := <-resultChan\\n\\t\\tresultFromAcocunt := result.FromAccount\\n\\t\\tresultToAccount := result.ToAccount\\n\\n\\t\\tfmt.Println(\\"before tx:\\", \\"fromAccount\\", account1.Balance, \\"toAccount\\", account2.Balance, \\"amount\\", int64(i+1)*amount)\\n\\t\\tfmt.Println(\\"after tx: \\", \\"fromAccount\\", resultFromAcocunt.Balance, \\"toAccount\\", resultToAccount.Balance)\\n\\t\\tfmt.Println(\\"-------\\")\\n\\n\\t\\tdiff1 := account1.Balance - resultFromAcocunt.Balance\\n\\t\\tdiff2 := resultToAccount.Balance - account2.Balance\\n\\n\\t\\trequire.Equal(t, diff1, diff2)\\n\\t\\trequire.True(t, diff1 > 0)\\n\\t\\trequire.True(t, diff1 > 0)\\n\\t}\\n}\\n\\n\\n##### Test Fails\\n\\ntext\\nbefore tx: fromAccount 650 toAccount 976 amount 10\\nafter tx: fromAccount 640 toAccount 986\\n-------\\nbefore tx: fromAccount 650 toAccount 976 amount 20\\nafter tx: fromAccount 630 toAccount 996\\n-------\\nbefore tx: fromAccount 650 toAccount 976 amount 30\\nafter tx: fromAccount 630 toAccount 1006\\n-------\\n--- FAIL: TestTransferTx (0.09s)\\n c:\\\\Users\\\\user\\\\Repos\\\\Go\\\\2023-11-04-api-time\\\\internal\\\\db\\\\store_test.go:62:\\n \\tError Trace:\\tc:/Users/user/Repos/Go/2023-11-04-api-time/internal/db/store_test.go:62\\n \\tError: \\tNot equal:\\n \\t \\texpected: 20\\n \\t \\tactual : 30\\n \\tTest: \\tTestTransferTx\\nFAIL\\nFAIL\\tgithub.com/machingclee/2023-11-04-go-gin/internal/db\\t0.283s\\n\\n\\n#### Transactions in Raw SQL and Transition into Go\\n\\n##### SQL\\n\\nWe ssh into pgSQL server by\\n\\ntext\\ndocker exec -it <container-name> bash\\nroot@3b68a2663049:/# psql -d pgdb -U pguser\\n\\n\\nLet\'s start a transaction on the first terminal:\\n\\nsql\\npgdb=# begin;\\nBEGIN\\npgdb=*# select * from accounts where id = 1 for update;\\n\\n\\nNext if we repeat the same transaction at another terminal, we get blocked until the first transaction has committed the result.\\n\\nWe continue to update the account and commit the result:\\n\\nsql\\npgdb=# begin;\\nBEGIN\\npgdb=*# select * from accounts where id = 1 for update;\\n id | owner | balance | currency | created_at\\n----+--------+---------+----------+-------------------------------\\n 1 | agqcdw | 0 | USD | 2023-11-05 12:21:30.566415+00\\n(1 row)\\n\\npgdb=*# update accounts set balance = 500 where id = 1;\\nUPDATE 1\\npgdb=*# commit;\\nCOMMIT\\npgdb=#\\n\\n\\nThe second transaction is now unblocked immediately.\\n\\n##### Same Scenario in Go\\n\\nLet\'s update this raw SQL in our sql/queries/account and use sqlc to generate new GetAccountForUpdate function:\\n\\nsql\\n-- name: GetAccountForUpdate :one\\nSELECT * FROM accounts\\nWHERE id = $1 LIMIT 1\\nFOR UPDATE;\\n\\n\\nNow a new version of GetAccount has been generated:\\n\\ngo\\nconst getAccountForUpdate = -- name: GetAccountForUpdate :one\\nSELECT id, owner, balance, currency, created_at FROM accounts\\nWHERE id = $1 LIMIT 1\\nFOR UPDATE\\n\\n\\nfunc (q *Queries) GetAccountForUpdate(ctx context.Context, id int64) (Account, error) {\\n\\trow := q.db.QueryRowContext(ctx, getAccountForUpdate, id)\\n\\tvar i Account\\n\\terr := row.Scan(\\n\\t\\t&i.ID,\\n\\t\\t&i.Owner,\\n\\t\\t&i.Balance,\\n\\t\\t&i.Currency,\\n\\t\\t&i.CreatedAt,\\n\\t)\\n\\treturn i, err\\n}\\n\\n\\nLet\'s rerun our test:\\n\\n##### Deadlock\\n\\ntext\\nbefore tx: fromAccount 488 toAccount 962 amount 10\\nafter tx: fromAccount 478 toAccount 972\\n-------\\n--- FAIL: TestTransferTx (1.07s)\\n c:\\\\Users\\\\user\\\\Repos\\\\Go\\\\2023-11-04-api-time\\\\internal\\\\db\\\\store_test.go:37:\\n \\tError Trace:\\tc:/Users/user/Repos/Go/2023-11-04-api-time/internal/db/store_test.go:37\\n \\tError: \\tReceived unexpected error:\\n \\t \\tpq: deadlock detected\\n \\tTest: \\tTestTransferTx\\n\\n\\nThis is because in a course of a transaction we are going to create a record in transfers table, which refers\\nto an entity in accounts table (the foreign key constraint). Since we have not committed any transaction yet, any select\\nstatement selecting that particular account for update will be locked, resulting in a dead lock.\\n\\n##### Solution\\n\\nWe just need to **_inform_** postgres that we are updating that row **_without touching the primary key_**, then our transaction will not need to acquire a lock for updating the account entity:\\n\\nsql\\n-- name: GetAccountForUpdate :one\\nSELECT * FROM accounts\\nWHERE id = $1 LIMIT 1\\nFOR NO KEY UPDATE;\\n\\n\\nsqlc genreate and now if we run the test again:\\n\\ntext\\nok \\tgithub.com/machingclee/2023-11-04-go-gin/internal/db\\t0.288s\\n\\n\\n#### Combine Get and Update SQL into One, sqlc Trick to Change Parameter Name\\n\\nIn TransferTx we have Get and Update the account in two separate queries.\\n\\nIf we know that we are just doing an add operation (+ve or -ve), we may combine the queries into one:\\n\\nsql\\n-- name: AddAccountBalance :one\\nUPDATE accounts\\nSET balance = balance + $2\\nWHERE id = $1\\nRETURNING *;\\n\\n\\nBy sqlc generate we get\\n\\ngo\\ntype AddAccountBalanceParams struct {\\n\\tID int64 json:\\"id\\"\\n\\tBalance int64 json:\\"balance\\"\\n}\\n\\n\\nThe field name Balance is very confusing and it should be amount instead.\\n\\nsqlc allows us to change parameter name arbitrarily:\\n\\nsql\\n-- name: AddAccountBalance :one\\nUPDATE accounts\\nSET balance = balance + sqlc.arg(amount)\\nWHERE id = sqlc.arg(id)\\nRETURNING *;\\n\\n\\nNow sqlc geneate again:\\n\\ngo\\ntype AddAccountBalanceParams struct {\\n\\tAmount int64 json:\\"amount\\"\\n\\tID int64 json:\\"id\\"\\n}\\n\\n\\nas desired.\\n\\nNow our transaction has changed from\\n\\ngo\\n\\t\\taccount1, err := q.GetAccountForUpdate(ctx, arg.FromAccountID)\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t\\tresult.FromAccount, err = q.UpdateAccount(ctx, UpdateAccountParams{\\n\\t\\t\\tID: arg.FromAccountID,\\n\\t\\t\\tBalance: account1.Balance - arg.Amount,\\n\\t\\t})\\n\\n\\t\\taccount2, err := q.GetAccountForUpdate(ctx, arg.ToAccountID)\\n\\t\\tif err != nil {\\n\\t\\t\\treturn err\\n\\t\\t}\\n\\t\\tresult.ToAccount, err = q.UpdateAccount(ctx, UpdateAccountParams{\\n\\t\\t\\tID: arg.ToAccountID,\\n\\t\\t\\tBalance: account2.Balance + arg.Amount,\\n\\t\\t})\\n\\n\\ninto\\n\\ngo\\n\\t\\tresult.FromAccount, err = q.AddAccountBalance(ctx, AddAccountBalanceParams{\\n\\t\\t\\tID: arg.FromAccountID,\\n\\t\\t\\tAmount: -arg.Amount,\\n\\t\\t})\\n\\n\\t\\tresult.ToAccount, err = q.AddAccountBalance(ctx, AddAccountBalanceParams{\\n\\t\\t\\tID: arg.ToAccountID,\\n\\t\\t\\tAmount: arg.Amount,\\n\\t\\t})\\n\\n\\n#### Transaction Isolation Levels\\n\\n- **Read Uncomitted.**\\n - In this level uncommitted changes in a course of transaction is **_visible_** by other transaction, which results in a **_dirty read_**.\\n - pgSQL **does not have this level**. Even though you can set isolation level to Read Uncommitted, which is the same as Read Committed.\\n- **Read Committed.**\\n - In this level **dirty read is avoided** but the same select query may result in two different results of the same row, an **_non-repeatable read phenomenon_** occurs.\\n - Worse still, same query can result in different number of rows, which is a **_phantom-read phenomenon_**.\\n- **Repeatable Read.**\\n - In this level both **non-repeatable read and phantom-read are prevented**, but the data of the select query from a \\"second\\" transaction will keep out-dated even \\"first\\" transaction has committed.\\n - If we try to run an update in the second transaction, different database engines have different interpretation.\\n - In mySQL we get consistent updated result from the perspective of database, but from the view of second transaction updated values are inconsistent.\\n - In pgSQL we directly get an error.\\n- **Serializable.**\\n - All select statement in this level is forced to be SELECT FOR SHARE, meaning all **_other_** transactions are only allowed to read.\\n - A commission of a read-only transaction is needed to release the lock for any update statement from other transactions.\\n\\n#### Reference\\n\\n- [Backend Master Class \\\\[Golang + Postgres + Kubernetes + gRPC\\\\]](https://www.udemy.com/course/backend-master-class-golang-postgresql-kubernetes/learn/lecture/25822282#overview)\\n","title":"Transactions, Go and sqlc","date":"2023-11-06T00:00:00.000Z","id":"blog0211","tag":"sql, go","intro":"Explore transactions with go and raw SQL"},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n<Center></Center>\\n\\n#### dbdiagram.io\\n\\nFirst you may need to register and sign in:\\n\\n- https://dbdiagram.io/\\n\\n#### Problem Description: Add a new Table users, With accounts.owner ref: > users.username\\n\\nThis usually gives rise to the following:\\n\\n**Problems.** An error Violation of foreign key constraint\\n\\n**Reason.** This is because the existing data in table A has no reference to Table B (as a new table there is no data yet), therefore the reference key constraint must fail **_unless all table has no data_**.\\n\\n#### Model the Problem with Real SQL Code\\n\\n##### Create accounts, transfers and entries\\n\\nLet\'s model this situation in code, let\'s first create the following tables, which in dbdiagram.io has a very nice visuallization:\\n\\n<center></center>\\n\\n[![](/assets/tech/212/image.png)](/assets/tech/212/image.png)\\n\\nThe table schemas above can be written in DBML:\\n\\ndbml\\nTable accounts as A {\\n id bigserial [pk]\\n owner varchar [not null]\\n balance bigint [not null]\\n currency varchar [not null]\\n created_at timestamptz [not null, default: now()]\\n\\n Indexes{\\n owner\\n }\\n}\\n\\nTable entries {\\n id bigserial [pk]\\n account_id bigint [ref: > A.id]\\n amount bigint [not null, note: \\"can be +ve or -ve\\"]\\n created_at timestamptz [not null, default: now()]\\n\\n Indexes{\\n account_id\\n }\\n}\\n\\nTable transfers{\\n id bigserial [pk]\\n from_account_id bigint [ref: > A.id, not null]\\n to_account_id bigint [ref: > A.id, not null]\\n amount bigint [not null, note: \\"must be positive\\"]\\n created_at timestamptz [not null, default: now()]\\n\\n Indexes{\\n from_account_id\\n to_account_id\\n (from_account_id, to_account_id)\\n }\\n}\\n\\n\\n##### Create User That owns Accounts\\n\\nAfter playing around (like CRUD) with these old tables, our accounts table will be full of record with owner field having no reference to any user data (not yet created).\\n\\nNow in the course of developement we have the following decisions:\\n\\n- Add a users table\\n- Create foreign key constraint in accounts table\\n- Create indexes (owner, currency) in accounts table to prevent accounts with repreated currencies.\\n\\n[![](/assets/tech/212/image-1.png)](/assets/tech/212/image-1.png)\\n\\ndbml\\nTable users as U {\\n username varchar [pk]\\n hashed_password varchar [not null]\\n full_name varchar [not null]\\n email varchar [unique]\\n password_changed_at timestamptz [not null, default: \\"0001-01-01 00:00:00Z\\"]\\n created_at timestamptz [not null, default: now()]\\n}\\n\\nTable accounts as A {\\n id bigserial [pk]\\n owner varchar [ref: > U.username, not null]\\n balance bigint [not null]\\n currency varchar [not null]\\n created_at timestamptz [not null, default: now()]\\n\\n Indexes{\\n owner\\n (owner, currency) [unique]\\n }\\n}\\n\\nTable entries {\\n id bigserial [pk]\\n account_id bigint [ref: > A.id]\\n amount bigint [not null, note: \\"can be +ve or -ve\\"]\\n created_at timestamptz [not null, default: now()]\\n\\n Indexes{\\n account_id\\n }\\n}\\n\\nTable transfers{\\n id bigserial [pk]\\n from_account_id bigint [ref: > A.id, not null]\\n to_account_id bigint [ref: > A.id, not null]\\n amount bigint [not null, note: \\"must be positive\\"]\\n created_at timestamptz [not null, default: now()]\\n\\n Indexes{\\n from_account_id\\n to_account_id\\n (from_account_id, to_account_id)\\n }\\n}\\n\\n\\nNow Export to PGSQL and only extract the parts corresponding to our changes:\\n\\nsql\\nCREATE TABLE \\"users\\" (\\n \\"username\\" varchar PRIMARY KEY,\\n \\"hashed_password\\" varchar NOT NULL,\\n \\"full_name\\" varchar NOT NULL,\\n \\"email\\" varchar UNIQUE,\\n \\"password_changed_at\\" timestamptz NOT NULL DEFAULT \'0001-01-01 00:00:00Z\',\\n \\"created_at\\" timestamptz NOT NULL DEFAULT (now())\\n);\\n\\nALTER TABLE \\"accounts\\" ADD FOREIGN KEY (\\"owner\\") REFERENCES \\"users\\" (\\"username\\");\\n\\nCREATE UNIQUE INDEX owner_currency ON \\"accounts\\" (\\"owner\\", \\"currency\\");\\n\\n-- +goose Down\\n\\nDROP TABLE \\"users\\"\\n\\nALTER TABLE \\"accounts\\" DROP FOREIGN KEY \\"owner\\";\\n\\nDROP INDEX \\"owner_currency\\" ON \\"accounts\\"\\n\\n\\n##### Problem and Resolution\\n\\n- **Problem.** Now existing owners block the table migrataion becuase creating users table with accounts.owner referencing to users.username is **_impossible_**.\\n\\n- **Solution.** We rename accounts.owner to accounts.deprecated_owner, drop not null constraint, and create an nullable owner column.\\n\\n Nullability of both columns is essential to let both columns to exist. When table is stable, we can drop the deprecated column and set not-null constraint to the new ower column.\\n\\n- **Additional Change.**\\n\\n sql\\n -- +goose Up\\n ALTER TABLE \\"accounts\\" RENAME COLUMN \\"owner\\" to \\"_deprecated_owner\\";\\n ALTER TABLE \\"accounts\\" ALTER COLUMN \\"_deprecated_owner\\" DROP NOT NULL;\\n ALTER TABLE accounts ADD \\"owner\\" varchar;\\n \\n\\n- **With Old Planing.**\\n\\n sql\\n CREATE TABLE \\"users\\" (\\n \\t\\"username\\" varchar PRIMARY KEY,\\n \\t\\"hashed_password\\" varchar NOT NULL,\\n \\t\\"full_name\\" varchar NOT NULL,\\n \\t\\"email\\" varchar UNIQUE,\\n \\t\\"password_changed_at\\" timestamptz NOT NULL DEFAULT \'0001-01-01 00:00:00Z\',\\n \\t\\"created_at\\" timestamptz NOT NULL DEFAULT (now())\\n );\\n ALTER TABLE \\"accounts\\" ADD FOREIGN KEY (\\"owner\\") REFERENCES \\"users\\" (\\"username\\");\\n CREATE UNIQUE INDEX owner_currency ON \\"accounts\\" (\\"owner\\", \\"currency\\");\\n\\n -- +goose Down\\n\\n ALTER TABLE IF EXISTS \\"accounts\\" DROP CONTRAINT IF EXISTS \\"owner_currency\\";\\n ALTER TABLE IF EXISTS \\"accounts\\" DROP CONTRAINT IF EXISTS \\"accounts_owner_fkey\\";\\n DROP TABLE IF EXISTS \\"users\\";\\n ALTER TABLE \\"accounts\\" DROP COLUMN owner;\\n ALTER TABLE \\"accounts\\" ALTER COLUMN \\"_deprecated_owner\\" SET NOT NULL;\\n ALTER TABLE \\"accounts\\" RENAME COLUMN \\"_deprecated_owner\\" to \\"owner\\";\\n \\n\\nNow new and old records will look like:\\n\\n<center></center>\\n\\n[![](/assets/tech/212/image-2.png)](/assets/tech/212/image-2.png)\\n\\nWe will be deleting old records (rows without owner) at a suitable timing.\\n","title":"SQL for Table Migrations in the Course of Developement","date":"2023-11-11T00:00:00.000Z","id":"blog0212","tag":"sql","intro":"Record the workflow used in database migration, integrated with tools like dbdiagram.io."},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n<Center></Center>\\n\\n#### Why not Sendgrid?\\n##### How easy is Using Sendgrid?\\nSendgrid is easy to use, we just need to plug our email information into sendgrid\'s webpage, then it will generate an API key for us. \\n\\nEmail can then be sent as simply as writing\\n\\njs\\nimport sgMail from \'@sendgrid/mail\';\\nimport { EmailSender } from \\"../dto/dto\\";\\n\\nsgMail.setApiKey(process.env.SENDGRID_API_KEY || \\"\\")\\n\\nconst sendMessage: EmailSender = async (props: {\\n to: string,\\n subject: string,\\n text: string,\\n html: string\\n}) => {\\n const { html, subject, text, to } = props;\\n\\n const msg = {\\n to,\\n from: \\"abc@email.com\\",\\n subject,\\n text,\\n html,\\n }\\n\\n try {\\n await sgMail.send(msg);\\n } catch (err) {\\n throw new Error(JSON.stringify(err));\\n }\\n}\\n\\nand I am sure this email-sending feature can be set up ***within less than 15 minutes*** with zero knowledge about this service. \\n\\n##### Problem of Sendgrid\\n\\nThe problem lies in its pricing ([click me](https://sendgrid.com/pricing)). A feature that is supposed to be free (have you ever paid google for sending email?) has a 100 emails/day cap, and uncapping it (still not unlimited) requires at least ***19.95 usd*** (approximately ***155.79 hkd***) per month.\\n\\n##### Problem of Gmail API\\n\\nThe following\\n- Complexity of oAuth2 credential authentication and \\n- the ease of use of the Google Cloud Console \\nare the only barrier, if we can get around this then there is no reason not to use gmail api (which is free).\\n\\n\\n#### Prerequisite: OAuth2 Credential in Json\\n\\nFirst you need to create an OAuth2 Credential about your account in json format. Detailed precedures have been included in\\n\\n- [this blog post](/blog/article/Gmail-and-Inbox-Push-Notification),\\n\\nunder the **_Create Credentials_** session.\\n\\n#### A gmailService File\\n\\n##### Environment Variables\\n\\nLet\'s create a gmailService.ts in our service directory, then:\\n\\njs-1\\nimport fs from \\"fs\\";\\nimport path from \\"path\\";\\nimport process from \\"process\\";\\nimport { authenticate } from \\"@google-cloud/local-auth\\";\\nimport google from \\"googleapis\\"\\nimport { EmailSender, GMailUserJson as GoogleUserJson } from \\"../dto/dto\\";\\nimport nodemailer from \\"nodemailer\\"\\n\\nconst SCOPES = [\'https://mail.google.com/\'];\\n\\nconst {\\n GOOGLE_API_OAUTH2_CREDENTIAL_JSON = \\"\\",\\n GOOGLE_API_REQUIRE_LOGIN_FOR_NEW_TOKEN = \\"\\",\\n GOOGLE_API_CREDENTIAL_JSON = \\"\\",\\n} = process.env;\\nconst OAUTH2_CREDENTIAL_PATH = path.join(process.cwd(), GOOGLE_API_OAUTH2_CREDENTIAL_JSON);\\nconst REQUIRE_LOGIN_FOR_NEW_TOKEN = GOOGLE_API_REQUIRE_LOGIN_FOR_NEW_TOKEN === \\"true\\";\\nconst CREDENTIALS_PATH = path.join(process.cwd(), GOOGLE_API_CREDENTIAL_JSON);\\n\\n\\nLet\'s explain the usage of the 3 environment variables.\\n\\n- GOOGLE_API_OAUTH2_CREDENTIAL_JSON Pointing to the oauth2 credentials relative to the root project level. This file **_must exist_** and should be **_obtained from the prerequisite above_**. \\\\\\n The oauth2 credentials should look like:\\n json\\n {\\n \\"installed\\": {\\n \\"client_id\\": \\"em0kcr5.apps.googleusercontent.com\\",\\n \\"project_id\\": \\"gmailapi-123456\\",\\n \\"auth_uri\\": \\"https://accounts.google.com/o/oauth2/auth\\",\\n \\"token_uri\\": \\"https://oauth2.googleapis.com/token\\",\\n \\"auth_provider_x509_cert_url\\": \\"https://www.googleapis.com/oauth2/v1/certs\\",\\n \\"client_secret\\": \\"secret\\",\\n \\"redirect_uris\\": [\\n \\"http://localhost\\"\\n ]\\n }\\n }\\n \\n- REQUIRE_LOGIN_FOR_NEW_TOKEN=true (initially)\\n - At the first time we run the application we will be asked to authenticate ourself by using authorize function below (line-68).\\n\\n - After authentication succeeds, our login information will be saved in a json file saved at GOOGLE_API_CREDENTIAL_JSON.\\n - We can set REQUIRE_LOGIN_FOR_NEW_TOKEN=false after the first authentication succeeds given that we have made sure the file pointed by GOOGLE_API_CREDENTIAL_JSON exists.\\n- GOOGLE_API_CREDENTIAL_JSON This is the path of the **_login information_** relative to the root project level. Initially **_we don\'t have this file yet_**.\\\\\\n The login credentials should look like:\\n json\\n {\\n \\"type\\": \\"authorized_user\\",\\n \\"client_id\\": \\"em0kcr5.apps.googleusercontent.com\\",\\n \\"client_secret\\": \\"secret\\",\\n \\"refresh_token\\": \\"1//some-string\\"\\n }\\n \\n \\n##### Cache Useful Variables\\n\\nNext let\'s create two variables that cache the variables we created when launching the application:\\n\\njs-19\\nlet authClient: google.Auth.OAuth2Client | null = null;\\nlet loginJson: GoogleUserJson | null = null;\\n\\n\\n##### Function to Send Email\\n\\njs-21\\nconst getCredentialJson = () => {\\n if (loginJson) {\\n return loginJson\\n } else {\\n const content = fs.readFileSync(CREDENTIALS_PATH, { encoding: \\"utf-8\\" });\\n const credentials = JSON.parse(content) as GoogleUserJson;\\n loginJson = credentials;\\n return credentials;\\n }\\n}\\n\\nconst getAuthClient = () => {\\n if (authClient) {\\n return authClient;\\n } else {\\n try {\\n const content = fs.readFileSync(CREDENTIALS_PATH, { encoding: \\"utf-8\\" });\\n const credentials = JSON.parse(content);\\n authClient = new google.Auth.OAuth2Client(\\n credentials.client_id,\\n credentials.client_secret,\\n \'http://localhost\'\\n );\\n return authClient;\\n } catch (err) {\\n return null;\\n }\\n }\\n}\\n\\nconst saveCredentials = (client: google.Auth.OAuth2Client) => {\\n const content = fs.readFileSync(OAUTH2_CREDENTIAL_PATH, { encoding: \\"utf-8\\" });\\n const keys = JSON.parse(content);\\n const key = keys.installed || keys.web;\\n const payload = JSON.stringify({\\n type: \'authorized_user\',\\n client_id: key.client_id,\\n client_secret: key.client_secret,\\n refresh_token: client.credentials.refresh_token,\\n });\\n fs.writeFileSync(CREDENTIALS_PATH, payload);\\n}\\n\\n/**\\n * Load or request or authorization to call APIs.\\n *\\n */\\nasync function authorize() {\\n if (REQUIRE_LOGIN_FOR_NEW_TOKEN) {\\n let client = await getAuthClient();\\n if (client) {\\n return client;\\n }\\n client = await authenticate({\\n scopes: SCOPES,\\n keyfilePath: OAUTH2_CREDENTIAL_PATH,\\n });\\n if (client.credentials) {\\n await saveCredentials(client);\\n }\\n }\\n}\\n\\nconst getTransporter = async () => {\\n const { client_id, client_secret, refresh_token, type } = getCredentialJson();\\n\\n return nodemailer.createTransport({\\n service: \\"gmail\\",\\n auth: {\\n type: \\"OAuth2\\",\\n user: process.env?.GOOGLE_API_EMAIL_SEND_ACCOUNT || \\"\\",\\n clientId: client_id,\\n clientSecret: client_secret,\\n refreshToken: refresh_token\\n }\\n });\\n}\\n\\nconst sendMessage: EmailSender = async ({\\n html, subject, text, to\\n}) => {\\n const t = await getTransporter();\\n t.sendMail({\\n html, subject, text, to\\n })\\n}\\n\\nexport default {\\n authorize,\\n sendMessage\\n}\\n\\n\\n##### Entrypoint for Google Authentication\\n\\nFinally:\\n\\n- Let\'s run await gmailService.authorize() before app.listen(). \\n- Set GOOGLE_API_REQUIRE_LOGIN_FOR_NEW_TOKEN=true for the first time.\\n- Turn it off GOOGLE_API_REQUIRE_LOGIN_FOR_NEW_TOKEN=false from the first time onwards.\\n","title":"Send Google Gmail Without Sendgrid","date":"2023-11-12T00:00:00.000Z","id":"blog0213","tag":"nodejs","intro":"We record how to make use of the native gmail api to send email without sendgrid which cost money with only 200 mails a date in free plan (while gmail api is free)."},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n<Center></Center>\\n\\n#### Repository\\n\\n- https://github.com/machingclee/2023-11-04-go-gin\\n\\n#### When is Go Over Nodejs?\\n\\nWhen IO bound is not a concern, and if we are concerned about the high-concurrency and cpu capability, then go has an advantage that cpu-intensive task **_does not block_**, ensuring the performance of the application.\\n\\nIf our application is full of cpu-bound tasks, then Go is a good choice.\\n\\nFor example, my messaging app needs to format all incoming voice messages from .m4a into .mp3, which is cpu-intensive as users use voice more often the using text messages.\\n\\nWhat if we offload this reformatting task to other microservices?\\n\\n- **To Lambda Services.** It depends on **_how frequent this formatting task is_**. In my use case, the formatting task is one of the main feature of the app, which is **_frequent_**. Designating this task to a lambda can introduce huge cost in the future.\\n\\n- **To Other Backend Service (additional EC2/Fargate Instance).** This is of course a good solution if our task to be offloaded is language-specific.\\n\\n But note that it will also increase the architectual complexity and we don\'t want api-dependencies explosion for no good reason.\\n\\n**_To sum up_**, if our application is **_flooded with_**\\n\\n- cpu-bounded and\\n- non-language-specific tasks,\\n\\nthen just go with golang!\\n\\n#### api\\n\\n##### api/server.go\\n\\ngo\\ntype Server struct {\\n\\tconfig *util.Config\\n\\tstore db.Store\\n\\ttokenMaker token.Maker\\n\\trouter *gin.Engine\\n}\\n\\nfunc NewServer(config *util.Config, store db.Store) (*Server, error) {\\n\\ttokenMaker, err := token.NewJWTMaker(config.TokenSymmetricKey)\\n\\tif err != nil {\\n\\t\\treturn nil, err\\n\\t}\\n\\tserver := &Server{\\n\\t\\tconfig: config,\\n\\t\\tstore: store,\\n\\t\\ttokenMaker: tokenMaker,\\n\\t}\\n\\n\\tserver.setupRouter()\\n\\n\\tif v, ok := binding.Validator.Engine().(*validator.Validate); ok {\\n\\t\\tv.RegisterValidation(\\"currency\\", validCurrency)\\n\\t}\\n\\n\\treturn server, nil\\n}\\n\\nfunc errorResponse(err error) gin.H {\\n\\treturn gin.H{\\"error\\": err.Error()}\\n}\\n\\nfunc (server *Server) Start(address string) error {\\n\\terr := server.router.Run(address)\\n\\treturn err\\n}\\n\\nfunc (server *Server) setupRouter() {\\n\\trouter := gin.Default()\\n\\n\\tuser := router.Group(\\"/user\\")\\n\\tuser.POST(\\"/\\", server.createUser)\\n\\tuser.POST(\\"/login\\", server.loginUser)\\n\\n\\taccount := router.Group(\\"/account\\")\\n\\taccount.Use(authMiddleware(server.tokenMaker))\\n\\taccount.POST(\\"/\\", server.createAccount)\\n\\taccount.POST(\\"/transfers\\", server.createTransfer)\\n\\taccount.GET(\\"/:id\\", server.getAccount)\\n\\taccount.GET(\\"/list\\", server.listAccount)\\n\\n\\tserver.router = router\\n}\\n\\n\\n##### api/middleware.go\\n\\ngo\\nconst (\\n\\tauthorizationHeaderKey = \\"authorization\\"\\n\\tauthorizationType = \\"bearer\\"\\n\\tauthorizationPayloadKey = \\"auth_payload\\"\\n)\\n\\nfunc authMiddleware(tokenMaker token.Maker) gin.HandlerFunc {\\n\\treturn func(ctx *gin.Context) {\\n\\t\\tauthHeader := ctx.GetHeader(authorizationHeaderKey)\\n\\t\\tif len(authHeader) == 0 {\\n\\t\\t\\terr := errors.New(\\"auth header is not provided\\")\\n\\t\\t\\tctx.AbortWithStatusJSON(http.StatusUnauthorized, errorResponse(err))\\n\\t\\t\\treturn\\n\\t\\t}\\n\\n\\t\\tfields := strings.Split(authHeader, \\" \\")\\n\\t\\tif len(fields) < 2 {\\n\\t\\t\\terr := errors.New(\\"invalid authorization header\\")\\n\\t\\t\\tctx.AbortWithStatusJSON(http.StatusUnauthorized, errorResponse(err))\\n\\t\\t\\treturn\\n\\t\\t}\\n\\t\\tauthType := strings.ToLower(fields[0])\\n\\t\\tif authorizationType != authType {\\n\\t\\t\\terr := errors.New(\\"only support beaer token\\")\\n\\t\\t\\tctx.AbortWithStatusJSON(http.StatusUnauthorized, errorResponse(err))\\n\\t\\t\\treturn\\n\\t\\t}\\n\\n\\t\\tauthToken := fields[1]\\n\\t\\tpayload, err := tokenMaker.VerifyToken(authToken)\\n\\t\\tif err != nil {\\n\\t\\t\\tctx.AbortWithStatusJSON(http.StatusUnauthorized, errorResponse(err))\\n\\t\\t\\treturn\\n\\t\\t}\\n\\t\\tctx.Set(authorizationPayloadKey, payload)\\n\\t\\tctx.Next()\\n\\t}\\n}\\n\\n\\n##### api/account.go\\n\\ngo\\npackage api\\n\\nimport (\\n\\t\\"database/sql\\"\\n\\t\\"errors\\"\\n\\t\\"fmt\\"\\n\\t\\"log\\"\\n\\t\\"net/http\\"\\n\\n\\t\\"github.com/gin-gonic/gin\\"\\n\\t\\"github.com/lib/pq\\"\\n\\t\\"github.com/machingclee/2023-11-04-go-gin/internal/db\\"\\n\\t\\"github.com/machingclee/2023-11-04-go-gin/token\\"\\n)\\n\\ntype createAccountRequest struct {\\n\\tOwner string json:\\"owner\\" binding:\\"required\\"\\n\\tCurrency string json:\\"currency\\" binding:\\"required,currency\\"\\n}\\n\\nfunc (server *Server) createAccount(ctx *gin.Context) {\\n\\tvar req createAccountRequest\\n\\n\\tif err := ctx.ShouldBindJSON(&req); err != nil {\\n\\t\\tctx.JSON(http.StatusBadRequest, errorResponse(err))\\n\\t\\treturn\\n\\t}\\n\\n\\tauthPayload := ctx.MustGet(authorizationPayloadKey).(*token.Payload)\\n\\n\\targ := db.CreateAccountParams{\\n\\t\\tOwner: sql.NullString{String: authPayload.Username, Valid: true},\\n\\t\\tCurrency: req.Currency,\\n\\t\\tBalance: 0,\\n\\t}\\n\\n\\taccount, err := server.store.CreateAccount(ctx, arg)\\n\\n\\tif err != nil {\\n\\t\\tif pqErr, ok := err.(*pq.Error); ok {\\n\\t\\t\\tlog.Println(pqErr.Code.Name())\\n\\t\\t}\\n\\t\\tctx.JSON(http.StatusInternalServerError, errorResponse(err))\\n\\t\\treturn\\n\\t}\\n\\n\\tctx.JSON(http.StatusOK, account)\\n}\\n\\ntype getAccountRequest struct {\\n\\tID int64 uri:\\"id\\" binding:\\"required,min=1\\"\\n}\\n\\nfunc (server *Server) getAccount(ctx *gin.Context) {\\n\\tvar req getAccountRequest\\n\\n\\tif err := ctx.ShouldBindUri(&req); err != nil {\\n\\t\\tctx.JSON(http.StatusBadRequest, errorResponse(err))\\n\\t\\treturn\\n\\t}\\n\\n\\tauthPayload := ctx.MustGet(authorizationPayloadKey).(*token.Payload)\\n\\n\\taccount, err := server.store.GetAccount(ctx, req.ID)\\n\\n\\tisOwner := account.Owner.Valid && (account.Owner.String == authPayload.Username)\\n\\n\\tif !isOwner {\\n\\t\\terr := errors.New(\\"Request user is not the account owner\\")\\n\\t\\tctx.JSON(http.StatusUnauthorized, errorResponse(err))\\n\\t}\\n\\n\\tif err != nil {\\n\\t\\tctx.JSON(http.StatusInternalServerError, errorResponse(err))\\n\\t\\treturn\\n\\t}\\n\\n\\tctx.JSON(http.StatusOK, account)\\n}\\n\\ntype listAccountRequest struct {\\n\\tPage int32 form:\\"page\\" binding:\\"required,min=1\\"\\n\\tSize int32 form:\\"size\\" binding:\\"required,min=5,max=100\\"\\n}\\n\\nfunc (server *Server) listAccount(ctx *gin.Context) {\\n\\tvar req listAccountRequest\\n\\n\\tif err := ctx.ShouldBindQuery(&req); err != nil {\\n\\t\\tctx.JSON(http.StatusBadRequest, errorResponse(err))\\n\\t\\treturn\\n\\t}\\n\\n\\tauthPayload := ctx.MustGet(authorizationPayloadKey).(*token.Payload)\\n\\n\\targ := db.ListAccountsParams{\\n\\t\\tOwner: sql.NullString{String: authPayload.Username, Valid: true},\\n\\t\\tLimit: req.Size,\\n\\t\\tOffset: req.Size * (req.Page - 1),\\n\\t}\\n\\n\\tfmt.Println(\\"arg\\", arg)\\n\\n\\taccounts, err := server.store.ListAccounts(ctx, arg)\\n\\tif err != nil {\\n\\t\\tctx.JSON(http.StatusInternalServerError, errorResponse(err))\\n\\t\\treturn\\n\\t}\\n\\n\\tctx.JSON(http.StatusOK, accounts)\\n}\\n\\n\\n#### util/config.go\\n\\nThis essentially captures the values from .env file.\\n\\ngo\\npackage util\\n\\nimport (\\n\\t\\"github.com/spf13/viper\\"\\n\\t\\"time\\"\\n)\\n\\ntype Config struct {\\n\\tDBDriver string mapstructure:\\"DB_DRIVER\\"\\n\\tDBSource string mapstructure:\\"DB_SOURCE\\"\\n\\tServerAddress string mapstructure:\\"SERVER_ADDRESS\\"\\n\\tTokenSymmetricKey string mapstructure:\\"TOKEN_SYMMETRIC_KEY\\"\\n\\tAccessTokenDuration time.Duration mapstructure:\\"ACCESS_TOKEN_DURATION\\"\\n}\\n\\nfunc LoadConfig(parentDir string) (*Config, error) {\\n\\tviper.AddConfigPath(parentDir)\\n\\tviper.SetConfigName(\\"app\\")\\n\\tviper.SetConfigType(\\"env\\")\\n\\n\\tviper.AutomaticEnv()\\n\\n\\terr := viper.ReadInConfig()\\n\\tif err != nil {\\n\\t\\treturn nil, err\\n\\t}\\n\\tconfig := Config{}\\n\\terr = viper.Unmarshal(&config)\\n\\tif err != nil {\\n\\t\\treturn nil, err\\n\\t}\\n\\treturn &config, nil\\n}\\n\\n\\n#### internal/db/store.go\\n\\ngo\\ntype SQLStore struct {\\n\\t*Queries\\n\\tdb *sql.DB\\n}\\n\\nfunc NewStore(db *sql.DB) Store {\\n\\treturn &SQLStore{\\n\\t\\tdb: db,\\n\\t\\tQueries: New(db),\\n\\t}\\n}\\n\\n\\nHere both Queries and New come from db.go generated from sqlc, which we don\'t have much to care.\\n\\n#### token\\n\\n##### token/maker.go, The Interface\\n\\nOur file just contain an interface, as apart from jwt-token, we also use paseto-token in Golang world (which we don\'t introduce here).\\n\\ngo\\npackage token\\n\\nimport \\"time\\"\\n\\ntype Maker interface {\\n\\tCreateToken(username string, duration time.Duration) (string, error)\\n\\tVerifyToken(token string) (*Payload, error)\\n}\\n\\n\\n##### token/jwt_maker.go, The Implementation\\n\\ngo\\npackage token\\n\\nimport (\\n\\t\\"errors\\"\\n\\t\\"fmt\\"\\n\\t\\"time\\"\\n\\n\\t\\"github.com/golang-jwt/jwt/v5\\"\\n)\\n\\nconst minSecretKeySize = 10\\n\\ntype JWTMaker struct {\\n\\tsecretKey string\\n}\\n\\nfunc NewJWTMaker(secretKey string) (Maker, error) {\\n\\tif len(secretKey) < minSecretKeySize {\\n\\t\\treturn nil, fmt.Errorf(\\"invalid key size:, must be at least %d characters\\", minSecretKeySize)\\n\\t}\\n\\treturn &JWTMaker{secretKey}, nil\\n}\\n\\nfunc (jwtMaker *JWTMaker) CreateToken(username string, duration time.Duration) (string, error) {\\n\\tpayload, err := NewPayload(username, duration)\\n\\n\\tif err != nil {\\n\\t\\treturn \\"\\", err\\n\\t}\\n\\n\\tjwtToken := jwt.NewWithClaims(jwt.SigningMethodHS256, payload)\\n\\treturn jwtToken.SignedString([]byte(jwtMaker.secretKey))\\n}\\n\\nfunc (jwtMaker *JWTMaker) VerifyToken(token string) (*Payload, error) {\\n\\tkeyFunc := func(token *jwt.Token) (interface{}, error) {\\n\\t\\t_, ok := token.Method.(*jwt.SigningMethodHMAC)\\n\\t\\tif !ok {\\n\\t\\t\\treturn nil, ErrInvalidToken\\n\\t\\t}\\n\\t\\treturn []byte(jwtMaker.secretKey), nil\\n\\t}\\n\\tjwtToken, err := jwt.ParseWithClaims(token, &Payload{}, keyFunc)\\n\\n\\tif err != nil {\\n\\t\\treturn nil, errors.New(err.Error())\\n\\t}\\n\\n\\tpayload, ok := jwtToken.Claims.(*Payload)\\n\\tif !ok {\\n\\t\\treturn nil, ErrInvalidToken\\n\\t}\\n\\n\\treturn payload, nil\\n}\\n\\n\\n##### token/payload.go\\n\\ngo\\npackage token\\n\\nimport (\\n\\t\\"errors\\"\\n\\t\\"time\\"\\n\\n\\tjwt \\"github.com/golang-jwt/jwt/v5\\"\\n\\t\\"github.com/google/uuid\\"\\n)\\n\\nvar (\\n\\tErrInvalidToken = errors.New(\\"Invalid Token\\")\\n\\tErrExpiredToken = errors.New(\\"token has invalid claims: token is expired\\")\\n)\\n\\ntype Payload struct {\\n\\tID uuid.UUID json:\\"id\\"\\n\\tUsername string json:\\"username\\"\\n\\tIssuedAt time.Time json:\\"issued_at\\"\\n\\tExpiredAt time.Time json:\\"expired_at\\"\\n}\\n\\nfunc (*Payload) GetAudience() (jwt.ClaimStrings, error) {\\n\\treturn []string{\\"james-auth\\"}, nil\\n}\\n\\nfunc (p *Payload) GetExpirationTime() (*jwt.NumericDate, error) {\\n\\treturn jwt.NewNumericDate(p.ExpiredAt), nil\\n}\\n\\nfunc (p *Payload) GetIssuedAt() (*jwt.NumericDate, error) {\\n\\treturn jwt.NewNumericDate(p.IssuedAt), nil\\n}\\n\\nfunc (*Payload) GetIssuer() (string, error) {\\n\\treturn \\"James\\", nil\\n}\\n\\nfunc (p *Payload) GetNotBefore() (*jwt.NumericDate, error) {\\n\\treturn jwt.NewNumericDate(p.IssuedAt), nil\\n}\\n\\nfunc (p *Payload) GetSubject() (string, error) {\\n\\treturn p.Username, nil\\n}\\n\\nfunc NewPayload(username string, duration time.Duration) (*Payload, error) {\\n\\ttokenID, err := uuid.NewRandom()\\n\\tif err != nil {\\n\\t\\treturn nil, err\\n\\t}\\n\\n\\tpayload := &Payload{\\n\\t\\tID: tokenID,\\n\\t\\tUsername: username,\\n\\t\\tIssuedAt: time.Now(),\\n\\t\\tExpiredAt: time.Now().Add(duration),\\n\\t}\\n\\n\\treturn payload, nil\\n}\\n\\n","title":"Complete Golang Project Structure","date":"2023-11-13T00:00:00.000Z","id":"blog0214","tag":"go","intro":"We record how to create a complete web application project in go."},{"content":"\\nshell\\ndocker run golang:1.21.1-alpine sleep infinity\\n\\n","title":"Docker Run Indefinitely","date":"2023-06-25T00:00:00.000Z","id":"blog0215","tag":"docker","intro":"We record a command to run an image in non-stop mode for debugging.","toc":false},{"content":"\\n#### Simple Case for a Go Backend\\n\\ndocker\\n# Build Stage\\nFROM golang:1.21.1-alpine3.18 AS builder\\nWORKDIR /app\\nCOPY . .\\nRUN go build -o main main.go\\n\\n# Run Stage\\nFROM alpine:3.18\\nWORKDIR /app\\nCOPY --from=builder /app/main .\\nCOPY app.env .\\n\\nEXPOSE 8080\\n\\nCMD [\\"/app/main\\"]\\n\\n\\n#### More Sophisticated case Where PGSql DB and Migration are Considered\\n\\n##### Dockerize The Go Backend\\n\\nLete\'s build our go programme as well as download the goose binary.\\n\\ndocker\\n# Build Stage\\nFROM golang:1.21.1-alpine3.18 AS builder\\nWORKDIR /app\\nCOPY . .\\nRUN go install github.com/pressly/goose/v3/cmd/goose@latest\\nRUN go build -o main main.go\\n\\n# Run Stage\\nFROM alpine:3.18\\nWORKDIR /app\\nCOPY --from=builder /app/main .\\nRUN mkdir -p /go/bin\\nCOPY --from=builder /go/bin/goose /go/bin\\nCOPY sql ./sql\\nCOPY app.env .\\nCOPY wait-for.sh .\\nCOPY docker-compose-start.sh .\\nRUN chmod +x wait-for.sh\\n\\nEXPOSE 8080\\n\\nCMD [\\"/app/main\\"]\\n\\n\\n- Here wait-for.sh is downloaded from [here](https://github.com/eficode/wait-for/releases/tag/v2.2.4?fbclid=IwAR1suC95oCkB63bRmBcUOcPuLi5MC4GV6JGJ_41qs-Ou2JsomDSv7CQzSYI)\\n- docker-compose-start.sh is simply the following:\\n bash\\n cd /app/sql/schema\\n /go/bin/goose postgres $DB_SOURCE up\\n cd /app\\n /app/main\\n \\n Note that the goose binary is copied form builder stage.\\n\\n##### Use wait-for.sh in our docker-compose.yml\\n\\nNote that depends_on in docker-compose has no guarantee on launching service B until service A is **_completely up_**.\\n\\nIf we want to make sure our docker-compose is run in the following order:\\n\\n1. PGSQL db is up and running, then\\n2. Run db migration and web application in Go from our docker image.\\n\\nThen we need the wait-for.sh script downloaded in the previous section, and achieve the correct sequential order by:\\n\\nyaml\\nversion: \\"2\\"\\nservices:\\n postgres:\\n image: postgres\\n volumes:\\n - ./data/db:/var/lib/postgresql/data\\n ports:\\n - \\"5432:5432\\"\\n environment:\\n POSTGRES_USER: pguser\\n POSTGRES_PASSWORD: pguser\\n POSTGRES_DB: pgdb\\n api:\\n build:\\n context: .\\n dockerfile: Dockerfile\\n ports:\\n - \\"8080:8080\\"\\n environment:\\n DB_SOURCE: postgresql://pguser:pguser@postgres:5432/pgdb?sslmode=disable\\n depends_on:\\n - postgres\\n entrypoint:\\n [\\n \\"/app/wait-for.sh\\",\\n \\"postgres:5432\\",\\n \\"--\\",\\n \\"/app/docker-compose-start.sh\\",\\n ]\\n\\n\\nNote that entrypoint will override the default entrypoint in the docker image and clear the existing CMD.\\n\\nResult:\\n\\nbash\\npostgres_1 | PostgreSQL Database directory appears to contain a database; Skipping initialization\\npostgres_1 |\\npostgres_1 | 2023-11-13 14:34:03.736 UTC [1] LOG: starting PostgreSQL 16.0 (Debian 16.0-1.pgdg120+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit\\npostgres_1 | 2023-11-13 14:34:03.736 UTC [1] LOG: listening on IPv4 address \\"0.0.0.0\\", port 5432\\npostgres_1 | 2023-11-13 14:34:03.736 UTC [1] LOG: listening on IPv6 address \\"::\\", port 5432\\npostgres_1 | 2023-11-13 14:34:03.746 UTC [1] LOG: listening on Unix socket \\"/var/run/postgresql/.s.PGSQL.5432\\"\\npostgres_1 | 2023-11-13 14:34:03.770 UTC [30] LOG: database system was shut down at 2023-11-13 14:33:04 UTC\\npostgres_1 | 2023-11-13 14:34:03.803 UTC [1] LOG: database system is ready to accept connections\\napi_1 | 2023/11/13 14:34:04 goose: no migrations to run. current version: 4\\napi_1 | [GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\\napi_1 |\\napi_1 | [GIN-debug] [WARNING] Running in \\"debug\\" mode. Switch to \\"release\\" mode in production.\\napi_1 | - using env: export GIN_MODE=release\\napi_1 | - using code: gin.SetMode(gin.ReleaseMode)\\napi_1 | [GIN-debug] POST /account/transfers --\x3e github.com/machingclee/2023-11-04-go-gin/api.(*Server).createTransfer-fm (4 handlers)\\napi_1 | [GIN-debug] GET /account/:id --\x3e github.com/machingclee/2023-11-04-go-gin/api.(*Server).getAccount-fm (4 handlers)\\napi_1 | [GIN-debug] GET /account/list --\x3e github.com/machingclee/2023-11-04-go-gin/api.(*Server).listAccount-fm (4 handlers)\\napi_1 | [GIN-debug] [WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\napi_1 | Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\\n\\n","title":"Two Stage Docker Image; Docker-Compose with Service B waiting for the Connection of Service A","date":"2023-11-14T00:00:00.000Z","id":"blog0216","tag":"docker, go, sql","intro":"We build smaller docker image by copying the only used components into new docker image","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px;\\n }\\n video {\\n border-radius: 8px;\\n }\\n</style>\\n\\n<Center></Center>\\n\\n#### Local Aws Secret as Environment Variables\\n\\nIn AWS Secrets Manager we create a secret named simple_bank_local (create also the env simple_bank_dev and simple_bank_prod if they exist in the future).\\n\\nNext, create key-value pairs in the course of creating the secret.\\n\\n[![](/assets/tech/217/image.png)](/assets/tech/217/image.png)\\n\\nThen we load our secret by aws-sdk:\\n\\ngo\\npackage util\\n\\nimport (\\n\\t\\"bytes\\"\\n\\t\\"context\\"\\n\\t\\"encoding/json\\"\\n\\t\\"fmt\\"\\n\\t\\"log\\"\\n\\t\\"os\\"\\n\\t\\"os/exec\\"\\n\\t\\"path/filepath\\"\\n\\t\\"runtime\\"\\n\\t\\"time\\"\\n\\n\\t\\"github.com/aws/aws-sdk-go-v2/aws\\"\\n\\t\\"github.com/aws/aws-sdk-go-v2/config\\"\\n\\t\\"github.com/aws/aws-sdk-go-v2/service/secretsmanager\\"\\n)\\n\\ntype Env struct {\\n\\tDBDriver string json:\\"DB_DRIVER\\"\\n\\tDBSource string json:\\"DB_SOURCE\\"\\n\\tServerAddress string json:\\"SERVER_ADDRESS\\"\\n\\tTokenSymmetricKey string json:\\"TOKEN_SYMMETRIC_KEY\\"\\n\\tAccessTokenDuration time.Duration json:\\"ACCESS_TOKEN_DURATION\\"\\n}\\n\\ntype intermediateEnv struct {\\n\\tAccessTokenDuration string json:\\"ACCESS_TOKEN_DURATION\\"\\n}\\n\\nvar ENV *Env = nil\\n\\nfunc LoadEnv() (*Env, error) {\\n\\tif ENV != nil {\\n\\t\\treturn ENV, nil\\n\\t}\\n\\tenv := \\"local\\"\\n\\tenv_override := os.Getenv(\\"env\\")\\n\\tif env_override != \\"\\" {\\n\\t\\tenv = env_override\\n\\t}\\n\\n\\tsecretName := fmt.Sprintf(\\"simple_bank_%s\\", env)\\n\\tregion := \\"ap-northeast-1\\"\\n\\n\\tfmt.Printf(\\"Getting Environment variables [%s] from aws screts ... \\", secretName)\\n\\n\\tconfig, err := config.LoadDefaultConfig(context.TODO(), config.WithRegion(region))\\n\\tif err != nil {\\n\\t\\tlog.Fatal(err)\\n\\t}\\n\\tsvc := secretsmanager.NewFromConfig(config)\\n\\tinput := &secretsmanager.GetSecretValueInput{\\n\\t\\tSecretId: aws.String(secretName),\\n\\t\\tVersionStage: aws.String(\\"AWSCURRENT\\"),\\n\\t}\\n\\tresult, err := svc.GetSecretValue(context.TODO(), input)\\n\\tif err != nil {\\n\\t\\tlog.Fatal(err.Error())\\n\\t}\\n\\tvar secretString string = *result.SecretString\\n\\tENV = &Env{}\\n\\tenvIntermediate := &intermediateEnv{}\\n\\tjson.Unmarshal([]byte(secretString), ENV)\\n\\tjson.Unmarshal([]byte(secretString), envIntermediate)\\n\\n\\tENV.AccessTokenDuration, err = time.ParseDuration(envIntermediate.AccessTokenDuration)\\n\\tif err != nil {\\n\\t\\tlog.Fatal(err)\\n\\t}\\n\\n\\tfmt.Println(\\"Env variables retrieved\\")\\n\\treturn ENV, nil\\n}\\n\\n\\nEveryone who has access to this aws resource can start the project without needing to pass the env file in an ad-hoc manner.\\n\\nNow we run the programme locally by\\n\\ntext\\nenv=local main\\n\\n\\nNote that we usually build an image via RUN go build -o main main.go that contains only the binary named main, and hence we use the command above.\\n\\n#### Execute Shell Script Using Aws Secrets with DB Schema Migration as a Sample\\n\\ngo-1\\nfunc RunDbMigration() error {\\n\\tworkingDir, err := os.Getwd()\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tenv, err := LoadEnv()\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\tfmt.Println(\\"Running Migration Script\\")\\n\\n\\nThe strategy is simply setting desired secret into environment variable\\n\\ngo-11\\n\\tos.Setenv(\\"DB_URL\\", env.DBSource)\\n\\n\\nand then run the shell script:\\n\\ngo-12\\n\\tmigrationScriptPath := filepath.Join(workingDir, \\"script_db_migrate_up.sh\\")\\n\\n\\tvar cmd *exec.Cmd\\n\\n\\tif runtime.GOOS == \\"windows\\" {\\n\\t\\tvar winBash string = C:\\\\Program Files\\\\Git\\\\usr\\\\bin\\\\sh.exe\\n\\t\\tif where := os.Getenv(\\"bin_where\\"); where != \\"\\" {\\n\\t\\t\\twinBash = where\\n\\t\\t}\\n\\t\\tcmd = exec.Command(winBash, migrationScriptPath)\\n\\t} else {\\n\\t\\tcmd = exec.Command(migrationScriptPath)\\n\\t}\\n\\n\\nNext we print the std outputs from the shell script.\\n\\ngo-25\\n\\tvar outb, errb bytes.Buffer\\n\\tcmd.Stdout = &outb\\n\\tcmd.Stderr = &errb\\n\\n\\terr = cmd.Run()\\n\\n\\tif err != nil {\\n\\t\\treturn err\\n\\t}\\n\\n\\tfmt.Println(\\"out:\\", outb.String())\\n\\tfmt.Println(\\"err:\\", errb.String())\\n\\n\\treturn nil\\n}\\n\\n\\nThe std output of my web-app:\\n\\nshell\\n$ go run main.go\\ngo run main.go\\nGetting Environment variables [simple_bank_local] from aws screts ... Env variables retrieved\\nRunning Migration Script\\nout:\\nerr: 2023/11/16 02:07:51 goose: no migrations to run. current version: 5\\n\\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\\n\\n[GIN-debug] [WARNING] Running in \\"debug\\" mode. Switch to \\"release\\" mode in production.\\n - using env: export GIN_MODE=release\\n - using code: gin.SetMode(gin.ReleaseMode)\\n\\n[GIN-debug] POST /user/ --\x3e github.com/machingclee/2023-11-04-go-gin/api.(*Server).createUser-fm (3 handlers)\\n[GIN-debug] POST /user/login --\x3e github.com/machingclee/2023-11-04-go-gin/api.(*Server).loginUser-fm (3 handlers)\\n[GIN-debug] POST /account/ --\x3e github.com/machingclee/2023-11-04-go-gin/api.(*Server).createAccount-fm (4 handlers)\\n[GIN-debug] POST /account/transfers --\x3e github.com/machingclee/2023-11-04-go-gin/api.(*Server).createTransfer-fm (4 handlers)\\n[GIN-debug] GET /account/:id --\x3e github.com/machingclee/2023-11-04-go-gin/api.(*Server).getAccount-fm (4 handlers)\\n[GIN-debug] GET /account/list --\x3e github.com/machingclee/2023-11-04-go-gin/api.(*Server).listAccount-fm (4 handlers)\\n[GIN-debug] [WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\nPlease check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\\n[GIN-debug] Listening and serving HTTP on :8080\\n\\n\\nSurprisingly even no migrations to run. current version: 5 is not an error, our migration message still comes from stderr.\\n","title":"Load Environment Variables from Aws Secret Managers and Run Shell Script in Go for DB Schema Migration Based on DB_URL from Secrets","date":"2023-11-15T00:00:00.000Z","id":"blog0217","tag":"go","intro":"Methods of sharing environment variabls has always been being diversified. Some may just keep the .env in the project. Some may share it via instant messagers. Some may store it in a webpage that require a login. We share a method that should be a norm for intense aws users!","toc":true},{"content":"\\n#### Tables\\n\\n###### Create Standard Table\\n\\nsql\\nCREATE TABLE \\"accounts\\" (\\n \\"id\\" bigserial PRIMARY KEY, -- or \\"id\\" uuid PRIMARY KEY\\n \\"owner\\" varchar NOT NULL,\\n \\"balance\\" bigint NOT NULL,\\n \\"is_blocked\\" boolean NOT NULL DEFAULT false,\\n \\"currency\\" varchar NOT NULL,\\n \\"created_at\\" timestamptz NOT NULL DEFAULT (now())\\n);\\n\\n\\n###### Create Index\\n\\nsql\\nCREATE INDEX ON \\"accounts\\" (\\"owner\\");\\n\\n\\n###### Create Unique Index (prevent row with duplicate field)\\n\\nsql\\nCREATE UNIQUE INDEX ON \\"accounts\\" (\\"owner\\", \\"currency\\");\\n\\n\\n###### Create Join Index\\n\\nsql\\nCREATE INDEX ON \\"transfers\\" (\\"from_account_id\\", \\"to_account_id\\");\\n\\n\\n###### Add Foreign Key\\n\\nsql\\nALTER TABLE \\"tableA\\" ADD FOREIGN KEY (\\"tableB_id\\") REFERENCES \\"tableB\\" (\\"id\\");\\n\\n\\n###### Drop Foreign Key\\n\\nsql\\nALTER TABLE IF EXISTS \\"table_name\\" DROP CONTRAINT IF EXISTS \\"tableA_tableB_fkey\\";\\n\\n\\n###### Set a Field to NOT NULL or NULL\\n\\nsql\\nALTER TABLE \\"table_name\\" ALTER COLUMN \\"field\\" SET NOT NULL;\\nALTER TABLE \\"table_name\\" ALTER COLUMN \\"field\\" DROP NOT NULL;\\n\\n\\n###### Add a Column\\n\\nsql\\nALTER TABLE \\"table_name\\" ADD \\"field\\" varchar;\\n\\n\\n###### Drop a Column\\n\\nsql\\nALTER TABLE \\"table_name\\" DROP COLUMN \\"field\\";\\n\\n\\n###### Rename a Column\\n\\nsql\\nALTER TABLE \\"table_name\\" RENAME COLUMN \\"fieldA\\" to \\"fieldB\\";\\n\\n\\n#### Models\\n\\n###### Create\\n\\nsql\\n-- name: CreateUser :one\\nINSERT INTO users (\\n username, email, passowrdHash\\n) VALUES (\\n $1, $2, $3\\n)\\nRETURNING *;\\n\\n\\n###### Get One\\n\\nsql\\n-- name: GetAccount :one\\nSELECT * FROM accounts\\nWHERE id = $1 LIMIT 1;\\n\\n\\n###### Get Many\\n\\nsql\\n-- name: ListAccounts :many\\nSELECT * FROM accounts\\nWHERE owner = $1\\nORDER BY id\\nLIMIT $2\\nOFFSET $3;\\n\\n\\n###### Update One\\n\\nsql\\n-- name: UpdateAccount :one\\nUPDATE accounts\\nSET balance = $2\\nWHERE id = $1\\nRETURNING *;\\n\\n\\n###### Delete One\\n\\nsql\\n-- name: DeleteAccount :exec\\nDELETE FROM accounts WHERE id = $1;\\n\\n\\n###### Do Math in SQL\\n\\nsql\\n-- name: AddAccountBalance :one\\nUPDATE accounts\\nSET balance = balance + sqlc.arg(amount)\\nWHERE id = sqlc.arg(id)\\nRETURNING *;\\n\\n","title":"Record Table Migration Script in SQL","date":"2023-11-17T00:00:00.000Z","id":"blog0218","tag":"sql","intro":"A list of sql script for copy and paste.","toc":true},{"content":"\\n#### Use Scala\\n\\nOpen [this github project](https://github.com/gatling/gatling-maven-plugin-demo-scala) in **_intelliJ_** and install everything that we are asked to install.\\n\\n<p></p>\\n\\n<a href=\\"/assets/tech/219/01.png\\"><img src=\\"/assets/tech/219/01.png\\" width=\\"340\\"></a></Center>\\n\\n<p></p>\\n<center></center>\\n\\nAnd the following should be shown in the console:\\n\\n<p></p>\\n\\n<a href=\\"/assets/tech/219/02.png\\"><img src=\\"/assets/tech/219/02.png\\" width=\\"420\\"></a></Center>\\n\\n<p></p>\\n<center></center>\\n\\nUp to this point we are sure that installation is completed.\\n\\n#### Custom Script\\n\\nNow follow this architecture\\n\\n<p></p>\\n\\n<a href=\\"/assets/tech/219/03.png\\"><img src=\\"/assets/tech/219/03.png\\" width=\\"400\\"></a></Center>\\n\\n<p></p>\\n<center></center>\\n\\nAnd create our first simiulation script:\\n\\nscala\\npackage videogamedb\\n\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nclass MyFirstTest extends Simulation\\n{\\n //http config\\n val httpProtocal = http.baseUrl(url=\\"https://videogamedb.uk/api\\")\\n .acceptHeader(value=\\"application/json\\")\\n\\n // scenario def\\n val scn = scenario(name=\\"My First Test\\")\\n .exec(\\n http(\\"Get all games\\")\\n .get(\\"/videogame\\")\\n )\\n\\n // load scenario\\n setUp(scn.inject(atOnceUsers(1))).protocols(httpProtocal)\\n}\\n\\n\\nNow run our engine again, as usual java framework try to grab all Simulation class files **_in a blackbox_** secretly. Once we type test, we get:\\n\\nshell\\nvideogamedb.MyFirstTest is the only simulation, executing it.\\nSelect run description (optional)\\ntest\\nSimulation videogamedb.MyFirstTest started...\\n\\n================================================================================\\n2023-11-19 19:01:19 0s elapsed\\n---- Requests ------------------------------------------------------------------\\n> Global (OK=1 KO=0 )\\n> Get all games (OK=1 KO=0 )\\n\\n---- My First Test -------------------------------------------------------------\\n[##########################################################################]100%\\n waiting: 0 / active: 0 / done: 1\\n================================================================================\\n\\nSimulation videogamedb.MyFirstTest completed in 0 seconds\\nParsing log file(s)...\\nParsing log file(s) done\\nGenerating reports...\\n\\n================================================================================\\n---- Global Information --------------------------------------------------------\\n> request count 1 (OK=1 KO=0 )\\n> min response time 721 (OK=721 KO=- )\\n> max response time 721 (OK=721 KO=- )\\n> mean response time 721 (OK=721 KO=- )\\n> std deviation 0 (OK=0 KO=- )\\n> response time 50th percentile 721 (OK=721 KO=- )\\n> response time 75th percentile 721 (OK=721 KO=- )\\n> response time 95th percentile 721 (OK=721 KO=- )\\n> response time 99th percentile 721 (OK=721 KO=- )\\n> mean requests/sec 1 (OK=1 KO=- )\\n---- Response Time Distribution ------------------------------------------------\\n> t < 800 ms 1 (100%)\\n> 800 ms <= t < 1200 ms 0 ( 0%)\\n> t >= 1200 ms 0 ( 0%)\\n> failed 0 ( 0%)\\n================================================================================\\n\\nReports generated in 0s.\\nPlease open the following file: file:///C:/Users/user/Repos/gatling/2023-11-19-gatling-maven-plugin-demo-scala-main/target/gatling/myfirsttest-20231119110117759/index.html\\n\\nProcess finished with exit code 0\\n\\n\\nWe also get a full review in an html file:\\n\\n<p></p>\\n\\n<a href=\\"/assets/tech/219/04.png\\"><img src=\\"/assets/tech/219/04.png\\" width=\\"600\\"></a></Center>\\n\\n<p></p>\\n<center></center>\\n\\n#### Scripts Fundamental in Gatling\\n\\n##### Pause\\n\\nscala\\npackage videogamedb.scriptfundamentals\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nimport scala.concurrent.duration.DurationInt\\n\\nclass AddPauseTime extends Simulation{\\n val httpProtocol = http.baseUrl(\\"https://videogamedb.uk/api\\")\\n .acceptHeader(\\"application/json\\")\\n\\n val scn = scenario(\\"Video Game DB - 3 calls\\")\\n .exec(http(\\"Get all video games\\").get(\\"/videogame\\"))\\n .pause(5)\\n\\n .exec(http(\\"Get specific game\\").get(\\"/videogame/1\\"))\\n .pause(1, 10) // random pause time between 1 and 10\\n\\n .exec(http(\\"Get all video games - 2nd call\\").get(\\"/videogame\\"))\\n .pause(3000.milliseconds)\\n\\n setUp(scn.inject(atOnceUsers(users=1)).protocols(httpProtocol))\\n}\\n\\n\\n##### Check Response Code\\n\\nscala\\npackage videogamedb.scriptfundamentals\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nimport scala.concurrent.duration.DurationInt\\n\\nclass CheckResponseCode extends Simulation{\\n val httpProtocol = http.baseUrl(\\"https://videogamedb.uk/api\\")\\n .acceptHeader(\\"application/json\\")\\n\\n val scn = scenario(\\"Video Game DB - 3 calls\\")\\n .exec(\\n http(\\"Get all video games\\")\\n .get(\\"/videogame\\")\\n .check(status.is(200))\\n )\\n .pause(5)\\n\\n .exec(http(\\"Get specific game\\")\\n .get(\\"/videogame/1\\")\\n .check(status.in(200 to 210))\\n )\\n .pause(1, 10) // random pause time between 1 and 10\\n\\n .exec(http(\\"Get all video games - 2nd call\\")\\n .get(\\"/videogame\\")\\n .check(status.not(404), status.not(500))\\n )\\n .pause(3000.milliseconds)\\n\\n setUp(scn.inject(atOnceUsers(users=1)).protocols(httpProtocol))\\n}\\n\\n\\n##### Check Response Body\\n\\nscala\\npackage videogamedb.scriptfundamentals\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nimport scala.concurrent.duration.DurationInt\\n\\nclass CheckResponseBodyAndExtract extends Simulation{\\n val httpProtocol = http.baseUrl(\\"https://videogamedb.uk/api\\")\\n .acceptHeader(\\"application/json\\")\\n\\n val scn = scenario(\\"Check with JSON path\\").exec(\\n http(\\"Get specific game\\")\\n .get(\\"/videogame/1\\")\\n .check(jsonPath(\\"$.name\\").is(\\"Resident Evil 4\\"))\\n ).exec()\\n\\n\\n setUp(scn.inject(atOnceUsers(users=1)).protocols(httpProtocol))\\n}\\n\\n\\n##### Save the Result in Variable and Reuse it\\n\\nscala\\npackage videogamedb.scriptfundamentals\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nimport scala.concurrent.duration.DurationInt\\n\\nclass CheckResponseBodyAndExtract extends Simulation{\\n val httpProtocol = http.baseUrl(\\"https://videogamedb.uk/api\\")\\n .acceptHeader(\\"application/json\\")\\n\\n val scn = scenario(\\"Check with JSON path\\")\\n .exec(http(\\"Get specific game\\")\\n .get(\\"/videogame/1\\")\\n .check(jsonPath(\\"$.name\\").is(\\"Resident Evil 4\\"))\\n )\\n\\n .exec(http(\\"Get all video games\\")\\n .get(\\"/videogame\\")\\n .check(jsonPath(path=\\"$[1].id\\").saveAs(\\"gameId\\"))\\n )\\n\\n .exec(http(\\"Get specific game\\")\\n .get(\\"/videogame/#{gameId}\\")\\n .check(jsonPath(\\"$.name\\").is(\\"Gran Turismo 3\\"))\\n )\\n\\n setUp(scn.inject(atOnceUsers(users=1)).protocols(httpProtocol))\\n}\\n\\n\\n##### Logging for Debugging\\n\\nscala\\npackage videogamedb.scriptfundamentals\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nimport scala.concurrent.duration.DurationInt\\n\\nclass CheckResponseBodyAndExtract extends Simulation{\\n val httpProtocol = http.baseUrl(\\"https://videogamedb.uk/api\\")\\n .acceptHeader(\\"application/json\\")\\n\\n val scn = scenario(\\"Check with JSON path\\")\\n .exec(http(\\"Get specific game\\")\\n .get(\\"/videogame/1\\")\\n .check(jsonPath(\\"$.name\\").is(\\"Resident Evil 4\\"))\\n )\\n\\n .exec(http(\\"Get all video games\\")\\n .get(\\"/videogame\\")\\n .check(jsonPath(path=\\"$[1].id\\").saveAs(\\"gameId\\"))\\n )\\n .exec { session => println(session); session}\\n\\n .exec(http(\\"Get specific game\\")\\n .get(\\"/videogame/#{gameId}\\")\\n .check(jsonPath(\\"$.name\\").is(\\"Gran Turismo 3\\"))\\n .check(bodyString.saveAs(\\"responseBody\\"))\\n )\\n .exec { session => println(session(\\"responseBody\\").as[String]); session}\\n\\n setUp(scn.inject(atOnceUsers(users=1)).protocols(httpProtocol))\\n}\\n\\n\\n##### Code Reuse\\n\\nscala\\npackage videogamedb.scriptfundamentals\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nclass CodeReuse extends Simulation {\\n val httpProtocol = http.baseUrl(\\"https://videogamedb.uk/api\\")\\n .acceptHeader(\\"application/json\\")\\n\\n def getAllVideoGames() = {\\n exec(\\n http(\\"Get all video games\\")\\n .get(\\"/videogame\\")\\n .check(status.is(200))\\n )\\n }\\n\\n def getSpecificGame() = {\\n exec(\\n http(\\"Get specific game\\")\\n .get(\\"/videogame/1\\")\\n .check(status.in(200 to 210))\\n )\\n }\\n\\n val scn = scenario(\\"Code reuse\\")\\n .exec(getAllVideoGames())\\n .pause(5)\\n .exec(getSpecificGame())\\n .pause(5)\\n .exec(getAllVideoGames())\\n\\n setUp(scn.inject(atOnceUsers(users=1)).protocols(httpProtocol))\\n}\\n\\n\\n##### Repeat Requests Several Times (Single User)\\n\\nscala\\npackage videogamedb.scriptfundamentals\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nclass CodeReuse extends Simulation {\\n val httpProtocol = http.baseUrl(\\"https://videogamedb.uk/api\\")\\n .acceptHeader(\\"application/json\\")\\n\\n def getAllVideoGames() = {\\n repeat(3){\\n exec(\\n http(\\"Get all video games\\")\\n .get(\\"/videogame\\")\\n .check(status.is(200))\\n )\\n }\\n }\\n\\n def getSpecificGame() = {\\n repeat(5, \\"counter\\"){\\n exec(\\n http(s\\"Get specific game with id: #{counter}\\")\\n .get(\\"/videogame/#{counter}\\")\\n .check(status.in(200 to 210))\\n )\\n }\\n }\\n\\n val scn = scenario(\\"Code reuse\\")\\n .exec(getAllVideoGames())\\n .pause(5)\\n .exec(getSpecificGame())\\n .pause(5)\\n .repeat(2){\\n getAllVideoGames()\\n }\\n\\n setUp(scn.inject(atOnceUsers(users=1)).protocols(httpProtocol))\\n}\\n\\n\\n##### Authenticate and Use that Token for Post Request\\n\\nThe authentication post request returns the following json\\n\\ntext\\n{\\n \\"token\\": \\"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJhZG1pbiIsImlhdCI6MTcwMDQwMjQ0MCwiZXhwIjoxNzAwNDA2MDQwfQ.MgyULBwMTd_Kj13E7UwrMALNctO6NUTL9qxS_sOk39k\\"\\n}\\n\\n\\nWe can use jsonPath $.token to extract this value and use it elsewhere.\\n\\nscala\\npackage videogamedb.scriptfundamentals\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nclass Authenticate extends Simulation {\\n val httpProtocol = http.baseUrl(\\"https://videogamedb.uk/api\\")\\n .acceptHeader(\\"application/json\\")\\n .contentTypeHeader(\\"application/json\\")\\n\\n def authenticate() = {\\n exec(http(\\"Authenticate\\")\\n .post(\\"/authenticate\\")\\n .body(StringBody(\\"{\\\\n \\\\\\"password\\\\\\": \\\\\\"admin\\\\\\",\\\\n \\\\\\"username\\\\\\": \\\\\\"admin\\\\\\"\\\\n}\\"))\\n .check(jsonPath(\\"$.token\\").saveAs(\\"jwtToken\\"))\\n )\\n }\\n\\n def createNewGame() = {\\n exec(http(\\"Create new game\\")\\n .post(\\"/videogame\\")\\n .header(\\"Authorization\\", \\"Bearer #{jwtToken}\\")\\n .body(StringBody(\\n \\"{\\\\n \\\\\\"category\\\\\\": \\\\\\"Platform\\\\\\",\\\\n \\\\\\"name\\\\\\": \\\\\\"Mario\\\\\\",\\\\n \\\\\\"rating\\\\\\": \\\\\\"Mature\\\\\\",\\\\n \\\\\\"releaseDate\\\\\\": \\\\\\"2012-05-04\\\\\\",\\\\n \\\\\\"reviewScore\\\\\\": 85\\\\n}\\"\\n ))\\n )\\n }\\n\\n val scn = scenario(\\"Authenticate\\")\\n .exec(authenticate())\\n .exec(createNewGame())\\n\\n setUp(scn.inject(atOnceUsers(users = 1)).protocols(httpProtocol))\\n}\\n\\n\\n#### Feed Data into the Test\\n\\n##### CsvFeeder\\n\\nscala\\npackage videogamedb.feeders\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nimport scala.concurrent.duration.DurationInt\\n\\nclass CsvFeeder extends Simulation{\\n val httpProtocol = http.baseUrl(\\"https://videogamedb.uk/api\\")\\n .acceptHeader(\\"application/json\\")\\n\\n val csvFeeder = csv(\\"data/gameCsvFile.csv\\").circular\\n\\n def getSpecificVideoGame() = {\\n repeat(10){\\n feed(csvFeeder)\\n .exec(http(\\"Get video game with name - #{gameName}\\")\\n .get(\\"/videogame/#{gameId}\\")\\n .check(jsonPath(\\"$.name\\").is(\\"#{gameName}\\"))\\n .check(status.is(200))\\n )\\n .pause(1)\\n }\\n }\\n\\n val scn = scenario(\\"Csv feeder test\\")\\n .exec(getSpecificVideoGame())\\n setUp(scn.inject(atOnceUsers(users=1)).protocols(httpProtocol))\\n}\\n\\n\\n##### Complex Feeder with Json Template, Random Data (String, Integer, Date)\\n\\n###### Json Template\\n\\nLet\'s create a template here:\\n\\n<p></p>\\n\\n<a href=\\"/assets/tech/219/05.png\\"><img src=\\"/assets/tech/219/05.png\\" width=\\"340\\"></a></Center>\\n\\n<p></p>\\n<center></center>\\n\\nNote that for non-string content we **_should not_** enclose it by \\"\'s.\\n\\ntext\\n// resources/bodies/newGameTemplate.txt\\n{\\n \\"id\\": #{gameId} ,\\n \\"category\\": \\"#{category}\\",\\n \\"name\\": \\"#{name}\\",\\n \\"rating\\": \\"#{rating}\\",\\n \\"releaseDate\\": \\"#{releaseDate}\\",\\n \\"reviewScore\\": #{reviewScore}\\n}\\n\\n\\n###### Script\\n\\nscala\\npackage videogamedb.feeders\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nimport java.time.LocalDate\\nimport java.time.format.DateTimeFormatter\\nimport scala.util.Random\\n\\nclass ComplexCustomFeeder extends Simulation{\\n val httpProtocol = http.baseUrl(\\"https://videogamedb.uk/api\\")\\n .acceptHeader(\\"application/json\\")\\n .contentTypeHeader(\\"application/json\\")\\n\\n val idNumbers = (1 to 10).iterator\\n val rnd = new Random()\\n val now = LocalDate.now()\\n val pattern = DateTimeFormatter.ofPattern(\\"yyyy-MM-dd\\")\\n\\n def randomString(length:Int) ={\\n rnd.alphanumeric.filter(_.isLetter).take(length).mkString\\n }\\n\\n def randomDate(startDate: LocalDate, random:Random):String = {\\n startDate.minusDays(random.nextInt(30)).format(pattern)\\n }\\n\\n val customFeeder = Iterator.continually(Map(\\n \\"gameId\\" -> idNumbers.next(),\\n \\"name\\" -> (\\"Game-\\" + randomString(5)),\\n \\"releaseDate\\" -> randomDate(now, rnd),\\n \\"reviewScore\\" -> rnd.nextInt(100),\\n \\"category\\" -> (\\"Category-\\" + randomString(6)),\\n \\"rating\\" -> (\\"Rating-\\" + randomString(4)),\\n ))\\n\\n def authenticate() = {\\n exec(http(\\"Authenticate\\")\\n .post(\\"/authenticate\\")\\n .body(StringBody(\\"{\\\\n \\\\\\"password\\\\\\": \\\\\\"admin\\\\\\",\\\\n \\\\\\"username\\\\\\": \\\\\\"admin\\\\\\"\\\\n}\\"))\\n .check(jsonPath(\\"$.token\\").saveAs(\\"jwtToken\\"))\\n )\\n }\\n\\n def createNewGame()={\\n repeat(10){\\n feed(customFeeder)\\n .exec(http(\\"Create new game - #{name}\\").\\n post(\\"/videogame\\")\\n .header(\\"authorization\\", \\"Bearer #{jwtToken}\\")\\n .body(ElFileBody(\\"bodies/newGameTemplate.json\\")).asJson\\n .check(bodyString.saveAs(\\"responseBody\\"))\\n )\\n .exec{session => println(session(\\"responseBody\\").as[String]); session}\\n .pause(1)\\n }\\n }\\n\\n val scn = scenario(\\"Csv feeder test\\")\\n .exec(authenticate())\\n .exec(createNewGame())\\n\\n setUp(scn.inject(atOnceUsers(users = 1)).protocols(httpProtocol))\\n}\\n\\n\\n#### Load Test with Scenario: Increase Number of Users Concurrently for a Period\\n\\nscala\\npackage videogamedb.simulations\\n\\nimport io.gatling.core.Predef._\\nimport io.gatling.http.Predef._\\n\\nclass BasicLoadSimulation extends Simulation {\\n\\n val httpProtocol = http.baseUrl(\\"https://videogamedb.uk/api\\")\\n .acceptHeader(\\"application/json\\")\\n\\n def getAllVideoGames() = {\\n exec(\\n http(\\"Get all video games\\")\\n .get(\\"/videogame\\")\\n )\\n }\\n\\n def getSpecificGame() = {\\n exec(\\n http(\\"Get specific game\\")\\n .get(\\"/videogame/2\\")\\n )\\n }\\n\\n val scn = scenario(\\"Basic Load Simulation\\")\\n .exec(getAllVideoGames())\\n .pause(5)\\n .exec(getSpecificGame())\\n .pause(5)\\n .exec(getAllVideoGames())\\n\\n setUp(\\n scn.inject(\\n nothingFor(5),\\n atOnceUsers(5),\\n rampUsers(10).during(10)\\n ).protocols(httpProtocol)\\n )\\n}\\n\\n\\n<p></p>\\n\\n<a href=\\"/assets/tech/219/07.png\\"><img src=\\"/assets/tech/219/07.png\\" width=\\"600\\"></a></Center>\\n\\n<p></p>\\n<center></center>\\n\\n<p></p>\\n\\n<a href=\\"/assets/tech/219/06.png\\"><img src=\\"/assets/tech/219/06.png\\" width=\\"600\\"></a></Center>\\n\\n<p></p>\\n<center></center>\\n\\n#### More Setup for Different Scenario from Official Documentation\\n\\n- [Documentation](https://gatling.io/docs/gatling/reference/current/core/injection/#open-model)\\n","title":"Load Test and Stress Test by Gatling","date":"2023-11-19T00:00:00.000Z","id":"blog0219","tag":"gatling, test","intro":"Record the steps to run load and stress test.","toc":true},{"content":"\\n#### The Key in Error Interceptor\\n\\nThe trick is to set\\n\\njs\\nconst originalConfig = error.config;\\n\\n\\nin the axios response interceptor, and at the end we return\\n\\njs\\noriginalConfig._retry = true;\\napiclient(originalConfig);\\n\\n\\n#### The Code\\n\\njs\\nimport axios, { AxiosInstance } from \'axios\'\\nimport { ReduxToolkitStore, RootState } from \'../redux/app/store\';\\nimport apiRoutes from \'./apiRoutes\';\\nimport { WBResponse } from \'./responseTypes\';\\nimport msgUtil from \'../util/msgUtil\';\\nimport authSlice from \'../redux/slices/authSlice\';\\n\\nconst { EXPO_PUBLIC_BACKEND_URL: baseURL } = process.env;\\n\\nconst apiClient = axios.create({\\n baseURL,\\n responseEncoding: \\"utf8\\",\\n headers: { \'Content-type\': \'application/json\' },\\n})\\n\\napiClient.defaults.withCredentials = true;\\nconst chatSocketRef = { current: \\"\\" };\\nconst notificationSocketRef = { current: \\"\\" };\\n\\n// inject store into interceptor at _layout.tsx\\nexport const configApiClient = (apiClient: AxiosInstance, store: ReduxToolkitStore) => {\\n ...\\n apiClient.interceptors.response.use(\\n response => response,\\n async error => {\\n const originalConfig = error.config;\\n if (\\n error?.response?.status === 403 ||\\n error?.response?.status === 401\\n ) {\\n const errorMessage = error?.response?.data?.errorMessage || \\"\\";\\n if (errorMessage === \\"JWT_EXPIRED\\") {\\n originalConfig._retry = true;\\n const refreshToken = (store?.getState() as RootState)?.auth.refreshToken;\\n const res = await apiClient.post<WBResponse<{ accessToken: string }>>(apiRoutes.POST_REFRESH_TOKEN, { refreshToken });\\n const { success } = res.data;\\n if (!success) {\\n msgUtil.error(res.data.errorMessage || \\"\\");\\n } else {\\n const { result } = res.data;\\n const newAccessToken = result.accessToken;\\n store.dispatch(authSlice.actions.setAccessToken(newAccessToken));\\n return apiClient(originalConfig);\\n }\\n }\\n } else if (error?.response?.status === 404) {\\n //404 page\\n } else if (error?.response?.status === 500) {\\n //do nothing\\n } else {\\n // snackbarUtils.info(\\"Please try to login again\\");\\n }\\n return Promise.reject(error)\\n }\\n )\\n}\\n\\nexport default apiClient\\n\\n","title":"Frontend Retry Mechanism for Refresh Token","date":"2023-11-20T00:00:00.000Z","id":"blog0220","tag":"jwt","intro":"Record the actual implementation in frontend for fail > refresh access-token > retry.","toc":false},{"content":"\\n#### Installation\\n\\ntext\\nyarn add prisma @prisma/client kysely prisma-kysely\\n\\n\\n#### Prisma\\n\\n##### schema.prisma: datasource and generator\\n\\n- Create ./prisma/schema.prisma at root level.\\n\\n- In schema.prisma write\\n\\n js\\n datasource db {\\n provider = \\"postgresql\\"\\n url = env(\\"DATABASE_URL\\")\\n }\\n\\n generator client {\\n provider = \\"prisma-client-js\\"\\n }\\n\\n generator kysely {\\n provider = \\"prisma-kysely\\"\\n\\n // Optionally provide a destination directory for the generated file\\n // and a filename of your choice\\n output = \\"../db\\"\\n fileName = \\"types.ts\\"\\n // Optionally generate runtime enums to a separate file\\n enumFileName = \\"enums.ts\\"\\n }\\n \\n\\n- We mainly use prisma for table migration and kysely for actual table communication. prisma-kysely also makes use of the models defined in this schema.prisma to generate type-safed query builder that we shall see later.\\n\\n- Note that we **_use both_** generators because the types generated by kysely is not readily usable by writing our actual code (which works perfectly fine for its own query-builder).\\n\\n On the other hand, types generated by prisma-client-js can be brought into writing backend logics.\\n\\n##### ./db/database.ts\\n\\nNote that we have choosen ./db as the output directory for kysely generated types. Create a database.ts and write:\\n\\njs\\nimport { DB } from \\"./types\\"; // this is the Database interface we defined earlier\\nimport { Pool } from \\"pg\\";\\nimport { Kysely, PostgresDialect } from \\"kysely\\";\\n\\nconst { DB_DATABASE, DB_HOST, DB_USER, DB_PASSWORD } = process.env;\\n\\nconst dialect = new PostgresDialect({\\n pool: new Pool({\\n database: DB_DATABASE,\\n host: DB_HOST,\\n password: DB_PASSWORD,\\n user: DB_USER,\\n port: 5434,\\n max: 10,\\n }),\\n});\\n\\nexport const db =\\n new Kysely() <\\n DB >\\n {\\n dialect,\\n };\\n\\n\\nIf you don\'t have types.ts yet, run npx prisma generate, which will trigger kysely generator to produce one for us.\\n\\n##### schema.prisma: models\\n\\n###### 1-many\\n\\nprisma\\nmodel User {\\n id Int @id @default(autoincrement())\\n firstName String @db.Text\\n lastName String @db.Text\\n companyId Int @db.Integer\\n company Company @relation(fields: [companyId], references: [id])\\n}\\n\\nmodel Company {\\n id Int @id @default(autoincrement())\\n name String\\n User User[]\\n}\\n\\n\\n- Each user belongs to a company, so we add companyId field in User.\\n\\n- Whenever we add a field with model-type (like Company, User[]) inside a model, we are going to **_create a virtual relation_**. These fields will not be present in our actual table.\\n- The model with targetId should be responsible for creating a foreign key relation, that\'s why we add\\n text\\n company Company @relation(fields: [companyId], references: [id])\\n \\n in User model.\\n\\n###### 1-1\\n\\nThe modeling of 1-1 is identical to 1-many in prisma schema, the only difference is that we drop [] in the virtual relation.\\n\\nprisma\\nmodel File {\\n id Int @id @default(autoincrement())\\n url String\\n isDeleted Boolean @default(false)\\n jsonFileFromVoice Voice? @relation(\\"jsonFile\\")\\n audioFileFromVoice Voice? @relation(\\"audioFile\\")\\n image Image? @relation(\\"image-file\\")\\n createdAt DateTime @default(now())\\n updatedAt DateTime @updatedAt\\n}\\n\\nmodel Image {\\n id Int @id @default(autoincrement())\\n fileId Int @unique\\n file File @relation(\\"image-file\\", fields: [fileId], references: [id])\\n createdAt DateTime @default(now())\\n updatedAt DateTime @updatedAt\\n}\\n\\nmodel Voice {\\n id Int @id @default(autoincrement())\\n jsonFileId Int @unique\\n jsonFile File @relation(\\"jsonFile\\", fields: [jsonFileId], references: [id])\\n audioFileId Int @unique\\n audioFile File @relation(\\"audioFile\\", fields: [audioFileId], references: [id])\\n}\\n\\n\\n- Here each Image model and Voice model refer to a File entity which is responsible for handling File meta data.\\n\\n- Note that we use Voice?, Image? in File model. Which are virtual relations meaning File has at most one reference from Voice, and at most one reference from Image.\\n\\n- Simiarly, recall in 1-many section we have\\n\\n prisma\\n model Company {\\n id Int @id @default(autoincrement())\\n name String\\n User User[]\\n }\\n \\n\\n which means that Company model has references from many User\'s (sure, one company has many users).\\n\\n- When a model has two fields referencing to the same model, we need to add a name @relation(\\"audioFile\\") to distinguish the virtual relationship. In our example, a voice may have transcription result saved in a jsonFile file, and also the audioFile which saves the original sound track.\\n\\n###### many-many\\n\\nThis time we don\'t need to write any refereence id:\\n\\nprisma\\nmodel Project {\\n id Int @id @default(autoincrement())\\n name String @db.Text\\n joinedUsers User[] @relation(\\"UserToJoinedProjects\\")\\n createdAt DateTime @default(now())\\n updatedAt DateTime @updatedAt\\n}\\n\\nmodel User {\\n id Int @id @default(autoincrement())\\n firstName String @db.Text\\n lastName String @db.Text\\n passwordHash String? @db.Text\\n joinedProjects Project[] @relation(\\"UserToJoinedProjects\\")\\n}\\n\\n\\nPrisma will generate us a table using the relation name (_UserToJoinedProjects in our case).\\n\\n<p></p>\\n\\n<a href=\\"/assets/tech/221/image.png\\"><img src=\\"/assets/tech/221/image.png\\" width=\\"700\\"></a></Center>\\n\\n<p></p>\\n<center></center>\\n\\nHere A is the userId and B is the projectId.\\n\\nThings become complicated if we want to model a **_project invitation table_**. That table includes\\n\\n- user who invites\\n- user who is invited\\n- project\\n\\nmodel\\nmodel ProjectInvitation {\\n id Int @id @default(autoincrement())\\n fromUserId Int\\n fromUser User @relation(\\"fromUser\\", fields: [fromUserId], references: [id])\\n toUserId Int\\n toUser User @relation(\\"toUser\\", fields: [toUserId], references: [id])\\n projectId Int\\n project Project @relation(fields: [projectId], references: [id])\\n}\\n\\nmodel User {\\n id Int @id @default(autoincrement())\\n ...\\n projectInvitationSent ProjectInvitation[] @relation(\\"fromUser\\")\\n projectInvitationReceived ProjectInvitation[] @relation(\\"toUser\\")\\n ...\\n}\\n\\nmodel Project {\\n id Int @id @default(autoincrement())\\n ...\\n ProjectInvitation ProjectInvitation[]\\n ...\\n}\\n\\n\\n#### Kysely\\n\\n##### Left-join as a Json Object\\n\\njs\\n...\\n.select(eb => [\\n jsonObjectFrom(\\n eb.selectFrom(\\"User\\")\\n .select([\\n \\"User.companyEmail\\",\\n (eb) => {\\n const firstName = eb.ref(\\"User.firstName\\");\\n const lastName = eb.ref(\\"User.lastName\\");\\n return sql<string>concat(${firstName}, \' \', ${lastName}).as(\\"name\\")\\n }\\n ])\\n .whereRef(\\"User.id\\", \\"=\\", \\"RoomIssue.hostUserId\\")\\n ).as(\\"hostUser\\")\\n])\\n\\n\\n##### Left-join as an Array Object\\n\\njs\\n.select(eb => [\\n jsonArrayFrom(eb\\n .selectFrom(\\"Message\\")\\n .select(\\"id\\")\\n .whereRef(\\"Message.roomIssueId\\", \\"=\\", \\"RoomIssue.id\\")\\n .whereRef(\\"lastVisit\\", \\"<\\", \\"Message.createdAt\\")\\n .where(\\"Message.userId\\", \\"=\\", userId)\\n ).as(\\"unreadMessages\\")\\n])\\n\\n\\n##### Where OR\\n\\njs\\n.where(eb => eb.or([\\n eb(\\"RoomIssue.type\\", \\"=\\", \\"PUBLIC_CHATROOM\\"),\\n eb(\\"RoomIssue.hostUserId\\", \\"=\\", userId)\\n]))\\n\\n\\n#### Run Custom Migration Script\\n\\n##### Cast the data type of a Column Which Already has data\\n\\nSuppose that inside Project table we have a **_text_** column userId which should have been **_uuid_**. It is tempting to immediately annotate the type String by @db.Uuid. However, sometimes prisma **_doesn\'t know_** how we intent to modify a table, and it **_tends to erase all the data_** to make sure the migration must succeed (which is harmful of course).\\n\\nTo manually create a migration sql script, first we modify the existing prisma.schema, we keep migrating via prisma but **_don\'t let prisma run that auto-generated script_** by\\n\\ntext\\nnpx prisma migrate dev --create-only\\n\\n\\nNow a new migration folder has been created for us to modify.\\n\\nNext we encounter the error that warns us the prisma-generated sql will fail:\\n\\ntext\\n\u26a0\ufe0f We found changes that cannot be executed:\\n\\n \u2022 Step 0 Changed the type of userId on the Project table. No cast exists, the column would be dropped and recreated, which cannot be done since the column is required and there is data in the table.\\n\\n\\nNever mind, we are going to change it into working script.\\n\\nInside that new migration sql file, we write (we need to do experiments to obtain a working script)\\n\\ntext\\nalter table \\"Project\\" alter COLUMN \\"userId\\" set DATA TYPE uuid using \\"userId\\"::UUID;\\n\\n\\nThe function \\"userId\\"::UUID can be hinted from first trying to change the type in GUI program (and the function will be mentioned in the error).\\n\\nFinally we run npx prisma migrate dev to execute our custom migration file.\\n\\n#### Reference\\n\\n- [Official Documentation](https://kysely.dev/docs/getting-started)\\n","title":"Prisma, Prisma-Kysely and Kysely ","date":"2023-11-30T00:00:00.000Z","id":"blog0221","tag":"sql, nodejs, prisma","intro":"Record and introduce the workflow of using prisma and kysely.","toc":true},{"content":"\\n<center></center>\\n\\nWe add application.yaml at the root project level and write\\n\\nyml\\nspring:\\n profiles:\\n active: dev\\nperson:\\n lastName: Lovely James\\n age: 18\\n boss: false\\n birth: 2023/12/09\\n map:\\n k1: v1\\n k2: 12\\n lists: [hehe, haha]\\n dog:\\n name: Bobby\\n age: 2\\n\\n---\\nspring:\\n config:\\n activate:\\n on-profile: \\"dev\\"\\nserver:\\n port: 8080\\n\\n---\\nspring:\\n config:\\n activate:\\n on-profile: \\"uat\\"\\nserver:\\n port: 8081\\n\\n---\\nspring:\\n config:\\n activate:\\n on-profile: \\"prod\\"\\nserver:\\n port: 8082\\n\\n\\nNext we run mvn package to generate our target jar file, for different env we use\\n\\ntext\\njava -jar \\"./springboot-revisit-0.0.1-SNAPSHOT.jar\\" --spring.profiles.active=prod\\n\\n\\nto run the application with variables defined in prod.\\n\\nNote that springboot will look for application.properties/application.yaml in\\n\\n- ./config\\n- ./\\n- classpath:/config/\\n- classpath:/\\n\\nIt is not necesary to define the config file in resources/ (which spring initizlizr does by default).\\n","title":"Manage Environment Varisbles for Dev, Uat, Prod in Springboot Using a Single Yaml File","date":"2023-12-08T00:00:00.000Z","id":"blog0222","tag":"java, springboot","intro":"Revisit the basic of springboot application.","toc":false},{"content":"\\n#### Prisma\\n\\nIn the course of using prisma I have developed the following helper commands in package.json:\\n\\njson\\n\\"scripts\\": {\\n \\"migrate\\": \\"env-cmd -f .env-cmdrc -e default,dev npx prisma migrate dev\\",\\n \\"migrate:create-only\\": \\"env-cmd -f .env-cmdrc -e default,dev npx prisma migrate dev --create-only\\",\\n \\"migrate:resolve\\": \\"env-cmd -f .env-cmdrc -e default,dev npx prisma migrate resolve --applied\\",\\n \\"migrate:deploy:uat\\": \\"env-cmd -f .env-cmdrc -e default,uat npx prisma migrate deploy\\",\\n \\"migrate:deploy:poc\\": \\"env-cmd -f .env-cmdrc -e default,poc npx prisma migrate deploy\\",\\n \\"migrate:deploy:prod\\": \\"env-cmd -f .env-cmdrc -e default,prod npx prisma migrate deploy\\",\\n \\"migrate-resolve:uat\\": \\"env-cmd -f .env-cmdrc -e default,uat npx prisma migrate resolve --applied\\",\\n \\"migrate-resolve:poc\\": \\"env-cmd -f .env-cmdrc -e default,poc npx prisma migrate resolve --applied\\",\\n \\"migrate-resolve:prod\\": \\"env-cmd -f .env-cmdrc -e default,prod npx prisma migrate resolve --applied\\",\\n}\\n\\n\\n- migrate \\\\\\n We only run migration dev in local (or dev) developement to execute table migration and obtain migration file.\\n- migrate-create-only \\\\\\n Used when auto generated sql-migration script (generated from prisma) **_fails to persist data_**.\\n\\n We keep updating the schema.prisma, yarn migrate-create-only to obtain faulty migration script, **_correct it_**, and yarn migrate.\\n\\n- migrate:deploy:uat \\\\\\n Apply all migration scripts to production server. The scripts are well tested in local development.\\n- migrate-resolve:uat \\\\\\n Used when we have manaully updated the table (due to incorrect procedures).\\n\\n We record the changes in migration file and run migrate-resolve:uat <migration-name> to indicate that the changes has been applied.\\n\\n\\n#### SQL Migration Scripts\\n\\n##### Rename a Table\\n\\ntext\\nALTER TABLE \\"RoomIssue\\" RENAME TO \\"MessagesSession\\";\\n\\n\\n##### Change the Type of a Column\\n\\n###### Simple Case\\n\\ntext\\nALTER TABLE \\"Message\\" ALTER COLUMN \\"createdAt\\" SET DATA TYPE DOUBLE PRECISION;\\n\\n\\n###### Type-Casting, Conditional (via Regex) Casting, with Error Handling\\n\\nFor example, our createdAt is a string recording the unix timstamp started from 1970 in ms. We:\\n\\n- cast non-empty \\"1234\\" string to 1234.\\n- cast empty string \\"\\" to 0, here CAST(\\"\\" as numeric) will make PGSQL panic.\\n\\ntext\\nALTER TABLE \\"Session\\" ADD COLUMN \\"temp_column\\" DECIMAL(14, 0);\\nUPDATE \\"Session\\" SET \\"temp_column\\" =\\n CASE\\n WHEN \\"createdAt\\" ~ \'^[0-9]+$\'\\n THEN CAST(\\"createdAt\\" AS numeric)\\n ELSE 0\\n END;\\nALTER TABLE \\"Session\\" DROP COLUMN \\"createdAt\\";\\nALTER TABLE \\"Session\\" RENAME COLUMN \\"temp_column\\" TO \\"createdAt\\";\\n\\n\\n##### COALESCE, the PostgreSQL version of IFNULL\\n\\nExamples:\\n\\ntext\\nSELECT COALESCE(1, 2);\\n\\n\\nresolves to 1, and\\n\\ntext\\nSELECT COALESCE (NULL, 2 , 1);\\n\\n\\nresolved to 2.\\n","title":"Prisma Migration Script","date":"2023-02-25T00:00:00.000Z","id":"blog0223","tag":"sql, prisma","intro":"Record the commonly used migration scripts in prisma and sql.","toc":true},{"content":"\\n#### Overview\\n\\n##### Prerequisite\\n\\nThe reader is assumed to be familiar with creating database on your own. You may do it:\\n\\n- via prisma (which we do in this article) or;\\n- via flyway (java package) or;\\n- via gooose (cli application) or;\\n- do it manually if one wishes as long as you know how to maintain the schema across different production environments.\\n\\n##### From start.springboot.io\\n\\nWe can bring web, lombok and pgsql into our springboot project in start.springboot.io. No jpa (hiberate) nor any additional database related jars will be needed in our project from that starter page.\\n\\nLater we will add one addtional package jooq from pom.xml which serves as a **_type-safe_** sql query builder that runs sql command directly to pgsql server without any need of ORM library. We don\'t even need to create entity classes, jooq will create a StudentRecord class for us by reverse-engineering the table student in our database.\\n\\n#### Configurations with PostgreSQL\\n\\n##### application.yaml\\n\\nNote that by default application.properties/application.yaml will be loaded into env varariable only when we spin up our springboot server.\\n\\nFor code generation to work we still need to input the url, user, password manaully in our pom.xml.\\n\\nyaml\\nspring:\\n profiles:\\n active: dev\\n show-sql: true\\nlogging:\\n level:\\n root: warn\\n\\n---\\nspring:\\n config:\\n activate:\\n on-profile: \\"dev\\"\\n datasource:\\n url: jdbc:postgresql://localhost:5432/pgdb\\n username: pguser\\n password: pguser\\n\\nserver:\\n port: 8080\\n\\n---\\nspring:\\n config:\\n activate:\\n on-profile: \\"uat\\"\\nserver:\\n port: 8081\\n\\n---\\nspring:\\n config:\\n activate:\\n on-profile: \\"prod\\"\\nserver:\\n port: 8082\\n\\n\\n##### pom.xml\\n\\nxml\\n<?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?>\\n<project xmlns=\\"http://maven.apache.org/POM/4.0.0\\" xmlns:xsi=\\"http://www.w3.org/2001/XMLSchema-instance\\"\\n\\txsi:schemaLocation=\\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\\">\\n\\t<modelVersion>4.0.0</modelVersion>\\n ...\\n\\t<dependencies>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.springframework.boot</groupId>\\n\\t\\t\\t<artifactId>spring-boot-starter-web</artifactId>\\n\\t\\t</dependency>\\n\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.postgresql</groupId>\\n\\t\\t\\t<artifactId>postgresql</artifactId>\\n\\t\\t\\t<scope>runtime</scope>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.projectlombok</groupId>\\n\\t\\t\\t<artifactId>lombok</artifactId>\\n\\t\\t\\t<optional>true</optional>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.springframework.boot</groupId>\\n\\t\\t\\t<artifactId>spring-boot-starter-test</artifactId>\\n\\t\\t\\t<scope>test</scope>\\n\\t\\t</dependency>\\n\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.jooq</groupId>\\n\\t\\t\\t<artifactId>jooq</artifactId>\\n\\t\\t\\t<version>3.19.0</version>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.jooq</groupId>\\n\\t\\t\\t<artifactId>jooq-meta</artifactId>\\n\\t\\t\\t<version>3.19.0</version>\\n\\t\\t</dependency>\\n\\t\\t<dependency>\\n\\t\\t\\t<groupId>org.jooq</groupId>\\n\\t\\t\\t<artifactId>jooq-codegen</artifactId>\\n\\t\\t\\t<version>3.19.0</version>\\n\\t\\t</dependency>\\n\\t</dependencies>\\n <build>\\n\\t\\t<plugins>\\n\\t\\t\\t<plugin>\\n\\t\\t\\t\\t<groupId>org.springframework.boot</groupId>\\n\\t\\t\\t\\t<artifactId>spring-boot-maven-plugin</artifactId>\\n\\t\\t\\t\\t<configuration>\\n\\t\\t\\t\\t\\t<excludes>\\n\\t\\t\\t\\t\\t\\t<exclude>\\n\\t\\t\\t\\t\\t\\t\\t<groupId>org.projectlombok</groupId>\\n\\t\\t\\t\\t\\t\\t\\t<artifactId>lombok</artifactId>\\n\\t\\t\\t\\t\\t\\t</exclude>\\n\\t\\t\\t\\t\\t</excludes>\\n\\t\\t\\t\\t</configuration>\\n\\t\\t\\t</plugin>\\n \x3c!-- ================ the jooq part ================ --\x3e\\n\\t\\t\\t<plugin>\\n\\t\\t\\t\\t<groupId>org.jooq</groupId>\\n\\t\\t\\t\\t<artifactId>jooq-codegen-maven</artifactId>\\n\\t\\t\\t\\t<version>3.14.0</version>\\n\\t\\t\\t\\t<executions>\\n\\t\\t\\t\\t\\t<execution>\\n\\t\\t\\t\\t\\t\\t<goals>\\n\\t\\t\\t\\t\\t\\t\\t<goal>generate</goal>\\n\\t\\t\\t\\t\\t\\t</goals>\\n\\t\\t\\t\\t\\t</execution>\\n\\t\\t\\t\\t</executions>\\n\\t\\t\\t\\t<dependencies>\\n\\t\\t\\t\\t\\t<dependency>\\n\\t\\t\\t\\t\\t\\t<groupId>mysql</groupId>\\n\\t\\t\\t\\t\\t\\t<artifactId>mysql-connector-java</artifactId>\\n\\t\\t\\t\\t\\t\\t<version>8.0.26</version>\\n\\t\\t\\t\\t\\t</dependency>\\n\\t\\t\\t\\t</dependencies>\\n\\t\\t\\t\\t<configuration>\\n\\t\\t\\t\\t\\t<jdbc>\\n\\t\\t\\t\\t\\t\\t<driver>org.postgresql.Driver</driver>\\n\\t\\t\\t\\t\\t\\t<url>${spring.datasource.url}</url>\\n\\t\\t\\t\\t\\t\\t<user>${spring.datasource.username}</user>\\n\\t\\t\\t\\t\\t\\t<password>${spring.datasource.password}</password>\\n\\t\\t\\t\\t\\t</jdbc>\\n\\t\\t\\t\\t\\t<generator>\\n\\t\\t\\t\\t\\t\\t<database>\\n\\t\\t\\t\\t\\t\\t\\t<name>org.jooq.meta.postgres.PostgresDatabase</name>\\n\\t\\t\\t\\t\\t\\t\\t<includes>.*</includes>\\n\\t\\t\\t\\t\\t\\t\\t<inputSchema>public</inputSchema>\\n\\t\\t\\t\\t\\t\\t</database>\\n\\t\\t\\t\\t\\t\\t<target>\\n\\t\\t\\t\\t\\t\\t\\t<packageName>com.machingclee.jooq.generated</packageName>\\n\\t\\t\\t\\t\\t\\t\\t<directory>src/main/java</directory>\\n\\t\\t\\t\\t\\t\\t</target>\\n\\t\\t\\t\\t\\t</generator>\\n\\t\\t\\t\\t</configuration>\\n\\t\\t\\t</plugin>\\n \x3c!-- ================ the jooq part ================ --\x3e\\n\\t\\t</plugins>\\n\\t</build>\\n</project>\\n\\n\\n##### How to Debug if the Code-Geneation does not work?\\n\\nWe manually trigger the code-generation process with a flag -X for debugging\\n\\ntext\\nmvn generate-sources -X\\n\\n\\nand analyse the root cause.\\n\\n#### Tables and Operations\\n\\n##### prisma/schema.prisma\\n\\nNote that according to org.postgresql.Driver\'s definition:\\n\\n- Table name starts with **_small_** letter and\\n- Field name is separated by an \\"_\\", i.e,\\n - first_name works but\\n - firstName will **_fail_**.\\n\\ntext\\ndatasource db {\\n provider = \\"postgresql\\"\\n url = env(\\"DATABASE_URL\\")\\n}\\n\\ngenerator client {\\n provider = \\"prisma-client-js\\"\\n}\\n\\nmodel student {\\n id String @id @default(dbgenerated(\\"gen_random_uuid()\\")) @db.Uuid\\n first_name String\\n last_name String\\n email String\\n}\\n\\n\\n##### CRUD Examples --- Demonstrative DAO\\n\\njava\\npackage com.machingclee.jooq.dao;\\n\\nimport java.util.UUID;\\nimport java.util.List;\\n\\nimport static com.machingclee.jooq.generated.Tables.*;\\nimport org.jooq.DSLContext;\\nimport org.springframework.beans.factory.annotation.Autowired;\\nimport org.springframework.stereotype.Repository;\\nimport com.machingclee.jooq.generated.tables.records.StudentRecord;\\n\\n@Repository\\npublic class StudentDAO {\\n private DSLContext db;\\n\\n @Autowired\\n StudentDAO(DSLContext ctx) {\\n this.db = ctx;\\n }\\n\\n public void create(StudentRecord student) {\\n db.insertInto(STUDENT,\\n STUDENT.FIRST_NAME, STUDENT.LAST_NAME, STUDENT.EMAIL)\\n .values(student.getFirstName(), student.getLastName(), student.getEmail())\\n .execute();\\n };\\n\\n public StudentRecord findById(UUID uuid) {\\n var result = db\\n .selectFrom(STUDENT)\\n .where(STUDENT.ID.equal(uuid))\\n .fetchOne()\\n .into(StudentRecord.class);\\n return result;\\n }\\n\\n public List<StudentRecord> getStudents() {\\n var result = db\\n .select()\\n .from(STUDENT) // if we start from .select, we can start to left-joining here\\n .orderBy(STUDENT.LAST_NAME.asc())\\n .fetch()\\n .into(StudentRecord.class);\\n return result;\\n }\\n\\n public StudentRecord findByEmail(String email) {\\n var result = db.selectFrom(STUDENT)\\n .where(STUDENT.EMAIL.equal(email))\\n .fetchOne()\\n .into(StudentRecord.class);\\n return result;\\n }\\n}\\n\\n\\n#### Springboot3 Functional Endpoints to get JOOQ\'s Output\\n\\n##### StudentController\\n\\nAs usual our controller consists of many handlers, but this time we don\'t annotate the controller as @RestController (which is a combination of @Controller and @ResponseBody):\\n\\njava\\npackage com.machingclee.experiments.controller;\\n\\nimport static com.machingclee.experiments.generated.Tables.STUDENT;\\nimport org.jooq.DSLContext;\\nimport org.springframework.beans.factory.annotation.Autowired;\\nimport org.springframework.stereotype.Component;\\nimport org.springframework.web.servlet.function.ServerRequest;\\nimport org.springframework.web.servlet.function.ServerResponse;\\nimport com.machingclee.experiments.dto.StudentDTO;\\n\\n@Component\\npublic class StudentController {\\n\\n private DSLContext db;\\n\\n @Autowired\\n public StudentController(DSLContext ctx) {\\n this.db = ctx;\\n }\\n\\n public ServerResponse getStudents(ServerRequest req) {\\n var students = db.select(STUDENT.FIRST_NAME, STUDENT.LAST_NAME, STUDENT.EMAIL)\\n .from(STUDENT)\\n .fetch()\\n .into(StudentDTO.class);\\n return ServerResponse.ok().body(students);\\n }\\n}\\n\\n\\n##### Configure StudentRouter\\n\\njava\\npackage com.machingclee.experiments.config;\\n\\nimport org.springframework.context.annotation.Bean;\\nimport org.springframework.context.annotation.Configuration;\\nimport org.springframework.http.MediaType;\\nimport org.springframework.web.servlet.function.RequestPredicates;\\nimport org.springframework.web.servlet.function.RouterFunction;\\nimport org.springframework.web.servlet.function.RouterFunctions;\\nimport org.springframework.web.servlet.function.ServerResponse;\\n\\nimport com.machingclee.experiments.controller.StudentController;\\n\\n@Configuration\\npublic class RoutingConfig {\\n @Bean\\n public RouterFunction<ServerResponse> studentRouter(StudentController studentController) {\\n return RouterFunctions.route()\\n .GET(\\"/students\\", RequestPredicates.accept(MediaType.ALL), studentController::getStudents)\\n .build();\\n }\\n}\\n\\n","title":"Springboot with JOOQ and Functional Endpoints","date":"2023-12-17T00:00:00.000Z","id":"blog0224","tag":"sql, java, springboot","intro":"We record the config for a starter project in springboot that mimics the workflow of prisma+kysely in nodejs world.","toc":true},{"content":"\\n#### Installation\\n\\ntext\\nyarn add tss-react\\n\\n\\n#### Styles\\n\\n##### Style that Intake a State\\n\\njs\\nconst useButtonStyles = tss\\n .withParams<{ disabled: boolean }>()\\n .create(({ disabled }) => ({\\n disabled: {\\n opacity: disabled ? 0.5 : 1,\\n pointerEvents: disabled ? \\"none\\" : \\"auto\\"\\n }\\n }));\\n\\n\\n##### Style Without a State\\n\\njs\\nconst useStyles = tss.create(() => ({\\n example: {\\n paddingLeft: \\"calc(2em - 3px)\\",\\n marginTop: 40,\\n marginBottom: 60,\\n borderLeft: \\"3px solid rgba(123,185,209,0.3)\\",\\n },\\n proof: {\\n \\"& > p:nth-child(1)\\": {\\n textIndent: \\"calc(3px - 2em) !important\\",\\n },\\n paddingLeft: \\"calc(2em - 3px)\\",\\n marginTop: 20,\\n marginBottom: 20,\\n },\\n codeBlock: {\\n \\"& .react-syntax-highlighter-line-number\\": {\\n minWidth: \\"2.5em !important\\",\\n },\\n \\"& pre\\": {\\n paddingTop: \\"0.95em !important\\",\\n background: \\"#F5F5F5!important\\",\\n // paddingLeft: \\"0.55em !important\\",\\n border: \\"1px solid #E0E0E0\\",\\n borderRadius: 2,\\n },\\n\\n \\"& .no-bg\\": {\\n background: \\"transparent !important\\",\\n \\"& code\\": {\\n background: \\"transparent !important\\",\\n },\\n },\\n },\\n}));\\n\\n\\n#### Usage\\n\\njs\\nconst SomeDiv = (props: SomeProps) => {\\n const { classes, cx } = useStyles();\\n ...\\n\\n return <div className={cx(classes.codeBlock)}>...</div>;\\n};\\n\\n","title":"TSS as a Replacement of makeStyles","date":"2023-12-10T00:00:00.000Z","id":"blog0225","tag":"react","intro":"In recent mui makeStyles is no longer maintained, but css in tss is still a very good idea so we record an alternative in this article.","toc":true},{"content":"\\n<p></p>\\n<center></center>\\n\\n\\nIn express if with log the query built by kysely via the config:\\n\\njs\\nexport const db = new Kysely<DB>({\\n dialect,\\n log(event) {\\n if (event.level === \\"query\\") {\\n console.log(event.query.sql);\\n console.log(event.query.parameters);\\n }\\n }\\n})\\n\\nthen we eventually get the following kind of queries to prevent sql injection:\\n\\njs\\nselect *, (select to_json(obj) from (select count(\\"MessagesSession\\".\\"id\\") as \\"count\\" from \\"MessagesSession\\" where \\"MessagesSession\\".\\"channelId\\" = \\"Channel\\".\\"id\\" and \\"MessagesSession\\".\\"hostUserId\\" = $1 and \\"MessagesSession\\".\\"isDeleted\\" != $2 and \\"MessagesSession\\".\\"isDraftInstantIssue\\" = $3) as obj) as \\"draft\\" from \\"Channel\\" where \\"Channel\\".\\"projectId\\" = $4 and \\"Channel\\".\\"isDeleted\\" = $5\\n[\\n \'77af0d8c-46aa-44c1-bb2b-730033cbf188\',\\n true,\\n true,\\n \'ff515aff-d53f-407c-9c11-602998b74ba8\',\\n false\\n]\\n\\n\\nTo exeucute this in our ordinary SQL shell we can prepare and execute it:\\n\\nsql\\nprepare plan(uuid, bool, bool, uuid, bool) as \\n\\tselect *, (select to_json(obj) from (select count(\\"MessagesSession\\".\\"id\\") as \\"count\\" from \\"MessagesSession\\" where \\"MessagesSession\\".\\"channelId\\" = \\"Channel\\".\\"id\\" and \\"MessagesSession\\".\\"hostUserId\\" = $1 and \\"MessagesSession\\".\\"isDeleted\\" != $2 and \\"MessagesSession\\".\\"isDraftInstantIssue\\" = $3) as obj) as \\"draft\\" from \\"Channel\\" where \\"Channel\\".\\"projectId\\" = $4 and \\"Channel\\".\\"isDeleted\\" = $5;\\n\\nexecute plan(\\n \'77af0d8c-46aa-44c1-bb2b-730033cbf188\',\\n true,\\n true,\\n \'ff515aff-d53f-407c-9c11-602998b74ba8\',\\n false\\n)\\n\\nFrom this we get:\\n\\n<p></p>\\n\\n<a href=\\"/assets/tech/226/image.png\\"><img src=\\"/assets/tech/226/image.png\\" width=\\"660\\"></a></Center>\\n\\n<p></p>\\n<center></center>\\n\\nNow we can investigate what\'s wrong with our query.","title":"Run Kysely Generated SQL in Tableplus","date":"2023-12-23T00:00:00.000Z","id":"blog0226","tag":"sql","intro":"Record how to experiment with the sqls generated by query builder.","toc":false},{"content":"\\n#### Repository\\n\\n- https://github.com/machingclee/2023-12-24-Dead-and-Delayed-Queues\\n\\n#### RabbitMQUtil\\n\\njava\\npackage com.machingclee.rabbitmq.util;\\n\\nimport java.io.IOException;\\nimport java.util.concurrent.TimeoutException;\\n\\nimport com.rabbitmq.client.Channel;\\nimport com.rabbitmq.client.ConnectionFactory;\\n\\npublic class RabbitMQUtil {\\n public static Channel getChannel() throws IOException, TimeoutException {\\n var factory = new ConnectionFactory();\\n factory.setHost(\\"localhost\\");\\n factory.setPort(5672);\\n factory.setUsername(\\"guest\\");\\n factory.setPassword(\\"guest\\");\\n\\n var connection = factory.newConnection();\\n var channel = connection.createChannel();\\n return channel;\\n }\\n}\\n\\n\\n#### Dead Exchange\\n\\n##### Dead Due to TTL\\n\\n###### Normal Consumer (TTL)\\n\\njava\\npackage com.machingclee.rabbitmq.experiment_queues.dead_exchange_ttl;\\n\\nimport java.io.IOException;\\nimport java.util.HashMap;\\nimport java.util.Map;\\nimport java.util.concurrent.TimeoutException;\\n\\nimport com.machingclee.rabbitmq.util.RabbitMQUtil;\\nimport com.rabbitmq.client.BuiltinExchangeType;\\n\\npublic class NormalConsumer {\\n public static final String NORMAL_EXCHANGE = \\"normal_exchange\\";\\n public static final String DEAD_EXCHANGE = \\"dead_exchange\\";\\n public static final String NORMAL_QUEUE = \\"normal_queue\\";\\n public static final String DEAD_QUEUE = \\"dead_queue\\";\\n\\n public static final String NORMAL_ROUTING_KEY = \\"normal_route\\";\\n public static final String DEAD_ROUTING_KEY = \\"dead_route\\";\\n\\n public static void main(String[] args) throws IOException, TimeoutException {\\n var channel = RabbitMQUtil.getChannel();\\n\\n // create exchanges\\n try {\\n channel.exchangeDeclare(NORMAL_EXCHANGE, BuiltinExchangeType.DIRECT);\\n channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT);\\n } catch (Exception err) {\\n System.out.println(err);\\n }\\n\\n // special config for normal queue to communicate with dead_exchange\\n // and dead_queue\\n Map<String, Object> arguments = new HashMap<>();\\n arguments.put(\\"x-dead-letter-exchange\\", DEAD_EXCHANGE);\\n arguments.put(\\"x-dead-letter-routing-key\\", DEAD_ROUTING_KEY);\\n\\n // create queues\\n channel.queueDeclare(NORMAL_QUEUE, false, false, false, arguments);\\n channel.queueDeclare(DEAD_QUEUE, false, false, false, null);\\n\\n channel.queueBind(NORMAL_QUEUE, NORMAL_EXCHANGE, NORMAL_ROUTING_KEY);\\n channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, DEAD_ROUTING_KEY);\\n }\\n}\\n\\n\\n###### Dead Queue Consumer (TTL)\\n\\njava\\npackage com.machingclee.rabbitmq.experiment_queues.dead_exchange_ttl;\\n\\nimport java.io.IOException;\\nimport java.util.concurrent.TimeoutException;\\n\\nimport com.machingclee.rabbitmq.util.RabbitMQUtil;\\nimport com.rabbitmq.client.BuiltinExchangeType;\\nimport com.rabbitmq.client.CancelCallback;\\nimport com.rabbitmq.client.DeliverCallback;\\n\\npublic class DeadQueueConsumer {\\n public static final String NORMAL_EXCHANGE = \\"normal_exchange\\";\\n public static final String DEAD_EXCHANGE = \\"dead_exchange\\";\\n public static final String NORMAL_QUEUE = \\"normal_queue\\";\\n public static final String DEAD_QUEUE = \\"dead_queue\\";\\n\\n public static final String NORMAL_ROUTING_KEY = \\"normal_route\\";\\n public static final String DEAD_ROUTING_KEY = \\"dead_route\\";\\n\\n public static void main(String[] args) throws IOException, TimeoutException {\\n var channel = RabbitMQUtil.getChannel();\\n\\n // create exchanges\\n try {\\n channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT);\\n } catch (Exception err) {\\n System.out.println(err);\\n }\\n channel.queueDeclare(DEAD_QUEUE, false, false, false, null);\\n channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, DEAD_ROUTING_KEY);\\n\\n DeliverCallback deliverCallback = (consumerTag, message) -> {\\n System.out.println(\\"[DeadQueue Consumer Digested]\\" + new String(message.getBody()));\\n channel.basicAck(message.getEnvelope().getDeliveryTag(), false);\\n };\\n\\n CancelCallback cancelCallback = consumerTag -> {\\n System.out.println(\\"[Message Cancelled]\\");\\n };\\n\\n channel.basicConsume(DEAD_QUEUE, false, deliverCallback, cancelCallback);\\n }\\n}\\n\\n\\n###### Producer (TTL)\\n\\njava\\npackage com.machingclee.rabbitmq.experiment_queues.dead_exchange_ttl;\\n\\nimport java.io.IOException;\\nimport java.util.concurrent.TimeoutException;\\n\\nimport com.machingclee.rabbitmq.util.RabbitMQUtil;\\nimport com.rabbitmq.client.AMQP.BasicProperties;\\nimport com.rabbitmq.client.BuiltinExchangeType;\\n\\npublic class Producer {\\n public static final String NORMAL_EXCHANGE = \\"normal_exchange\\";\\n public static final String NORMAL_ROUTING_KEY = \\"normal_route\\";\\n\\n public static void main(String[] args) throws IOException, TimeoutException {\\n var channel = RabbitMQUtil.getChannel();\\n try {\\n channel.exchangeDeclare(NORMAL_EXCHANGE, BuiltinExchangeType.DIRECT);\\n } catch (Exception err) {\\n System.out.println(err);\\n }\\n\\n BasicProperties properites = new BasicProperties().builder()\\n .expiration(\\"10000\\")\\n .build();\\n\\n for (int i = 0; i < 10; i++) {\\n var message = \\"info \\" + (i + 1);\\n channel.basicPublish(NORMAL_EXCHANGE, NORMAL_ROUTING_KEY, properites, message.getBytes());\\n System.out.println(\\"info \\" + (i + 1) + \\" was sent\\");\\n }\\n }\\n}\\n\\n\\n##### Dead Due to Rejection\\n\\n###### Normal Consumer (Rejection)\\n\\njava\\npackage com.machingclee.rabbitmq.experiment_queues.dead_exchange_rejected;\\n\\nimport java.io.IOException;\\nimport java.util.HashMap;\\nimport java.util.Map;\\nimport java.util.concurrent.TimeoutException;\\n\\nimport com.machingclee.rabbitmq.util.RabbitMQUtil;\\nimport com.rabbitmq.client.BuiltinExchangeType;\\nimport com.rabbitmq.client.CancelCallback;\\nimport com.rabbitmq.client.DeliverCallback;\\n\\npublic class NormalConsumer {\\n public static final String NORMAL_EXCHANGE = \\"normal_exchange\\";\\n public static final String DEAD_EXCHANGE = \\"dead_exchange\\";\\n public static final String NORMAL_QUEUE = \\"normal_queue\\";\\n public static final String DEAD_QUEUE = \\"dead_queue\\";\\n\\n public static final String NORMAL_ROUTING_KEY = \\"normal_route\\";\\n public static final String DEAD_ROUTING_KEY = \\"dead_route\\";\\n\\n public static void main(String[] args) throws IOException, TimeoutException {\\n var channel = RabbitMQUtil.getChannel();\\n\\n // create exchanges\\n try {\\n channel.exchangeDeclare(NORMAL_EXCHANGE, BuiltinExchangeType.DIRECT);\\n channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT);\\n } catch (Exception err) {\\n System.out.println(err);\\n }\\n\\n // special config for normal queue to communicate with dead_exchange\\n // and dead_queue\\n Map<String, Object> arguments = new HashMap<>();\\n arguments.put(\\"x-dead-letter-exchange\\", DEAD_EXCHANGE);\\n arguments.put(\\"x-dead-letter-routing-key\\", DEAD_ROUTING_KEY);\\n\\n // create queues\\n channel.queueDeclare(NORMAL_QUEUE, false, false, false, arguments);\\n channel.queueDeclare(DEAD_QUEUE, false, false, false, null);\\n\\n channel.queueBind(NORMAL_QUEUE, NORMAL_EXCHANGE, NORMAL_ROUTING_KEY);\\n channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, DEAD_ROUTING_KEY);\\n\\n DeliverCallback deliverCallback = (consumerTag, message) -> {\\n String msg = new String(message.getBody());\\n System.out.println(\\"[DeadQueue Consumer Digested]\\" + msg);\\n if (msg.equals(\\"info \\" + 5)) {\\n System.out.println(\\"reject message: \\" + msg);\\n channel.basicReject(message.getEnvelope().getDeliveryTag(), false);\\n } else {\\n System.out.println(\\"message acked: \\" + msg);\\n channel.basicAck(message.getEnvelope().getDeliveryTag(), false);\\n }\\n };\\n\\n CancelCallback cancelCallback = consumerTag -> {\\n System.out.println(\\"[Message Cancelled]\\");\\n };\\n\\n channel.basicConsume(NORMAL_QUEUE, false, deliverCallback, cancelCallback);\\n }\\n}\\n\\n\\n###### Dead Queue Consumer (Rejection)\\n\\njava\\npackage com.machingclee.rabbitmq.experiment_queues.dead_exchange_rejected;\\n\\nimport java.io.IOException;\\nimport java.util.concurrent.TimeoutException;\\n\\nimport com.machingclee.rabbitmq.util.RabbitMQUtil;\\nimport com.rabbitmq.client.BuiltinExchangeType;\\nimport com.rabbitmq.client.CancelCallback;\\nimport com.rabbitmq.client.DeliverCallback;\\n\\npublic class DeadQueueConsumer {\\n public static final String NORMAL_EXCHANGE = \\"normal_exchange\\";\\n public static final String DEAD_EXCHANGE = \\"dead_exchange\\";\\n public static final String NORMAL_QUEUE = \\"normal_queue\\";\\n public static final String DEAD_QUEUE = \\"dead_queue\\";\\n\\n public static final String NORMAL_ROUTING_KEY = \\"normal_route\\";\\n public static final String DEAD_ROUTING_KEY = \\"dead_route\\";\\n\\n public static void main(String[] args) throws IOException, TimeoutException {\\n var channel = RabbitMQUtil.getChannel();\\n\\n // create exchanges\\n try {\\n channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT);\\n } catch (Exception err) {\\n System.out.println(err);\\n }\\n channel.queueDeclare(DEAD_QUEUE, false, false, false, null);\\n channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, DEAD_ROUTING_KEY);\\n\\n DeliverCallback deliverCallback = (consumerTag, message) -> {\\n System.out.println(\\"[DeadQueue Consumer Digested]\\" + new String(message.getBody()));\\n channel.basicAck(message.getEnvelope().getDeliveryTag(), false);\\n };\\n\\n CancelCallback cancelCallback = consumerTag -> {\\n System.out.println(\\"[Message Cancelled]\\");\\n };\\n\\n channel.basicConsume(DEAD_QUEUE, false, deliverCallback, cancelCallback);\\n }\\n}\\n\\n\\n###### Producer (Rejection)\\n\\njava\\npackage com.machingclee.rabbitmq.experiment_queues.dead_exchange_rejected;\\n\\nimport java.io.IOException;\\nimport java.util.concurrent.TimeoutException;\\n\\nimport com.machingclee.rabbitmq.util.RabbitMQUtil;\\nimport com.rabbitmq.client.AMQP.BasicProperties;\\nimport com.rabbitmq.client.BuiltinExchangeType;\\n\\npublic class Producer {\\n public static final String NORMAL_EXCHANGE = \\"normal_exchange\\";\\n public static final String NORMAL_ROUTING_KEY = \\"normal_route\\";\\n\\n public static void main(String[] args) throws IOException, TimeoutException {\\n var channel = RabbitMQUtil.getChannel();\\n try {\\n channel.exchangeDeclare(NORMAL_EXCHANGE, BuiltinExchangeType.DIRECT);\\n } catch (Exception err) {\\n System.out.println(err);\\n }\\n\\n BasicProperties properites = new BasicProperties().builder()\\n .expiration(\\"10000\\")\\n .build();\\n\\n for (int i = 0; i < 10; i++) {\\n var message = \\"info \\" + (i + 1);\\n channel.basicPublish(NORMAL_EXCHANGE, NORMAL_ROUTING_KEY, properites, message.getBytes());\\n System.out.println(\\"info \\" + (i + 1) + \\" was sent\\");\\n }\\n }\\n}\\n\\n\\n##### Dead Due to Max Queue Length\\n\\n###### Normal Consumer (Max Queue Length)\\n\\njava\\npackage com.machingclee.rabbitmq.dead_exchange_with_max_queue_length;\\n\\nimport java.io.IOException;\\nimport java.util.HashMap;\\nimport java.util.Map;\\nimport java.util.concurrent.TimeoutException;\\n\\nimport com.machingclee.rabbitmq.util.RabbitMQUtil;\\nimport com.rabbitmq.client.BuiltinExchangeType;\\n\\npublic class NormalConsumer {\\n public static final String NORMAL_EXCHANGE = \\"normal_exchange\\";\\n public static final String DEAD_EXCHANGE = \\"dead_exchange\\";\\n public static final String NORMAL_QUEUE = \\"normal_queue\\";\\n public static final String DEAD_QUEUE = \\"dead_queue\\";\\n\\n public static final String NORMAL_ROUTING_KEY = \\"normal_route\\";\\n public static final String DEAD_ROUTING_KEY = \\"dead_route\\";\\n\\n public static final Integer MAX_QUEUE_LENGTH = 6;\\n\\n public static void main(String[] args) throws IOException, TimeoutException {\\n var channel = RabbitMQUtil.getChannel();\\n\\n // create exchanges\\n try {\\n channel.exchangeDeclare(NORMAL_EXCHANGE, BuiltinExchangeType.DIRECT);\\n channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT);\\n } catch (Exception err) {\\n System.out.println(err);\\n }\\n\\n // special config for normal queue to communicate with dead_exchange\\n // and dead_queue\\n Map<String, Object> arguments = new HashMap<>();\\n arguments.put(\\"x-dead-letter-exchange\\", DEAD_EXCHANGE);\\n arguments.put(\\"x-dead-letter-routing-key\\", DEAD_ROUTING_KEY);\\n arguments.put(\\"x-max-length\\", MAX_QUEUE_LENGTH);\\n\\n // create queues\\n channel.queueDeclare(NORMAL_QUEUE, false, false, false, arguments);\\n channel.queueDeclare(DEAD_QUEUE, false, false, false, null);\\n\\n channel.queueBind(NORMAL_QUEUE, NORMAL_EXCHANGE, NORMAL_ROUTING_KEY);\\n channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, DEAD_ROUTING_KEY);\\n }\\n}\\n\\n\\n###### Dead Queue Consumer (Max Queue Length)\\n\\njava\\npackage com.machingclee.rabbitmq.dead_exchange_with_max_queue_length;\\n\\nimport java.io.IOException;\\nimport java.util.concurrent.TimeoutException;\\n\\nimport com.machingclee.rabbitmq.util.RabbitMQUtil;\\nimport com.rabbitmq.client.BuiltinExchangeType;\\nimport com.rabbitmq.client.CancelCallback;\\nimport com.rabbitmq.client.DeliverCallback;\\n\\npublic class DeadQueueConsumer {\\n public static final String NORMAL_EXCHANGE = \\"normal_exchange\\";\\n public static final String DEAD_EXCHANGE = \\"dead_exchange\\";\\n public static final String NORMAL_QUEUE = \\"normal_queue\\";\\n public static final String DEAD_QUEUE = \\"dead_queue\\";\\n\\n public static final String NORMAL_ROUTING_KEY = \\"normal_route\\";\\n public static final String DEAD_ROUTING_KEY = \\"dead_route\\";\\n\\n public static void main(String[] args) throws IOException, TimeoutException {\\n var channel = RabbitMQUtil.getChannel();\\n\\n // create exchanges\\n try {\\n channel.exchangeDeclare(DEAD_EXCHANGE, BuiltinExchangeType.DIRECT);\\n } catch (Exception err) {\\n System.out.println(err);\\n }\\n channel.queueDeclare(DEAD_QUEUE, false, false, false, null);\\n channel.queueBind(DEAD_QUEUE, DEAD_EXCHANGE, DEAD_ROUTING_KEY);\\n\\n DeliverCallback deliverCallback = (consumerTag, message) -> {\\n System.out.println(\\"[DeadQueue Consumer Digested]\\" + new String(message.getBody()));\\n channel.basicAck(message.getEnvelope().getDeliveryTag(), false);\\n };\\n\\n CancelCallback cancelCallback = consumerTag -> {\\n System.out.println(\\"[Message Cancelled]\\");\\n };\\n\\n channel.basicConsume(DEAD_QUEUE, false, deliverCallback, cancelCallback);\\n }\\n}\\n\\n\\n###### Producer (Max Queue Length)\\n\\njava\\npackage com.machingclee.rabbitmq.dead_exchange_with_max_queue_length;\\n\\nimport java.io.IOException;\\nimport java.util.concurrent.TimeoutException;\\n\\nimport com.machingclee.rabbitmq.util.RabbitMQUtil;\\nimport com.rabbitmq.client.BuiltinExchangeType;\\n\\npublic class Producer {\\n public static final String NORMAL_EXCHANGE = \\"normal_exchange\\";\\n public static final String NORMAL_ROUTING_KEY = \\"normal_route\\";\\n\\n public static void main(String[] args) throws IOException, TimeoutException {\\n var channel = RabbitMQUtil.getChannel();\\n try {\\n channel.exchangeDeclare(NORMAL_EXCHANGE, BuiltinExchangeType.DIRECT);\\n } catch (Exception err) {\\n System.out.println(err);\\n }\\n\\n // BasicProperties properites = new BasicProperties().builder()\\n // .expiration(\\"10000\\")\\n // .build();\\n\\n for (int i = 0; i < 10; i++) {\\n var message = \\"info \\" + (i + 1);\\n channel.basicPublish(NORMAL_EXCHANGE, NORMAL_ROUTING_KEY, null, message.getBytes());\\n System.out.println(\\"info \\" + (i + 1) + \\" was sent\\");\\n }\\n }\\n}\\n\\n\\n#### Integration of Dead and Delayed Queues with Springboot\\n\\n##### Routing for Experiements (Producer)\\n\\njava\\npackage com.machingclee.rabbitmq.config;\\n\\nimport org.springframework.context.annotation.Bean;\\nimport org.springframework.context.annotation.Configuration;\\nimport org.springframework.http.MediaType;\\nimport org.springframework.web.servlet.function.RequestPredicates;\\nimport org.springframework.web.servlet.function.RouterFunction;\\nimport org.springframework.web.servlet.function.RouterFunctions;\\nimport org.springframework.web.servlet.function.ServerResponse;\\n\\nimport com.machingclee.rabbitmq.controller.MessageController;\\n\\n@Configuration\\npublic class RoutingConfig {\\n @Bean\\n public RouterFunction<ServerResponse> studentRouter(MessageController msgController) {\\n return RouterFunctions.route()\\n .GET(\\"/ttl/msg/{message}\\", RequestPredicates.accept(MediaType.ALL), msgController::sendMessage)\\n .GET(\\"/delayed/msg/{ttl}/{msg}\\", RequestPredicates.accept(MediaType.ALL), msgController::delayedMessage)\\n .POST(\\"/ttl/msg\\", RequestPredicates.accept(MediaType.ALL), msgController::sendMessageWithTTL)\\n .build();\\n }\\n}\\n\\n\\n##### TTL (Works Well for Constant TTL)\\n\\n- In this example we want to provide variable ttl\'s in NORMAL_Q_C but problem occurs as queues cannot be consumed asynchronously.\\n- Q_A and Q_B work perfectly well as the ttl\'s of these two queues are constant.\\n- Problem of Q_C can be solved by using delayed-message-plugin of RabbitMQ, details are provided in the next section.\\n- But the need for variable ttl should be rare and therefore queues like Q_A and Q_B should be totally fine.\\n\\n###### TTLQueuConfig.java\\n\\njava\\npackage com.machingclee.rabbitmq.config;\\n\\nimport java.util.HashMap;\\nimport java.util.Map;\\n\\nimport org.springframework.amqp.core.Binding;\\nimport org.springframework.amqp.core.BindingBuilder;\\nimport org.springframework.amqp.core.DirectExchange;\\nimport org.springframework.amqp.core.Queue;\\nimport org.springframework.amqp.core.QueueBuilder;\\nimport org.springframework.beans.factory.annotation.Qualifier;\\nimport org.springframework.context.annotation.Bean;\\nimport org.springframework.context.annotation.Configuration;\\n\\n@Configuration\\npublic class TTLQueueConfig {\\n public static final String NORMAL_EXCHANGE = \\"NORMAL_EXCHANGE\\";\\n public static final String DEAD_LETTER_EXCANGE = \\"DEAD_LETTER_EXCANGE\\";\\n\\n public static final String NORMAL_Q_A = \\"NORMAL_Q_A\\";\\n public static final String NORMAL_Q_A_ROUTING_KEY = \\"NORMAL_Q_A_ROUTING_KEY\\";\\n public static final String NORMAL_Q_B = \\"NORMAL_Q_B\\";\\n public static final String NORMAL_Q_B_ROUTING_KEY = \\"NORMAL_Q_B_ROUTING_KEY\\";\\n public static final String DEAD_LETTER_Q = \\"DEAD_LETTER_Q\\";\\n public static final String DEAD_LETTER_Q_ROUTING_KEY = \\"DEAD_LETTER_Q_ROUTING_KEY\\";\\n\\n public static final String NORMAL_Q_C = \\"NORMAL_Q_C\\";\\n public static final String NORMAL_Q_C_ROUTING_KEY = \\"NORMAL_Q_C_ROUTING_KEY\\";\\n\\n @Bean\\n public DirectExchange xExchange() {\\n return new DirectExchange(NORMAL_EXCHANGE);\\n }\\n\\n @Bean\\n public DirectExchange deadExchange() {\\n return new DirectExchange(DEAD_LETTER_EXCANGE);\\n }\\n\\n @Bean\\n public Queue queueA() {\\n Map<String, Object> arguments = new HashMap<>();\\n arguments.put(\\"x-dead-letter-exchange\\", DEAD_LETTER_EXCANGE);\\n arguments.put(\\"x-dead-letter-routing-key\\", DEAD_LETTER_Q_ROUTING_KEY);\\n arguments.put(\\"x-message-ttl\\", 10000);\\n\\n return QueueBuilder.durable(NORMAL_Q_A).withArguments(arguments).build();\\n }\\n\\n @Bean\\n public Queue queueB() {\\n Map<String, Object> arguments = new HashMap<>();\\n arguments.put(\\"x-dead-letter-exchange\\", DEAD_LETTER_EXCANGE);\\n arguments.put(\\"x-dead-letter-routing-key\\", DEAD_LETTER_Q_ROUTING_KEY);\\n arguments.put(\\"x-message-ttl\\", 40000);\\n\\n return QueueBuilder.durable(NORMAL_Q_B).withArguments(arguments).build();\\n }\\n\\n @Bean\\n public Queue queueC() {\\n Map<String, Object> arguments = new HashMap<>();\\n arguments.put(\\"x-dead-letter-exchange\\", DEAD_LETTER_EXCANGE);\\n arguments.put(\\"x-dead-letter-routing-key\\", DEAD_LETTER_Q_ROUTING_KEY);\\n\\n return QueueBuilder.durable(NORMAL_Q_C).withArguments(arguments).build();\\n }\\n\\n @Bean\\n public Queue queueD() {\\n return QueueBuilder.durable(DEAD_LETTER_Q).build();\\n }\\n\\n @Bean\\n public Binding bindAToNormalExchange(@Qualifier(\\"queueA\\") Queue queueA,\\n @Qualifier(\\"xExchange\\") DirectExchange xExchange) {\\n return BindingBuilder.bind(queueA).to(xExchange).with(NORMAL_Q_A_ROUTING_KEY);\\n }\\n\\n @Bean\\n public Binding bindBToNormalExchange(\\n @Qualifier(\\"queueB\\") Queue queueB,\\n @Qualifier(\\"xExchange\\") DirectExchange xExchange) {\\n return BindingBuilder.bind(queueB).to(xExchange).with(NORMAL_Q_B_ROUTING_KEY);\\n }\\n\\n @Bean\\n public Binding bindCToNormalExchange(\\n @Qualifier(\\"queueC\\") Queue queueC,\\n @Qualifier(\\"xExchange\\") DirectExchange xExchange) {\\n return BindingBuilder.bind(queueC).to(xExchange).with(NORMAL_Q_C_ROUTING_KEY);\\n }\\n\\n @Bean\\n public Binding bindDeadQueueToDeadExchange(\\n @Qualifier(\\"queueD\\") Queue queueD,\\n @Qualifier(\\"deadExchange\\") DirectExchange deadExchange) {\\n return BindingBuilder.bind(queueD).to(deadExchange).with(DEAD_LETTER_Q_ROUTING_KEY);\\n }\\n}\\n\\n\\n###### Dead-Letter Queue Consumer\\n\\njava\\npackage com.machingclee.rabbitmq.consumer;\\n\\nimport org.springframework.amqp.core.Message;\\nimport org.springframework.amqp.rabbit.annotation.RabbitListener;\\nimport org.springframework.stereotype.Component;\\nimport com.rabbitmq.client.Channel;\\nimport lombok.extern.slf4j.Slf4j;\\n\\n@Slf4j\\n@Component\\npublic class DeadLetterQueueConsumer {\\n @RabbitListener(queues = \\"DEAD_LETTER_Q\\")\\n public void receiveDeadMsg(Message message, Channel channel) {\\n\\n String msg = new String(message.getBody());\\n log.info(\\"Current time: {}, dead queue message: {}\\",\\n new java.util.Date().toString(),\\n msg);\\n }\\n}\\n\\n\\n###### HTTP Handlers to Publish TTL Messages\\n\\njava\\npackage com.machingclee.rabbitmq.controller;\\n\\nimport java.io.IOException;\\nimport java.util.Date;\\nimport java.util.Map;\\n\\nimport org.springframework.amqp.rabbit.core.RabbitTemplate;\\nimport org.springframework.beans.factory.annotation.Autowired;\\nimport org.springframework.stereotype.Component;\\nimport org.springframework.web.servlet.function.ServerRequest;\\nimport org.springframework.web.servlet.function.ServerResponse;\\n\\nimport com.machingclee.rabbitmq.config.DelayedQueueConfig;\\nimport com.machingclee.rabbitmq.config.TTLQueueConfig;\\nimport com.machingclee.rabbitmq.controller.dto.MessageWithTtlDTO;\\n\\nimport jakarta.servlet.ServletException;\\nimport lombok.extern.slf4j.Slf4j;\\n\\n@Slf4j\\n@Component\\npublic class MessageController {\\n\\n private RabbitTemplate rabbitTemplate;\\n\\n @Autowired\\n MessageController(RabbitTemplate template) {\\n this.rabbitTemplate = template;\\n }\\n\\n public ServerResponse sendMessage(ServerRequest req) {\\n var message = req.pathVariable(\\"message\\");\\n\\n rabbitTemplate.convertAndSend(\\n TTLQueueConfig.NORMAL_EXCHANGE,\\n TTLQueueConfig.NORMAL_Q_A_ROUTING_KEY,\\n \\"[Send to 10s ttl queue] \\" + message);\\n\\n log.info(\\"Current Time: {}, sent an message t10s-ttl queues: {}\\",\\n new Date().toString(),\\n message);\\n rabbitTemplate.convertAndSend(\\n TTLQueueConfig.NORMAL_EXCHANGE,\\n TTLQueueConfig.NORMAL_Q_B_ROUTING_KEY,\\n \\"[Send to 40s ttl queue] \\" + message);\\n log.info(\\"Current Time: {}, sent an message to 40s-ttl queues: {}\\",\\n new Date().toString(),\\n message);\\n\\n return ServerResponse.ok().body(Map.of(\\"result\\", message));\\n }\\n\\n public ServerResponse sendMessageWithTTL(ServerRequest req) throws IOException, ServletException {\\n MessageWithTtlDTO body = req.body(MessageWithTtlDTO.class);\\n\\n var message = body.getMessage();\\n var ttl = body.getTtl();\\n\\n rabbitTemplate.convertAndSend(\\n TTLQueueConfig.NORMAL_EXCHANGE,\\n TTLQueueConfig.NORMAL_Q_C_ROUTING_KEY,\\n \\"[custom ttl message: ]\\" + message,\\n msg -> {\\n msg.getMessageProperties().setExpiration(ttl);\\n return msg;\\n });\\n log.info(\\"Custom ttl message was sent: \\" + message);\\n return null;\\n }\\n}\\n\\n\\n##### Delayed Configuration\\n\\n###### DelayedQueueConfig\\n\\njava\\npackage com.machingclee.rabbitmq.config;\\n\\nimport java.util.HashMap;\\nimport java.util.Map;\\n\\nimport org.springframework.amqp.core.Binding;\\nimport org.springframework.amqp.core.BindingBuilder;\\nimport org.springframework.amqp.core.CustomExchange;\\nimport org.springframework.context.annotation.Bean;\\nimport org.springframework.context.annotation.Configuration;\\n\\nimport org.springframework.amqp.core.Queue;\\nimport org.springframework.beans.factory.annotation.Qualifier;\\n\\n@Configuration\\npublic class DelayedQueueConfig {\\n public static final String DELAYED_QUEUE_EXCHANGE = \\"delayed.exchange\\";\\n public static final String DELAYED_QUEUE_NAME = \\"delayed.queue\\";\\n public static final String DELAYED_QUEUE_ROUTING_KEY = \\"delayed.routingKey\\";\\n\\n @Bean\\n public Queue delayedQueue() {\\n return new Queue(DELAYED_QUEUE_NAME);\\n }\\n\\n @Bean\\n public CustomExchange delayedExchange() {\\n Map<String, Object> arguments = new HashMap<>();\\n arguments.put(\\"x-delayed-type\\", \\"direct\\");\\n\\n return new CustomExchange(\\n DELAYED_QUEUE_EXCHANGE,\\n \\"x-delayed-message\\",\\n true,\\n false,\\n arguments);\\n }\\n\\n @Bean\\n public Binding bindDelayedQueueToDelayedExchange(\\n @Qualifier(\\"delayedQueue\\") Queue delayedQueue,\\n @Qualifier(\\"delayedExchange\\") CustomExchange delayedExchange) {\\n return BindingBuilder\\n .bind(delayedQueue)\\n .to(delayedExchange)\\n .with(DELAYED_QUEUE_ROUTING_KEY).noargs();\\n }\\n}\\n\\n\\n###### DelayedQueueConsumer\\n\\njava\\npackage com.machingclee.rabbitmq.consumer;\\n\\nimport java.util.Date;\\n\\nimport org.springframework.amqp.core.Message;\\nimport org.springframework.amqp.rabbit.annotation.RabbitListener;\\nimport org.springframework.stereotype.Component;\\n\\nimport com.machingclee.rabbitmq.config.DelayedQueueConfig;\\n\\nimport lombok.extern.slf4j.Slf4j;\\n\\n@Slf4j\\n@Component\\npublic class DelayedQueueConsumer {\\n @RabbitListener(queues = DelayedQueueConfig.DELAYED_QUEUE_NAME)\\n public void receiveDelayedQueue(Message message) {\\n String msg = new String(message.getBody());\\n log.info(\\"[Delayed Consumer] {}, {}\\", new Date().toString(), msg);\\n }\\n}\\n\\n\\n###### HTTP Handlers to Publish Delayed Messages\\n\\njava\\n@Slf4j\\n@Component\\npublic class MessageController {\\n public ServerResponse delayedMessage(ServerRequest req) {\\n var ttl = Integer.parseInt(req.pathVariable(\\"ttl\\"));\\n var message = req.pathVariable(\\"msg\\");\\n\\n log.info(\\"[message delayed for {}s]: {}\\", ttl, message);\\n rabbitTemplate.convertAndSend(\\n DelayedQueueConfig.DELAYED_QUEUE_EXCHANGE,\\n DelayedQueueConfig.DELAYED_QUEUE_ROUTING_KEY,\\n \\"[message with ttl: \\" + ttl + \\"] \\" + message,\\n msg -> {\\n msg.getMessageProperties().setDelay(ttl);\\n return msg;\\n });\\n\\n return ServerResponse.ok().body(Map.of(\\"result\\", message));\\n }\\n}\\n\\n","title":"Dead-letter and Delayed Queues","date":"2023-12-24T00:00:00.000Z","id":"blog0227","tag":"rabbitMQ, message-broker, java, springboot","intro":"Record the implementation of various queues and integrate them with springboot.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 600px\\n }\\n</style>\\n\\n#### Problem\\n\\nIt is a built-in function in vscode to paste image **_from Clipboard into markdown file_**. However, the behaviour is like this when you simply ctrl+v:\\n\\n![](/assets/img/2024-01-04-03-34-12.png)\\n\\nThis default behaviour has **_two problems_**:\\n\\n- Sometimes you want your directory consists of **only** md files.\\n- images should sit inside static file directory.\\n\\n#### Use Paste Image Plugin\\n\\n<center></center>\\n\\n- First we install the Paste Image plugin:\\n\\n ![](/assets/img/2024-01-04-02-53-59.png)\\n\\n- Next in our project (blog project, documentation project, etc) we add\\n\\n json\\n // .vscode/settings.json\\n {\\n \\"pasteImage.path\\": \\"${projectRoot}/static/img\\",\\n \\"pasteImage.basePath\\": \\"${projectRoot}\\",\\n \\"pasteImage.forceUnixStyleSeparator\\": true,\\n \\"pasteImage.prefix\\": \\"/\\",\\n \\"pasteImage.insertPattern\\": \\"${imageSyntaxPrefix}/img/${imageFileName}${imageSyntaxSuffix}\\"\\n }\\n \\n\\n- Here pasteImage.path is your image destination (to be saved).\\n\\n- Here pasteImage.insertPattern is the path that will be inserted into your md file.\\n\\n- Now you can paste your image from clipbaord by ctrl+alt+v.\\n\\n- For example:\\n json\\n \\"pasteImage.insertPattern\\": \\"${imageSyntaxPrefix}/assets/img/${imageFileName}${imageSyntaxSuffix}\\"\\n \\n produces me a link (because \\"pasteImage.prefix\\": \\"/\\"):\\n md\\n ![](/assets/img/2024-01-04-02-53-59.png)\\n \\n and my project will transform this url to some public asset file path.\\n","title":"Paste Image in Markdown Files from Clipboard","date":"2024-01-04T00:00:00.000Z","id":"blog0230","tag":"md, vscode","intro":"We record a vscode configuration to customize where to save our image when pasting image in md files with automatically generated url inside markdown file.","toc":true},{"content":"\\n- yarn add prisma\\n\\n- Create prisma/schema.prisma\\n\\n js\\n datasource db {\\n provider = \\"postgresql\\"\\n url = env(\\"DATABASE_URL\\")\\n }\\n\\n model student {\\n // or id Int @id @default(autoincrement())\\n id String @id @default(dbgenerated(\\"gen_random_uuid()\\")) @db.Uuid\\n first_name String\\n last_name String\\n email String\\n }\\n \\n\\n- Set up a .env\\n\\n- Use [this env tool](/blog/article/Environment-Variable-by-env-cmdrc) if necessary to distinguish different env\'s.\\n\\n- Use [these docker file](/blog/article/Simple-Postgresql-and-MySQL-Server-from-Docker-Compose) to create a database if necessary for experiment.\\n\\n- Use these [commonly used commands](/blog/article/Commonly-Used-Command-in-Prisma-and-More-Table-Migration-Script) for table migrations.\\n","title":"Minimal Code for Setting Prisma as Just a Table Migration tool","date":"2024-01-05T00:00:00.000Z","id":"blog0231","tag":"prisma, nodejs","intro":"Record minimal code needed to use prisma as just a table migration tool.","toc":false},{"content":"\\n#### Repository\\n\\n- https://github.com/machingclee/2024-01-06-experiment-prisma-with-go-jet\\n\\n#### Usage\\n\\n##### Installations\\n\\n- We will be using this package: https://github.com/go-jet/jet\\n- go get -u github.com/go-jet/jet/v2\\n- go install github.com/go-jet/jet/v2/cmd/jet@latest\\n\\n##### Makefile\\n\\n- Create a Makefile in root directory with content\\n\\n makefile\\n DB_URL = postgresql://pguser:pguser@localhost:5432/udemy\\n\\n from-db:\\n jet -dsn=$(DB_URL)?sslmode=disable -schema=public -path=./.gen -ignore-tables=_prisma_migrations\\n run:\\n go run ./cmd/api/*.go\\n \\n\\n If you are windows user you may choco install make in powershell with admin mode.\\n\\n- make from-db\\n\\n##### pkg/pgsql/pgsql.go\\n\\n- go get github.com/lib/pq (if we use pgsql)\\n- Now in pkg/pgsql/pgsql\\n\\n text\\n package pgsql\\n\\n import (\\n \\"database/sql\\"\\n \\"log\\"\\n\\n _ \\"github.com/lib/pq\\"\\n )\\n\\n var DB *sql.DB\\n\\n func NewDB() *sql.DB {\\n db, err := sql.Open(\\n \\"postgres\\",\\n \\"postgresql://pguser:pguser@localhost:5432/udemy?sslmode=disable\\"\\n )\\n if err != nil {\\n log.Fatal(err)\\n }\\n DB = db\\n return DB\\n }\\n \\n\\n##### pkg/user/handler.go\\n\\n- text\\n package user\\n\\n import (\\n \\"authentication/.gen/udemy/public/model\\"\\n \\"authentication/.gen/udemy/public/table\\"\\n \\"database/sql\\"\\n\\n . \\"github.com/go-jet/jet/v2/postgres\\"\\n )\\n\\n type HandlerRepo struct {\\n db *sql.DB\\n }\\n\\n var Repo *HandlerRepo\\n\\n func NewHandler(db *sql.DB) {\\n Repo = &HandlerRepo{\\n db: db,\\n }\\n }\\n\\n func (m *HandlerRepo) GetUsers() []model.User {\\n db := m.db\\n statement := SELECT(table.User.AllColumns).FROM(table.User)\\n var users []model.User\\n statement.Query(db, &users)\\n return users\\n }\\n\\n \\n\\n##### cmd/api/routes.go\\n\\n- text\\n package main\\n\\n import (\\n \\"authentication/pkg/user\\"\\n \\"fmt\\"\\n \\"net/http\\"\\n\\n \\"github.com/go-chi/chi/v5\\"\\n \\"github.com/go-chi/chi/v5/middleware\\"\\n )\\n\\n func routes() http.Handler {\\n r := chi.NewRouter()\\n r.Use(middleware.Logger)\\n r.Use(middleware.Recoverer)\\n\\n r.Route(\\"/user\\", func(r chi.Router) {\\n r.Get(\\"/\\", func(w http.ResponseWriter, r *http.Request) {\\n u := user.Repo.GetUsers()\\n fmt.Println(u)\\n })\\n })\\n return r\\n }\\n \\n\\n##### cmd/api/main.go\\n\\n- text\\n package main\\n\\n import (\\n \\"authentication/pkg/pgsql\\"\\n \\"authentication/pkg/user\\"\\n \\"log\\"\\n \\"net/http\\"\\n )\\n\\n func main() {\\n db := pgsql.NewDB()\\n user.NewHandler(db)\\n\\n srv := http.Server{\\n Addr: \\":8080\\",\\n Handler: routes(),\\n }\\n\\n err := srv.ListenAndServe()\\n log.Fatal(err)\\n }\\n \\n\\n- We get:\\n\\n text\\n {\\n \\"ID\\": 1,\\n \\"FirstName\\": \\"James\\",\\n \\"LastName\\": \\"Lee\\",\\n \\"Password\\": \\"123\\",\\n \\"Active\\": 1,\\n \\"CreatedAt\\": 1704544094240,\\n \\"UpdatedAt\\": 1704544094240\\n }\\n \\n","title":"Reverse Engineer PostgreSQL to Go Structs","date":"2024-01-06T00:00:00.000Z","id":"blog0232","tag":"go, jet, sql","intro":"Record the procedure the reverse-engineer a PGSQL database.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px\\n }\\n</style>\\n\\n#### Basic Structure by Real Example\\n\\nEvery workflow must be stored inside .github/workflows/name.yml\\n\\n\\n##### Deploy Documentation Project\\n\\nyaml\\nname: Deploy wonderbricks wiki\\non:\\n push:\\n branches:\\n - main\\njobs:\\n deploy:\\n permissions:\\n id-token: write\\n contents: read\\n runs-on: ubuntu-latest\\n environment: deployment\\n steps:\\n - name: Get code\\n uses: actions/checkout@v4\\n - name: Configure AWS Credentials\\n uses: aws-actions/configure-aws-credentials@v4\\n with:\\n role-to-assume: arn:aws:iam::798404461798:role/github-wonderbricks-wiki-cicd\\n aws-region: ap-southeast-2\\n - name: install nodejs\\n uses: actions/setup-node@v4\\n with:\\n node-version: 18\\n - name: Cache dependencies\\n id: cache\\n uses: actions/cache@v3\\n with:\\n path: node_modules\\n key: wonderbricks-wiki-modules-${{ hashFiles(\'**/yarn.lock\') }}\\n - name: NPM Install by yarn\\n if: steps.cache.outputs.cache-hit != \'true\'\\n run: yarn\\n - name: Start Deployment\\n run: yarn deploy\\n\\n\\n#### Fundamentals\\n\\n##### Dependencies\\n\\nyaml\\njobs:\\n deploy:\\n needs: [test, job2]\\n\\n\\n##### Trigger Actions Manually and more Event Triggers\\n\\n- [Documentation of Event Triggers](https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows)\\n- yaml\\n name: Deploy Project\\n on:\\n push:\\n branches:\\n - main\\n workflow_dispatch:\\n \\n\\n##### Access Context Variables\\n\\n- [Documentation of all Context Variables](https://docs.github.com/en/actions/learn-github-actions/contexts)\\n- Accessible by ${{ github }}\\n\\n##### Multiple Branch that Triggers an Action\\n\\nyaml\\nname: Events\\non:\\n pull_request:\\n types:\\n - opened\\n workflow_dispatch:\\n push:\\n branches:\\n - main\\n - \\"dev-*\\"\\n - \\"feat/**\\"\\n\\n\\n##### Skip an Action\\n###### By Specifying Filepaths\\nyaml\\n push:\\n branches:\\n - main\\n - \\"dev-*\\"\\n - \\"feat/**\\"\\n paths-ignore:\\n - \'.github/workflows/*\'\\n\\n###### By Commit Message\\nInclude one of the following in our commit message:\\n- [skip ci]\\n- [ci skip]\\n- [no ci]\\n- [skip actions]\\n- [actions skip]\\n\\n##### Artifacts\\n\\n[Documentation on Artifacts](https://github.com/actions/upload-artifact)\\n\\n###### Upload and zip an Artifact\\nyaml\\n build:\\n runs-on: ubuntu-latest\\n steps:\\n - name: Build website\\n run: npm run build\\n - name: Upload artifacts\\n uses: actions/upload-artifact@v4\\n with:\\n name: dist-files\\n path: |\\n dist\\n package.json\\n\\n###### Retrieve and Unzip the Artifact\\nyaml\\n deploy:\\n needs: build\\n runs-on: ubuntu-latest\\n steps:\\n - name: Get build artifacts\\n uses: actions/download-artifact@v4\\n with:\\n name: dist-files\\n path: some/dir (optional)\\n\\n\\n##### Share Variables Among Jobs\\n###### Declare Output Variable in One Job\\n\\n\\nyaml\\n build:\\n outputs:\\n script-file: ${{ steps.publish.outputs.something }}\\n steps:\\n - name: Get Code\\n uses: actions/checkout@v3\\n - name: Install Dependencies\\n run: yarn\\n - name: Build Website\\n run: yarn build\\n - name: Publish Js Filename\\n id: publish\\n run: find dist/assets/*.js -type f -execdir echo \'something={}\' >> $GITHUB_OUTPUT \';\'\\n\\n\\n\\n###### Retrive this Variable in Other job\\nyaml\\n deploy:\\n needs: build\\n runs-on: ubuntu-latest\\n steps:\\n - name: Get Build Artifacts\\n uses: actions/download-artifact@v4\\n with:\\n name: dist-files\\n - name: Output filename\\n run: echo \\"${{ needs.build.outputs.script-file }}\\"\\n\\n\\n##### Environment Variabls (Workflow Level, Job Level)\\nNote that env can be at **workflow** level or at **job** level.\\nyaml\\nname: Deployment\\non:\\n push:\\n branches:\\n - main\\nenv:\\n MONGODB_DB_NAME: github-actions\\njobs:\\n test:\\n env:\\n MONGO_DB_HOST: localhost:20717\\n MONGODB_USERNAME: test\\n MONGODB_PASSWORD: test\\n\\n\\n##### Versioning Environment Variable\\nIn paid version we can choose the version (via environment key)\\nyaml\\njobs:\\n test:\\n environment: testing\\n env:\\n MONGO_DB_HOST: localhost\\n MONGODB_USERNAME: ${{ secrets.MONGODB_USERNAME }}\\n MONGODB_PASSWORD: ${{ secrets.MONGODB_PASSWORD }}\\n PORT: 8080\\n\\n\\n##### Catching Failure\\n###### Catching for Step\\nOnce any of the previous step fails, we can use failure() the catch it.\\nyaml\\n - name: Test Code\\n id: run-tests\\n run: yarn test\\n - name: Upload Test Report\\n if: failure() && steps.run-tests.output == \'failure\'\\n uses: actions/upload-artifact@v4\\n with:\\n name: test-report\\n path: test.json\\n\\n###### Catching for jobs\\nWe can define a job to wait for at least one failure of the other jobs\\nyaml\\n report:\\n needs: [lint, deploy]\\n if: failure()\\n runs-on: ubuntu-latest\\n steps:\\n - name: Output Information\\n run: |\\n echo \\"Something went wrong\\"\\n echo \\"${{ github }}\\"\\n\\n\\n##### Caching\\nyaml\\n - name: Cache dependencies\\n id: cache\\n uses: actions/cache@v3\\n with:\\n path: node_modules\\n key: modules-${{ hashFiles(\'**/yarn.lock\') }}\\n - name: NPM Install by yarn\\n if: steps.cache.outputs.cache-hit != \'true\'\\n run: yarn\\n\\n\\n##### Continue a job even Error Occurs\\n\\nyaml\\n - name: Test Code\\n continue-on-error: true\\n id: run-tests\\n run: yarn test\\n\\n\\n\\n##### Matrix\\n###### Catesian Products\\n\\n6 jobs will be executed:\\n\\nyaml\\n repeated-job:\\n strategy:\\n matrix:\\n node-version: [12, 14, 16]\\n operating-system: [ubuntu-latest, windows-latest]\\n runs-on: ${{ matrix.operatoring-system }}\\n steps:\\n - name: Echo Node Version\\n run: echo ${{ matrix.node-version }}\\n\\n\\n###### Combinations\\nyaml\\n repeated-job:\\n strategy:\\n matrix:\\n # node-version: [12, 14, 16]\\n # operating-system: [ubuntu-latest, windows-latest]\\n include:\\n - node-version: 18\\n operating-system: ubuntu-latest\\n - node-version: 12\\n operating-system: windows-latest\\n runs-on: ${{ matrix.operatoring-system }}\\n steps:\\n - name: Echo Node Version\\n run: echo ${{ matrix.node-version }}\\n\\n\\n\\n\\n##### Reusable Job\\n\\n###### Define the Reusable Workflow Detail\\nThough it is called **reusable workflow**, but such a workflow is called directly whin a job, they are actually **reusuable jobs**.\\n\\nyaml\\nname: Reusable Deploy\\non:\\n workflow_call:\\n inputs:\\n artifact-name:\\n description: The name of the deployable artifact files\\n required: false\\n default: dist\\n type: string\\n outputs:\\n result:\\n description: The result of the deployment operation\\n value: ${{ jobs.deploy.outputs.outcome }} # declare a variable to be set in subseq job\\n secrets:\\n some-secret:\\n required: false\\n\\njobs:\\n deploy:\\n outputs:\\n outcomes: ${{ steps.set-result.outputs.step-result }} # declare another variable to be set in subseq step\\n runs-on: ubuntu-latest\\n steps:\\n - name: Get Code\\n uses: actions/download-artifact@v4\\n with: \\n name: ${{ inputs.artifact-name }}\\n - name: List Files\\n run: ls\\n - name: Set Step Output\\n id: set-result\\n run: echo \\"step-result=success\\" >> $GITHUB_OUTPUT\\n\\n\\n\\n###### Reuse that job\\n\\nyaml\\n another_deploy:\\n needs: build\\n uses: ./.github/workflows/reusable.yml\\n with:\\n artificat-name: dist-files\\n secrets:\\n some-secret: ${{ secrets.some-secret }}\\n print-deploy-result:\\n needs: another_deploy\\n runs-on: ubuntu-latest\\n steps:\\n - name: Print reusuable deploy output\\n run: echo \\"${{ needs.another_deploy.outputs.result }}\\"\\n\\n\\n\\n##### Container and Service Container\\n###### Running jobs in Container\\n- We can define a container and *run our steps inside that container* as if we are running those jobs in ubuntu-latest as before.\\n\\n- Working inside a container makes perfect sense if ubuntu-latest does not provide everything we need (like we want a virtual environment with specific packages pre-installed).\\n\\nyaml\\njobs:\\n test:\\n environment: testing\\n runs-on: ubuntu-latest\\n container: \\n image: node:16\\n env:\\n MONGO_CONNECTON_PROTOCOL: mongodb+srv\\n MONGODB_CLUSTER_ADDRESS: some-address\\n MONGODB_USERNAME: ${{ secrets.MONCODB_USERNAME }}\\n MONGODB_PASSWORD: ${{ secrets.MONCODB_PASSWORD }}\\n PORT: 8080\\n steps:\\n - name: Get Code\\n uses: action/checkout@v3\\n - name: Cache dependencies\\n ...\\n\\n\\n###### Adding Service Container\\n\\nAdding services attributes will make a workflow very similar to a docker-compose file.\\n\\nyaml\\njobs:\\n test:\\n environment: testing\\n runs-on: ubuntu-latest\\n container: \\n image: node:16\\n env:\\n MONGO_CONNECTON_PROTOCOL: mongodb\\n MONGODB_CLUSTER_ADDRESS: mongodb\\n MONGODB_USERNAME: root\\n MONGODB_PASSWORD: example\\n PORT: 8080\\n services: # <--------------------------------- additional\\n mongodb:\\n image: mongo\\n env:\\n MONGO_INITDB_ROOT_USERNAME: root\\n MONGO_INITDB_ROOT_PASSWORD: example\\n steps:\\n - name: Get Code\\n uses: action/checkout@v3\\n - name: Cache dependencies\\n ...\\n\\n\\n#### Custom Actions\\n\\n##### Composite Actions\\n\\n- First we create a workflow file at\\n\\n - actions/cached-deps/action.yml\\n\\n The filename has to be action.yml. \\n\\n###### Format Without Inputs and Outputs\\n\\n- A standard format of a composite action is:\\n \\n yaml\\n name: \'Get & Cache Dependencies\'\\n decsription: \'Get the dependencies (via npm) and cache them.\'\\n runs:\\n using: \'composite\'\\n steps:\\n - name: Cache dependencies\\n id: cache\\n uses: actions/cache@v3\\n with:\\n path: node_modules\\n key: node-modules-${{ hashFiles(\'**/package-lock.json\') }}\\n - name: Install dependencies\\n if: steps.cache.outputs.cache-hit != \'true\'\\n run: npm ci\\n shell: bash\\n \\n \\n- Note that in composite actions if we use run, then it must be followed by shell, and we need to declare using: \'composite\'\\n\\n- then use it in this way:\\n\\n yaml\\n steps:\\n - name: Custom Load & Cache action\\n uses: ./.github/actions/cached-deps\\n \\n\\n###### Format With Inputs\\n\\n- Custom Action can also accept parameter:\\n\\n yaml\\n name: \'Get & Cache Dependencies\'\\n decsription: \'Get the dependencies (via npm) and cache them.\'\\n inputs:\\n caching:\\n description: \'Whether to cache dependencies or not.\'\\n required: false\\n default: \'true\'\\n runs:\\n using: \'composite\'\\n steps:\\n - name: Cache dependencies\\n if: inputs.caching # <----------- our input\\n id: cache\\n uses: actions/cache@v3\\n with:\\n path: node_modules\\n key: node-modules-${{ hashFiles(\'**/package-lock.json\') }}\\n - name: Install dependencies\\n if: steps.cache.outputs.cache-hit != \'true\'\\n run: npm ci\\n shell: bash\\n \\n\\n##### Javascript Actions\\n\\n###### Without Inputs\\n\\n- We need:\\n - ./.github/actions/deploy-s3-javascript/action.yml\\n - ./.github/actions/deploy-s3-javascript/main.js\\n\\n- yaml\\n # ./.github/actions/deploy-s3-javascript/action.yml\\n name: \'Deploy to AWS S3\'\\n description: \'Deploy a static website via AWS S3\'\\n runs:\\n using: \'node18\'\\n main: \'main.js\'\\n \\n\\n- js\\n // ./.github/actions/deploy-s3-javascript/main.js\\n\\n const core = require(\\"@actions/core\\")\\n const github = require(\\"@actions/github\\")\\n const exec = require(\\"@actions/exec\\");\\n\\n function run() {\\n core.notice(\\"Hello from my custom Javascript Action.\\")\\n }\\n\\n run();\\n \\n- with \\n\\n text\\n npm install @actions/core @actions/github @actions/exec\\n \\n\\n- Return to deploy.yml and define a new job using this javascript action:\\n\\n yaml\\n jobs:\\n information:\\n runs-on: ubuntu-latest\\n steps:\\n - name: Run custom action\\n uses: ./.github/actions/deploy-s3-javascript\\n \\n > Note that at this point if we commit the changes we will get the error:\\n > [![/assets/img/2024-01-19-03-18-59.png](/assets/img/2024-01-19-03-18-59.png)](/assets/img/2024-01-19-03-18-59.png)\\n > this is because we need to checkout the main.js file as well.\\n\\n- Therefore we need the following adjustment:\\n\\n yaml\\n jobs:\\n information:\\n runs-on: ubuntu-latest\\n steps:\\n - name: Get code\\n uses: actions/checkout@v4\\n - name: Run custom action\\n uses: ./.github/actions/deploy-s3-javascript\\n \\n\\n###### With Inputs and Outputs\\n\\n- yaml\\n # ./.github/actions/deploy-s3-javascript/action.yml\\n\\n name: \'Deploy to AWS S3\'\\n description: \'Deploy a static website via AWS S3\'\\n inputs:\\n bucket:\\n description: \'The S3 bucket name\'\\n required: true\\n bucket-region:\\n description: \'The region of the S3 bucket\'\\n required: false\\n default: \'us-east-1\'\\n dist-folder:\\n description: \'The folder containing the deployable files\'\\n required: true\\n outputs:\\n website-url:\\n description: \'The URL of the deployed website\'\\n runs:\\n using: \'node18\'\\n main: \'main.js\'\\n \\n- js\\n // ./.github/actions/deploy-s3-javascript/main.js\\n\\n const core = require(\\"@actions/core\\")\\n const github = require(\\"@actions/github\\")\\n const exec = require(\\"@actions/exec\\");\\n\\n function run() {\\n const bucket = core.getInput(\'bucket\', {required: true});\\n const bucketRegion = core.getInput(\'bucket-region\', {required: true});\\n const distFolder = core.getInput(\'dist-folder\', {required: true});\\n\\n const s3Uri = s3://${bucket};\\n exec.exec(aws s3 sync ${distFolder} ${s3Uri} --region ${bucketRegion})\\n\\n const website = http://${bucket}.s3-website-${bucketRegion}.amazonaws.com\\n core.setOutput(\'website-url\', webSiteUrl);\\n }\\n\\n run(); \\n \\n\\n- We can use this output via the standard trick in other run:\\n yaml\\n run: echo ${{ steps.deploy.outputs.website-url }}\\n \\n\\n##### Docker Actions\\n\\n- We need (take python as an example):\\n - ./.github/actions/deploy-s3-docker/action.yml\\n - ./.github/actions/deploy-s3-docker/deployment.py\\n - ./.github/actions/deploy-s3-docker/Dockerfile\\n - ./.github/actions/deploy-s3-docker/requirements.txt\\n\\n- yaml\\n # ./.github/actions/deploy-s3-docker/action.yml\\n\\n name: \'Deploy to AWS S3\'\\n description: \'Deploy a static website via AWS S3\'\\n inputs:\\n bucket:\\n description: \'The S3 bucket name\'\\n required: true\\n bucket-region:\\n description: \'The region of the S3 bucket\'\\n required: false\\n default: \'us-east-1\'\\n dist-folder:\\n description: \'The folder containing the deployable files\'\\n required: true\\n outputs:\\n website-url:\\n description: \'The URL of the deployed website\'\\n runs:\\n using: \'docker\'\\n main: \'Dockerfile\'\\n \\n- Note that github actions generate env variable with the format:\\n - INPUT_ + <our variable in all cap>\\n py\\n # ./.github/actions/deploy-s3-docker/deployment.py\\n\\n import os\\n import boto3\\n from botocore.config import Config\\n\\n def run():\\n bucket = os.environ[\\"INPUT_BUCKET\\"]\\n bucket_region = os.environ[\\"INPUT_BUCKET_REGION\\"]\\n dist_folder = os.environ[\\"INPUT_DIST-FOLDER\\"]\\n\\n configuration = Config(region_name=bucket_region)\\n\\n s3_client = boto3.client(\\"s3\\", config = configuration)\\n\\n for root, subdirs, files in os.walk(dist_folder):\\n for file in files:\\n s3_client.upload_file(os.path.join(root, file), bucket, file)\\n\\n website_url = f\\"http://{bucket}.s3-website-{bucket_region}.amazonaws.com\\"\\n print(f\\"::set-output name=website-url::{website_url}\\")\\n\\n if __name__ == \\"__main__\\":\\n run()\\n \\n\\n- Dockerfile\\n # ./.github/actions/deploy-s3-docker/Dockerfile\\n\\n FROM python:3\\n\\n COPY requirements.txt /requirements.txt\\n\\n RUN pip install -r requirements.txt\\n\\n COPY deployment.py /deployment.py\\n\\n CMD [\\"python\\", \\"/deployment.py\\"]\\n \\n- ./.github/actions/deploy-s3-docker/requirements.txt\\n text\\n boto3==1.24.71\\n botocore==1.27.71\\n jmespath==1.0.1\\n python-dateutil==2.8.2\\n s3transfer==0.6.0\\n six==1.16.0\\n urllib3==1.26.12\\n \\n\\n","title":"Fundamentals of Github Actions","date":"2024-01-16T00:00:00.000Z","id":"blog0233","tag":"cicd, github-actions","intro":"Fundamental and basic use of github actions.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px\\n }\\n</style>\\n\\n#### The Backend Counterpart in Java\\n\\n- In the past we have written an Algolia search backend in java [**here**](/blog/article/Build-a-Search-Function).\\n\\n- This time we simplify everything by writing all components in nodejs.\\n\\n- Some detail will be ignored in this article as we mainly just focus on the implementation of the corrsponding concept in nodejs.\\n\\n\\n#### Backend/algoliaService\\n\\njs\\n// algoliaService.ts\\nimport algoliasearch, { SearchClient } from \\"algoliasearch\\";\\nimport { jsonArrayFrom } from \\"kysely/helpers/postgres\\";\\nimport { Types } from \\"mongoose\\";\\nimport { db } from \\"../db/kysely/database\\";\\nimport { LLMResultInMongo, LLMSummaryModel } from \\"../db/mongo/models/LLMSummary\\";\\nimport { AlgoliaRecord } from \\"../dto/dto\\";\\nimport logger from \\"../util/logger\\";\\n\\nconst APPLICATION_ID = process.env?.ALGOLIA_APPLICATION_ID || \\"\\";\\nconst ADMIN_API_KEY = process.env?.ALGOLIA_ADMIN_API_KEY || \\"\\";\\nconst INDEX_NAME = process.env?.ALGOLIA_INDEX_NAME || \\"\\"\\nconst FRONTEND_SEARCH_API_KEY = process.env?.ALGOLIA_SEARCH_KEY || \\"\\";\\n\\nlet searchClient: { current: SearchClient | null } = { current: null };\\nconst client = algoliasearch(APPLICATION_ID, ADMIN_API_KEY);\\nconst index = client.initIndex(INDEX_NAME);\\n\\nconst newSearchClient = () => {\\n client.initIndex(INDEX_NAME);\\n logger.info(\\"Algolia Client Inited\\");\\n searchClient.current = client;\\n}\\n\\nconst getSearchClient = () => {\\n return searchClient.current;\\n}\\n\\nconst addRecord = async (newRecord: AlgoliaRecord): Promise<string> => {\\n return new Promise((resolve, reject) => {\\n index.saveObject(newRecord).then((result) => {\\n const { objectID } = result;\\n resolve(objectID);\\n }).catch(err => {\\n reject(err);\\n });\\n })\\n}\\n\\n##### Initialize Algolia DB for Searching, Define Search and Filter attribute.\\n\\n- Here we qurey our database by kysely, this part of code is project-dependnent.\\n\\n- You should have your own logic to get target documents to be searched.\\n\\n- The main point is to exceute index.saveObject(record) to save the search target with objectID being the search target id (used by algolia as an id attribute).\\n\\n- **Search attributes** are the ones whose content will be being seached.\\n\\n- **Filter attributes** are the ones whose content will be used to filter users who have right to get that document.\\n\\njs\\nconst initAlgoliaDB = async () => {\\n const index = client.initIndex(INDEX_NAME);\\n index.clearObjects();\\n const allSessions = await db.selectFrom(\\"MessagesSession\\")\\n .leftJoin(\\"Channel\\", \\"Channel.id\\", \\"MessagesSession.channelId\\")\\n .leftJoin(\\"Project\\", \\"Project.id\\", \\"Channel.projectId\\")\\n .leftJoin(\\"User\\", \\"User.id\\", \\"MessagesSession.hostUserId\\")\\n .innerJoin(\\"LLMSummary\\", \\"LLMSummary.messagesSessionId\\", \\"MessagesSession.id\\")\\n .where(\\"LLMSummary.llmResultMongoOid\\", \\"is not\\", null)\\n .where(eb => eb.or([\\n eb(\\"MessagesSession.type\\", \\"=\\", \\"PUBLIC_CHATROOM\\"),\\n eb.and([\\n eb(\\"MessagesSession.type\\", \\"=\\", \\"PERSONAL_CHATROOM\\"),\\n eb(\\"MessagesSession.isDraftInstantIssue\\", \\"=\\", false)\\n ])\\n ]))\\n .selectAll(\\"MessagesSession\\")\\n .select(\\"LLMSummary.llmResultMongoOid\\")\\n .select(eb => [\\n jsonArrayFrom(eb.\\n selectFrom(\\"UserToJoinedProject\\")\\n .select(\\"UserToJoinedProject.userId as joinedUserId\\")\\n .whereRef(\\"UserToJoinedProject.projectId\\", \\"=\\", \\"Project.id\\")\\n ).as(\\"members\\")\\n ])\\n .select([\\"User.companyId as hostCompanyId\\", \\"User.firstName as hostFirstname\\", \\"User.lastName as hostLastname\\"])\\n .execute();\\n\\n const oids = allSessions.map(session => session.llmResultMongoOid) as string[];\\n const nonnullSummaries = await LLMSummaryModel.find({ _id: { $in: oids } }).lean();\\n const nonnullSummarySessionIdAndResult = allSessions.map(s => {\\n const target = nonnullSummaries.find(summary => summary._id.toString() === s.llmResultMongoOid);\\n return {\\n sessionId: s.id,\\n result: { enResult: target?.result, tcResult: target?.zhResult }\\n }\\n })\\n\\n const resultsToSearch = nonnullSummarySessionIdAndResult.map(data => {\\n const { result, sessionId } = data;\\n const { enResult, tcResult } = result;\\n const aiResult = llmResultToAlgoliaResult({ enResult: enResult || [], tcResult: tcResult || [] });\\n const originalSession = allSessions.find(s => s.id === sessionId);\\n const { name, members } = originalSession!;\\n return {\\n objectID: sessionId,\\n members: members.map(m => m.joinedUserId),\\n name,\\n aiResult,\\n }\\n }) as AlgoliaRecord[];\\n\\n try {\\n for (const result of resultsToSearch) {\\n index.saveObject(result).catch(err => {\\n console.log(err);\\n })\\n }\\n } catch (err) {\\n console.log(JSON.stringify(err));\\n }\\n\\n index.setSettings({\\n attributesForFaceting: [\\n \\"searchable(name)\\",\\n \\"searchable(aiResult)\\",\\n \\"searchable(username)\\",\\n \\"filterOnly(members)\\"\\n ],\\n })\\n}\\n\\n\\n##### Util Functions\\n\\n\\njs\\n// by the way the flatten here can be simply replaced by flatMap\\nconst flatten = (results: ((string | undefined | null)[])[]) => {\\n return results.reduce<string[]>((prev, curr) => {\\n const curr_ = curr?.filter(c => c) as string[];\\n const prev_ = prev?.filter(p => p) as string[];\\n return [...curr_, ...prev_]\\n }, [])\\n}\\n\\nconst llmResultToAlgoliaResult = (params: {\\n enResult: LLMResultInMongo[],\\n tcResult: LLMResultInMongo[]\\n}) => {\\n const { enResult, tcResult } = params;\\n const enTitle = enResult?.[0]?.title || \\"\\";\\n const zhTitle = tcResult?.[0]?.title || \\"\\";\\n const titles = [enTitle, zhTitle].filter(str => str) as string[];\\n const enSummary = enResult.map(result => result.summary);\\n const enKeyPoints = flatten(enResult.map(result => result.keyPoints));\\n const enTags = flatten(enResult.map(result => result.keywords));\\n const tcSummary = tcResult?.map(result => result.summary) || [];\\n const tcKeyPoints = flatten(tcResult?.map(result => result.keyPoints) || []);\\n const tcTags = flatten(tcResult?.map(result => result.keywords) || []);\\n\\n const aiResult = [\\n ...titles,\\n ...enKeyPoints,\\n ...tcKeyPoints,\\n ...enSummary,\\n ...tcSummary,\\n ...enTags,\\n ...tcTags\\n ] as string[]\\n\\n return aiResult;\\n}\\n\\nconst getAllMembers = async (sessionId: string) => {\\n const idResults = await db.selectFrom(\\"MessagesSession\\")\\n .leftJoin(\\"Channel\\", \\"Channel.id\\", \\"MessagesSession.channelId\\")\\n .leftJoin(\\"Project\\", \\"Project.id\\", \\"Channel.projectId\\")\\n .leftJoin(\\"UserToJoinedProject\\", \\"UserToJoinedProject.projectId\\", \\"Project.id\\")\\n .select(\\"UserToJoinedProject.userId\\")\\n .where(\\"MessagesSession.id\\", \\"=\\", sessionId)\\n .execute();\\n const ids = idResults.map(result => result?.userId).filter(id => id) as string[];\\n return ids;\\n}\\n\\nconst createSearchApiKey = (userId: string) => {\\n const client = getSearchClient();\\n // doc: https://www.algolia.com/doc/deprecated/api-clients/javascript/v3/methods/generate-secured-api-key/?client=javascript#examples\\n const restrictedAPIKey = client?.generateSecuredApiKey(\\n FRONTEND_SEARCH_API_KEY,\\n { filters: members:${userId} }\\n );\\n\\n return { restrictedAPIKey, applicationId: APPLICATION_ID, index: INDEX_NAME };\\n}\\n\\nexport default {\\n getAllMembers,\\n llmResultToAlgoliaResult,\\n addRecord,\\n initAlgoliaDB,\\n newSearchClient,\\n getSearchClient,\\n createSearchApiKey\\n}\\n\\n\\n##### Create API searchKey to Restrict Document Access\\n\\nRecall the createSearchApiKey function in algoliaService defined above, we have restrict the access of documents by the criterion:\\njs\\n{ filters: members:${userId} }\\n\\nwhich means that:\\n- We have an attribute members: string[] in our document. \\n- The new apikey for searching can hit the target only when members.include(userId).\\n\\nNext, since the applicationId and searchIndex can be defined in the backend and passed to frontend (though you can save the same set of strings in frontend as well, but then we loss the single source of truth), we also return them.\\n\\njs\\nsearchRouter.get(\\"/api-key\\", (req, res) => {\\n const userId = req.user?.userId;\\n if (!userId) {\\n throw new Error(\\"uesrId cannot be found\\");\\n }\\n const { restrictedAPIKey, applicationId, index } = algoliaService.createSearchApiKey(userId);\\n res.json({\\n success: true,\\n result: { securedAPIKey: restrictedAPIKey, applicationId, index }\\n })\\n});\\n\\n\\n\\n\\n#### Frontend \\n##### Algolia Class and Static Methods\\n\\njs\\nimport algoliasearch, { SearchClient } from \'algoliasearch/lite\';\\nimport apiClient from \\"../axios/apiClient\\";\\nimport apiRoutes from \\"../axios/apiRoutes\\";\\nimport { WBResponse } from \\"../axios/responseTypes\\";\\nimport msgUtil from \\"./msgUtil\\";\\n\\nexport class Algolia {\\n private static client: SearchClient | null = null;\\n private static index: string = \\"\\";\\n private static applicationId = \\"\\";\\n private static securedAPIKey = \\"\\";\\n\\n public static getSearchData = () => {\\n return {\\n index: this.index,\\n applicationId: this.applicationId,\\n securedAPIKey: this.securedAPIKey\\n };\\n }\\n\\n public static getClient = async () => {\\n if (!this.client) {\\n const res = await apiClient.get<WBResponse<{ applicationId: string, securedAPIKey: string, index: string }>>(apiRoutes.GET_SECURED_API_KEY);\\n const { success } = res.data;\\n if (success) {\\n const { applicationId, securedAPIKey, index } = res.data.result;\\n this.index = index;\\n this.applicationId = applicationId;\\n this.securedAPIKey = securedAPIKey;\\n this.client = algoliasearch(applicationId, securedAPIKey);\\n }\\n else {\\n msgUtil.persistedError(\\"Cannot get API key\\");\\n }\\n }\\n if (!this.client) {\\n msgUtil.persistedError(\\"Cannot get search client\\");\\n return null;\\n }\\n return this.client;\\n }\\n}\\n\\n\\n##### Usage\\n\\njs\\nexport type SearchResult = {\\n results: {\\n hits: {\\n aiResult: string[],\\n objectID: string,\\n username: string\\n }[]\\n }[]\\n}\\nexport type Hits = SearchResult[\\"results\\"][0][\\"hits\\"];\\n // inside a component\\n const updateSearchResult = useMemo(() => debounce(async (text: string) => {\\n const algoliaClient = await Algolia.getClient();\\n const { index } = Algolia.getSearchData();\\n const searchResult = await algoliaClient?.search([{\\n indexName: index,\\n query: text,\\n params: { hitsPerPage: 5, attributesToRetrieve: [\\"aiResult\\", \\"username\\", \\"objectID\\"] }\\n }], {}) as SearchResult;\\n setHits(searchResult.results?.[0].hits);\\n }, 500), []);\\n\\n\\n- Here you can save a more comprehensive search target in algolia db to directly display the searchResult. \\n\\n- Or you can make use of the objectID to directly fetch desired target from your own database.","title":"Algolia Revisit","date":"2024-01-21T00:00:00.000Z","id":"blog0234","tag":"react, algolia, searching, nodejs","intro":"In the past we have discussed algolia backend and the corresponding frontend, revisit this topic with all code written in nodejs","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px\\n }\\n</style>\\n\\n#### .github/workflows/deployment.yml\\n\\nyaml\\nname: Deploy backend\\non:\\n push:\\n branches:\\n - \\"release/*/*\\"\\njobs:\\n deployment:\\n runs-on: ubuntu-latest\\n environment: deployment\\n env:\\n AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\\n AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\\n REGION: ap-southeast-2\\n steps:\\n - name: Get code\\n uses: actions/checkout@v4\\n - name: Run custom action to deploy aws fargate\\n uses: ./.github/actions/fargate-deployment\\n with:\\n branch_name: ${{ github.ref_name }}\\n image-registry: \\"798404461798.dkr.ecr.ap-southeast-2.amazonaws.com\\"\\n stage: poc\\n image-name: billie-v3-poc\\n task-family: billie-chat-poc\\n cluster-name: billie-chat-poc\\n service-name: billie-chat-poc\\n region: ap-southeast-2\\n\\n\\n#### .github/actions/fargate-deployment/action.yml\\n\\nyaml\\nname: \\"Deploy to AWS Fargate\\"\\ndescription: \\"Build and Deploy an image to AWS fargate\\"\\n\\ninputs:\\n branch_name:\\n description: The target branch being deployed\\n required: true\\n stage:\\n description: uat, poc or prod\\n required: true\\n image-registry:\\n description: \\"Image registry\\"\\n required: true\\n image-name:\\n description: \\"Image name\\"\\n required: true\\n task-family:\\n description: \\"Task family\\"\\n required: true\\n cluster-name:\\n description: \\"Cluster name\\"\\n required: true\\n service-name:\\n description: \\"Service name\\"\\n required: true\\n region:\\n description: \\"region\\"\\n required: true\\n\\nruns:\\n using: \\"node20\\"\\n main: \\"main.js\\"\\n\\n\\n#### .github/actions/fargate-deployment/main.js\\n\\ntext\\nnpm install @actions/core @actions/github @actions/exec\\n\\n\\njs\\nconst core = require(\\"@actions/core\\");\\nconst github = require(\\"@actions/github\\");\\nconst exec = require(\\"@actions/exec\\");\\nconst fs = require(\\"fs\\");\\nconst { v4: uuid } = require(\\"uuid\\");\\n\\nconst cmd = async (...commands) => {\\n const filepath = uuid() + \\".sh\\";\\n let commandStr = \\"\\";\\n for (const command of commands) {\\n commandStr += \\"\\\\n\\" + command;\\n }\\n fs.writeFileSync(filepath, commandStr, { encoding: \\"utf-8\\" });\\n await exec.exec(sh ${filepath});\\n};\\n\\nasync function run() {\\n const branchName = core.getInput(\\"branch_name\\", { required: true });\\n const stage = core.getInput(\\"stage\\", { required: true });\\n const IMAGE_REGISTRY = core.getInput(\\"image-registry\\", { required: true });\\n const IMAGE_NAME = core.getInput(\\"image-name\\", { required: true });\\n const TASK_FAMILY = core.getInput(\\"task-family\\", { required: true });\\n const REGION = core.getInput(\\"region\\", { required: true });\\n const CLUSTER_NAME = core.getInput(\\"cluster-name\\", { required: true });\\n const SERVICE_NAME = core.getInput(\\"service-name\\", { required: true });\\n\\n core.notice(I am working on branch ${branchName});\\n\\n const context = github.context;\\n const runNumber = context.runNumber;\\n const splitData = branchName.split(\\"/\\"); // [\\"release\\", \\"v3\\", \\"uat\\"];\\n const version = splitData[1];\\n const newTag = ${stage}-${version}-${runNumber};\\n\\n await cmd(\\n aws ecr get-login-password --region ${REGION} | docker login --username AWS --password-stdin ${IMAGE_REGISTRY},\\n docker build -t ${IMAGE_REGISTRY}/${IMAGE_NAME}:${newTag} -f Dockerfile.${stage} .,\\n docker push ${IMAGE_REGISTRY}/${IMAGE_NAME}:${newTag}\\n );\\n\\n const tmpTaskDefinitionPath = \\"latest_task_definition_cicd.json\\";\\n\\n await cmd(\\n latest_task_definition=$(aws ecs describe-task-definition --task-definition ${TASK_FAMILY} --query \'taskDefinition\' --region ${REGION}),\\n echo $latest_task_definition > \\"${tmpTaskDefinitionPath}\\"\\n );\\n await exec.exec(\\"ls\\");\\n const jsonString = fs.readFileSync(tmpTaskDefinitionPath, {\\n encoding: \\"utf-8\\",\\n });\\n core.notice(I am working on jsonString ${jsonString});\\n const taskDefinition = JSON.parse(jsonString);\\n const containerDefinition = taskDefinition.containerDefinitions[0];\\n const imageUri = containerDefinition.image;\\n const imguriTagRegex = /(?<=:).*?$/g; // should be only 1 occurrence\\n const newimageUri = imageUri.replace(imguriTagRegex, (tag) => newTag); // replace\\n\\n // https://github.com/aws/aws-sdk/issues/406\\n await cmd(\\n TASK_DEFINITION=$(aws ecs describe-task-definition --task-definition ${TASK_FAMILY} --region ${REGION}),\\n NEW_TASK_DEFINTIION=$(echo $TASK_DEFINITION | jq --arg IMAGE ${newimageUri} \'.taskDefinition | .containerDefinitions[0].image = $IMAGE | del(.taskDefinitionArn) | del(.revision) | del(.status) | del(.requiresAttributes) | del(.compatibilities) | del(.registeredAt) | del(.registeredBy)\'),\\n aws ecs register-task-definition --region ${REGION} --cli-input-json \\"$NEW_TASK_DEFINTIION\\"\\n );\\n\\n await cmd(\\n TASK_DEFINITION=$(aws ecs describe-task-definition --task-definition ${TASK_FAMILY} --region ${REGION}),\\n echo $TASK_DEFINITION,\\n revision=$(echo $TASK_DEFINITION | jq \'.taskDefinition.revision\'),\\n aws ecs update-service --cluster ${CLUSTER_NAME} --service ${SERVICE_NAME} --region ${REGION} --task-definition ${TASK_FAMILY}:$revision\\n );\\n}\\n\\nrun();\\n\\n","title":"Github Action for Deployment on ECS Fargate","date":"2024-01-22T00:00:00.000Z","id":"blog0235","tag":"cicd, github-actions","intro":"We study how to automate the deployment process with containerized backend image.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px\\n }\\n</style>\\n\\n#### Queues Structure\\n\\n[![](/assets/img/2024-03-02-23-57-02.png)](/assets/img/2024-03-02-23-57-02.png)\\n\\n\\n#### File Structure\\n\\n![](/assets/img/2024-01-28-00-46-02.png)\\n\\nLet\'s explore these files one by one.\\n\\n##### QueueName.ts\\n\\njs\\nenum QueueName {\\n LLM_SUMMARY_STEP = \\"LLM_SUMMARY_STEP\\",\\n LLM_TRANSLATION_STEP = \\"LLM_TRANSLATION_STEP\\",\\n LLM_REPLY_STEP = \\"LLM_REPLY_STEP\\",\\n INSERT_ALGOLIOA = \\"INSERT_ALGOLIOA\\",\\n UPDATE_ALGOLIA = \\"UPDATE_ALGOLIA\\",\\n FLASK_EXCEL_GENERATION = \\"FLASK_EXCEL_GENERATION\\",\\n SNOOZE_AND_PIN = \\"SNOOZE_AND_PIN\\",\\n SNOOZE_AND_PIN_DEAD_LETTER = \\"SNOOZE_AND_PIN_DEAD_LETTER\\",\\n EXPORT_REPORT = \\"EXPORT_REPORT\\",\\n LLM_USAGE_LOG = \\"LLM_USAGE_LOG\\",\\n LLM_IMPACT_UPDATE = \\"LLM_IMPACT_UPDATE\\"\\n}\\n\\nexport default QueueName;\\n\\n\\n##### RoutingKey.ts\\n\\njs\\nenum QueueName {\\n LLM_SUMMARY_STEP = \\"LLM_SUMMARY_STEP\\",\\n LLM_TRANSLATION_STEP = \\"LLM_TRANSLATION_STEP\\",\\n LLM_REPLY_STEP = \\"LLMREPLY_STEP\\",\\n INSERT_ALGOLIOA = \\"INSERT_ALGOLIOA\\",\\n UPDATE_ALGOLIA = \\"UPDATE_ALGOLIA\\",\\n FLASK_EXCEL_GENERATION = \\"FLASK_EXCEL_GENERATION\\",\\n SNOOZE_AND_PIN = \\"SNOOZE_AND_PIN\\",\\n SNOOZE_AND_PIN_DEAD_LETTER = \\"SNOOZE_AND_PIN_DEAD_LETTER\\",\\n EXPORT_REPORT = \\"EXPORT_REPORT\\",\\n LLM_USAGE_LOG = \\"LLM_USAGE_LOG\\",\\n LLM_IMPACT_UPDATE = \\"LLM_IMPACT_UPDATE\\"\\n}\\n\\nexport default QueueName;\\n\\n\\n##### channels.ts\\n\\njs\\nimport amqplib, { Channel, Replies } from \\"amqplib\\";\\nimport logger from \\"../util/logger\\";\\nimport QueueName from \\"./QueueName\\";\\nimport queueBinding from \\"./queueBinding\\";\\n\\nconst AMQP_URL = process.env.AMQP_URL || \\"\\"\\n\\nexport const NORMAL_EXCHANGE = \\"Billie\\"\\nexport const GENERAL_DEAD_EXCHANGE = \\"GENERAL_DEAD_EXCHANGE\\"\\n\\nconst Q_CAPACITY = Number(process.env.Q_CAPACITY || \\"10\\");\\n\\ntype ChannelRef = { current: Channel | null };\\n\\nconst llmTaskChannelRef: ChannelRef = { current: null };\\nconst normalTaskChanenlRef: ChannelRef = { current: null };\\n\\n\\nconst getLLMTaskChannel = () => llmTaskChannelRef.current;\\nconst getNormalTaskChannel = () => normalTaskChanenlRef.current;\\n\\nconst queues: { [k in QueueName]?: Replies.AssertQueue } = {};\\n\\nconst getQueue = (queueName: QueueName) => queues?.[queueName];\\n\\nconst initChannels = async (consumptions: (() => void)[]) => {\\n // create channels\\n const connection = await amqplib.connect(AMQP_URL);\\n const llmTaskChannel = await connection.createChannel();\\n const normalTaskChannel = await connection.createChannel();\\n\\n llmTaskChannel.prefetch(Q_CAPACITY);\\n llmTaskChannelRef.current = llmTaskChannel;\\n normalTaskChanenlRef.current = normalTaskChannel;\\n\\n try {\\n await llmTaskChannel.assertExchange(NORMAL_EXCHANGE, \\"direct\\", { durable: true });\\n await llmTaskChannel.assertExchange(GENERAL_DEAD_EXCHANGE, \\"direct\\", { durable: true });\\n } catch (err) {\\n console.log(err);\\n }\\n\\n await queueBinding({ queues, llmTaskChannel, normalTaskChannel });\\n\\n for (const consumption of consumptions) {\\n consumption();\\n }\\n\\n logger.info(\\"Channels Inited\\")\\n}\\n\\nexport default {\\n getQueue,\\n getLLMTaskChannel,\\n getNormalTaskChannel,\\n initChannels\\n}\\n\\n\\n- Here queueBinding and consumptions are the most important components of our queue system.\\n\\n##### queueBinding.ts\\n\\njs\\nimport { Channel, Replies } from \\"amqplib\\";\\nimport QueueName from \\"./QueueName\\";\\nimport RoutingKey from \\"./RoutingKey\\";\\nimport { GENERAL_DEAD_EXCHANGE, NORMAL_EXCHANGE } from \\"./channels\\";\\nconst SNOOZE_PIN_TTL = Number(process.env.SNOOZE_PIN_TTL || \\"604800000\\");\\n\\ntype KeyBindingConfig<Q, R> = {\\n queueName: Q\\n routingKey: R,\\n channel: Channel,\\n exchange: string,\\n deadLetter?: {\\n deadLetterExchange?: string,\\n deadLetterRoutingKey?: R,\\n messageTtl?: number\\n }\\n}\\n\\nconst bind = async (\\n queues: { [key in QueueName]?: Replies.AssertQueue },\\n config: KeyBindingConfig<QueueName, RoutingKey>\\n) => {\\n const { channel, queueName, exchange, routingKey,\\n deadLetter = {}\\n } = config;\\n const q = await channel.assertQueue(queueName, {\\n durable: true,\\n ...deadLetter\\n });\\n queues[queueName] = q;\\n await channel.bindQueue(queueName, exchange, routingKey);\\n}\\n\\nconst queueBinding = async (args: {\\n queues: { [key in QueueName]?: Replies.AssertQueue },\\n llmTaskChannel: Channel,\\n normalTaskChannel: Channel\\n}) => {\\n const { queues, llmTaskChannel, normalTaskChannel } = args;\\n\\n const queueConfigs: KeyBindingConfig<QueueName, RoutingKey>[] = [\\n {\\n queueName: QueueName.LLM_SUMMARY_STEP,\\n routingKey: RoutingKey.LLM_SUMMARY_STEP,\\n channel: llmTaskChannel,\\n exchange: NORMAL_EXCHANGE\\n },\\n {\\n queueName: QueueName.LLM_TRANSLATION_STEP,\\n routingKey: RoutingKey.LLM_TRANSLATION_STEP,\\n channel: llmTaskChannel,\\n exchange: NORMAL_EXCHANGE\\n },\\n {\\n queueName: QueueName.LLM_REPLY_STEP,\\n routingKey: RoutingKey.LLM_REPLY_STEP,\\n channel: llmTaskChannel,\\n exchange: NORMAL_EXCHANGE\\n },\\n {\\n queueName: QueueName.SNOOZE_AND_PIN,\\n routingKey: RoutingKey.SNOOZE_AND_PIN,\\n channel: normalTaskChannel,\\n exchange: NORMAL_EXCHANGE,\\n deadLetter: {\\n deadLetterExchange: GENERAL_DEAD_EXCHANGE,\\n deadLetterRoutingKey: RoutingKey.SNOOZE_AND_PIN_DEAD_LETTER,\\n messageTtl: SNOOZE_PIN_TTL\\n }\\n },\\n {\\n queueName: QueueName.SNOOZE_AND_PIN_DEAD_LETTER,\\n routingKey: RoutingKey.SNOOZE_AND_PIN_DEAD_LETTER,\\n channel: normalTaskChannel,\\n exchange: GENERAL_DEAD_EXCHANGE,\\n },\\n {\\n queueName: QueueName.INSERT_ALGOLIOA,\\n routingKey: RoutingKey.INSERT_ALGOLIOA,\\n channel: normalTaskChannel,\\n exchange: NORMAL_EXCHANGE,\\n },\\n {\\n queueName: QueueName.UPDATE_ALGOLIA,\\n routingKey: RoutingKey.UPDATE_ALGOLIA,\\n channel: normalTaskChannel,\\n exchange: NORMAL_EXCHANGE,\\n },\\n {\\n queueName: QueueName.FLASK_EXCEL_GENERATION,\\n routingKey: RoutingKey.FLASK_EXCEL_GENERATION,\\n channel: normalTaskChannel,\\n exchange: NORMAL_EXCHANGE,\\n },\\n {\\n queueName: QueueName.EXPORT_REPORT,\\n routingKey: RoutingKey.EXPORT_REPORT,\\n channel: normalTaskChannel,\\n exchange: NORMAL_EXCHANGE,\\n },\\n {\\n queueName: QueueName.LLM_USAGE_LOG,\\n routingKey: RoutingKey.LLM_USAGE_LOG,\\n channel: normalTaskChannel,\\n exchange: NORMAL_EXCHANGE\\n },\\n {\\n queueName: QueueName.LLM_IMPACT_UPDATE,\\n routingKey: RoutingKey.LLM_IMPACT_UPDATE,\\n channel: normalTaskChannel,\\n exchange: NORMAL_EXCHANGE\\n }\\n ]\\n\\n for (const queueConfig of queueConfigs) {\\n await bind(queues, queueConfig);\\n }\\n}\\n\\nexport default queueBinding;\\n\\n\\n##### consumptions.ts\\n\\njs\\nimport llmSummaryQueue from \\"./llmSummaryQueue\\";\\nimport algoliaUpdateQueue from \\"./algoliaUpdateQueue\\";\\nimport excelGenReqToFlaskQueue from \\"./excelGenReqToFlaskQueue\\";\\nimport llmReplyQueue from \\"./llmReplyQueues\\";\\nimport llmTranslationQueue from \\"./llmTranslationQueue\\";\\nimport llmUsageLogQueue from \\"./llmUsageLogQueues\\";\\nimport snoozeAndPinDeadLetterQueue from \\"./snoozeAndPinDeadLetterQueue\\";\\nimport snoozeAndPinQueue from \\"./snoozeAndPinQueue\\";\\nimport llmImpactUpdateQueue from \\"./llmImpactUpdateQueue\\";\\n\\nexport default [\\n algoliaUpdateQueue.initConsumption,\\n excelGenReqToFlaskQueue.initConsumption,\\n llmReplyQueue.initConsumption,\\n llmSummaryQueue.initConsumption,\\n llmTranslationQueue.initConsumption,\\n llmUsageLogQueue.initConsumption,\\n snoozeAndPinDeadLetterQueue.initConsumption,\\n // snoozeAndPinQueue.initConsumption, <---- don\'t add this \\n llmImpactUpdateQueue.initConsumption,\\n];\\n\\n\\ninitConsumption is a method of our custom MessageQueue class which simplify our code by templating the boilerplate code:\\n\\n##### MessageQueue Class (model/MessageQueue.ts)\\n\\njs\\nimport { Channel } from \\"amqplib\\";\\nimport { MessageErrorModel } from \\"../../db/mongo/models/MessageErrorLog\\";\\nimport gmailService from \\"../../service/gmailService\\";\\nimport logger from \\"../../util/logger\\";\\nimport QueueName from \\"../QueueName\\";\\nimport RoutingKey from \\"../RoutingKey\\";\\nimport channels, { NORMAL_EXCHANGE } from \\"../channels\\";\\n\\nconst ERROR_EMAIL_RECEIVER = process.env.ERROR_EMAIL_RECEIVER;\\nconst env = process.env.env;\\n\\nexport default class MessageQueue<MessageType> {\\n private queueName: QueueName;\\n private routingKey: RoutingKey;\\n private channel: () => (Channel | null);\\n private exchange = NORMAL_EXCHANGE;\\n public consumption?: (decoded: MessageType) => void | Promise<void>;\\n\\n constructor(args: {\\n queueName: QueueName,\\n routingKey: RoutingKey,\\n channel: () => (Channel | null),\\n consumption?: (decoded: MessageType) => void | Promise<void>,\\n exchange?: string\\n }) {\\n const { exchange = NORMAL_EXCHANGE } = args\\n this.queueName = args.queueName;\\n this.routingKey = args.routingKey;\\n this.channel = args.channel;\\n this.consumption = args.consumption;\\n this.exchange = exchange;\\n }\\n\\n public publish = (msg: MessageType) => {\\n const refinedMsg = msg as MessageType & { routingKey: RoutingKey };\\n refinedMsg.routingKey = this.routingKey;\\n const msg_ = JSON.stringify(refinedMsg);\\n\\n this.channel()?.publish(\\n this.exchange,\\n this.routingKey,\\n Buffer.from(msg_)\\n );\\n }\\n\\n public initConsumption = () => {\\n const q = channels.getQueue(this.queueName);\\n if (!this.channel()) {\\n throw new Error(llmTaskChannel cannot be established);\\n }\\n if (!q) {\\n throw new Error( q: ${this.routingKey} cannot be established);\\n }\\n this.channel()?.consume(q.queue, (msg) => {\\n const msg_ = msg?.content.toString();\\n const decodedMsg: MessageType = JSON.parse(msg_ || \'{ \\"message\\": \\"msg_ is null\\" }\');\\n if (!msg?.content) {\\n logger.info(\\"null message\\");\\n return;\\n }\\n try {\\n logger.info([${this.routingKey}]: processing msg ${msg_})\\n const result = this.consumption?.(decodedMsg);\\n if (result instanceof Promise) {\\n // synchronous call cannot catch the error thrown inside a promise.\\n result\\n .then(() => {\\n this.channel()?.ack(msg);\\n }).catch(err => {\\n const errorLog = new MessageErrorModel({\\n msg: {\\n err: err?.message || \\"\\",\\n step: (decodedMsg as { routingKey?: string }).routingKey || \\"\\",\\n param: decodedMsg\\n }\\n })\\n gmailService.sendEmail({\\n to: ERROR_EMAIL_RECEIVER || \\"\\",\\n html: <div>\\n <div>Error Message:\\n <p/>\\n <div>\\n ${JSON.stringify(decodedMsg, null, 2)}\\n </div>\\n <p>The same message is also logged in mongodb.</p>\\n </div>,\\n subject: Error message from ${env?.toUpperCase()} environment,\\n text: Error message from ${env?.toUpperCase()} environment\\n })\\n errorLog.save().then(() => {\\n this.channel()?.nack(msg, false, false);\\n })\\n });\\n } else {\\n this.channel()?.ack(msg);\\n }\\n } catch (err) {\\n const errorLog = new MessageErrorModel({\\n msg: {\\n err: (err as { message?: string })?.message || \\"\\",\\n step: (decodedMsg as { routingKey?: string }).routingKey || \\"\\",\\n param: decodedMsg\\n }\\n })\\n gmailService.sendEmail({\\n to: ERROR_EMAIL_RECEIVER || \\"\\",\\n html: <div>\\n <div>Error Message:\\n <p/>\\n <div>\\n ${JSON.stringify(decodedMsg, null, 2)}\\n </div>\\n <p>The same message is also logged in mongodb.</p>\\n </div>,\\n subject: Error message from ${env?.toUpperCase()} environment,\\n text: Error message from ${env?.toUpperCase()} environment\\n })\\n errorLog.save().then(() => {\\n this.channel()?.nack(msg, false, false);\\n })\\n }\\n }, { noAck: false });\\n }\\n}\\n\\n\\n- Here the try-catch logic seems a bit repetitive.\\n- But note that a normal try-catch would not catch the error thrown inside a promise, therefore we need to repeatedly catch the error.\\n- Here we reject a message by either throwing an error explicitly (for example, we might want to try catch to execute custom logging logic, and then throw the error again)\\n- or by letting the program throw any error.\\n\\n#### Example of Queues\\n\\nNote that by using MessageQueue class we can pay all our attention to writing consumption logic.\\n\\n##### Normal Task Queue\\n###### excelGenReqToFlaskQueue\\n\\njs\\nimport LLMStatus from \\"../../constants/LLMStatus\\";\\nimport { db } from \\"../../db/kysely/database\\";\\n\\nimport { MessageErrorModel } from \\"../../db/mongo/models/MessageErrorLog\\";\\nimport { SummaryLangChoice } from \\"../../dto/dto\\";\\nimport chatService from \\"../../service/chatService\\";\\nimport RedisUtil from \\"../../util/RedisUtil\\";\\nimport QueueName from \\"../QueueName\\";\\nimport RoutingKey from \\"../RoutingKey\\";\\nimport channels from \\"../channels\\";\\nimport MessageQueue from \\"../model/MessageQueue\\";\\n\\nconst llmTaskChannel = () => channels.getLLMTaskChannel();\\n\\nconst excelGenReqToFlaskQueue = new MessageQueue<{\\n roomId: string, lang: SummaryLangChoice\\n}>({\\n channel: llmTaskChannel,\\n queueName: QueueName.FLASK_EXCEL_GENERATION,\\n routingKey: RoutingKey.FLASK_EXCEL_GENERATION,\\n consumption: async (decoded) => {\\n const { lang, roomId } = decoded;\\n try {\\n await chatService.dispatchExcelGenerationTaskToFlask(roomId, lang);\\n await RedisUtil.setLLMStatus(roomId, LLMStatus.FINISHED);\\n } catch (err) {\\n const errorLog = new MessageErrorModel({\\n msg: {\\n err: err,\\n step: \\"excelGenReqToFlaskQueue\\",\\n param: decoded\\n }\\n })\\n // only session will use this excel generation function\\n await db.updateTable(\\"MessagesSession\\")\\n .set({ isLiveEnded: false }).where(\\"MessagesSession.id\\", \\"=\\", roomId).execute();\\n await errorLog.save();\\n RedisUtil.setLLMStatus(roomId, LLMStatus.FAILED);\\n throw err;\\n }\\n }\\n})\\n\\nexport default excelGenReqToFlaskQueue;\\n\\n\\n##### Dead-Letter Queues\\n\\n###### snoozeAndPinQueue.ts\\n\\nNote that this queue is supposed to be a delayed task queue, ***no consumption should be inited***. Otherwise we have to at least ack, nack, reject which violates our purpose to let the message expire automatically.\\n\\njs\\nimport { SnoozeAndPinMessage } from \\"../../dto/dto\\";\\nimport QueueName from \\"../QueueName\\";\\nimport RoutingKey from \\"../RoutingKey\\";\\nimport channels from \\"../channels\\";\\nimport MessageQueue from \\"../model/MessageQueue\\";\\n\\nconst normalTaskChannel = () => channels.getNormalTaskChannel();\\n\\nconst snoozeAndPinQueue = new MessageQueue<SnoozeAndPinMessage> ({\\n channel: normalTaskChannel,\\n queueName: QueueName.SNOOZE_AND_PIN,\\n routingKey: RoutingKey.SNOOZE_AND_PIN,\\n});\\n\\nexport default snoozeAndPinQueue;\\n\\nHere we have leave the consumption field empty and we have not put it inside the list of consumptions.ts.\\n\\n###### snoozeAndPinDeadLetterQueue.ts\\n\\nAccording to our configuration in queueBinding.ts, after SNOOZE_PIN_TTL ms, the message from QueueName.SNOOZE_AND_PIN will be redirected to RoutingKey.SNOOZE_AND_PIN_DEAD_LETTER via GENERAL_DEAD_EXCHANGE exchange.\\n\\njs\\nimport nonDraftsCache from \\"../../caching/nonDraftsCache\\";\\nimport { db } from \\"../../db/kysely/database\\";\\nimport { SnoozeAndPinMessage } from \\"../../dto/dto\\";\\nimport QueueName from \\"../QueueName\\";\\nimport RoutingKey from \\"../RoutingKey\\";\\nimport channels from \\"../channels\\";\\nimport MessageQueue from \\"../model/MessageQueue\\";\\n\\nconst normalTaskChannel = () => channels.getNormalTaskChannel();\\n\\nconst snoozeAndPinDeadLetterQueue = new MessageQueue<SnoozeAndPinMessage>({\\n channel: normalTaskChannel,\\n queueName: QueueName.SNOOZE_AND_PIN_DEAD_LETTER,\\n routingKey: RoutingKey.SNOOZE_AND_PIN_DEAD_LETTER,\\n consumption: async (decoded) => {\\n const { sessionId, channelId, isAdmin } = decoded;\\n await db.updateTable(\\"MessagesSession\\")\\n .set({\\n prioritizedOrSnoozedAt: 0,\\n sortingTimestamp: eb => eb.selectFrom(\\"MessagesSession as NewSession\\")\\n .select(\\"NewSession.createdAt\\")\\n .where(\\"NewSession.id\\", \\"=\\", sessionId)\\n })\\n .where(\\"MessagesSession.id\\", \\"=\\", sessionId)\\n .execute();\\n\\n const { customClearCache } = nonDraftsCache.setCacheKey({ channelId, isAdmin });\\n await customClearCache();\\n }\\n});\\n\\nexport default snoozeAndPinDeadLetterQueue;\\n\\n","title":"Code Organization for RabbitMQ","date":"2024-03-03T00:00:00.000Z","id":"blog0236","tag":"rabbitMQ, message-broker, nodejs","intro":"The concept of message queue is easy but writing them can easily be messy due to boilerplate code, we summarize how to organize them into a MessageQueue class.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px\\n }\\n</style>\\n\\nInstall autohotkey [here](https://www.autohotkey.com/) and write the following in the .ahk script:\\n\\nautohotkey\\n!1::\\nRun %comspec% /k code \\"C:\\\\Repos\\\\javascripts\\\\frontend-1\\",, Hide\\nreturn\\n!2::\\nRun %comspec% /k code \\"C:\\\\Repos\\\\javascripts\\\\frontend-2\\",, Hide\\nreturn\\n!3::\\nRun %comspec% /k code \\"C:\\\\Repos\\\\some\\\\backend\\",, Hide\\nreturn\\n!4::\\nRun %comspec% /k start \\"\\" \\"some\\\\SourceTree.exe\\" -f \\"%cd%\\",, Hide\\nreturn\\n","title":"Auto-hotkey Config for Vscode Switching (Windows)","date":"2024-01-26T00:00:00.000Z","id":"blog0237","tag":"autohotkey","intro":"We introduce how to alt+1, alt+2, ... to switch among different vscode window.","toc":false},{"content":"\\n<style>\\n img {\\n max-width: 660px\\n }\\n</style>\\n\\n#### File Structure \\n\\n- Similar to [Code Organization for RabbitMQ](/blog/article/Code-Organization-for-RabbitMQ), we will declare everything related to caching in side caching folder.\\n\\n ![](/assets/img/2024-01-27-17-21-45.png)\\n\\n- In go-lang all folder will be a package, I find it to be a very nice convention.\\n- Usually the entry-point of a package will be a file named same as the package name, namely, some-package/some-package.ts (which we don\'t have in this case since we have nothing to initialize, unlike rabbitmq).\\n\\n\\n##### model/Caching.ts\\n\\njs\\nimport { getInitedRedis } from \\"../../redis/redis\\";\\nimport logger from \\"../../util/logger\\";\\n\\nconst GENERAL_CACHING_DURATION = Number(process.env.GENERAL_CACHING_DURATION || \\"86400\\");\\n\\nclass Caching<CacheKey extends Object> {\\n private identifierKey: string = \\"\\";\\n private cacheKey: CacheKey | null = null;\\n private customDelete?: (cannoicalUnink: (unlinkFunc: CacheKey) => Promise<number>, cacheKey: CacheKey) => Promise<void>;\\n constructor(args: {\\n identifierKey: string,\\n customUnlink?: (cannoicalUnink: (unlinkFunc: CacheKey) => Promise<number>, cacheKey: CacheKey)\\n => Promise<void>;\\n }) {\\n this.identifierKey = args.identifierKey;\\n if (args.customUnlink) {\\n this.customDelete = args.customUnlink;\\n }\\n }\\n\\n public setCacheKey = (key: CacheKey) => {\\n this.cacheKey = key;\\n return { getCache: this.get, setCache: this.set, clearCache: this.unlink, customClearCache: this.customUnlink };\\n }\\n\\n private keyFromObject = (object: Object) => {\\n return Object.entries(object).map(([key, value]) => ${key}=${value}).join(\\"&\\")\\n }\\n private getRedis = () => {\\n return getInitedRedis();\\n }\\n\\n private get = async () => {\\n const redis = this.getRedis();\\n if (!this.cacheKey) {\\n throw new Error(\\"Cache key has not been set yet\\");\\n }\\n const key = this.identifierKey + this.keyFromObject(this.cacheKey);\\n logger.info(Get result from key: [${key}])\\n return await redis.get(key);\\n }\\n\\n private set = async (cache: string, expireInSeconds?: number) => {\\n const redis = this.getRedis();\\n if (!this.cacheKey) {\\n throw new Error(\\"Cache key has not been set yet\\");\\n }\\n const key = this.identifierKey + this.keyFromObject(this.cacheKey);\\n await redis.set(key, cache, \\"EX\\", expireInSeconds || GENERAL_CACHING_DURATION)\\n }\\n\\n private unlink = async () => {\\n if (!this.cacheKey) {\\n throw new Error(\\"Cache key has not been set yet\\");\\n }\\n const key = this.identifierKey + this.keyFromObject(this.cacheKey);\\n const redis = this.getRedis();\\n await redis.unlink(key);\\n }\\n\\n // \\n public cannonicalUnlink = async (cacheKey: CacheKey) => {\\n const redis = this.getRedis();\\n return await redis.unlink(this.identifierKey + this.keyFromObject(cacheKey));\\n }\\n\\n private customUnlink = async () => {\\n if (!this.customDelete) {\\n await this.unlink();\\n }\\n const redis = this.getRedis();\\n if (!this.cacheKey) {\\n throw new Error(\\"Cache key has not been set yet\\");\\n }\\n\\n await this.customDelete?.(this.cannonicalUnlink, this.cacheKey);\\n }\\n}\\n\\nexport default Caching\\n\\n\\nHere customUnlink enables user of the class to designed his own cache-unlinking logic. We shall see this in the next section:\\n\\n\\n\\n##### draftsCache.ts\\n\\n- Here we define they type of key needed in caching and pass it via generic type argument.\\n\\n- The customUnlink is to delete a number of pages simultaneously, instead of us executing the unlink logic page by page manaully.\\n\\n- This file caches the user-dependent data, therefore an id is passed into the generic type.\\n\\njs\\nimport Caching from \\"./model/Caching\\";\\n\\nconst SESSIONS_N_CAHCHING_PAGE = Number(process.env.SESSIONS_N_CAHCHING_PAGE || \\"1\\");\\n\\nconst draftsCache = new Caching<{\\n channelId: string,\\n isAdmin: boolean,\\n userId: string,\\n page?: number,\\n}>({\\n identifierKey: \\"USER_ID_DEPENDENT_UNPUBLISHED_INSTANT_ISSUES\\",\\n customUnlink: async (cannoicalUnlink, currCacheKey) => {\\n const { channelId, isAdmin, userId } = currCacheKey;\\n await Promise.all(Array(SESSIONS_N_CAHCHING_PAGE).fill(null).map(\\n (_, i) => cannoicalUnlink({ channelId, isAdmin, userId, page: i }))\\n );\\n }\\n});\\n\\nexport default draftsCache;\\n\\n\\n##### nonDraftsCache.ts\\n\\nThis one is more or less the same as draftsCache.ts but it is caching user-independent \\n\\njs\\nimport Caching from \\"./model/Caching\\";\\n\\nconst SESSIONS_N_CAHCHING_PAGE = Number(process.env.SESSIONS_N_CAHCHING_PAGE || \\"1\\");\\n\\nconst nonDraftsCache = new Caching<{\\n channelId: string,\\n isAdmin: boolean,\\n page?: number,\\n}>({\\n identifierKey: \\"USER_ID_INDEPENDENT_MESSAGES_SESSIONS\\",\\n customUnlink: async (cannoicalUnlink, currCacheKey) => {\\n const { channelId, isAdmin } = currCacheKey;\\n await Promise.all(Array(SESSIONS_N_CAHCHING_PAGE).fill(null).map(\\n (_, i) => cannoicalUnlink({ channelId, isAdmin, page: i })\\n ));\\n }\\n});\\n\\nexport default nonDraftsCache\\n\\n\\n\\n\\n\\n#### Real Case: Caching Both User-Dependent and User-Independent Data\\n\\n- Suppose that a page will show both draft messages as well as published messages known as nonDraft (the opposite).\\n- This kind of pages are difficult to cache because different users will fetch different data.\\n- User will see \\n - published messages (user-independent) and \\n - unpiblished draft messages (user-dependent).\\n- That means we need to create two caches to store different data.\\n- Not only that, they need to have separate logic to be invalidated.\\n\\n##### Caching Strategy\\n\\n- Suppose now we need to fetch data from page=0 with limit=10.\\n- Don\'t be constrainted by the number 10, the page=0&limit=10 in caching does not necessarily mean we need to just return 10 data. \\n- We first fetch the first row from the table (ORDER BY created_at desc), if that row is a draft, we fetch 10 drafts and cache it, then try to query the opposite (nonDraft) with some constraint, then cache it. \\n\\n We reverse the above step if the first row is a nonDraft.\\n\\n- Let\'s study the description above in more detail with pseudo code.\\n\\n- WLOG, suppose that the first row is a nonDraft, then we query the first 10 nonDraft\'s, call this array ndArr.\\n- Next we query time-ranged user-dependent draft\'s (the opposite of nonDraft) and insert them into ndArr according to createdAt, to be precise:\\n- Let\'s define the draft array dArr as \\n - const dArr = query(all drafts between t and T) (pseudo code), where \\n - t = min(sort(ndArr, sortby=createdAt)) (pseudo code)\\n - T = max(sort(ndArr, sortby=createdAt)) (pseudo code)\\n\\n- Then we finally execute cache_1(ndArr) and cache_2(dArr).\\n\\n- Next time when we query the same key, we return \\n js\\n [...getCache_1(some_key), ...getCache_2(some_key)]\\n .sort((a,b) => b.createdAt - a.createdAt)\\n \\n\\n\\n\\n##### Usage of our Caching Class, turn the Strategy into real code\\n\\njs\\nconst getCachedMessagesSessions = async (req: Request, res: Response) => {\\n const { channelId } = req.params as { channelId: string };\\n const { page, limit } = req.query as { page: string, limit: string }\\n const page_ = Number(page);\\n const limit_ = Number(limit);\\n const skip = limit_ * page_;\\n const userId = req.user?.userId || \\"\\";\\n const compId = req.user?.compId || \\"\\";\\n const isAdmin = req.user?.isAdmin || false\\n\\n\\n // const useCaching = page_ <= 1;\\n const useCaching = MESSAGES_SESSIONS_USE_CACHING;\\n const cachingCondition = page_ < SESSIONS_N_CAHCHING_PAGE;\\n\\n if (useCaching && cachingCondition) {\\n const { getCache: getNonDraftIssueCache, setCache: setNonDraftCache } = nonDraftCache.setCacheKey({\\n channelId, isAdmin, page: page_\\n });\\n const { getCache: getDraftIssueCache, setCache: setDraftCache } = draftsCache.setCacheKey({\\n channelId, isAdmin, userId, page: page_\\n });\\n\\n const [\\n nonDraftIssues,\\n draftIssues\\n ] = await Promise.all([\\n getNonDraftIssueCache(),\\n getDraftIssueCache()\\n ])\\n\\n if (nonDraftIssues && draftIssues) {\\n // first one page only\\n const parsedResult = JSON.parse(nonDraftIssues) as { sortingTimestamp: number }[];\\n const rangedDrafts = JSON.parse(draftIssues) as { sortingTimestamp: number }[];\\n const merged = [...parsedResult, ...rangedDrafts].sort((a, b) => b.sortingTimestamp - a.sortingTimestamp);\\n\\n return res.json({\\n success: true,\\n result: { roomAsIssues: merged }\\n })\\n }\\n\\n // the caching mechanism will defer by whether we \\n // take 10 draft-issues, and grab non-unpublished-issue in-between by sorting time stamp, or \\n // take 10 non-draft-issue, and grab unpublished-issues in-between\\n // certainly this depends on the type of latest session.\\n const { sessions: initialSession } = await chatService.getMessagesSessionsWithHost({\\n userId: userId,\\n channelId: channelId,\\n skip: 0,\\n isAdmin,\\n limit: 1,\\n });\\n\\n if (!initialSession || initialSession.length === 0) {\\n return res.json({\\n success: true,\\n result: { roomAsIssues: [] }\\n })\\n }\\n\\n const firstSession = initialSession?.[0];\\n const isDraft = firstSession?.type === \\"PERSONAL_CHATROOM\\" && firstSession?.isDraftInstantIssue;\\n\\n const { sessions } = await chatService.getMessagesSessionsWithHost({\\n userId: userId,\\n channelId: channelId,\\n draftOnly: isDraft,\\n notDraft: !isDraft,\\n skip,\\n isAdmin,\\n limit: limit_,\\n });\\n\\n const rangedOppositeSessions = await chatService.getRangedSessions(sessions, {\\n channelId, isAdmin, limit: limit_, skip, userId, draftOnly: !isDraft, notDraft: isDraft\\n });\\n\\n const drafts = isDraft ? sessions : rangedOppositeSessions;\\n const nonDrafts = isDraft ? rangedOppositeSessions : sessions;\\n\\n await Promise.all([\\n setNonDraftCache(JSON.stringify(nonDrafts)),\\n setDraftCache(JSON.stringify(drafts))\\n ])\\n const merged = [...sessions, ...rangedOppositeSessions].sort((a, b) => b.sortingTimestamp - a.sortingTimestamp);\\n res.json({\\n success: true,\\n result: { roomAsIssues: merged }\\n })\\n\\n } else {\\n const { sessions } = await chatService.getMessagesSessionsWithHost({\\n userId: userId,\\n channelId: channelId,\\n skip,\\n isAdmin,\\n limit: limit_,\\n });\\n res.json({\\n success: true,\\n result: { roomAsIssues: sessions }\\n })\\n }\\n}\\n\\n##### Cache Invalidation\\n\\n\\n- Cache Invalidation is deeply associated with your business logic and we will not discuss here. \\n\\n- But whenever you want to do it, you may do it by (assume that we need to invalidate everything):\\n\\n js\\n const { customClearCache: customClearNonDrafts } = nonDraftsCache.setCacheKey({\\n channelId, isAdmin \\n });\\n const { customClearCache: customClearDrafts } = draftsCache.setCacheKey({ \\n channelId, isAdmin, userId \\n });\\n\\n await Promise.all([\\n customClearNonDrafts(),\\n customClearDrafts()\\n ])\\n \\n\\n\\n\\n\\n\\n##### Remarks\\n\\n- You should always allow yourself to switch between cached and non-cached mode in order to check whether the caching is done correctly.\\n\\n- Caching should not be done in every page unless you are sure that page can never be chagned (like displaying a message-thread in a forum with ascending order in createdAt).\\n\\n- However, in real applications we usually sort createdBy in desc order in order to display latest information, caching every page is not pragmatic.\\n\\n- Caching does help a bit to make your application seems performant, but remember invalidation of cache is very complicated. Don\'t try to cache everything.\\n","title":"Code Organization for Redis","date":"2024-01-27T00:00:00.000Z","id":"blog0238","tag":"redis, nodejs","intro":"We study how to organize code for redis caching in a nodejs project","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px\\n }\\n</style>\\n\\n#### Procedures\\n\\n- Sometimes we don\'t want prisma to notice the changes in migration file as we have done some adjustment ***manually*** (and make the corresponding changes in migration file as well so that it automatically migrates to production).\\n\\n\\n\\n- Run to the target migration:\\n text\\n shasum -a 256 prisma/migrations/20220510001642_my_migration/migration.sql\\n \\n\\n\\n- This will produce something like:\\n text\\n fc27e97b9a61877f7f59d59a69c8c0bd2cd3271bc44b9f208800ed458d18a10b prisma/migrations/20220510001642_my_migration/migration.sql\\n \\n- Update the database of target row with checksum column with the hash: \\n text\\n fc27e97b9a61877f7f59d59a69c8c0bd2cd3271bc44b9f208800ed458d18a10b\\n \\n\\n ![](/assets/img/2024-02-26-01-00-33.png)\\n\\n- Then yarn migrate should have no warning such as:\\n text\\n The migration 20240219084746_constraint_admin_to_invite_people was modified after it was applied.\\n We need to reset the \\"public\\" schema at \\"ep-aged-morning-26453078.ap-southeast-1.aws.neon.tech\\"\\n\\n Do you want to continue? All data will be lost.\\n \\n which is of course ***NO***!\\n\\n#### Reference \\n\\n- [https://echobind.com/post/make-prisma-ignore-a-migration-change](https://echobind.com/post/make-prisma-ignore-a-migration-change)","title":"Forcefully Make Prisma Ignore The Changes in a Migrated .sql File","date":"2024-02-25T00:00:00.000Z","id":"blog0239","tag":"nodejs, sql, prisma","intro":"We record how to mark a manually changed sql file as a successful migration in prisma.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px\\n }\\n</style>\\n\\n#### SQL Funcitons\\n\\n##### generate_ulid\\n\\n\\n- [Source](https://github.com/geckoboard/pgulid/blob/master/pgulid.sql)\\n\\n- Example Result of this Function: 01HQGKW8ZK4FVKT8N5WYEQNB7F\\n\\n- sql\\n CREATE EXTENSION IF NOT EXISTS pgcrypto;\\n\\n CREATE OR REPLACE FUNCTION generate_ulid()\\n RETURNS TEXT\\n AS $$\\n DECLARE\\n -- Crockford\'s Base32\\n encoding BYTEA = \'0123456789ABCDEFGHJKMNPQRSTVWXYZ\';\\n timestamp BYTEA = E\'\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\';\\n output TEXT = \'\';\\n\\n unix_time BIGINT;\\n ulid BYTEA;\\n BEGIN\\n -- 6 timestamp bytes\\n unix_time = (EXTRACT(EPOCH FROM CLOCK_TIMESTAMP()) * 1000)::BIGINT;\\n timestamp = SET_BYTE(timestamp, 0, (unix_time >> 40)::BIT(8)::INTEGER);\\n timestamp = SET_BYTE(timestamp, 1, (unix_time >> 32)::BIT(8)::INTEGER);\\n timestamp = SET_BYTE(timestamp, 2, (unix_time >> 24)::BIT(8)::INTEGER);\\n timestamp = SET_BYTE(timestamp, 3, (unix_time >> 16)::BIT(8)::INTEGER);\\n timestamp = SET_BYTE(timestamp, 4, (unix_time >> 8)::BIT(8)::INTEGER);\\n timestamp = SET_BYTE(timestamp, 5, unix_time::BIT(8)::INTEGER);\\n\\n -- 10 entropy bytes\\n ulid = timestamp || gen_random_bytes(10);\\n\\n -- Encode the timestamp\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 0) & 224) >> 5));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 0) & 31)));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 1) & 248) >> 3));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 1) & 7) << 2) | ((GET_BYTE(ulid, 2) & 192) >> 6)));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 2) & 62) >> 1));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 2) & 1) << 4) | ((GET_BYTE(ulid, 3) & 240) >> 4)));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 3) & 15) << 1) | ((GET_BYTE(ulid, 4) & 128) >> 7)));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 4) & 124) >> 2));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 4) & 3) << 3) | ((GET_BYTE(ulid, 5) & 224) >> 5)));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 5) & 31)));\\n\\n -- Encode the entropy\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 6) & 248) >> 3));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 6) & 7) << 2) | ((GET_BYTE(ulid, 7) & 192) >> 6)));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 7) & 62) >> 1));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 7) & 1) << 4) | ((GET_BYTE(ulid, 8) & 240) >> 4)));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 8) & 15) << 1) | ((GET_BYTE(ulid, 9) & 128) >> 7)));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 9) & 124) >> 2));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 9) & 3) << 3) | ((GET_BYTE(ulid, 10) & 224) >> 5)));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 10) & 31)));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 11) & 248) >> 3));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 11) & 7) << 2) | ((GET_BYTE(ulid, 12) & 192) >> 6)));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 12) & 62) >> 1));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 12) & 1) << 4) | ((GET_BYTE(ulid, 13) & 240) >> 4)));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 13) & 15) << 1) | ((GET_BYTE(ulid, 14) & 128) >> 7)));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 14) & 124) >> 2));\\n output = output || CHR(GET_BYTE(encoding, ((GET_BYTE(ulid, 14) & 3) << 3) | ((GET_BYTE(ulid, 15) & 224) >> 5)));\\n output = output || CHR(GET_BYTE(encoding, (GET_BYTE(ulid, 15) & 31)));\\n\\n RETURN output;\\n END\\n $$\\n LANGUAGE plpgsql\\n VOLATILE;\\n \\n\\n##### ulid_to_uuid\\n\\n- [Source](https://github.com/scoville/pgsql-ulid/blob/main/ulid-to-uuid.sql)\\n\\n- Next we turn 01HQGKW8ZK4FVKT8N5WYEQNB7F into the standard UUID format:\\n sql\\n CREATE OR REPLACE FUNCTION parse_ulid(ulid text) RETURNS bytea AS $$\\n DECLARE\\n -- 16byte \\n bytes bytea = E\'\\\\\\\\x00000000 00000000 00000000 00000000\';\\n v char[];\\n -- Allow for O(1) lookup of index values\\n dec integer[] = ARRAY[\\n 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\\n 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\\n 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\\n 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,\\n 255, 255, 255, 255, 255, 255, 255, 0, 1, 2,\\n 3, 4, 5, 6, 7, 8, 9, 255, 255, 255,\\n 255, 255, 255, 255, 10, 11, 12, 13, 14, 15,\\n 16, 17, 1, 18, 19, 1, 20, 21, 0, 22,\\n 23, 24, 25, 26, 255, 27, 28, 29, 30, 31,\\n 255, 255, 255, 255, 255, 255, 10, 11, 12, 13,\\n 14, 15, 16, 17, 1, 18, 19, 1, 20, 21,\\n 0, 22, 23, 24, 25, 26, 255, 27, 28, 29,\\n 30, 31\\n ];\\n BEGIN\\n IF NOT ulid ~* \'^[0-7][0-9ABCDEFGHJKMNPQRSTVWXYZ]{25}$\' THEN\\n RAISE EXCEPTION \'Invalid ULID: %\', ulid;\\n END IF;\\n\\n v = regexp_split_to_array(ulid, \'\');\\n\\n -- 6 bytes timestamp (48 bits)\\n bytes = SET_BYTE(bytes, 0, (dec[ASCII(v[1])] << 5) | dec[ASCII(v[2])]);\\n bytes = SET_BYTE(bytes, 1, (dec[ASCII(v[3])] << 3) | (dec[ASCII(v[4])] >> 2));\\n bytes = SET_BYTE(bytes, 2, (dec[ASCII(v[4])] << 6) | (dec[ASCII(v[5])] << 1) | (dec[ASCII(v[6])] >> 4));\\n bytes = SET_BYTE(bytes, 3, (dec[ASCII(v[6])] << 4) | (dec[ASCII(v[7])] >> 1));\\n bytes = SET_BYTE(bytes, 4, (dec[ASCII(v[7])] << 7) | (dec[ASCII(v[8])] << 2) | (dec[ASCII(v[9])] >> 3));\\n bytes = SET_BYTE(bytes, 5, (dec[ASCII(v[9])] << 5) | dec[ASCII(v[10])]);\\n\\n -- 10 bytes of entropy (80 bits);\\n bytes = SET_BYTE(bytes, 6, (dec[ASCII(v[11])] << 3) | (dec[ASCII(v[12])] >> 2));\\n bytes = SET_BYTE(bytes, 7, (dec[ASCII(v[12])] << 6) | (dec[ASCII(v[13])] << 1) | (dec[ASCII(v[14])] >> 4));\\n bytes = SET_BYTE(bytes, 8, (dec[ASCII(v[14])] << 4) | (dec[ASCII(v[15])] >> 1));\\n bytes = SET_BYTE(bytes, 9, (dec[ASCII(v[15])] << 7) | (dec[ASCII(v[16])] << 2) | (dec[ASCII(v[17])] >> 3));\\n bytes = SET_BYTE(bytes, 10, (dec[ASCII(v[17])] << 5) | dec[ASCII(v[18])]);\\n bytes = SET_BYTE(bytes, 11, (dec[ASCII(v[19])] << 3) | (dec[ASCII(v[20])] >> 2));\\n bytes = SET_BYTE(bytes, 12, (dec[ASCII(v[20])] << 6) | (dec[ASCII(v[21])] << 1) | (dec[ASCII(v[22])] >> 4));\\n bytes = SET_BYTE(bytes, 13, (dec[ASCII(v[22])] << 4) | (dec[ASCII(v[23])] >> 1));\\n bytes = SET_BYTE(bytes, 14, (dec[ASCII(v[23])] << 7) | (dec[ASCII(v[24])] << 2) | (dec[ASCII(v[25])] >> 3));\\n bytes = SET_BYTE(bytes, 15, (dec[ASCII(v[25])] << 5) | dec[ASCII(v[26])]);\\n\\n RETURN bytes;\\n END\\n $$\\n LANGUAGE plpgsql\\n IMMUTABLE;\\n \\n sql\\n CREATE OR REPLACE FUNCTION ulid_to_uuid(ulid text) RETURNS uuid AS $$\\n BEGIN\\n RETURN encode(parse_ulid(ulid), \'hex\')::uuid;\\n END\\n $$\\n LANGUAGE plpgsql\\n IMMUTABLE;\\n \\n\\n##### ulid_as_uuid\\n\\nFinally we combine all of the above:\\n\\nsql\\nCREATE OR REPLACE FUNCTION ulid_as_uuid() RETURNS uuid AS $$\\nBEGIN\\n\\t RETURN ulid_to_uuid(generate_ulid());\\nEND\\n$$\\nLANGUAGE plpgsql;\\n\\n\\n\\n#### Example\\n\\nNow by \\nsql\\nSELECT ulid_as_uuid();\\n\\nwe get \\nsql\\n018de143-60a9-7476-6bdc-c4736b7f19e8\\n\\n\\n#### References \\n\\n- [SQL Function to get ULID](https://github.com/geckoboard/pgulid/blob/master/pgulid.sql)\\n\\n- [Parse ULID to UUID](https://github.com/scoville/pgsql-ulid/blob/main/ulid-to-uuid.sql)","title":"Use ULID in PostgreSQL","date":"2024-02-26T00:00:00.000Z","id":"blog0240","tag":"sql, prisma","intro":"We record how to use ULID instead of UUID in PostgreSQL","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px\\n }\\n</style>\\n\\n#### The Result\\n\\n- Inside our table:\\n \\n ![](/assets/img/2024-02-28-06-40-03.png)\\n\\n- createdAt\\n - It is js-compatible, which is used mostly in frontend;\\n - It is also convenient to database for ORDER BY statements, \\n - Sorting integers (by numerical value) is much easier than \\n - sorting String\'s (by internal sorting rules of characters) or \\n - sortingDatetime\'s (by internal data structure composing of year, month, timezone, etc).\\n\\n- createdAtHK is human-readable in HK timezone\\n\\n\\n#### Prisma\\n\\nWe add the following in any prisma model:\\n\\nprisma\\nmodel SomeModel {\\n ...\\n createdAt Float @default(dbgenerated(\\"gen_created_at()\\"))\\n createdAtHK String @default(dbgenerated(\\"gen_created_at_hk_timestr()\\"))\\n}\\n\\n\\nThen we execute\\n\\ntext\\nnpx prisma migrate dev --create-only\\n\\n\\nto create a migration .sql file for further editing (but not execute the changes).\\n\\n\\n#### SQL Functions\\n\\nNext we fill in the missing implementation of gen_created_at and gen_created_at_hk_timestr in the generated migration .sql file.\\n\\n\\nsql\\n-- in generated migration SQL file\\nCREATE OR REPLACE FUNCTION gen_created_at() RETURNS float as $$\\nBEGIN\\n\\treturn ROUND(extract(epoch from NOW()::TIMESTAMPTZ) * 1000, 0)::float;\\nEND\\n$$\\nLANGUAGE plpgsql;\\n\\nCREATE OR REPLACE FUNCTION gen_created_at_hk_timestr() RETURNS text as $$\\nBEGIN\\n\\treturn TO_CHAR((NOW()::TIMESTAMPTZ AT TIME ZONE \'UTC\' AT TIME ZONE \'GMT+8\'), \'YYYY-MM-DD HH24:MI:SS\');\\nEND\\n$$\\nLANGUAGE plpgsql;\\n\\n-- prisma should have generated something similar:\\nALTER TABLE your_table ALTER COLUMN \\"created_at\\" SET DEFAULT gen_created_at();\\nALTER TABLE your_table ADD COLUMN \\"createdAtHK\\" TEXT NOT NULL DEFAULT gen_created_at_hk_timestr();\\n\\nNow we can start the migration.\\n\\n","title":"Javascript Compatible Timestamp in PostgreSQL","date":"2024-02-27T00:00:00.000Z","id":"blog0241","tag":"sql, prisma","intro":"The default Datetime object in PGSQL by default is UTC+0. Application-wise we wish an absolute timestamp that is compatible with frontend (i.e., javascript), here is how.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px\\n }\\n</style>\\n\\n\\n#### Objective\\n\\nThe use case:\\n\\n- Due to the nature of left-join\'s our table will get bigger and bigger with rows that we might think of as \\"duplicated\\".\\n\\n- For example, in an invitation table (we call this UserToChannel) there can be plenty of users inviting a single user into a channel, in that case our business logic would ***require only the first invitation be effective*** (but we still wish to leave the invitation record).\\n\\n- There can be even more duplicated rows due to ***left-joins*** without very refined where clauses (or there can\'t be such conditions).\\n\\n- Therefore when we want to get all invitation, we wish to ***collapse*** all invitations into 1 invitation per channel.\\n\\n- However, we want to sort the invitation by createdAt in descending order.\\n\\n***Problem.*** We cannot \\n- Collapse (deduplicate, using the first occurence) by createdAt in ascending order and \\n- Sort by createdAt in descending order \\n\\nat the same time.\\n\\n<center></center>\\n\\n***Solution.*** \\n- We use With Collapsed AS (SELECT ...) to create a deduplicated table, then \\n- we sort this deduplicated table as if we are sorting an existing table\\n\\n\\n**Remark.** This technique is also helpful if we want to create a table of id\'s for intersection (inner-join) to another table on the id condition.\\n\\n\\n\\n#### SQL\\n\\nWe skip the tedious select and where clauses and only leave the important part that we use to \\"deduplicate\\", the detail can be seen in kysely part of the next section:\\n\\ntext\\nWITH collapsedChannels AS (\\n SELECT DISTINCT ON (\\"Channel\\".id) \\"Channel\\".*, \\"UserToChannel\\".joined, ... (not important) \\n FROM \\"Channel\\"\\n WHERE ...\\n ORDER BY \\"Channel\\".id \\"Channel\\".\\"createdAt\\" asc\\n) \\nSELECT * FROM \\"collapsedChannels\\"\\nORDER BY \\"collapsedChannels\\".\\"createdAt\\" desc\\n\\n\\nNote that we first group createdAt by asc and then order that by desc.\\n\\n\\n#### Kysely\\n\\nThe following is the actual code implementation of a request handler:\\n\\njs\\nconst getChannels = async (req: Request, res: Response) => {\\n const userId = req.user?.userId || \\"\\";\\n const userEmail = req.user?.email || \\"\\";\\n const isAdmin = req.user?.isAdmin || false;\\n const { projectId } = req.params;\\n let reportChannel: Channel | undefined\\n\\n reportChannel = await db.selectFrom(\\"Channel\\").selectAll().where(\\"Channel.type\\", \\"=\\", \\"EXPORT_REPORT\\").executeTakeFirst();\\n\\n if (!reportChannel) {\\n reportChannel = await db.insertInto(\\"Channel\\").values({\\n type: \\"EXPORT_REPORT\\",\\n isDeleted: false,\\n name: \\"Export Report\\",\\n })\\n .returningAll()\\n .executeTakeFirst();\\n }\\n\\n const standardChannels_ = db.with(\\"collapsedChannels\\", db => db.selectFrom(\\"Channel\\")\\n .leftJoin(\\"UserToChannel\\", \\"UserToChannel.channelId\\", \\"Channel.id\\")\\n .leftJoin(\\"UserToProjectInvitation\\", \\"UserToProjectInvitation.linkedByChannelId\\", \\"Channel.id\\")\\n .leftJoin(\\"User as Inviter\\", \\"Inviter.companyEmail\\", \\"UserToProjectInvitation.invitedByEmail\\")\\n .leftJoin(\\"Company as InviterCompany\\", \\"InviterCompany.id\\", \\"Inviter.companyId\\")\\n .selectAll(\\"Channel\\")\\n .select(\\"UserToChannel.joined\\")\\n .select(eb => {\\n const firstName = eb.ref(\\"Inviter.firstName\\");\\n const lastName = eb.ref(\\"Inviter.lastName\\");\\n const fullName = sql<string>concat(${firstName}, \' \', ${lastName});\\n return fullName.as(\\"inviterName\\")\\n })\\n .select(\\"InviterCompany.name as inviterCompanyName\\")\\n .select(\\"UserToProjectInvitation.invitedByUser\\")\\n .select(eb => [\\n jsonObjectFrom(\\n eb.selectFrom(\\"MessagesSession\\")\\n .select([\\n eb => eb.fn.count(\\"MessagesSession.id\\").as(\\"count\\")\\n ])\\n .whereRef(\\"MessagesSession.channelId\\", \\"=\\", \\"Channel.id\\")\\n .where(\\"MessagesSession.hostUserId\\", \\"=\\", userId)\\n .where(\\"MessagesSession.isDeleted\\", \\"!=\\", true)\\n .where(\\"MessagesSession.isSessionConfirmed\\", \\"=\\", true)\\n .where(\\"MessagesSession.isDraftInstantIssue\\", \\"=\\", true)\\n ).as(\\"draft\\")\\n ])\\n .where(\\"Channel.projectId\\", \\"=\\", projectId)\\n .where(\\"Channel.isDeleted\\", \\"=\\", false)\\n .$if(!isAdmin, qb => qb.where(\\"UserToChannel.userEmail\\", \\"=\\", userEmail))\\n .distinctOn(\\"Channel.id\\")\\n .orderBy([\\"Channel.id\\", \\"Channel.createdAt asc\\"])\\n )\\n\\n const standardChannels = await standardChannels_\\n .selectFrom(\\"collapsedChannels\\")\\n .selectAll()\\n .orderBy(\\"collapsedChannels.createdAt desc\\")\\n .execute();\\n\\n res.json({\\n success: true,\\n result: { channels: [reportChannel, ...standardChannels] }\\n });\\n}\\n","title":"As-With Clause in SQL","date":"2024-02-28T00:00:00.000Z","id":"blog0242","tag":"kysely, sql","intro":"Some simple yet common use case of as-with clause in sql, and its counterpart in kysely.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n\\n#### To Receive Push Notification\\n\\n##### Android (Configure Firebase Project)\\n\\n\\n- [Firebase Console](https://console.firebase.google.com/u/0/)\\n\\n\\n- For now since we don\'t have a google account for all mobile application, the following credentials are generated by using james.lee@wonderbricks.com. \\n\\n- The generation steps are simple to replicate:\\n\\n\\n- We can create ***one*** project for ***four*** environments.\\n - ***Repeatedly*** Click Plus Sign\\n\\n ![](/assets/img/2024-02-28-07-37-19.png)\\n\\n - ***Repeatedly*** Click\\n\\n ![](/assets/img/2024-02-28-07-37-29.png)\\n\\n - ***Repeatedly*** Fill in the package name\\n\\n ![](/assets/img/2024-02-28-07-37-39.png)\\n\\n\\n- We create 4 package name with only ***characters and underscores*** (android don\'t allow - in package name):\\n\\n - com.XXX.XXXX_dev\\n - com.XXX.XXXX_uat\\n - com.XXX.XXXX_poc\\n - com.XXX.XXXX_prod\\n\\n- Project Setting\\n\\n ![](/assets/img/2024-02-28-07-37-48.png)\\n\\n- Manage API in Google Cloud Console:\\n\\n ![](/assets/img/2024-02-28-07-37-54.png)\\n\\n- Enable\\n\\n ![](/assets/img/2024-02-28-07-38-01.png)\\n\\n- After we enabled successfully:\\n\\n ![](/assets/img/2024-02-28-07-38-09.png)\\n\\n text\\n AAAA95L1NUI:XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\n \\n\\n\\n- [Expo Account Dashboard](https://expo.dev/accounts/machingclee)\\n\\n- Click Credentials:\\n\\n ![](/assets/img/2024-02-28-07-38-17.png)\\n\\n- Add a FCM (Legacy) server key:\\n\\n ![](/assets/img/2024-02-28-07-38-30.png)\\n\\n\\n\\n\\n##### iOS (Nothong)\\n- Nothing to add as we are using EAS-build. \\n\\n\\n#### Page to Create Access Token\\n\\nBy default everyone who owns the device\'s expo-push-notification-token can send push-notification to that device. Expo provides one additional layer to prevent malacious use of a notification token (just in case someone gets the token for some reason)\\n\\n\\n- [Get Access Token Here](https://expo.dev/accounts/machingclee/settings/access-tokens)\\n\\nYou can use this token in <a href=\\"#pushNotificationUtil.ts\\" target=\\"_self\\">pushNotificationUtil.ts</a> section.\\n\\n\\n##### Personal Access Tokens\\ntext\\nvWXyXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\n\\n\\n##### Create Robot Users\\n\\ntext\\ni-QtXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\n\\n\\n##### Toggle the Enhanced Security for Push Notifications (Optional)\\n\\n![](/assets/img/2024-02-28-07-39-10.png)\\n\\n<center></center>\\n\\nAfter that any naked POST request cannot send notification to your user even they own the push-notification-token of that user.\\n\\n![](/assets/img/2024-02-28-07-39-15.png)\\n\\n\\n\\n\\n\\n#### Receive Notification in Mobile\\n \\n##### Frontend\\n###### Create a PushNotification Token at Entrypoint\\n\\nWe use the following hook \\n\\njs\\n// hooks/usePushNotification.ts\\n\\nimport { useState, useEffect, useRef } from \'react\';\\nimport { Text, View, Button, Platform } from \'react-native\';\\nimport * as Device from \'expo-device\';\\nimport * as Notifications from \'expo-notifications\';\\nimport Constants from \'expo-constants\';\\nimport msgUtil from \'../util/msgUtil\';\\nimport { useAppDispatch, useAppSelector } from \'../redux/app/hooks\';\\nimport appSlice, { appThunkAction } from \'../redux/slices/appSlice\';\\nimport { PushNotificationData } from \'../dto/dto\';\\nimport { useRouter } from \'expo-router\';\\nimport projectAndChannelSlice from \'../redux/slices/projectAndChannelSlice\';\\n\\nconst configNotificationHandler = () => {\\n Notifications.setNotificationHandler({\\n handleNotification: async () => ({\\n shouldShowAlert: true,\\n shouldPlaySound: true,\\n shouldSetBadge: false,\\n }),\\n });\\n}\\n\\nasync function getNotificationTokenAsync(userId: string) {\\n if (Platform.OS === \'android\') {\\n Notifications.setNotificationChannelAsync(\'default\', {\\n name: \'default\',\\n importance: Notifications.AndroidImportance.MAX,\\n vibrationPattern: [0, 250, 250, 250],\\n lightColor: \'#FF231F7C\',\\n });\\n }\\n\\n if (Device.isDevice) {\\n const { status: existingStatus } = await Notifications.getPermissionsAsync();\\n let finalStatus = existingStatus;\\n console.log(\\"finalStatusfinalStatusfinalStatus\\", finalStatus);\\n if (existingStatus !== \'granted\') {\\n const { status } = await Notifications.requestPermissionsAsync();\\n finalStatus = status;\\n }\\n if (finalStatus !== \'granted\') {\\n alert(\'Failed to get push token for push notification!\');\\n return;\\n }\\n const token = await Notifications.getExpoPushTokenAsync({\\n projectId: Constants?.expoConfig?.extra?.eas.projectId || \\"\\",\\n });\\n\\n return token.data;\\n } else {\\n throw new Error(\\"Must use physical device for Push Notifications\\");\\n }\\n}\\n\\nexport default function usePushNotification() {\\n const dispatch = useAppDispatch();\\n const router = useRouter();\\n configNotificationHandler();\\n const notificationListener = useRef<Notifications.Subscription>();\\n const responseListener = useRef<Notifications.Subscription>();\\n\\n const subscribeNotification = async (userId: string) => {\\n try {\\n const token = await getNotificationTokenAsync(userId);\\n console.log(\\"tokentokentokentoken\\", token);\\n if (token) {\\n await dispatch(appThunkAction.providePushNotificationToken({ pushNotificationToken: token })).unwrap();\\n }\\n } catch (err) {\\n msgUtil.persistedError(JSON.stringify(err));\\n }\\n\\n notificationListener.current = Notifications.addNotificationResponseReceivedListener(response => {\\n const { actionIdentifier, notification } = response;\\n const { date, request } = response;\\n // actionA\\n });\\n\\n responseListener.current = Notifications.addNotificationResponseReceivedListener(response => {\\n const { notification } = response;\\n const data = notification.request.content.data as PushNotificationData;\\n // action B\\n });\\n\\n return () => {\\n if (notificationListener?.current) {\\n Notifications.removeNotificationSubscription(notificationListener?.current);\\n }\\n if (responseListener?.current) {\\n Notifications.removeNotificationSubscription(responseListener?.current);\\n }\\n };\\n\\n }\\n\\n return { subscribeNotification }\\n}\\n\\n\\n\\n\\n\\nWe will call subscribeNotification() whenever we try to login. \\n\\nFor example, in the entrypoint of our application:\\n\\njs\\nexport default () => {\\n const [_, setHydrated] = useState(false); // force rerendering only\\n const { accessToken, userId } = useAppSelector(s => s.auth);\\n const dispatch = useAppDispatch();\\n const { subscribeNotification } = usePushNotification();\\n \\n ...\\n\\n useEffect(() => {\\n if (!accessToken) {\\n router.push(\\"/login\\")\\n } else {\\n subscribeNotification(userId);\\n }\\n }, [accessToken]);\\n\\n useEffect(() => {\\n setTimeout(() => {\\n setHydrated(true);\\n }, 1);\\n }, [])\\n\\n return (\\n ...\\n )\\n}\\n\\nThis effect will be executed everytime we login or launch the application.\\n\\n\\nNote that action A and action B inside subscribeNotification have different behaviour:\\n\\n###### Control the User\'s Behaviour (User being Passive)\\n\\n- Look at the hook ahove, actionA will be executed only when user is in-app, moreover, the action will take place without user\'s consent.\\n\\n###### User Controles the Behaviour (User being Active)\\n\\n- Look at the hook above, actionB will be executed when user ***tap into*** the application.\\n\\n\\n\\n##### Backend (Receive Push Notification Token via Upsert)\\n\\njs\\n// controller: \\n\\nconst providePushNotificationToken = async (req: Request, res: Response) => {\\n const userEmail = req.user?.email || \\"\\";\\n const { pushNotificationToken } = req.body as { pushNotificationToken: string };\\n\\n await db.insertInto(\\"PushNotification\\")\\n .values({ userEmail, token: pushNotificationToken })\\n .onConflict(oc => oc\\n // userEmail is the only unique-key constraint\\n .columns([\\"userEmail\\"])\\n .doUpdateSet(eb => ({ token: eb.ref(\\"excluded.token\\") }))\\n )\\n .execute();\\n res.json({ success: true });\\n}\\n\\n\\n\\n[![](/assets/img/2024-02-28-07-57-43.png)](/assets/img/2024-02-28-07-57-43.png)\\n\\n#### Send Notification via Backend\\n##### Nodejs Library to Send Notification \\n- Resource: [send-push-notifications-using-a-server](https://docs.expo.dev/push-notifications/sending-notifications/#send-push-notifications-using-a-server)\\n\\n- We use this [npm package](https://www.npmjs.com/package/expo-server-sdk).\\n\\n- Note that the npm package also do the ***Limit Concurrent Connections*** for us.\\n \\n##### pushNotificationUtil.ts\\n\\njs\\nimport { Expo, ExpoPushMessage, ExpoPushTicket } from \'expo-server-sdk\';\\n\\ntype PushNotificationData = {\\n projectId: string,\\n channelId: string,\\n}\\n\\nconst expo = new Expo({ accessToken: process.env.EXPO_ACCESS_TOKEN });\\n\\nconst forwardMessages = async (params: {\\n tokens: string[],\\n message: {\\n title: string,\\n body: string,\\n data: PushNotificationData\\n }\\n}) => {\\n const { message, tokens } = params;\\n const { body, data, title } = message;\\n const messages: ExpoPushMessage[] = []\\n for (const token of tokens) {\\n messages.push({\\n to: token,\\n sound: \'default\',\\n body: body,\\n data,\\n title\\n })\\n }\\n const chunks = expo.chunkPushNotifications(messages);\\n const tickets: ExpoPushTicket[] = []\\n for (const chunk of chunks) {\\n try {\\n const ticketChunk = await expo.sendPushNotificationsAsync(chunk);\\n console.log(ticketChunk);\\n tickets.push(...ticketChunk);\\n // NOTE: If a ticket contains an error code in ticket.details.error, you\\n // must handle it appropriately. The error codes are listed in the Expo\\n // documentation:\\n // https://docs.expo.io/push-notifications/sending-notifications/#individual-errors\\n } catch (error) {\\n console.error(error);\\n }\\n }\\n\\n const successReceiptIds = [];\\n for (let ticket of tickets) {\\n // NOTE: Not all tickets have IDs; for example, tickets for notifications\\n // that could not be enqueued will have error information and no receipt ID.\\n if (ticket.status === \\"ok\\" && ticket?.id) {\\n successReceiptIds.push(ticket.id);\\n }\\n }\\n\\n let receiptIdChunks = expo.chunkPushNotificationReceiptIds(successReceiptIds);\\n (async () => {\\n // Like sending notifications, there are different strategies you could use\\n // to retrieve batches of receipts from the Expo service.\\n for (let chunk of receiptIdChunks) {\\n try {\\n let receipts = await expo.getPushNotificationReceiptsAsync(chunk);\\n console.log(receipts);\\n\\n // The receipts specify whether Apple or Google successfully received the\\n // notification and information about an error, if one occurred.\\n for (let receiptId in receipts) {\\n const receipt = receipts[receiptId];\\n let { status, details } = receipt\\n\\n if (status === \'ok\') {\\n continue;\\n } else if (status === \'error\') {\\n console.error(\\n There was an error sending a notification: ${JSON.stringify(details)}\\n );\\n }\\n }\\n } catch (error) {\\n console.error(error);\\n }\\n }\\n })();\\n}\\n\\nexport default {\\n forwardMessages\\n}\\n\\n\\n\\n##### notificationService.ts\\n\\n\\nHere the backend logic differs from the businesss, for us we push notification for different channels, and each user has their own token in the PushNotification Table:\\n\\njs\\nimport { channel } from \\"diagnostics_channel\\";\\nimport { db } from \\"../db/kysely/database\\";\\nimport pushNotificationUtil from \\"../util/pushNotificationUtil\\";\\n\\nconst notifyChannel = async (params: {\\n excludeUserEmails: string[],\\n channelId: string,\\n title?: string,\\n body?: string\\n}) => {\\n const { excludeUserEmails, channelId, title, body } = params;\\n const targetChannel = await db.selectFrom(\\"Channel\\")\\n .select([\\"Channel.name\\", \\"Channel.projectId\\"])\\n .where(\\"Channel.id\\", \\"=\\", channelId)\\n .executeTakeFirst();\\n if (!targetChannel) {\\n throw new Error(\\"Channel does not exist\\");\\n }\\n const allTokenExceptMeInsideChannel = await db.selectFrom(\\"UserToChannel\\")\\n .leftJoin(\\"User\\", \\"User.companyEmail\\", \\"UserToChannel.userEmail\\")\\n .leftJoin(\\"PushNotification\\", \\"PushNotification.userEmail\\", \\"User.companyEmail\\")\\n .select(\\"PushNotification.token\\")\\n .where(\\"PushNotification.userEmail\\", \\"not in\\", excludeUserEmails)\\n .where(\\"UserToChannel.channelId\\", \\"=\\", channelId)\\n .execute();\\n const tokens = allTokenExceptMeInsideChannel.filter(t => t.token).map(t => t.token!);\\n pushNotificationUtil.forwardMessages({\\n tokens,\\n message: {\\n title: title || \\"Issue Update\\",\\n data: {\\n channelId,\\n projectId: targetChannel.projectId || \\"\\",\\n },\\n body: body || New Issue in Channel ${targetChannel.name},\\n }\\n })\\n}\\n\\nexport default {\\n notifyChannel\\n}\\n\\n\\nOf course you can build more reusable notification strategy in this file!","title":"Push-Notification in Android and iOS via Expo","date":"2024-02-20T00:00:00.000Z","id":"blog0243","tag":"react-native, nodejs, expo","intro":"We record how to implement push-notification in both Android and iOS.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n#### Scripts\\n##### Automatic updatedAt\\nsql\\nDROP TRIGGER IF EXISTS upd_trig on \\"MessagesSession\\";\\n\\ncreate or replace FUNCTION upd_trig() RETURNS trigger\\n LANGUAGE plpgsql AS\\n$$BEGIN\\n NEW.\\"updatedAt\\" := gen_created_at();\\n RETURN NEW;\\nEND;$$;\\n\\nCREATE TRIGGER upd_trig BEFORE UPDATE ON \\"MessagesSession\\"\\n FOR EACH ROW EXECUTE PROCEDURE upd_trig();\\n\\n\\n##### Automatic createdAt\\nsql\\nCREATE OR REPLACE FUNCTION gen_created_at() RETURNS float as $$\\nBEGIN\\n\\treturn ROUND(extract(epoch from NOW()::TIMESTAMPTZ) * 1000, 0)::float;\\nEND\\n$$\\nLANGUAGE plpgsql;\\n\\n##### Automatic human readble createdAt in HK\\nsql\\nCREATE OR REPLACE FUNCTION gen_created_at_hk_timestr() RETURNS text as $$\\nBEGIN\\n\\treturn TO_CHAR((NOW()::TIMESTAMPTZ AT TIME ZONE \'UTC\' AT TIME ZONE \'GMT+8\'), \'YYYY-MM-DD HH24:MI:SS\');\\nEND\\n$$\\nLANGUAGE plpgsql;\\n\\n\\n#### Usages\\n\\n##### Prisma\\nprisma\\nmodel SomeModel {\\n ...\\n createdAt Float @default(dbgenerated(\\"gen_created_at()\\"))\\n createdAtHK String @default(dbgenerated(\\"gen_created_at_hk_timestr()\\"))\\n}\\n\\n\\n##### Plain SQL\\n\\nsql\\nALTER TABLE \\"SomeModel\\" ADD COLUMN \\"createdAt\\" DOUBLE PRECISION NOT NULL DEFAULT gen_created_at(),\\nADD COLUMN \\"createdAtHK\\" TEXT NOT NULL DEFAULT gen_created_at_hk_timestr();\\n\\n\\n\\n\\n\\n","title":"SQL Functions to Generate created_at, human-readable created_at and updated_at","date":"2024-03-01T00:00:00.000Z","id":"blog0244","tag":"sql","intro":"Convenient simple functions to set as default in SQL.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n\\n#### Usage\\n\\nWe execute the script via \\n\\ntext\\nnode env-to-eas-json.js --env=uat --debug=false --store=true\\n\\nthen the parameters will be passed into function as string\'s\\n\\n#### How to Implement\\n\\nWe use an npm package called [minimist](https://www.npmjs.com/package/minimist). \\n\\n\\n##### The Main Part\\n\\n\\njs\\n// script.js\\nconst fs = require(\\"fs\\");\\nvar argv_ = require(\\"minimist\\")(process.argv.slice(2));\\n\\nconst main = (argv) => {\\n const { env, debug, store } = argv; // e.g., env = \\"dev_voice_only\\"\\n const debug_ = debug === \\"true\\";\\n const publish = store === \\"true\\";\\n\\n // end of this topic, you can implement your own logic below:\\n}\\n\\nmain(argv_);\\n\\n\\n##### A Complete Example\\n\\nThe following script will copy variables inside .env-cmdrc described by [this post](/blog/article/Environment-Variable-by-env-cmdrc), to the corresponding fields inside eas.json for expo projects.\\n\\n\\njs\\nconst fs = require(\\"fs\\");\\nvar argv_ = require(\\"minimist\\")(process.argv.slice(2));\\n\\nconst main = (argv) => {\\n const { env, debug, store } = argv; // e.g., env = \\"dev_voice_only\\"\\n const debug_ = debug === \\"true\\";\\n const publish = store === \\"true\\";\\n\\n console.log(\\"Copying environment variables from .env-cmdrc to eas.json\\");\\n\\n // then the reamining are messy logics of copying custom.env \\n // into eas.json for expo project\\n\\n const envFile = fs.readFileSync(\\".env-cmdrc\\", { encoding: \\"utf-8\\" });\\n const envJson = JSON.parse(envFile);\\n const defaultConfig = envJson[\\"default\\"];\\n const envConfig = envJson[env];\\n\\n // json to be copied to eas.json at \\"[env]\\" key\\n const easConfig = { ...defaultConfig, ...envConfig };\\n // in case debug == true, we inject the following attributes to debug development build\\n const debugAttributes = {\\n android: {\\n gradleCommand: \\":app:assembleDebug\\",\\n },\\n ios: {\\n buildConfiguration: \\"Debug\\",\\n },\\n };\\n // load the existing eas.json\\n const easJsonToBeAdjusted = JSON.parse(\\n fs.readFileSync(\\"eas.json\\", { encoding: \\"utf-8\\" })\\n );\\n\\n // adjust the target build environment (prod, dev, uat, ..., etc)\\n easJsonToBeAdjusted[\\"build\\"][env] = {\\n distribution: publish ? \\"store\\" : \\"internal\\",\\n env: easConfig,\\n ...(debug_ ? debugAttributes : {}),\\n };\\n const adjustedEasJsonString = JSON.stringify(easJsonToBeAdjusted, null, 2);\\n fs.writeFileSync(\\"eas.json\\", adjustedEasJsonString);\\n};\\n\\nmain(argv_);\\n","title":"How to node your-script.js --env=some_env","date":"2024-03-02T00:00:00.000Z","id":"blog0245","tag":"nodejs","intro":"We record how to make custom script with custom predefined variable in command line via running js script.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n\\n#### Table Design\\n\\n\\nWe divide notifications into two types:\\n\\n##### IndividualUserNotification\\n\\n- This one contains all user-specific data, it is only viewable by the user who requests it. \\n\\n- Without that, we would have to sub-query all the data using an userId, making an API non-cachable.\\n\\n- IndividualUserNotificationType indicates the ***notification purpose*** of that notification. \\n\\n - For example, type: NEW_ISSUE means that we want to notify userEmail there is a new issue. \\n\\n **Which issue is it?** According to business logic, which is the issue corr. to sessionId.\\n\\n - Similarly, type: NEW_CHANNEL_TO_JOIN means that we want to notify userEmail there is a new channel available.\\n\\n **Which channel is it?** Which is the channel corr. to channelId.\\n \\n\\n\\nprisma \\nenum IndividualUserNotificationType {\\n NEW_ISSUE\\n NEW_LLM_REPLY\\n NEW_DRAFT\\n NEW_CHANNEL_TO_JOIN\\n SESSION_NEW_LIVE\\n SESSION_NEW_LIVE_REPLY\\n ASSIGNED_AS_FOLLOWER\\n WATCH\\n}\\n\\nmodel IndividualUserNotification {\\n id Int @id @default(autoincrement())\\n type IndividualUserNotificationType\\n sessionId String? @db.Uuid\\n channelId String? @db.Uuid\\n userEmail String\\n createdAt Float @default(dbgenerated(\\"gen_created_at()\\"))\\n createdAtHK String @default(dbgenerated(\\"gen_created_at_hk_timestr()\\"))\\n \\n @@index([userEmail, type])\\n}\\n\\n\\n##### GlobalNotification\\n\\n- This one contains data that is viewable ***whenever a user has certain right***, it is like a real-time status.\\n\\n- For example, when a Live is held currently but not ended inside a channel, we notify all people there is a ONGOING_LIVE status.\\n\\n- We don\'t treat this as IndividualUserNotification because it needs a userEmail, but the status ONGOING_LIVE can be viewed even if a user join the channel much later than the Live is held.\\n\\n- This notification should not have target user as the ***notification target is uncertain at any time*** .\\n\\nprisma \\nenum GlobalNotificationType {\\n ONGOING_LIVE\\n}\\n\\nmodel GlobalNotification {\\n id Int @id @default(autoincrement())\\n type GlobalNotificationType\\n channelId String? @db.Uuid\\n Channel Channel? @relation(fields: [channelId], references: [id], onDelete: Cascade)\\n createdAt Float @default(dbgenerated(\\"gen_created_at()\\"))\\n createdAtHK String @default(dbgenerated(\\"gen_created_at_hk_timestr()\\"))\\n}\\n\\n\\n#### Backend Notification Design:\\n\\n\\n\\n##### What to Receive in Frontend?\\n\\n\\n- First of all we need to know from business logic:\\n\\n $\\\\text{Project}\\\\xrightarrow{\\\\text{many}}\\\\text{Channel}\\\\xrightarrow{\\\\text{many}}\\\\text{Session}$\\n\\n- Therefore each channel-notification must have a parent projectId. \\n\\n- Simiarly, each session-notification must have a channelId and projectId.\\n\\n- We prototype the frontend notification data as follows:\\n\\n js\\n export type IndividualType = {\\n channel: \\"NEW_CHANNEL_TO_JOIN\\"\\n session: \\"NEW_ISSUE\\"\\n | \\"NEW_LLM_REPLY\\"\\n | \\"NEW_DRAFT\\"\\n | \\"SESSION_NEW_LIVE\\"\\n | \\"SESSION_NEW_LIVE_REPLY\\"\\n | \\"ASSIGNED_AS_FOLLOWER\\"\\n | \\"WATCH\\"\\n }\\n\\n export type GlobalNotificationType = \\"ONGOING_LIVE\\"\\n\\n export type InappNotification = {\\n global?: {\\n channels: {\\n [projectId in string]?: {\\n [channelId in string]?: {\\n [type in GlobalNotificationType]?: number\\n }\\n }\\n }\\n },\\n individual?: {\\n channels: {\\n [projectId in string]?: {\\n [channelId in string]?: {\\n [type in IndividualType[\\"channel\\"]]?: number\\n }\\n }\\n },\\n sessions: {\\n [projectId in string]?: {\\n [channelId in string]?: {\\n [sessionId in string]?: {\\n [type in IndividualType[\\"session\\"]]?: number\\n }\\n }\\n }\\n }\\n }\\n }\\n \\n\\n##### How Frontend Consume the Data?\\n\\nWe store the data inside state.inappNotification of some slice.\\n\\njs\\n...\\nconst notification = useAppSelector(s => s.chat\\n .inappNotification\\n ?.individual\\n ?.sessions\\n ?.[selectedProjectId]\\n ?.[selectedChannelId]\\n ?.[roomId]\\n);\\n\\nconst numOfNotification = (() => {\\n if (type === \\"LIVE\\") {\\n if (isLiveEnded) {\\n return notification?.NEW_LLM_REPLY;\\n } else {\\n if (notification?.SESSION_NEW_LIVE) {\\n // this specialCount is -1, indicating NEW\\n return NotificationSpecialCount.NEW; \\n }\\n else {\\n return notification?.SESSION_NEW_LIVE_REPLY\\n }\\n }\\n } else {\\n if (notification?.NEW_ISSUE) {\\n return NotificationSpecialCount.NEW;\\n } else {\\n return notification?.NEW_LLM_REPLY\\n }\\n }\\n})();\\n...\\n\\n\\n\\n##### Backend Handler that Responses Desired Prototype\\n###### General Idea\\n\\n\\nThe general idea is to form an object that contains all the notification.\\n\\n- **Notification Due to Session Level Information.**\\n\\n json\\n {\\n [projectId] : {\\n [channelId]: {\\n [sessionId]: {\\n NEW_ISSUE: 1\\n NEW_DRAFT: 3\\n ...\\n }\\n }\\n }\\n }\\n \\n- **Notification Due to Channel Level Information.**\\n json\\n {\\n [projectId] : {\\n [channelId]: {\\n NEW_CHANNEL: 1\\n }\\n }\\n }\\n \\n\\n###### Code Implementation\\n\\nThis is a little bit long processing. \\n\\nSince we want to get all data in one single query. If it is hard to read, it is suggested to separate individual notifications into two separate queries for channels and for sessions respectively.\\n\\njs-1\\nconst getInappNotifications = async (req: Request, res: Response) => {\\n const userEmail = req.user?.email || \\"\\";\\n\\n // this is a fixed array of notification types that is suppose to be \\"session notification\\"\\n const individualSession: IndividualUserNotification[\\"type\\"][] = [\\n \\"ASSIGNED_AS_FOLLOWER\\",\\n \\"NEW_DRAFT\\",\\n \\"NEW_ISSUE\\",\\n \\"NEW_LLM_REPLY\\",\\n \\"SESSION_NEW_LIVE\\",\\n \\"SESSION_NEW_LIVE_REPLY\\",\\n \\"WATCH\\"\\n ];\\n // that to be \\"channel notification\\"\\n const individualChannel: IndividualUserNotification[\\"type\\"][] = [\\n \\"NEW_CHANNEL_TO_JOIN\\"\\n ];\\n\\n const [globalChannelsResult, notificationBySessionsResult] = await Promise.all([\\n db.selectFrom(\\"GlobalNotification\\")\\n .leftJoin(\\"UserToChannel\\", \\"UserToChannel.channelId\\", \\"GlobalNotification.channelId\\")\\n .leftJoin(\\"Channel\\", \\"Channel.id\\", \\"GlobalNotification.channelId\\")\\n .leftJoin(\\"Project\\", \\"Project.id\\", \\"Channel.projectId\\")\\n .select([\\n \\"GlobalNotification.type\\",\\n \\"GlobalNotification.channelId\\",\\n \\"Project.id as projectId\\"\\n ])\\n .where(\\"UserToChannel.userEmail\\", \\"=\\", userEmail)\\n .execute(),\\n\\nIn the follwoing query we make the following aliases:\\n\\n- NotificationChannel = channel being notified\\n- NotificationSessionChannel = the channel of the session being notified\\n- NotificationProject = project being notified\\n- NotificationSessionProject = project of session being notified (forget to add Notification at the prefix)\\n\\nThis will introduce sparsities (nulls) to each selected row.\\n\\nJust recall that project contains many channel, channel contains many messagesSession, then the nullity check will make sense\\n\\n\\nIn frontend each session, channeland project will calculate what notification to show based on these informations\\njs-31\\n db.selectFrom(\\"IndividualUserNotification\\")\\n .leftJoin(\\"MessagesSession\\", \\"MessagesSession.id\\", \\"IndividualUserNotification.sessionId\\")\\n .leftJoin(\\"Channel as NotificationChannel\\", \\"NotificationChannel.id\\", \\"IndividualUserNotification.channelId\\")\\n .leftJoin(\\"Channel as NotificationSessionChannel\\", \\"NotificationSessionChannel.id\\", \\"MessagesSession.channelId\\")\\n .leftJoin(\\"Project as NotificationChannelProject\\", \\"NotificationChannelProject.id\\", \\"NotificationChannel.projectId\\")\\n .leftJoin(\\"Project as NotificationSessionProject\\", \\"NotificationSessionProject.id\\", \\"NotificationSessionChannel.projectId\\")\\n .select([\\n \\"IndividualUserNotification.sessionId as sessionId\\",\\n \\"IndividualUserNotification.type as notificationType\\",\\n \\"NotificationSessionChannel.id as notificationSessionChannelId\\",\\n \\"NotificationChannel.id as notificationChannelId\\",\\n \\"NotificationChannelProject.id as notificationChannelProjectId\\",\\n \\"NotificationSessionProject.id as notificationSessionProjectId\\"\\n ])\\n .where(\\"IndividualUserNotification.userEmail\\", \\"=\\", userEmail)\\n .execute()\\n ])\\n const globalChannels: {\\n [projectId in string]?: {\\n [channelId in string]?: {\\n [type in GlobalNotification[\\"type\\"]]?: number\\n }\\n }\\n } = {};\\n\\n const notificationByChannels: {\\n [projectId in string]?: {\\n [channelId in string]?: {\\n [type in IndividualUserNotification[\\"type\\"]]?: number\\n }\\n }\\n } = {};\\n\\n const notificationBySessions: {\\n [sectionId in string]?: {\\n [projectId in string]?: {\\n [channelId in string]?: {\\n [sessionId in string]?: { [type in IndividualUserNotification[\\"type\\"]]?: number }\\n }\\n }\\n }\\n } = {};\\n\\n // set global channels notifications\\n for (const result of globalChannelsResult) {\\n const { channelId = \\"\\", type = \\"\\", projectId = \\"\\" } = result;\\n if (projectId && channelId && type) {\\n const key = [${projectId}][${channelId}][${type}];\\n const count = lodash.get(globalChannels, key, 0) as number;\\n lodash.set(globalChannels, key, count + 1);\\n }\\n }\\n\\n // set individual sessions and channels notifications\\n for (const result of notificationBySessionsResult) {\\n const {\\n notificationChannelId = \\"\\",\\n notificationSessionChannelId = \\"\\",\\n notificationType,\\n notificationChannelProjectId = \\"\\",\\n notificationSessionProjectId = \\"\\",\\n sessionId = \\"\\"\\n } = result;\\n\\n if (!notificationType) {\\n continue;\\n }\\n if (individualChannel.includes(notificationType)) {\\n if (notificationChannelProjectId && notificationChannelId && notificationType) {\\n const key = [${notificationChannelProjectId}][${notificationChannelId}][${notificationType}];\\n const count = lodash.get(notificationByChannels, key, 0) as number;\\n lodash.set(notificationByChannels, key, count + 1);\\n }\\n }\\n if (individualSession.includes(notificationType)) {\\n if (notificationSessionProjectId && notificationSessionChannelId && sessionId && notificationType) {\\n const key = [${notificationSessionProjectId}][${notificationSessionChannelId}][${sessionId}][${notificationType}];\\n const count = lodash.get(notificationBySessions, key, 0) as number;\\n lodash.set(notificationBySessions, key, count + 1);\\n }\\n }\\n }\\n\\n res.json({\\n success: true,\\n result: {\\n global: {\\n channels: globalChannels\\n },\\n individual: {\\n channels: notificationByChannels,\\n sessions: notificationBySessions\\n }\\n }\\n })\\n}\\n\\n\\n#### Sample from Real Data\\n\\njs\\n{\\n global: {\\n channels: {\\n \\"018e3606-3293-fce9-2f55-49f61657b978\\": {\\n \\"018e3624-aead-6e5f-86c4-87b4bceaf83e\\": {\\n ONGOING_LIVE: 4,\\n },\\n },\\n \\"018def71-630f-4767-2e78-0f0b9f7360a6\\": {\\n \\"018e3975-20be-40d6-2318-6db03ca7a23d\\": {\\n ONGOING_LIVE: 2,\\n },\\n },\\n \\"018e39e2-2a2e-2b20-2762-65943cddad3b\\": {\\n \\"018e39fd-2cd2-bb83-6676-be150087df97\\": {\\n ONGOING_LIVE: 1,\\n },\\n },\\n },\\n },\\n individual: {\\n channels: {\\n },\\n sessions: {\\n \\"018def71-630f-4767-2e78-0f0b9f7360a6\\": {\\n \\"018e3975-20be-40d6-2318-6db03ca7a23d\\": {\\n \\"018e3975-4fa0-0f04-3ba0-f57b7925f8c3\\": {\\n WATCH: 1,\\n },\\n },\\n \\"018e1877-2588-c90d-8688-623edd8282cd\\": {\\n \\"018e2e0e-241e-97c1-5dfe-59a09157e350\\": {\\n NEW_DRAFT: 1,\\n },\\n },\\n \\"018e2e0d-bc74-dbf2-1ea8-3786b974267d\\": {\\n \\"018e2e31-4b1a-a468-cd91-d6c1b95539fe\\": {\\n NEW_DRAFT: 1,\\n },\\n \\"018e2e59-8f93-3a4b-3270-dd4338468d2a\\": {\\n NEW_DRAFT: 1,\\n },\\n \\"018e2e0f-2c06-a0d4-289f-2a0752af6dde\\": {\\n NEW_DRAFT: 1,\\n },\\n },\\n },\\n \\"018e39e2-2a2e-2b20-2762-65943cddad3b\\": {\\n \\"018e39fd-2cd2-bb83-6676-be150087df97\\": {\\n \\"018e3fed-bda5-ceab-1334-01c89e14b315\\": {\\n SESSION_NEW_LIVE: 1,\\n },\\n \\"018e3fec-4d53-d9bd-b991-820552aba584\\": {\\n NEW_ISSUE: 1,\\n ASSIGNED_AS_FOLLOWER: 1,\\n },\\n \\"018e3f48-177f-2a78-ec95-09be41ca76a1\\": {\\n NEW_ISSUE: 2,\\n },\\n },\\n },\\n \\"018e39c6-4a23-dcaa-cebf-2a22e9842dd6\\": {\\n \\"018e39c6-dca4-e263-6dfd-b82a002c7ace\\": {\\n \\"018e39c6-ed70-868e-44c2-26e0500241e2\\": {\\n WATCH: 1,\\n },\\n \\"018e3c22-450a-06d8-00a0-f2bb56324c82\\": {\\n WATCH: 1,\\n },\\n },\\n },\\n \\"018e3606-3293-fce9-2f55-49f61657b978\\": {\\n \\"018e3624-aead-6e5f-86c4-87b4bceaf83e\\": {\\n \\"018e3fed-5323-24cb-6f60-eff184f31f2b\\": {\\n SESSION_NEW_LIVE: 1,\\n },\\n \\"018e4095-6879-eec7-3187-a4e4165283e4\\": {\\n NEW_ISSUE: 1,\\n },\\n \\"018e3f4c-4c18-dcac-69b4-10d8c307abb0\\": {\\n NEW_ISSUE: 1,\\n },\\n },\\n },\\n },\\n },\\n}\\n","title":"In-App Notification","date":"2024-03-14T00:00:00.000Z","id":"blog0246","tag":"sql","intro":"We record how to implement in-app notification which consequently makes caching much more effective and flexible.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n\\n#### Underlying Structs\\n\\ngo\\npackage main\\n\\nimport (\\n\\t\\"fmt\\"\\n\\t\\"strconv\\"\\n)\\n\\nconst ArraySize = 7\\n\\ntype HashTable struct {\\n\\tarray [ArraySize]*bucket\\n}\\n\\ntype bucket struct {\\n\\thead *bucketNode\\n}\\n\\ntype bucketNode struct {\\n\\tkey string\\n\\tnext *bucketNode\\n}\\n\\n\\n#### Hash Function\\n\\ngo\\nfunc hash(key string) int {\\n\\ttotal := 0\\n\\tfor i := 0; i < len(key); i++ {\\n\\t\\tchar := key[i]\\n\\t\\tcode := int(char)\\n\\t\\ttotal += code\\n\\t}\\n\\treturn total % ArraySize\\n}\\n\\n\\n#### Method Receivers \\n\\n##### Insert \\n\\ngo\\nfunc (h *HashMap) Insert(key string) {\\n\\tbucketIndex := hash(key)\\n\\tbucket := h.array[bucketIndex]\\n\\tbucket.Insert(key)\\n}\\n\\nfunc (b *bucket) Insert(key string) {\\n\\tif b.head == nil {\\n\\t\\tb.head = &bucketNode{key, nil}\\n\\t} else {\\n\\t\\tif b.head.key == key {\\n\\t\\t\\t// replace new value\\n\\t\\t} else {\\n\\t\\t\\tnewNode := &bucketNode{key, nil}\\n\\t\\t\\tnewNode.next = b.head\\n\\t\\t\\tb.head = newNode\\n\\t\\t}\\n\\t}\\n}\\n\\n\\n##### search \\n\\ngo\\nfunc (h *HashMap) search(key string) bool {\\n\\tbucketIndex := hash(key)\\n\\tbucket := h.array[bucketIndex]\\n\\tnode := bucket.head\\n\\n\\tfor node != nil {\\n\\t\\tif node.key == key {\\n\\t\\t\\treturn true\\n\\t\\t} else {\\n\\t\\t\\tnode = node.next\\n\\t\\t}\\n\\t}\\n\\n\\treturn false\\n}\\n\\n##### delete\\n\\ngo\\nfunc (h *HashMap) delete(key string) {\\n\\tbucketIndex := hash(key)\\n\\tbucket := h.array[bucketIndex]\\n\\n\\tif bucket.head == nil {\\n\\t\\treturn\\n\\t}\\n\\n\\tif bucket.head.key == key {\\n\\t\\tbucket.head = bucket.head.next\\n\\t}\\n\\n\\tnode := bucket.head\\n\\n\\tif node.next == nil {\\n\\t\\treturn\\n\\t}\\n\\n\\tfor node.next != nil {\\n\\t\\tif node.next.key == key {\\n\\t\\t\\tnode.next = node.next.next\\n\\t\\t} else {\\n\\t\\t\\tnode = node.next\\n\\t\\t}\\n\\t}\\n}\\n\\n\\n\\n\\n#### Experiments\\n\\n##### Init \\n\\ngo\\nfunc Init() *HashMap {\\n\\tresult := HashMap{}\\n\\tfor i := 0; i < 7; i++ {\\n\\t\\tresult.array[i] = &bucket{}\\n\\t}\\n\\treturn &result\\n}\\n\\n\\n##### print \\n\\ngo\\nfunc (h HashMap) print() {\\n\\tvar results [ArraySize]([]string)\\n\\tfor i := 0; i < ArraySize; i++ {\\n\\t\\tresults[i] = make([]string, 0, 100)\\n\\t}\\n\\n\\tfor i := 0; i < ArraySize; i++ {\\n\\t\\tbucket := h.array[i]\\n\\t\\tnode := bucket.head\\n\\t\\tfor node != nil {\\n\\t\\t\\tresults[i] = append(results[i], node.key)\\n\\t\\t\\tnode = node.next\\n\\t\\t}\\n\\t}\\n\\n\\tfor i := 0; i < ArraySize; i++ {\\n\\t\\tfmt.Printf(\\"%v-th bucket: \\", i)\\n\\t\\tfor j := 0; j < len(results[i]); j++ {\\n\\t\\t\\tfmt.Print(results[i][j] + \\" \\")\\n\\t\\t}\\n\\t\\tfmt.Println()\\n\\t}\\n}\\n\\n\\n##### Let\'s Playaround with it\\ngo\\nfunc main() {\\n\\thashTable := Init()\\n\\tfor i := 0; i < 20; i++ {\\n\\t\\thashTable.Insert(\\"abcd\\" + strconv.Itoa(i))\\n\\t}\\n\\n\\tprintln(\\"Before Deletion\\")\\n\\thashTable.print()\\n\\thashTable.delete(\\"abcd17\\")\\n\\thashTable.delete(\\"abcd1\\")\\n\\thashTable.delete(\\"abcd2\\")\\n\\thashTable.delete(\\"abcd3\\")\\n\\thashTable.delete(\\"abcd4\\")\\n\\thashTable.delete(\\"abcd5\\")\\n\\thashTable.delete(\\"abcd6\\")\\n\\thashTable.delete(\\"abcd7\\")\\n\\thashTable.delete(\\"abcd8\\")\\n\\thashTable.delete(\\"abcd18\\")\\n\\thashTable.delete(\\"abcd0\\")\\n\\thashTable.delete(\\"abcd12\\")\\n\\thashTable.delete(\\"abcd9\\")\\n\\tprintln(\\"After Deletion\\")\\n\\n\\thashTable.print()\\n}\\n\\nWhich results in:\\n\\ntext\\nBefore Deletion\\n0-th bucket: abcd16 abcd6 \\n1-th bucket: abcd17 abcd10 abcd7 abcd0 \\n2-th bucket: abcd18 abcd11 abcd8 abcd1 \\n3-th bucket: abcd19 abcd12 abcd9 abcd2\\n4-th bucket: abcd13 abcd3\\n5-th bucket: abcd14 abcd4\\n6-th bucket: abcd15 abcd5\\n\\nAfter Deletion\\n0-th bucket: abcd16\\n1-th bucket: abcd10\\n2-th bucket: abcd11\\n3-th bucket: abcd19\\n4-th bucket: abcd13\\n5-th bucket: abcd14\\n6-th bucket: abcd15\\n\\n","title":"Custom HashTable by Separate Chaining","date":"2024-03-15T00:00:00.000Z","id":"blog0247","tag":"go, data-structure","intro":"We implement a simple hashtable by using golang.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n\\n#### findPrimes\\n\\nWe study finding the number of primes in a specific range and how to break down this task by goroutines:\\n\\ngo\\nvar wg sync.WaitGroup\\n\\nfunc findPrime(start int, end int, primeChannel chan int) {\\n\\tdefer wg.Done()\\n\\tfor num := start; num <= end; num++ {\\n\\t\\tif num == 1 {\\n\\t\\t\\tcontinue\\n\\t\\t}\\n\\n\\t\\tisPrime := true\\n\\n\\t\\tfor k := 2; k < num; k++ {\\n\\t\\t\\tif num%k == 0 {\\n\\t\\t\\t\\tisPrime = false\\n\\t\\t\\t\\tbreak\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t\\tif isPrime {\\n\\t\\t\\tprimeChannel <- num\\n\\t\\t}\\n\\t}\\n}\\n\\n\\nIt takes 1263ms for start=2 and end=120000 in a single goroutine.\\n\\n#### Goroutines and Waitgroup\\n##### Method 1 (Read Data in Main Goroutine)\\n\\nLet\'s break down this into 24 goroutines:\\n\\ngo\\nfunc main() {\\n\\tstart := time.Now().UnixMilli()\\n\\tnumOfTask := 24\\n\\tpatchSize := 120000 / numOfTask\\n\\n\\tprimeChannel := make(chan int)\\n\\n\\tfor i := 0; i < numOfTask; i++ {\\n\\t\\twg.Add(1)\\n\\t\\tstart := i*patchSize + 1\\n\\t\\tend := (i + 1) * patchSize\\n\\t\\tgo findPrime(start, end, primeChannel)\\n\\t}\\n\\n\\tgo func() {\\n\\t\\twg.Wait()\\n\\t\\tclose(primeChannel)\\n\\t}()\\n\\n\\tnumOfPrimes := 0\\n\\n\\tfor range primeChannel {\\n\\t\\tnumOfPrimes++\\n\\t}\\n\\n\\tend := time.Now().UnixMilli()\\n\\tfmt.Println(\\"Time Taken:\\", fmt.Sprintf(\\"%vms\\", end-start))\\n\\tfmt.Println(\\"Number of Primes\\", numOfPrimes)\\n}\\n\\n- Here we use WaitGroup to count the number of completed goroutines.\\n\\n- We craete one additional goroutine to keep track of the completeness.\\n\\n- We close the channel when all goroutines are done.\\n\\n- ***Without*** this addtional goroutine we will get:\\n text\\n fatal error: all goroutines are asleep - deadlock!\\n \\n because the compiler has detected there is no attempt to close the channel.\\n\\nNow our execution time is reduced significantly: \\ntext\\nTime Taken: 100ms\\nNumber of Primes 11301\\n\\nOf course this number is still not optimal because all the large numbers accumulate at the later goroutines. We can further improve it by redistributing the numbers into 24 bins evenly.\\n\\n##### Method 2 (Read Data in Another Goroutine)\\nWe can also run the \\"data-reading/processing pipeline\\" in another goroutine:\\n\\ngo\\nfunc countPrimes(numOfPrimes *int, primeChannel chan int) {\\n\\tfor range primeChannel {\\n\\t\\t*numOfPrimes++\\n\\t}\\n}\\n\\ngo\\nfunc main() {\\n\\tstart := time.Now().UnixMilli()\\n\\tnumOfTask := 24\\n\\tpatchSize := 120000 / numOfTask\\n\\n\\tprimeChannel := make(chan int)\\n\\n\\tfor i := 0; i < numOfTask; i++ {\\n\\t\\twg.Add(1)\\n\\t\\tstart := i*patchSize + 1\\n\\t\\tend := (i + 1) * patchSize\\n\\t\\tgo findPrime(start, end, primeChannel)\\n\\t}\\n\\n\\tnumOfPrimes := 0\\n\\tgo countPrimes(&numOfPrimes, primeChannel)\\n\\n\\twg.Wait()\\n\\tclose(primeChannel)\\n\\n\\tend := time.Now().UnixMilli()\\n\\tfmt.Println(\\"Time Taken:\\", fmt.Sprintf(\\"%vms\\", end-start))\\n\\tfmt.Println(\\"Number of Primes\\", numOfPrimes)\\n}\\n\\n\\n- Note that we cannot add wg.Add(1) for go countPrimes(&numOfPrimes, primeChannel) because the for loop inside countPrimes only ends when the channel is closed.\\n\\n- That means it will automatically ends when the channel is closed. \\n\\n- Additionally wg.Add(1) will dead-lock indefinitely.\\n\\n- Add count and remove count to the WaitGroup ***only*** for the goroutine ***that you want to wait***.\\n\\n\\n##### Method 3 (Distribute Data by Channel in one Goroutine and Process Data in Other Goroutines)\\n\\nPreviously we need to partition our input manually, but we can also distribute data to data-processing goroutines ***by one single channel***:\\n\\ngo\\nfunc findPrime(intChannel chan int, primeChannel chan int) {\\n\\tdefer findPrime_wg.Done()\\n\\tfor num := range intChannel {\\n\\t\\tif num == 1 {\\n\\t\\t\\tcontinue\\n\\t\\t}\\n\\n\\t\\tisPrime := true\\n\\t\\tfor k := 2; k < int(math.Floor(float64(math.Sqrt(float64(num)))))+1; k++ {\\n\\t\\t\\tif num%k == 0 {\\n\\t\\t\\t\\tisPrime = false\\n\\t\\t\\t\\tbreak\\n\\t\\t\\t}\\n\\t\\t}\\n\\n\\t\\tif isPrime {\\n\\t\\t\\tprimeChannel <- num\\n\\t\\t}\\n\\t}\\n}\\n\\nvar distribute_wg sync.WaitGroup\\nvar findPrime_wg sync.WaitGroup\\n\\nfunc initNumbers(intChannel chan int, upperBound int) {\\n\\tdefer distribute_wg.Done()\\n\\n\\tfor i := 2; i <= upperBound; i++ {\\n\\t\\tintChannel <- i\\n\\t}\\n}\\n\\nfunc countPrimes(numOfPrimes *int, primeChannel chan int) {\\n\\tfor range primeChannel {\\n\\t\\t*numOfPrimes++\\n\\t}\\n}\\n\\nfunc main() {\\n\\tstart := time.Now().UnixMilli()\\n\\n\\tintChannel := make(chan int, 1000)\\n\\tprimeChannel := make(chan int)\\n\\n\\tdistribute_wg.Add(1)\\n\\tgo initNumbers(intChannel, 120000)\\n\\n\\tfor i := 0; i < 24; i++ {\\n\\t\\tfindPrime_wg.Add(1)\\n\\t\\tgo findPrime(intChannel, primeChannel)\\n\\t}\\n\\n\\tnumOfPrimes := 0\\n\\tgo countPrimes(&numOfPrimes, primeChannel)\\n\\n\\tgo func() {\\n\\t\\tdistribute_wg.Wait()\\n\\t\\tclose(intChannel)\\n\\t}()\\n\\n\\tfindPrime_wg.Wait()\\n\\tclose(primeChannel)\\n\\n\\tend := time.Now().UnixMilli()\\n\\tfmt.Println(\\"Time Taken:\\", fmt.Sprintf(\\"%vms\\", end-start))\\n\\tfmt.Println(\\"Number of Primes\\", numOfPrimes)\\n}\\n\\nHere we have made the following improvement:\\n\\n- intChannel := make(chan int, 1000) is set to be buffered because we don\'t need to wait one goroutine to consume the task, we can let all goroutines consume tasks simultaneously.\\n- We set upper bound int(math.Floor(float64(math.Sqrt(float64(num)))))+1 for finding the factor because for every decomposition $a\\\\cdot b= k$, with $a\\\\leq b$, we have $a^2\\\\leq k$, i.e., $a\\\\leq \\\\sqrt{k}$, which implies $a\\\\leq \\\\lfloor \\\\sqrt{k}\\\\rfloor +1$.\\n\\n\\nThe result is better!\\ntext\\nTime Taken: 19ms\\nNumber of Primes 11301\\n","title":"Channel and WaitGroup","date":"2024-03-23T00:00:00.000Z","id":"blog0248","tag":"go","intro":"Study how to share a SINGLE channel to all goroutines and how to close it properly.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\nWe study the following locks:\\n\\ngo\\npackage main\\n\\nimport (\\n\\t\\"fmt\\"\\n\\t\\"sync\\"\\n\\t\\"time\\"\\n)\\n\\nvar wg sync.WaitGroup\\nvar mutex sync.RWMutex\\n\\nfunc write() {\\n\\tdefer wg.Done()\\n\\n\\tmutex.Lock()\\n\\tfmt.Println(\\"Writing ...\\")\\n\\ttime.Sleep(time.Second * 1)\\n\\tmutex.Unlock()\\n}\\n\\nfunc read() {\\n\\tdefer wg.Done()\\n\\tmutex.RLock()\\n\\tfmt.Println(\\"Reading ...\\")\\n\\ttime.Sleep(time.Second * 1)\\n\\tmutex.RUnlock()\\n}\\n\\nfunc main() {\\n\\tfor r := 0; r < 10; r++ {\\n\\t\\twg.Add(1)\\n\\t\\tgo write()\\n\\t}\\n\\tfor r := 0; r < 10; r++ {\\n\\t\\twg.Add(1)\\n\\t\\tgo read()\\n\\t}\\n\\twg.Wait()\\n}\\n\\n\\n- In RWMutex, when we are writing we use Lock() to block any ***read*** and ***write*** access until we Unlock() it. \\n- However, when we use RLock(), we are telling compiler we **allow** concurrent reads.\\n- If we execute the code above, we have:\\n text\\n Writing ...\\n \\n and ***1s*** later:\\n text\\n Reading ...\\n Reading ...\\n Reading ...\\n Reading ...\\n Reading ...\\n Reading ...\\n Reading ...\\n Reading ...\\n Reading ...\\n Reading ...\\n \\n and then the following 1s later one another:\\n text\\n Writing ... // 1s\\n Writing ... // 1s\\n Writing ... // 1s\\n Writing ...\\n Writing ... .\\n Writing ... .\\n Writing ... .\\n Writing ...\\n Writing ... // 1s\\n \\n\\n","title":"Read Write Lock for Go","date":"2024-03-24T00:00:00.000Z","id":"blog0249","tag":"go","intro":"We study read lock and write lock.","toc":false},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n video {\\n border-radius: 4px;\\n max-width: 660px;\\n }\\n</style>\\n\\n#### Result\\n\\n\\n<video controls>\\n <source src=\\"/assets/tech/250/001.mp4\\" type=\\"video/mp4\\">\\n Sorry, your browser doesn\'t support embedded videos.\\n</video>\\n\\n\\n\\n#### Steps to Monitor Changes in Database\\n\\n##### Step 1. Create a Database with Logical Replication\\n\\n- Only with logical replica we can configure confluent to listen to our database update. \\n\\n- Suppose that we are using AWS RDS service, we need to create a new parameter group and change the value of rds.logical_replication to 1.\\n\\n- Finally ***reboot*** the db instance.\\n\\n\\n\\n##### Step 2. Create an Environment and a Cluster\\n\\n- Environment:\\n\\n ![](/assets/img/2024-03-29-23-43-02.png)\\n\\n- Cluster:\\n\\n ![](/assets/img/2024-03-29-23-43-33.png)\\n\\n##### Step 3. Add Connectors inside Cluster\\n\\n- Any connector that ***reads*** from DB and sends record to Kafka (Debezium) is named by some source.\\n\\n ![](/assets/img/2024-03-29-23-45-35.png)\\n\\n We call them ***source connector***.\\n\\n- In case we need to ***output*** those record into other storage, we use ***sink connector***.\\n\\n- In our case, we just want to listen to the changes inside the database using our backend, a single source connector will do.\\n\\n\\n##### Step 4. Choose Correct mode of Connector\\n\\n- Note that we are listening to the update from DB, ***by default*** a source connector is in bulk mode (which is to fetch all rows inside a table).\\n\\n- We need to change it to other mode (like timestamp or increment) to identify ***new records***:\\n\\n ![](/assets/img/2024-03-29-23-51-09.png)\\n\\n- Note that to use timestamp mode, our target column must be of type timestamp.\\n\\n- Only type integer in our database is available to increment mode. By experiment Float64 fails to be assigned as increment.\\n\\n\\n##### Step 5. Configure Consumption Clients Using a Nodejs Backend\\n\\nAt this point there are not much to do with the GUI in confluent, we will be using node-rdkafka in our monitoring backend. \\n\\nLet\'s define a dev container demonstrated by the official node-rdkafka documentation:\\n\\n- https://github.com/Blizzard/node-rdkafka/blob/master/examples/docker-alpine.md\\n\\n[This blog post](/blog/article/Dev-Container) provides more detail on dev container. Inside that container we execute the following simple implementation:\\n\\njs\\nconst fs = require(\\"fs\\");\\nconst Kafka = require(\\"node-rdkafka\\");\\n\\nconst topic = \\"db_changes_UserToChannel\\";\\n\\nfunction main() {\\n const topicConfig = { \\"auto.offset.reset\\": \\"earliest\\" };\\n const consumer = new Kafka.KafkaConsumer({\\n \\"group.id\\": \\"nodejs-group-1\\",\\n \\"security.protocol\\": \\"sasl_ssl\\",\\n \\"bootstrap.servers\\": \\"aaa\\",\\n \\"sasl.mechanisms\\": \\"PLAIN\\",\\n \\"sasl.username\\": \\"bbb\\",\\n \\"sasl.password\\": \\"ccc\\",\\n \\"session.timeout.ms\\": \\"45000\\"\\n }, topicConfig);\\n consumer.connect();\\n\\n consumer\\n .on(\\"ready\\", () => {\\n // subscribe to the topic and start polling for messages\\n console.log(\\"subscribe topic\\", topic);\\n consumer.subscribe([topic]);\\n consumer.consume();\\n })\\n .on(\\"data\\", (message) => {\\n // print incoming messages\\n console.log(Consumed message from topic ${message?.topic}: ${message?.value?.toString()});\\n });\\n}\\n\\nmain()\\n\\n\\nYou should have obtained the credentials when you try to create a cluster or configure your consumption client.\\n\\nSample messages:\\n\\ntext\\nC:\\\\Program Files\\\\nodejs\\\\node.exe .\\\\main.js\\nsubscribe topic db_changes_UserToChannel\\nmain.js:22\\nConsumed message from topic db_changes_UserToChannel: {\\"id\\":\\"018e8a48-bdf8-d76d-bdd7-615d1528e274\\",\\"channelId\\":\\"018e8439-19b0-125a-a93c-8c117a6ada7a\\",\\"userEmail\\":\\"test2@wonderbricks.com\\",\\"joined\\":false,\\"createdAt\\":1.711717006834E12,\\"createdAtHK\\":\\"2024-03-29 20:56:46\\",\\"confluent_createdAt_datetime\\":1711717006834}\\nmain.js:29\\nConsumed message from topic db_changes_UserToChannel: {\\"id\\":\\"018e8a4b-e1b4-09c6-da17-50ed74b43ce8\\",\\"channelId\\":\\"018e8439-19b0-125a-a93c-8c117a6ada7a\\",\\"userEmail\\":\\"test4@gmail.com\\",\\"joined\\":false,\\"createdAt\\":1.711717212594E12,\\"createdAtHK\\":\\"2024-03-29 21:00:12\\",\\"confluent_createdAt_datetime\\":1711717212594}\\nmain.js:29\\nConsumed message from topic db_changes_UserToChannel: {\\"id\\":\\"018e8a50-ceeb-569c-5d81-c90cef1777c2\\",\\"channelId\\":\\"018e8439-19b0-125a-a93c-8c117a6ada7a\\",\\"userEmail\\":\\"test5@gmail.com\\",\\"joined\\":false,\\"createdAt\\":1.711717535465E12,\\"createdAtHK\\":\\"2024-03-29 21:05:35\\",\\"confluent_createdAt_datetime\\":1711717535465}\\n\\n\\n#### How to Delete a Cluster?\\n\\nAfter you have done all the experiments, to delete a cluster you must first ***pause*** all of your connectors.\\n\\n![](/assets/img/2024-03-30-00-01-45.png)\\n\\nThen you are free to delete the cluster:\\n\\n![](/assets/img/2024-03-30-00-02-49.png)\\n\\n","title":"CDC in Confluent and Kafka","date":"2024-03-28T00:00:00.000Z","id":"blog0250","tag":"cdc, confluent, kafka","intro":"We study how to configure Debezium in Confluent to listen changes of database, which is very helpful in streaming real-time changes to frontend.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n#### Why \\n\\n- By looking at the Dockerfile clearly we wouldn\'t want to install those dependencies at the developer\'s machine because some may use windows, some may use mac. \\n\\n- By using dev container, we can unify the dev environment.\\n\\n#### Project Structure\\n\\n![](/assets/img/2024-03-29-23-18-18.png)\\n\\n#### devcontainer.json\\n\\njson\\n{\\n \\"name\\": \\"For Kafka\\",\\n \\"build\\": {\\n \\"dockerfile\\": \\"Dockerfile\\"\\n }\\n}\\n\\n\\n#### Dockerfile\\n\\nDockerfile\\nFROM node:20-alpine\\n\\nRUN apk --no-cache add \\\\\\n bash \\\\\\n g++ \\\\\\n ca-certificates \\\\\\n lz4-dev \\\\\\n musl-dev \\\\\\n cyrus-sasl-dev \\\\\\n openssl-dev \\\\\\n make \\\\\\n python3\\n\\nRUN apk add --no-cache --virtual .build-deps gcc zlib-dev libc-dev bsd-compat-headers py-setuptools bash\\n\\n- Note that we don\'t write RUN npm install <package-name> inside the Dockerfile because by default we are at the root level inside the container where we are not allowed to WRITE at that level. \\n\\n- Even we mkdir -p and WORKDIR at a directory and successfully npm install, but inside container we are at the workspace defined by the vscode extention Remote Development rather than the directory specified by WORKDIR.\\n\\n- The correct procedure should be to install just the native linux libraries, and then we install runtime-specific library ***inside*** the container.\\n\\n- Note that the node_modules installed inside the container will be linux-specific, running it in windows will fail.\\n\\n#### Open the Project Inside dev Container\\n\\n1. Click the lower-left button in vs-code:\\n\\n ![](/assets/img/2024-03-29-23-28-39.png)\\n\\n2. Choose Reopen in Container\\n\\n ![](/assets/img/2024-03-29-23-29-15.png)\\n\\n3. Choose the configuration we have just named:\\n\\n ![](/assets/img/2024-03-29-23-31-12.png)\\n","title":"Dev Container","date":"2024-03-29T00:00:00.000Z","id":"blog0251","tag":"docker","intro":"Sometimes a library may require user installing gcc, c++, ca-certificates, or some other linux-specific libraries. Let\'s use docker image to provide us a consistent working environment.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n#### Concurrency \\n##### Unreserved Concurrency\\n\\n- Free for all functions in $\\\\displaystyle \\\\frac{\\\\text{account}}{\\\\text{region}}$.\\n\\n- If one function concumes all concurrency, the others will get throttled.\\n\\n\\n##### Reserved Concurrency\\n\\n- This is the ***number of containers*** allowed to run concurrently for a specific lambda function.\\n\\n- This number is taken away from the unreserved concurrency pool.\\n\\n- You can use reserved concurrency to ***minimize*** or ***maximize*** the processing rate.\\n\\n- By default, the maximum number of concurrency is set to 1000, but that ***doesn\'t mean*** you are allowed to spawn 1000 containers by default.\\n\\n- By default my applied account-level quota value ***is just 10***.\\n\\n [![](/assets/img/2024-04-04-21-11-39.png)](/assets/img/2024-04-04-21-11-39.png)\\n\\n\\n- To get the maximum number of concurrency we need to request it from:\\n\\n ![](/assets/img/2024-04-04-21-32-42.png)\\n\\n- Go to Monitor > Throttles, which counts the number of function invokation that is beyond the concurrency limit. If it is **not** intentionally done to rate limit an api endpoint, then **any positive value is an alarm**.\\n\\n ![](/assets/img/2024-04-04-21-20-23.png)\\n\\n For experiment, in this picture we have 3 counts beyond the concurrency limit (which is set to be 0).\\n\\n- We can also monitor the number of **concurrent execution** to adjust the configuration of max number of concurrency:\\n\\n ![](/assets/img/2024-04-04-21-24-41.png)\\n\\n##### Provisioned Concurrency\\n\\n- As with reserved concurrency, it also subtracts from unreserved concurrency pool.\\n\\n- It is a pool of concurrency that is **always on** (for optimal latency).\\n\\n- Very **Expensive**.\\n\\n- It supports autoscaling group policies.\\n\\n\\n\\n#### Database Proxies for RDS\\n\\n- Lambda functions are executed inside a container, and when concurrency reaches to some level, the database is unable to handle large amount of concurrent requests for new connections\\n\\n- RDS Proxy helps minimize the number of requests for new connections unless it is necessary:\\n\\n ![](/assets/img/2024-04-05-01-38-01.png)\\n\\n\\n#### CloudWatch Events with Lambda\\n\\n- Choose CloudWatch and then choose Rules:\\n\\n ![](/assets/img/2024-04-05-02-08-06.png)\\n\\n- Give it a name and choose Schedule, we will be redirected to EventBridge Scheduler page:\\n\\n ![](/assets/img/2024-04-05-02-09-16.png)\\n\\n- Choose Recurring schedule and we are led to define a cron expression:\\n\\n ![](/assets/img/2024-04-05-02-10-19.png)\\n\\n- A list of templates and experiments on the cron expression:\\n\\n - https://crontab.guru/\\n\\n- Choose Lambda Invokation as a target and complete the invokation payload:\\n\\n ![](/assets/img/2024-04-05-02-18-51.png)","title":"More About Lambda Functions, Thottling, Concurrency, RDS Proxy and Integration with CloudWatch Events","date":"2024-04-04T00:00:00.000Z","id":"blog0253","tag":"aws","intro":"We study the concurrency models in lambda functions to avoid unexpected failure.","toc":true},{"content":"\\n\\n- Note that aws-xray-sdk is not a natively supported package in lambda.\\n\\n- Let\'s create a layer in order to make aws-xray-sdk importable into our lambda function in python.\\n\\n- Make sure we have conda installed, and put \\n text\\n source ~/anaconda3/etc/profile.d/conda.sh\\n \\n in our ~/.bashrc, this is to make sure conda is identifiable as a executable command.\\n\\n1. Open an empty folder and cd into it\\n\\n2. pip install -t $(pwd) aws-xray-sdk\\n\\n ![](/assets/img/2024-04-05-00-53-08.png)\\n\\n3. Create a new folder named python, drag all package related files into it, and zip that python folder\\n\\n ![](/assets/img/2024-04-05-00-56-15.png)\\n\\n4. Go to Layers and click Create Layer\\n\\n ![](/assets/img/2024-04-05-01-06-23.png)\\n\\n5. Upload the zip file and we are ready to go.\\n\\n ![](/assets/img/2024-04-05-01-07-02.png)\\n\\n6. Test the import statement which originally throwed error without our additional layer:\\n\\n ![](/assets/img/2024-04-05-01-15-58.png)\\n\\n python\\n import json\\n from aws_xray_sdk.core import xray_recorder\\n\\n def lambda_handler(event, context):\\n # TODO implement\\n return {\\n \'statusCode\': 200,\\n \'body\': json.dumps(\'Hello from Lambda!\')\\n }\\n ","title":"Create Custom Layer for Lambda Functions in Python","date":"2024-04-12T00:00:00.000Z","id":"blog0254","tag":"aws","intro":"We record a standard procedure to create a custom layer for 3rd party libraries.","toc":false},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n#### Project Typescript\\n\\n##### Initialization\\n\\n- We install aws-cdk by \\n text\\n npm install -g aws-cdk\\n \\n\\n- We then init a project by\\n\\n text\\n cdk init app --language typescript\\n \\n##### Project Structure\\n\\nThe project structure is as follows:\\n\\n ![](/assets/img/2024-04-17-02-39-04.png)\\n\\n As we will see, \\n - lib/ contains stack definition**s** of resources, \\n - bin/ contains the entry point of declaring all the stacks pointed in cdk.json.\\n\\n#### Rename bin/xxx.ts to bin/launch.ts\\n\\nThe default name in bin/ may be misleading, which is automatically generated according to the directory we are working with.\\n\\nTherefore we now rename bin/cdk_demo_app.ts into bin/launch.ts.\\n\\nIn cdk.json we change the app field to \\n json\\n \\"app\\": \\"npx ts-node --prefer-ts-exts bin/launch.ts\\"\\n \\n\\n#### Combining the Stack Definitions\\n\\nInstead of introducing various stack definitions, we first have a overview of how we put the stack togethers in bin/launch.json:\\n\\nts\\nimport \'source-map-support/register\';\\nimport * as cdk from \'aws-cdk-lib\';\\nimport { LambdaStack } from \'../lib/lambda-stack\';\\nimport { PhotoS3Stack } from \'../lib/photo-s3-stacks\';\\nimport { APIStack } from \'../lib/api-stack\';\\nimport { DataTableStack } from \'../lib/data-table-stack\';\\n\\nconst app = new cdk.App();\\n\\nconst photoStack = new PhotoS3Stack(app, \'photoS3Stacks\');\\n\\nconst dataTableStack = new DataTableStack(app, \\"spacesTableStack\\");\\n\\nconst lambdaStack = new LambdaStack(app, \'LambdaStack\', {\\n targetBucketArn: photoStack.photoBucketName,\\n spacesTable: dataTableStack.spaceTable\\n});\\n\\nconst apiStack = new APIStack(app, \\"ApiStack\\", {\\n helloLambdaIntegration: lambdaStack.helloLambdaIntegration,\\n spaceLambdaIntegration: lambdaStack.spaceLambdaIntegration\\n})\\n\\n- Lambda function will interact with **S3 bucket** and **dynamodb**, therefore we need to pass the bucketArn and tableArn into LambdaStack in order to grant proper policies to the automatically generated role.\\n\\n- In defining various stacks we will expose the generated resource as a public field in order to ***share resources among different stacks***.\\n\\n#### Create Stacks of Resources\\n\\n##### S3 (PhotoS3Stack)\\n\\n\\n\\nts\\nimport * as cdk from \'aws-cdk-lib\';\\nimport { Construct } from \'constructs\';\\n// import * as sqs from \'aws-cdk-lib/aws-sqs\';\\nimport { Function as LambdaFunction } from \\"aws-cdk-lib/aws-lambda\\"\\nimport { Bucket } from \'aws-cdk-lib/aws-s3\';\\n\\nexport class PhotoS3Stack extends cdk.Stack {\\n public readonly photoBucketName: string;\\n constructor(scope: Construct, id: string, props?: cdk.StackProps) {\\n super(scope, id, props);\\n const photosBucket = new Bucket(this, \\"photo-s3-bucket\\", {\\n bucketName: \\"photo-buckets-for-lambda-testing\\"\\n })\\n this.photoBucketName = photosBucket.bucketArn\\n }\\n}\\n\\n##### Lambda Function (LambdaStack)\\n\\nIn the root directory we create a folder called service\\n\\n![](/assets/img/2024-04-17-03-00-18.png)\\n\\n###### handler --- hello.ts (List all S3 Buckets)\\n\\n- This is a simple handler which lists all s3 buckets in my account:\\n\\n- For sure we will need to allow \\n\\n - \\"s3:ListAllMyBuckets\\" and \\n\\n - \\"s3:ListBucket\\" \\n\\n from all related resources\\n \\n- We will simply use resources: [\\"*\\"] in role policy to skip the process of finding suitable ***(FROM)*** Resource.\\n\\n\\nts\\nimport { APIGatewayProxyEvent, APIGatewayProxyResult, Context } from \\"aws-lambda\\"\\nimport { S3Client, ListBucketsCommand } from \\"@aws-sdk/client-s3\\"\\n\\nconst s3Client = new S3Client();\\n\\nconst handler = async (event: APIGatewayProxyEvent, context: Context) => {\\n const command = new ListBucketsCommand({});\\n const listBucketResults = (await s3Client.send(command)).Buckets;\\n const res: APIGatewayProxyResult = {\\n statusCode: 200,\\n body: JSON.stringify(Here is a list of my buckets: ${listBucketResults?.map(r => r.Name).join(\\", \\")})\\n }\\n console.log(event);\\n return res;\\n}\\n\\nexport {\\n handler\\n}\\n\\n\\n\\n###### handler --- spaces/handler.ts (CRUD with DynamoDB)\\n\\n- We defer the definitions of get, post, put, delete_ functions in [**this post**](/blog/article/CRUD-in-DynamoDB).\\n\\nts\\nimport { DynamoDBClient } from \\"@aws-sdk/client-dynamodb\\";\\nimport { APIGatewayProxyEvent, APIGatewayProxyResult, Context } from \\"aws-lambda\\"\\nimport post from \\"./methods/post\\";\\nimport get from \\"./methods/get\\";\\nimport put from \\"./methods/put\\";\\nimport delete_ from \\"./methods/delete\\";\\n\\nconst ddbClient = new DynamoDBClient({});\\n\\nconst handler = async (event: APIGatewayProxyEvent, context: Context): Promise<APIGatewayProxyResult> => {\\n let message: string = \\"\\";\\n try {\\n switch (event.httpMethod) {\\n case \\"GET\\":\\n return get(event, ddbClient);\\n case \\"POST\\":\\n return post(event, ddbClient);\\n case \\"PUT\\":\\n return put(event, ddbClient);\\n case \\"DELETE\\":\\n return delete_(event, ddbClient);\\n default:\\n break;\\n }\\n return res: APIGatewayProxyResult = {\\n statusCode: 200,\\n body: message\\n }\\n }\\n catch (err) {\\n return {\\n statusCode: 500,\\n body: JSON.stringify(err)\\n }\\n }\\n}\\n\\nexport {\\n handler\\n}\\n\\n\\n###### Create Stack Definition of Lambda Functions\\n\\n- We defined two lambda functions inside the set stack definition.\\n- Although we use typescript in this project, we can create handler in any other languages.\\n\\n\\n\\nts\\nimport * as cdk from \'aws-cdk-lib\';\\nimport { Construct } from \'constructs\';\\n// import * as sqs from \'aws-cdk-lib/aws-sqs\';\\nimport { Code, Function as LambdaFunction, Runtime } from \\"aws-cdk-lib/aws-lambda\\"\\nimport { join } from \\"path\\";\\nimport { LambdaIntegration } from \'aws-cdk-lib/aws-apigateway\';\\nimport { Lambda } from \'aws-cdk-lib/aws-ses-actions\';\\nimport { NodejsFunction } from \'aws-cdk-lib/aws-lambda-nodejs\';\\nimport { Effect, PolicyStatement } from \'aws-cdk-lib/aws-iam\';\\nimport { ITable } from \'aws-cdk-lib/aws-dynamodb\';\\n\\nexport class LambdaStack extends cdk.Stack {\\n public helloLambdaIntegration: LambdaIntegration\\n public spaceLambdaIntegration: LambdaIntegration\\n constructor(scope: Construct, id: string, props: cdk.StackProps & {\\n targetBucketArn: string,\\n spacesTable: ITable\\n }) {\\n super(scope, id, props);\\n \\n // ============ start of first lambda ============\\n const testLambda = new NodejsFunction(\\n this,\\n \\"lambda-from-file\\", // id of this lambda function,\\n {\\n runtime: Runtime.NODEJS_18_X,\\n handler: \\"handler\\",\\n entry: join(__dirname, \\"..\\", \\"services\\", \\"hello.ts\\"),\\n environment: {\\n TARGET_BUCKET: props.targetBucketArn\\n }\\n }\\n )\\n /**\\n * list of all condition keys:\\n * https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazons3.html\\n */\\n testLambda.addToRolePolicy(new PolicyStatement({\\n effect: Effect.ALLOW,\\n actions: [\\n \\"s3:ListAllMyBuckets\\",\\n \\"s3:ListBucket\\"\\n ],\\n resources: [\\"*\\"]\\n }))\\n // ============ end of first lambda ============\\n\\n // ============ start of second lambda ============\\n const spaceLambda = new NodejsFunction(\\n this,\\n \\"space-lambda\\", // id of this lambda function,\\n {\\n runtime: Runtime.NODEJS_18_X,\\n handler: \\"handler\\",\\n entry: join(__dirname, \\"..\\", \\"services\\", \\"spaces\\", \\"handler.ts\\"),\\n environment: {\\n TABLE_NAME: props.spacesTable.tableName\\n }\\n }\\n )\\n\\n spaceLambda.addToRolePolicy(new PolicyStatement({\\n effect: Effect.ALLOW,\\n resources: [props.spacesTable.tableArn],\\n actions: [\\n \\"dynamodb:GetItem\\",\\n \\"dynamodb:Scan\\",\\n \\"dynamodb:PutItem\\",\\n \\"dynamodb:DeleteItem\\"\\n ]\\n }))\\n // ============ end of second lambda ============\\n\\n this.helloLambdaIntegration = new LambdaIntegration(testLambda);\\n this.spaceLambdaIntegration = new LambdaIntegration(spaceLambda);\\n }\\n}\\n\\n\\n##### API Gateway (APIStack)\\n\\nts\\nexport class APIStack extends cdk.Stack {\\n\\n constructor(scope: Construct, id: string, props: cdk.StackProps & {\\n helloLambdaIntegration: LambdaIntegration,\\n spaceLambdaIntegration: LambdaIntegration\\n }) {\\n super(scope, id, props);\\n\\n const api = new RestApi(this, \\"test-api\\",);\\n const apiResources = api.root.addResource(\\"test\\");\\n apiResources.addMethod(\\"GET\\", props.helloLambdaIntegration);\\n\\n const spaceRouter = api.root.addResource(\\"spaces\\");\\n spaceRouter.addMethod(\\"GET\\", props.spaceLambdaIntegration);\\n spaceRouter.addMethod(\\"POST\\", props.spaceLambdaIntegration);\\n spaceRouter.addMethod(\\"PUT\\", props.spaceLambdaIntegration);\\n spaceRouter.addMethod(\\"DELETE\\", props.spaceLambdaIntegration);\\n }\\n}\\n\\n\\n\\n##### DynamoDB (DataTableStack)\\n\\n- ts\\n import * as cdk from \'aws-cdk-lib\';\\n import { Construct } from \'constructs\';\\n import { AttributeType, ITable, Table } from \\"aws-cdk-lib/aws-dynamodb\\";\\n import getSuffixFromStack from \'../utils/getSuffixFromStack\';\\n\\n export class DataTableStack extends cdk.Stack {\\n public spaceTable: ITable\\n constructor(scope: Construct, id: string, props?: cdk.StackProps) {\\n super(scope, id, props);\\n\\n const suffix = getSuffixFromStack(this);\\n\\n this.spaceTable = new Table(this, \\"SpacesTable\\", {\\n partitionKey: {\\n name: \\"id\\",\\n type: AttributeType.STRING\\n },\\n tableName: SpaceStack-${suffix},\\n })\\n }\\n }\\n \\n\\n- Here getSuffixFromStack is a simple util function defined by \\n ts\\n import { Fn, Stack } from \\"aws-cdk-lib\\";\\n\\n export default (stack: Stack) => {\\n /**\\n * exmaple of stackId: \\n * 1:562976154517:stack/spacesTableStack/ed3f63e0-fa7b-11ee-9900-0a107562c215 \\n */\\n const shortStakcId = Fn.select(2, Fn.split(\\"/\\", stack.stackId));\\n const suffix = Fn.select(4, Fn.split(\\"-\\", shortStakcId));\\n return suffix;\\n }\\n \\n\\n#### Deployment\\n##### Deploy the Whole Stacks\\n- We need to initiate a cloudformation stack definition of this project:\\n\\n text\\n cdk bootstrap\\n \\n\\n- Next we need to check whether our code can generate a cloudformation definition successfully:\\n\\n text\\n cdk synth\\n \\n\\n- If everything is done, we deploy our stacks by:\\n text\\n cdk deploy --all\\n \\n\\n##### Deploy Part of the Stacks\\n\\n- Note that we have defined many stacks in launch.ts, it is possible to inspect the names of stacks by\\n text\\n cdk list\\n \\n which yields:\\n\\n text\\n photoS3Stacks\\n spacesTableStack\\n LambdaStack\\n ApiStack\\n \\n- Suppose we just want to reploy the LambdaStack only, we execute\\n\\n text\\n cdk deploy LambdaStack\\n \\n\\n#### Debug a Lambda Function\\n\\n##### .vscode/launch.json\\n\\nNote that if our lambda function uses an env variable provided by other stack, then we may need to hard-code it for debugging.\\n\\n\\njson\\n{\\n \\"version\\": \\"0.2.0\\",\\n \\"configurations\\": [\\n {\\n \\"type\\": \\"node\\",\\n \\"request\\": \\"launch\\",\\n \\"name\\": \\"Debug Local File\\",\\n \\"runtimeArgs\\": [\\n \\"-r\\",\\n \\"ts-node/register\\"\\n ],\\n \\"args\\": [\\n \\"${relativeFile}\\"\\n ],\\n \\"env\\": {\\n \\"AWS_REGION\\": \\"ap-northeast-1\\",\\n \\"TABLE_NAME\\": \\"SpaceStack-0a107562c215\\"\\n }\\n }\\n ]\\n}\\n\\n\\n##### Write a Test File\\n\\nLet\'s create a file test/spaces-handler-test.ts and write \\n\\nts\\nimport { APIGatewayProxyEvent } from \\"aws-lambda\\";\\nimport { handler } from \\"../services/spaces/handler\\";\\n\\nhandler(\\n {\\n httpMethod: \\"DELETE\\",\\n queryStringParameters: {\\n id: \\"0206f811-3880-4065-bd5f-3a82b6e64de5\\"\\n },\\n body: JSON.stringify({ location: \\"Chipi Chipi ChapaChap\\" })\\n } as any as APIGatewayProxyEvent,\\n {} as any\\n);\\n\\nNote that handler must satisfy the interface:\\nts\\nimport { APIGatewayProxyEvent } from \\"aws-lambda\\";\\n\\nasync (event: APIGatewayProxyEvent, context: Context): Promise<APIGatewayProxyResult>\\n\\n\\n\\n#### Project Initialization in Python\\n\\nSuppose our lambda functions are written in python, it makes sense to start the project with Python since debugging the lambda functions will be more convenient. \\n\\n##### Initialization\\n\\nFirst we init the project by \\n text\\n cdk init sample-app --language python\\n \\n\\n##### Project Structure\\n\\n![](/assets/img/2024-04-17-04-08-26.png)\\n\\n##### lambda/handler\\n\\nIt is as simple as \\n\\npy\\ndef handler(event, context):\\n print(event)\\n return {\\n \\"statusCode\\": 200,\\n \\"body\\": \\"success\\"\\n }\\n\\n\\n##### lambda_from_cli/lambda_from_cli_stack.py (LambdaFromCliStack)\\n\\npy\\nfrom constructs import Construct\\nfrom aws_cdk import (\\n Duration,\\n Stack,\\n aws_iam as iam,\\n aws_sqs as sqs,\\n aws_sns as sns,\\n aws_sns_subscriptions as subs,\\n aws_lambda as lambda_,\\n aws_lambda_event_sources as lambda_event_sources\\n)\\n\\n\\nclass LambdaFromCliStack(Stack):\\n def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:\\n super().__init__(scope, construct_id, **kwargs)\\n\\n queue = sqs.Queue(\\n self, \\"LambdaFromCliQueue\\",\\n visibility_timeout=Duration.seconds(300),\\n )\\n\\n # create lambda function\\n sqs_lambda = lambda_.Function(self, \\n \\"SQSLambda\\", \\n handler=\\"lambda_handler.handler\\",\\n runtime=lambda_.Runtime.PYTHON_3_12,\\n code=lambda_.Code.from_asset(\\"lambda\\")\\n )\\n\\n # Create event source\\n sqs_event_source = lambda_event_sources.SqsEventSource(queue)\\n\\n # Add SQS event soruce to lambda\\n sqs_lambda.add_event_source(sqs_event_source)\\n\\n\\n##### app.py\\npy\\n#!/usr/bin/env python3\\n\\nimport aws_cdk as cdk\\nfrom lambda_from_cli.lambda_from_cli_stack import LambdaFromCliStack\\n\\napp = cdk.App()\\nLambdaFromCliStack(app, \\"LambdaFromCliStack\\")\\n\\napp.synth()\\n","title":"AWS CDK in Typescript and Python with \u2460 Application in S3 \u2461 Lambda Functions with Debugging \u2462 API Gateway and \u2463 DynamoDB","date":"2024-04-15T00:00:00.000Z","id":"blog0255","tag":"aws","intro":"We study how to create stack of aws resources and how they can interact with each other.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n\\n#### Detail on CRUD with DynamoDB\\n\\n\\nWe continue from section\\n- handler --- spaces/handler.ts (CRUD with DynamoDB)\\nin [**this post**](/blog/article/AWS-CDK-in-Typescript-and-Python-with-Application-in-S3-Lambda-Functions-with-Debugging-API-Gateway-and-DynamoDB#handler-----spaces/handler.ts-(CRUD-with-DynamoDB))\\n\\nSuppose that we have initiated a ddbClient by \\nts\\nconst ddbClient = new DynamoDBClient({});\\n\\nand passed into each of get, post, put, delete functions.\\n\\n##### get.ts\\n\\nts\\nimport { DynamoDBClient, GetItemCommand, ScanCommand } from \\"@aws-sdk/client-dynamodb\\";\\nimport { unmarshall } from \\"@aws-sdk/util-dynamodb\\";\\nimport { APIGatewayProxyEvent, APIGatewayProxyResult } from \\"aws-lambda\\";\\n\\nexport default async (event: APIGatewayProxyEvent, ddbClient: DynamoDBClient): Promise<APIGatewayProxyResult> => {\\n const spaceId = event.queryStringParameters?.[\\"id\\"];\\n if (event.queryStringParameters) {\\n if (!spaceId) {\\n return {\\n statusCode: 400,\\n body: JSON.stringify(\\"id required\\")\\n }\\n }\\n\\n event.queryStringParameters\\n const res = await ddbClient.send(new GetItemCommand({\\n TableName: process.env.TABLE_NAME,\\n Key: { id: { S: spaceId } }\\n }));\\n\\n if (res.Item) {\\n const unmarshalled = unmarshall(res.Item);\\n return {\\n statusCode: 200,\\n body: JSON.stringify(unmarshalled)\\n }\\n } else {\\n return {\\n statusCode: 404,\\n body: JSON.stringify(space with id: ${spaceId} not found)\\n }\\n }\\n } else {\\n const result = await ddbClient.send(new ScanCommand({\\n TableName: process.env.TABLE_NAME,\\n }));\\n const result_ = result.Items?.map(item => unmarshall(item));\\n return {\\n statusCode: 200,\\n body: JSON.stringify(result_)\\n }\\n }\\n}\\n\\n\\n##### post.ts\\n\\nts\\nimport { DynamoDBClient, PutItemCommand } from \\"@aws-sdk/client-dynamodb\\";\\nimport { marshall } from \\"@aws-sdk/util-dynamodb\\";\\nimport { APIGatewayProxyEvent, APIGatewayProxyResult } from \\"aws-lambda\\";\\nimport { v4 } from \\"uuid\\";\\n\\nexport default async (event: APIGatewayProxyEvent, ddbClient: DynamoDBClient): Promise<APIGatewayProxyResult> => {\\n const randomId = v4();\\n const item = JSON.parse(event.body || \\"\\");\\n\\n const result = await ddbClient.send(new PutItemCommand({\\n TableName: process.env.TABLE_NAME,\\n Item: marshall(item)\\n }));\\n\\n return {\\n statusCode: 201,\\n body: JSON.stringify({ id: randomId })\\n };\\n}\\n\\n\\n##### put.ts\\n\\nts\\nimport { DynamoDBClient, PutItemCommand, UpdateItemCommand } from \\"@aws-sdk/client-dynamodb\\";\\nimport { APIGatewayProxyEvent, APIGatewayProxyResult } from \\"aws-lambda\\";\\n\\nexport default async (event: APIGatewayProxyEvent, ddbClient: DynamoDBClient): Promise<APIGatewayProxyResult> => {\\n if (event.queryStringParameters && \\"id\\" in event.queryStringParameters && event.body) {\\n const spaceId = event.queryStringParameters[\\"id\\"] || \\"\\";\\n const [key, value] = Object.entries(JSON.parse(event.body))[0] as [string, string];\\n const updateResult = await ddbClient.send(new UpdateItemCommand({\\n TableName: process.env.TABLE_NAME,\\n Key: {\\n \\"id\\": { S: spaceId }\\n },\\n UpdateExpression: \\"set #attr = :new\\",\\n ExpressionAttributeNames: {\\n \\"#attr\\": key\\n },\\n ExpressionAttributeValues: {\\n \\":new\\": {\\n \\"S\\": value\\n }\\n },\\n ReturnValues: \\"UPDATED_NEW\\"\\n }))\\n\\n return {\\n statusCode: 204,\\n body: JSON.stringify(updateResult.Attributes)\\n }\\n }\\n\\n return {\\n statusCode: 400,\\n body: JSON.stringify(\\"Please provide value arguments\\")\\n };\\n}\\n\\n\\n##### delete.ts\\n\\nts\\nimport { DeleteItemCommand, DynamoDBClient, PutItemCommand } from \\"@aws-sdk/client-dynamodb\\";\\nimport { APIGatewayProxyEvent, APIGatewayProxyResult } from \\"aws-lambda\\";\\n\\nexport default async (event: APIGatewayProxyEvent, ddbClient: DynamoDBClient): Promise<APIGatewayProxyResult> => {\\n if (event.queryStringParameters && \\"id\\" in event.queryStringParameters && event.body) {\\n const spaceId = event.queryStringParameters[\\"id\\"] || \\"\\";\\n await ddbClient.send(new DeleteItemCommand({\\n TableName: process.env.TABLE_NAME,\\n Key: { id: { S: spaceId } }\\n }))\\n\\n return {\\n statusCode: 204,\\n body: JSON.stringify(deleted item with id ${spaceId})\\n }\\n }\\n\\n return {\\n statusCode: 201,\\n body: JSON.stringify(\\"Please provide valid arguments.\\")\\n };\\n}\\n","title":"CRUD in DynamoDB","date":"2024-04-16T00:00:00.000Z","id":"blog0256","tag":"aws","intro":"Basic CRUD with DynamoDB","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n\\n\\n#### Main Reference\\n\\n- [**Official Documentation**](https://docs.expo.dev/eas-update/introduction/)\\n\\n#### Package Installation\\n\\n- Install the packages for expo-updates\\n\\n text\\n npx expo install expo-updates\\n \\n\\n#### Config the eas.json\\n\\n- Set update config:\\n\\n text \\n eas update:configure\\n \\n\\n it should set the channel properties in our eas.json in all environments.\\n\\n#### Channels\\n\\n- According to [**this documentation**](https://docs.expo.dev/eas-update/eas-cli/) we can check existing channels (the deployment stages) by \\n\\n text\\n eas channel:list\\n \\n\\n- We can view a specific channel by \\n text\\n eas channel:view production\\n \\n\\n- We can create a channel by \\n\\n text\\n eas channel:create [channel-name]\\n \\n\\n Let\'s create a channel called prod:\\n\\n ![](/assets/img/2024-04-17-22-03-44.png)\\n\\n then in expo-development portal:\\n\\n ![](/assets/img/2024-04-17-22-03-38.png)\\n\\n#### Branches\\n\\n- A ***new branch*** version-1.0 is automatically created and attached to prod channel\\n\\n ![](/assets/img/2024-04-17-22-04-08.png)\\n\\n\\n#### Execution of the OTA Update and Caveat\\n\\n- We add a script in package.json:\\n\\n text\\n env-cmd -f .env-cmdrc -e default,prod eas update --branch uat --message \\"Update\\" --clear-cache\\n \\n\\n- Note that by default if no env variable is declared, all env variables ***will be erased***.\\n\\n- This is why we declare all environment variables by \\n text\\n env-cmd -f .env-cmdrc -e default,prod\\n \\n in the deployment script.","title":"EAS Update","date":"2024-04-17T00:00:00.000Z","id":"blog0257","tag":"eas, expo","intro":"We discuss the technical detail of OTA (Over The Air) update which avoid unnecessary rebuild for updating a mobile application.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n#### Repository\\n\\n- https://github.com/machingclee/2024-04-28-debezium-pgsql-monitoring-template\\n\\n\\n#### Docker-Compose File and Registration of Debezium Connector\\n\\n##### Spin up Instances\\n\\nThe docker-compose file is modified from [***this repository***](https://github.com/irtiza07/postgres_debezium_cdc/blob/master/docker-compose.yaml) with tutorial [***in this video***](https://www.youtube.com/watch?v=YZRHqRznO-o&t=583s)\\n\\n\\nyml\\nversion: \\"3.7\\"\\nservices:\\n zookeeper:\\n image: confluentinc/cp-zookeeper:5.5.3\\n environment:\\n ZOOKEEPER_CLIENT_PORT: 2181\\n\\n kafka:\\n image: confluentinc/cp-enterprise-kafka:5.5.3\\n depends_on: [ zookeeper ]\\n ports:\\n - 29092:29092\\n environment:\\n KAFKA_BROKER_ID: 1\\n KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\\n KAFKA_ZOOKEEPER_CONNECT: \\"zookeeper:2181\\"\\n KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092\\n KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT \\n\\n debezium:\\n image: debezium/connect:1.4\\n environment:\\n BOOTSTRAP_SERVERS: kafka:9092\\n GROUP_ID: 1\\n CONFIG_STORAGE_TOPIC: connect_configs\\n OFFSET_STORAGE_TOPIC: connect_offsets\\n KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter\\n VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter\\n VALUE_CONVERTER_SCHEMAS_ENABLE: false\\n CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081\\n CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081\\n depends_on: [ kafka ]\\n ports:\\n - 8083:8083\\n\\n schema-registry:\\n image: confluentinc/cp-schema-registry:5.5.3\\n environment:\\n - SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL=zookeeper:2181\\n - SCHEMA_REGISTRY_HOST_NAME=schema-registry\\n - SCHEMA_REGISTRY_LISTENERS=http://schema-registry:8081,http://localhost:8081\\n ports:\\n - 8081:8081\\n depends_on: [ zookeeper, kafka ]\\n\\n\\n##### Configure a project-root/debezium.json for an Instance of Connector\\n\\njson\\n{\\n \\"name\\": \\"billie-connector\\",\\n \\"config\\": {\\n \\"connector.class\\": \\"io.debezium.connector.postgresql.PostgresConnector\\",\\n \\"plugin.name\\": \\"pgoutput\\",\\n \\"database.hostname\\": \\"xxx.rds.amazonaws.com\\",\\n \\"database.port\\": \\"5432\\",\\n \\"database.user\\": \\"xxx\\",\\n \\"database.password\\": \\"yyy\\",\\n \\"database.dbname\\": \\"zzz\\",\\n \\"database.server.name\\": \\"postgres\\",\\n \\"table.include.list\\": \\"public.MessagesSession,public.SummaryFollow,public.LLMSummary,public.UserToProject,public.UserToChannel\\"\\n }\\n}\\n\\n\\nFrom experience this database.server.name will determine the topic name in Kafka. \\n\\nFor example, we will be having\\n- postgres.public.LLMSummary\\n- postgres.public.MessagesSession\\n- postgres.public.SummaryFollow\\n- postgres.public.UserToChannel\\n- postgres.public.UserToProject\\n\\nas our topics to listen. \\n\\n##### Create Debezium Source Connector from the Configuration\\n\\n###### Post Request to Create a Connector\\n\\nLet\'s execute the following in a bash shell:\\nbash\\ncurl -i -X POST -H \\"Accept:application/json\\" -H \\"Content-Type:application/json\\" 127.0.0.1:8083/connectors/ --data \\"@debezium.json\\"\\n\\n###### Health-check the connector\\nLet\'s health-check the latest connector:\\nbash\\ncurl -H \\"Accept:application/json\\" localhost:8083/connectors/billie-connector/status\\n\\n###### List all Topics Created\\nNote that the name of the running container ***depends on*** your ***working directory name***.\\nbash\\ndocker exec -it <directory-name>-kafka-1 bash\\n\\nand then run\\nbash\\n/usr/bin/kafka-topics --bootstrap-server localhost:9092 --list\\n\\n\\nIn my case I get:\\ntext\\n__confluent.support.metrics\\n__consumer_offsets\\n_schemas\\nconnect-status\\nconnect_configs\\nconnect_offsets\\npostgres.public.LLMSummary\\npostgres.public.MessagesSession\\npostgres.public.SummaryFollow\\npostgres.public.UserToChannel\\npostgres.public.UserToProject\\n\\n\\n\\n#### Adjust the Logical Replication Level\\n\\nWithout full identity we cannot capture the ***state before changes*** (which will be null in the messages from Kafka), let\'s adjust it:\\n\\nsql\\nALTER TABLE \\"LLMSummary\\" REPLICA IDENTITY FULL;\\nALTER TABLE \\"MessagesSession\\" REPLICA IDENTITY FULL;\\nALTER TABLE \\"SummaryFollow\\" REPLICA IDENTITY FULL;\\nALTER TABLE \\"UserToChannel\\" REPLICA IDENTITY FULL;\\nALTER TABLE \\"UserToProject\\" REPLICA IDENTITY FULL;\\n\\n\\n#### Listening to the Topics\\n\\n##### main.js\\n\\nLet\'s try [**kafkajs**](https://www.npmjs.com/package/kafkajs) instead of [**node-rdkafka**](https://www.npmjs.com/package/node-rdkafka):\\n\\ntext\\nyarn add kafkajs\\n\\nwith \\n\\njs\\nconst { Kafka } = require(\'kafkajs\')\\n\\nconst kafka = new Kafka({\\n clientId: \'my-app\',\\n brokers: [\'localhost:29092\']\\n})\\n\\nconst topics = [\\n \\"postgres.public.LLMSummary\\",\\n \\"postgres.public.MessagesSession\\",\\n \\"postgres.public.SummaryFollow\\",\\n \\"postgres.public.UserToChannel\\",\\n \\"postgres.public.UserToProject\\"\\n];\\nconst run = async () => {\\n const consumer = kafka.consumer({ groupId: \\"kafka\\" });\\n consumer.subscribe({ topics, fromBeginning: false })\\n await consumer.run({\\n eachMessage: async ({ topic, partition, message }) => {\\n const payload = JSON.parse(message.value.toString()).payload;\\n console.log(\\"------------------------\\")\\n console.log(topic)\\n console.log(payload)\\n },\\n })\\n}\\n\\nrun().catch(console.error);\\n\\nprocess.on(\'uncaughtException\', function (err) {\\n logger.error(err.stack);\\n logger.info(\\"Node NOT Exiting...\\");\\n});\\n\\n\\n##### Results\\n\\nSuppose that I have done an action in our frontend, then it is very clear what happened in the backend:\\n\\n[![](/assets/img/2024-04-28-03-57-11.png)](/assets/img/2024-04-28-03-57-11.png)\\n\\nFor example:\\n\\n- What is ***inserted*** into the database\\n- What is ***updated*** into the database\\n\\nIt helps understand the business from the tables and understand the tables from the business as well.","title":"Kafka and Debezium with Everything Hosted Locally Without Confluent","date":"2024-04-28T00:00:00.000Z","id":"blog0258","tag":"kafka, debezium","intro":"In the past we have studied CDC with the help of confluent [***here***](/blog/article/CDC-in-Confluent-and-Kafka). This time we host everything locally to prepare ourselves to host CDC without the dependency on confluent.","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n#### Installation\\n\\n- We installl kubectrl from [here](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/).\\n\\n- install minikube from [here](https://minikube.sigs.k8s.io/docs/start/).\\n- Note that kubectl will be configured by minikube to use minikube as to create cluster.\\n\\n#### Basic minikube Commands\\n\\n- There no need to install virtual box as we can use \\n text\\n minikube start --driver=docker\\n \\n to simulate our environment\\n\\n- We can spin up the dashboard for clusters by \\n text\\n minikube dashboard\\n \\n\\n- Build a basic docker image:\\n text\\n docker build -t kub-first-app .\\n \\n\\n#### Kubectl: Imperative Approach\\n\\nIn the sequel we will set the alias alias k=kubectl.\\n\\n##### Create a deployment object\\ntext\\nk create deployment first-app --image=kub-first-app\\n\\nAt this point no pods is running because minikube cannot pull image from my local machine\\n\\n##### Get all deployments\\ntext\\nk get deployments\\n\\n##### Check the status of all pods\\ntext\\nk get pods\\n\\n\\n##### Delete a deployment object\\ntext\\nk delete deployment first-app\\n\\n\\n\\n##### Create a service by an existing deployment\\ntext\\nk expose deployment first-app --type=LoadBalancer --port=8080\\n\\nhere type can be of:\\n- ClusterIP Only reachable inside the cluster\\n- NodePort Create an IP address for a specific worker node\\n- LoadBalancer Create an IP address to a loadbalancer that route the traffics to the deployment evenly\\n\\n##### List all services\\ntext\\nk get services\\n\\nexample: \\ntext\\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE\\nfirst-app LoadBalancer 10.107.213.78 <pending> 8080:31967/TCP 8s\\nkubernetes ClusterIP 10.96.0.1 <none> 443/TCP \\n\\n- If our cluster is provided by a cloud provider, then we will get an external-IP address.\\n\\n- In local environment, we can run minikube service first-app to let minikube map a local port to the service\\n\\n##### Scale to multiple pods\\ntext\\nk scale deployment/first-app --replicas=3\\n\\n##### Update the deployment\\n###### Update to new image\\nwe first build and pusb a new container to docker hub ***with new tag***.\\ntext\\nk set image deployment/first-app kub-first-app=machingclee/kub-first-app:2\\n\\n\\nHere kub-first-app is a running container inside our pod, the name can be found in the dashboard (recall we can use minikube dashboard):\\n\\n![](/assets/img/2024-05-04-01-58-38.png)\\n\\n###### Check update status\\ntext\\nk rollout status deployment/first-app\\n\\n\\n###### Undo the latest deployment\\ntext\\nk rollout undo deployment/first-app\\n\\n\\n###### View all the rollout histories\\ntext\\nk rollout history deployment/first-app\\n\\nWhich results in:\\ntext\\ndeployment.apps/first-app \\nREVISION CHANGE-CAUSE\\n2 <none>\\n3 <none\\n\\n###### Get the detail about the deployment\\ntext\\nk rollout history deployment/first-app --revision=2\\n\\nWhich results in:\\ntext\\ndeployment.apps/first-app with revision #2\\nPod Template:\\n Labels: app=first-app\\n pod-template-hash=7cd7d85cf5\\n Containers:\\n kub-first-app:\\n Image: machingclee/kub-first-app:2\\n Port: <none>\\n Host Port: <none>\\n Environment: <none>\\n Mounts: <none>\\n Volumes: <none>\\n Node-Selectors: <none>\\n Tolerations: <none>\\n\\n\\n###### Reset to specific deployment history\\ntext\\nk rollout undo deployment/first-app --to-revision=1\\n\\n##### Delete a Service\\ntext\\nk delete service first-app\\n\\n\\n##### Appendix: Make a new reference by a new tag to an existing image\\nThis is to push to docker hub.\\ntext\\ndocker tag kub-first-app machingclee/kub-first-app\\n\\n \\n\\n#### Kubectl: Declarative Approach\\n\\n##### deployment.yml\\nyml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n name: second-app-deployment\\nspec: # spec of deployment,\\n replicas: 1\\n selector:\\n matchLabels:\\n app: second-app\\n tier: backend\\n template: # in what follows k8s always expect a pod template spec\\n # <--- no need to add \\"kind: Pod\\" here.\\n metadata:\\n labels: # deplyment.yml is more strict, all labels must match in order to deploy a pod\\n app: second-app\\n tier: backend\\n spec: # sepc of pod\\n containers:\\n - name: second-nodejs-app\\n image: machingclee/kub-first-app\\n\\n\\n##### service.yml\\n\\nyml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n name: backend\\nspec:\\n selector:\\n # service is less strict,\\n # \\"app: second-app\\" will match all selectors that use this key-value\\n app: second-app\\n ports:\\n - protocol: \\"TCP\\"\\n port: 80\\n targetPort: 8080\\n type: LoadBalancer\\n\\n\\n##### Apply a service.yml and a deployment.yml\\ntext\\nk apply -f service.yml\\nk apply -f deployment.yml\\n\\n\\n##### Connect to the Service just Created\\nWe run (backend is the service name we just defined)\\ntext\\nminikube service backend\\n\\nbecause we don\'t have cloud provider for k8s for the moment.\\n\\n##### Delete Resources\\ntext\\nk delete -f deployment.yml -f service.yml\\n\\n\\n##### Delete Everything\\n\\ntext\\nk delete all --all\\n\\n\\n##### Merge service.yml and deployment.yml into one\\nyml\\napiVersion: v1\\nkind: Service\\nmetadata:\\n name: backend\\nspec:\\n selector:\\n # service is less strict,\\n # \\"app: second-app\\" will match all selectors that use this key-value\\n app: second-app\\n ports:\\n - protocol: \\"TCP\\"\\n port: 80\\n targetPort: 8080\\n type: LoadBalancer\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n name: second-app-deployment\\nspec: # spec of deployment,\\n replicas: 1\\n selector:\\n matchLabels:\\n app: second-app\\n tier: backend\\n template: # in what follows k8s always expect a pod template spec\\n # <--- no need to add \\"kind: Pod\\" here.\\n metadata:\\n labels: # deplyment.yml is more strict, all labels must match in order to deploy a pod\\n app: second-app\\n tier: backend\\n spec: # sepc of pod\\n containers:\\n - name: second-nodejs-app\\n image: machingclee/kub-first-app\\n\\n\\nThe key difference is that we need to separate them by ---.\\n\\nNote that the service object ***is a living organism*** in the cluster, it will monitor all the changes, therefore it is better to create a service object first, then apply deployment objects.\\n\\nNow we can deploy by k apply -f merged-file.yml.\\n\\n#### Volume\\n\\n##### emptyDir \\n\\nemptyDir is a pod specific volume. \\n\\n\\nyml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n name: story-deployment\\nspec:\\n replicas: 1\\n selector:\\n matchLabels:\\n app: story\\n template:\\n metadata:\\n labels:\\n app: story\\n spec:\\n containers:\\n - name: story\\n image: machingclee/kub-data-demo:2\\n imagePullPolicy: Always\\n volumeMounts:\\n - mountPath: /app/story\\n name: story-volume\\n volumes:\\n - name: story-volume\\n emptyDir: {} # <-- pod specific\\n\\n- When ***a*** container get restarted, the data in app/story inside container will be persisted as long as the pod is not removed.\\n\\n- This moethod fails when replicas: 2 because emptyDir is bound to a pod and cannot be shared among pods.\\n\\n##### hostPath\\n\\nVolume of type hostPath is bound to a node, with type DirectoryOrCreate the directly data will be created if there is no existing one.\\n\\nyml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n name: story-deployment\\nspec:\\n replicas: 2\\n selector:\\n matchLabels:\\n app: story\\n template:\\n metadata:\\n labels:\\n app: story\\n spec:\\n containers:\\n - name: story\\n image: machingclee/kub-data-demo:2\\n imagePullPolicy: Always\\n volumeMounts:\\n - mountPath: /app/story\\n name: story-volume\\n volumes:\\n - name: story-volume\\n hostPath: # <-- solve the problems in one worker-node env\\n path: /data\\n type: DirectoryOrCreate\\n\\n\\n##### PersistentVolume\\n###### host-pv.yml\\n\\nLet\'s define a persist-volume resource:\\n\\nyml\\napiVersion: v1\\nkind: PersistentVolume\\nmetadata:\\n name: host-pv\\nspec:\\n capacity:\\n storage: 1Gi\\n volumeMode: Filesystem # or Block\\n storageClassName: standard\\n accessModes:\\n - ReadWriteOnce # all pods inside a worker node get access and create once\\n hostPath:\\n path: /data\\n type: DirectoryOrCreate\\n\\n###### host-pvc.yml\\nNext we need to define persistent-volume-claim resource:\\nyml\\napiVersion: v1\\nkind: PersistentVolumeClaim\\nmetadata:\\n name: host-pvc\\nspec:\\n volumeName: host-pv\\n accessModes:\\n - ReadWriteOnce # this allow any one of the pods in a node to declare the volume once\\n resources:\\n requests:\\n storage: 1Gi\\n\\n\\n###### Modify deployment.yml\\nNow we apply the persistent-volume-claim: \\n\\nyml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n name: story-deployment\\nspec:\\n replicas: 2\\n selector:\\n matchLabels:\\n app: story\\n template:\\n metadata:\\n labels:\\n app: story\\n spec:\\n containers:\\n - name: story\\n image: machingclee/kub-data-demo:2\\n imagePullPolicy: Always\\n volumes:\\n - name: story-volume\\n persistentVolumeClaim:\\n claimName: host-pvc\\n\\n\\n#### Environment Variable\\n\\n##### ConfigMap\\n\\nHere to apply the environment variable from external file we need to create a config map resource:\\n\\nyml\\n# environment.yml\\n\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n name: data-store-env\\ndata:\\n STORY_FOLDER: \\"story\\"\\n ENV: \\"dev123\\"\\n\\n\\n##### Apply the config map\\n\\nLet\'s update the deployment.yml\\nyml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n name: story-deployment\\nspec:\\n replicas: 2\\n selector:\\n matchLabels:\\n app: story\\n template:\\n metadata:\\n labels:\\n app: story\\n spec:\\n containers:\\n - name: story\\n image: machingclee/kub-data-demo:2\\n imagePullPolicy: Always\\n env:\\n - name: STORY_FOLDER # method 1\\n value: \\"story\\"\\n - name: STORY_FOLDER_2 # method 2\\n valueFrom:\\n configMapKeyRef:\\n name: data-store-env\\n key: STORY_FOLDER\\n - name: ENV\\n valueFrom:\\n configMapKeyRef:\\n name: data-store-env\\n key: ENV\\n volumeMounts:\\n - mountPath: /app/story\\n name: story-volume\\n volumes:\\n - name: story-volume\\n persistentVolumeClaim:\\n claimName: host-pvc\\n\\n\\nand apply the ConfigMap:\\n\\ntext\\nk apply -f environment.yml\\n\\nFor the new environment variables to take effect, we need to restart all pods:\\ntext\\nk rollout restart deployment story-deployment\\n\\n\\n","title":"Fundamentals of Kubernetes","date":"2024-05-04T00:00:00.000Z","id":"blog0259","tag":"k8s","intro":"We study the basic objects in the world of k8s","toc":true},{"content":"\\n<style>\\n img {\\n max-width: 660px;\\n }\\n</style>\\n\\n#### Preface\\n\\nRecently I have developed a mobile application for the company I am working at. I am grateful to have an oppurtunity and precious experience to design the whole system from zero.\\n\\n<a href=\\"/assets/img/2024-05-04-20-39-27.png\\">![](/assets/img/2024-05-04-20-39-27.png)</a>\\n\\n#### What I failed\\n- **Sparsity of Table.** Some data looks identitcal, I have therefore tried to mix two interfaces into one table, for example:\\n\\n <a href=\\"/assets/img/2024-05-04-18-42-15.png\\"><img src=\\"/assets/img/2024-05-04-18-42-15.png\\" width=\\"180\\"/></a>\\n\\n where the type of this record depends on SessionType, which determines whether we use isDraftInstantIssue or isDraftReply. It causes some of the column being always null (i.e., ***sparsity***), and it causes confusion when do we use these booleans.\\n \\n From this lesson I should have made additional two tables named \\n - InstantIssueDetail and\\n - ReplyDetail, \\n \\n and link these two tables to the original table via \\n - InstantIssueDetail.session_id and \\n - ReplyDetail.session_id \\n respectively. The new tables group all the type-specific data.\\n\\n- Similar to the above case. A message can be of type Image, Voice, Text, and forturnately I can avoid the sparsity timely:\\n\\n <a href=\\"/assets/img/2024-05-04-20-44-21.png\\"><img src=\\"/assets/img/2024-05-04-20-44-21.png\\" width=\\"400\\"/></a>\\n\\n\\n- **Didn\'t Stop Bad Design at the Beginning (Data Duplication).** In the past we had a tech lead designing a UserRegistration table (where User table has been made already) which serves as an intermediate table before User.\\n\\n However ***90% of data are the same***. This results in unpredictable extra amount of works as every time we deal with registration we need to look at two tables. \\n\\n If we want to select data, we need to case, when, then, else, end many times.\\n\\n#### What I succeeded\\nResearched on various ORMs, youtube videos on those frameworks, pros and cons, combining the experience with sql tutorial from other languages, eventually \\n\\n- Adopted **Prisma** as a table migration tool in light of Flyway in spring boot and Goose in golang.\\n\\n- Used **Kysely** as a ***type-safe*** query builder.\\n\\n- Standardized the approach to ***version*** all the schema changes in database, and reproduce all the changes to different environments.\\n\\nOur product works well with the database and developers can easily write their query with good semantic meaning, for example: \\n\\n<a href=\\"/assets/img/2024-05-04-20-28-16.png\\">![](/assets/img/2024-05-04-20-28-16.png)</a>\\n\\nwhich an ORM cannot provide. Note that a table carries different meaning in differnent context!\\n\\n","title":"Self-Reflection on Database Schema Design","date":"2024-05-02T00:00:00.000Z","id":"blog0260","tag":"sql","intro":"Record some mistake that can be avoided when designing a database schema.","toc":true},{"content":"\\nAs a review of statistics I try to work on this problem:\\n\\n> **Problem.** Let $(X,Y,Z)$ be a 3-dimensional random variable which follows the distribution (i.e., the probability density function is)\\n>\\n> $$\\n> f(x,y,z) = \\\\begin{cases}\\n> \\\\displaystyle\\\\frac{24}{(1+x+y+z)^5}, & x>0,y>0,z>0,\\\\\\\\\\n> 0, &\\\\text{otherwise.}\\n> \\\\end{cases}\\n> $$\\n>\\n> Find the probability density function of $W:=X+Y+Z$.\\n\\nI start off by guessing the answer to be $24/(1+w)^5$ very sloppily, which is of course wrong and I tried to figure out how I can relate $W$ with $f(x,y,z)$.\\n\\n<details>\\n<summary> Solution </summary>\\n\\n---\\n\\nLet us start from the definition, what does $f$ tell us? In view of a distribution it tells us how are $(X,Y,Z)$\'s spreaded in $\\\\mathbb R^3$. In other words, for every given $A\\\\subseteq \\\\mathbb R^3$, we have\\n\\n$$\\n\\\\mathbb P\\\\big((X,Y,Z)\\\\in A\\\\big) = \\\\int_A f(x)\\\\,dV(x)\\n$$\\n\\nwhich is the proportion of $(X,Y,Z)$\'s lying within $A$ and $dV$ denotes the Lebesgue measure on $\\\\mathbb R^3$. From that recall also that to find the probability density function $p_W$ of $W$, it is sufficient to find its cummulative distribution $\\\\int_0^t p_W(x)\\\\,dx$ (since then we can differentiate pointwise).\\n\\nFrom this, consider the relation $W\\\\leq t$, which is\\n\\n$$\\n\\\\begin{aligned}\\nW\\\\leq t\\n&\\\\iff X+Y+Z\\\\leq t,X,Y,Z>0\\\\\\\\\\n&\\\\iff 0< Z\\\\leq t-X-Y, 0< t-X-Y, X,Y>0\\\\\\\\\\n&\\\\iff 0< Z\\\\leq T-X-Y, 0<Y < t-X, 0<t-X, X>0\\\\\\\\\\n&\\\\iff 0< Z\\\\leq T-X-Y, 0<Y < t-X, 0< X<t,\\n\\\\end{aligned}\\n$$\\n\\nwe conclude that $w=x+y+z\\\\leq t$ ($x,y,z>0$) if and only if $(x,y,z)$ lies in the set\\n\\n$$\\nA_t := \\\\{(x,y,z): x\\\\in (0,t), y\\\\in (0, t-x), z\\\\in (t-x-y)\\\\},\\n$$\\n\\ntherefore\\n\\n$$\\n\\\\begin{aligned}\\n\\\\int_0^tp_W(x)\\\\,dx\\n&=\\\\mathbb P(W\\\\leq t)\\\\\\\\\\n&=\\\\mathbb P\\\\big((X,Y,Z)\\\\in A_t\\\\big)\\\\\\\\\\n&=\\\\int_{A_t}f(u)\\\\,dV(u)\\\\\\\\\\n&=\\\\int_0^t \\\\int_0^{t-x}\\\\int_0^{t-x-y} f(x,y,z) \\\\,dzdydx.\\n\\\\end{aligned}\\n$$\\n\\nThe answer is\\n\\n$$\\np_W(t)=\\\\frac{d}{dt}\\\\left(\\\\int_0^t\\\\int_0^{t-x}\\\\int_0^{t-x-y} \\\\frac{24}{(1+x+y+z)^5}\\\\,dzdydx\\\\right).\\n$$\\n\\n$\\\\qed$\\n\\n</details>\\n","title":"Computational Example on Probability Distribution","date":"2021-08-19T00:00:00.000Z","id":"math001","tag":"math","intro":"Given $(X,Y,Z)$ and its (joint) distribution on $\\\\mathbb R^3$, we try to find the distribution of $W=X+Y+Z$ on $\\\\mathbb R$.","toc":false},{"content":"\\nFollow the idea of user04651 in <a href=\\"https://math.stackexchange.com/questions/2287213/closure-of-interior-of-closed-convex-set\\">this post</a> to prove this statement.\\n\\n> **Lemma.** Let $X$ be a normed space, $U\\\\subseteq X$ open and $x_0\\\\in U$. If $x\\\\not\\\\in U$, then the line segment\\n>\\n> $$\\n> [x_0,x)\\\\subseteq (\\\\mathrm{conv}(U\\\\cup \\\\{x\\\\}))^\\\\circ .\\n> $$\\n\\n<proof>\\n\\n**_Proof._** For every $x_1\\\\in [x_0,x)$, there is a $\\\\delta>1$ such that $x = x_0 + \\\\delta(x_1-x_0)$. Let $\\\\rho>0$ be such that $B(x_0,\\\\rho)\\\\subseteq U$.\\n\\nNow for every $x\' \\\\in B(x_1, (1-\\\\frac{1}{\\\\delta})\\\\rho)$ (we have discussed the intuition of this radius in <a href=\\"/blog/Convex-Analysis-More-on-Convex-Functions-and-Characterize-Convex-lsc-Functions-by-Biconjugate-Functionals\\">this post</a>), we have for some $z\\\\in B(0,\\\\rho)$,\\n\\n$$\\n\\\\begin{aligned}\\nx\'\\n&= x_1 + \\\\left(1-\\\\frac{1}{\\\\delta}\\\\right)z \\\\\\\\\\n&= \\\\frac{1}{\\\\delta}(x_0+\\\\delta(x_1-x_0)) + \\\\left(1-\\\\frac{1}{\\\\delta}\\\\right)(x_0+z).\\\\\\\\\\n&= \\\\frac{1}{\\\\delta} x + \\\\left(1-\\\\frac{1}{\\\\delta}\\\\right)\\\\underbrace{(x_0+z)}_{\\\\in U}\\\\\\\\\\n&\\\\in \\\\mathrm{conv}(U\\\\cup \\\\{x\\\\}).\\n\\\\end{aligned}\\n$$\\n\\nAs this is true for every $x\' \\\\in B(x_1, (1-\\\\frac{1}{\\\\delta})\\\\rho)$, we conclude $x_1\\\\in (\\\\mathrm{conv}(U\\\\cup \\\\{x\\\\}))^\\\\circ$.\\n\\n</proof>\\n\\n> **Proposition.** Let $X$ be a normed space and $C\\\\subseteq X$ a convex set, then $\\\\overline{C} = \\\\overline{C^\\\\circ}$.\\n\\n<proof>\\n\\n**_Proof._** The direction $\\\\overline{C^\\\\circ}\\\\subseteq \\\\overline C$ is clear.\\n\\nLet $x\\\\in \\\\overline C$, for the sake of contradiction suppose $x\\\\not\\\\in \\\\overline{C^\\\\circ}$, then there is an open neighborhood $V$ of $x$ such that $V\\\\cap C^\\\\circ=\\\\emptyset$. Since $x\\\\in \\\\overline C$, there is an $x\'\\\\in C$ such that $x\'\\\\in V$.\\n\\nNow $x\'$ being an element in $V$, $x\'\\\\not\\\\in C^\\\\circ$, fix an $x_0\\\\in C^\\\\circ$, by the lemma above the segment\\n\\n$$\\n[x_0,x\')\\\\subseteq (\\\\mathrm{conv}(C^\\\\circ\\\\cup \\\\underbrace{\\\\{x\'\\\\}}_{\\\\subseteq C}))^\\\\circ\\\\subseteq (\\\\mathrm{conv}(C))^\\\\circ = C^\\\\circ.\\n$$\\n\\nWe can pick an $x\'\'\\\\in [x_0,x\')$ that is close enough to $x\'$, so that $x\'\'\\\\in V$, but then $x\'\'\\\\in V\\\\cap C^\\\\circ$, a contradiction.\\n\\n</proof>\\n","title":"The Set Equality $\\\\overline{C} = \\\\overline{C^\\\\circ}$ for Convex Set $C$","date":"2021-08-20T00:00:00.000Z","id":"math002","tag":"math","intro":"Record a proof to this result in convex analysis.","toc":false},{"content":"\\nWhen taking elevator one is possible to calculate the probability of being the first one to leave the lift.\\n\\nIn my situation, assume that:\\n\\n- An elevator can just stop from floor $22$ to floor $40$.\\n- Infinite capacity.\\n- We start from ground floor.\\n\\nGiven that I will leave at floor $n$, where $22\\\\leq n < 40$, then the average probability of taking the first leave at floor $n$ is $\\\\displaystyle \\\\frac{1}{n-21}.$\\n\\n**Reason.** Denote $E_k$, $k\\\\ge 2$, the event that floor $n$ is the first one to stop with $k$ buttons on the elevator control panel being pressed.\\n\\nWe discard the case $k=1$ as $\\\\mathbb P(E_1)=1$ is too big that pollutes our average value.\\n\\nWe have (for $k\\\\ge 2$)\\n\\n$$\\n\\\\mathbb P(E_k) =\\\\displaystyle \\\\binom{40-n}{k-1} \\\\bigg/\\\\binom{18}{k-1}.\\n$$\\n\\n- Note that we are just considering choosing $k-1$ buttons because floor $n$ being the first to stop has conditioned our sample space to combintations of selected buttons that contain $n$.\\n\\n- Here the numerator $\\\\binom{40-n}{k-1}$ represents the combinations of buttons that is bigger than $n$.\\n\\n- The demoninator $\\\\binom{18}{k-1}$ represents the available combinations of buttons apart from $n$.\\n\\nNote that the edge case of $k$ is $k=2$ and $k= |\\\\mathbb N \\\\cap [n, 40]|=40-n +1$. Our average probability will be\\n\\n$$\\n\\\\frac{1}{\\\\underbrace{40-n+1-2+1}_{\\\\text{number of summands}}}\\n\\\\sum_{k=2}^{40-n+1} \\\\mathbb P(E_k) = \\\\frac{1}{40-n}\\n\\\\sum_{k=2}^{41-n} \\\\frac{\\\\displaystyle \\\\binom{40-n}{k-1}}{\\\\displaystyle \\\\binom{18}{k-1}} \\\\xlongequal{(*)}\\\\frac{1}{n-21}.\\n$$\\n\\nHere $(*)$ follows from the following lemma:\\n\\n> **Lemma.** Let $X, Y$ be positive integers and $X\\\\leq Y$, then\\n>\\n> $$\\n> \\\\sum_{k=1}^X \\\\frac{\\\\binom{X}{k}}{\\\\binom{Y}{k}} = \\\\frac{X}{Y-X+1}.\\n> $$\\n\\n<details>\\n<summary> <strong>Proof.</strong> [In case you want to try, don\'t unfold it]</summary>\\n\\n---\\n\\nDenote $A_k =\\\\binom{X}{k} / \\\\binom{Y}{k}$ and $S = \\\\sum_{k=1}^X A_k$. We note that\\n\\n$$\\n\\\\begin{aligned}\\nA_k& = \\\\frac{\\\\binom{X}{k}}{\\\\binom{Y}{k}}\\\\\\\\\\n&= \\\\frac{\\\\binom{X+1}{k+1} - \\\\binom{X}{k+1}}{\\\\binom{Y}{k}} \\\\\\\\\\n&= \\\\frac{\\\\frac{X+1}{k+1}\\\\binom{X}{k}}{\\\\binom{Y}{k}} - \\\\frac{\\\\binom{X}{k+1}}{\\\\binom{Y}{k}} \\\\\\\\\\n&= \\\\frac{\\\\frac{X+1}{k+1}\\\\binom{X}{k}}{\\\\binom{Y}{k}} - \\\\frac{\\\\binom{X}{k+1}}{\\\\frac{k+1}{Y-k}\\\\binom{Y}{k+1}} \\\\\\\\\\n&= \\\\frac{X+1}{k+1} A_k - \\\\frac{Y-k}{k+1}A_{k+1}.\\n\\\\end{aligned}\\\\\\\\\\n$$\\n\\nWe rearrange to conclude\\n\\n$$\\n\\\\begin{aligned}\\n(Y+1)A_{k+1}-XA_k &= (k+1)A_{k+1} - kA_k\\\\\\\\\\n\\\\sum_{k=1}^{X-1} \\\\big((Y+1)A_{k+1}-XA_k \\\\big) &= \\\\sum_{k=1}^{X-1}\\\\big((k+1)A_{k+1} - kA_k\\\\big)\\\\\\\\\\n(Y+1) (S-A_1) - X(S-A_X)&=XA_X - A_1\\\\\\\\\\n(Y-X+1)S &= YA_1\\\\\\\\\\nS &= \\\\frac{Y}{Y-X+1}A_1\\\\\\\\\\n&=\\\\frac{Y}{Y-X+1} \\\\cdot \\\\frac{X}{Y} \\\\\\\\\\n&= \\\\frac{X}{Y-X+1},\\n\\\\end{aligned}\\n$$\\n\\nas desired. $\\\\qquad \\\\blacksquare$\\n\\n</details>\\n\\nI am at floor $n=34$, therefore on average I just have\\n\\n$$\\n\\\\frac{1}{34-21} = \\\\frac{1}{13} \\\\approx 7.69\\\\%\\n$$\\n\\nchance of being the first one to leave if there are at least 2 buttons pressed in control panel.\\n","title":"First to Leave Problem of Elevator","date":"2021-10-01T00:00:00.000Z","id":"math003","tag":"math","intro":"Simple problem inspired by my life.","toc":false},{"content":"\\n> **Problem.** Given a function $f:\\\\mathbb R\\\\to \\\\mathbb R$, show that the following set $E$ is at most countable:\\n>\\n> $$\\n> E=\\\\left\\\\{ a\\\\in\\\\mathbb R: \\\\lim_{x\\\\to a}f(x) \\\\text{ exists and } \\\\lim_{x\\\\to a}f(x)\\\\neq f(a) \\\\right\\\\}\\n> $$\\n\\n<proof>\\n\\n**_My Solution._** Denote\\n\\n$$\\n\\\\begin{align*}\\nE&=\\\\left\\\\{a\\\\in \\\\mathbb R :\\\\lim_{x\\\\to a}f(x)\\\\text{ exists and }\\\\lim_{x\\\\to a}|f(x)-f(a)|>0\\\\right\\\\}\\\\\\\\\\n&= \\\\bigcup_{k\\\\ge 1}\\\\underbrace{\\\\left\\\\{a\\\\in \\\\mathbb R :\\\\lim_{x\\\\to a}f(x)\\\\text{ exists and }\\\\lim_{x\\\\to a}|f(x)-f(a)|>\\\\frac{1}{k}\\\\right\\\\}}_{=:E_k},\\n\\\\end{align*}\\n$$\\n\\nthen it remains to show that each of $E_k$\'s is countable.\\n\\nWe prove this by showing that each $a\\\\in E_k$ is in fact isolated by an open interval (and hence each point can be identified with a rational number).\\n\\n> **Fact.** For every $k\\\\in \\\\mathbb N$, each of points in $E_k$ is isolated.\\n\\nBy **_isolated_** we mean for every $a\\\\in E_k$, there is a $\\\\delta>0$ such that\\n\\n$$\\n\\\\big((a-\\\\delta, a+\\\\delta)\\\\setminus \\\\{a\\\\}\\\\big) \\\\cap E_k = \\\\emptyset.\\n$$\\n\\nIn other words, the derived set of $E_k$, $E_k\'$, satisfies $E_k\'\\\\cap E_k=\\\\emptyset$.\\n\\n**_Proof._** Suppose not, i.e., $a\\\\in E_k$ and $a$ can\'t be isolated, i.e., $a\\\\in E_k\'$. Then for every $n\\\\ge 1$, we can find an $a_n\\\\in B(a, \\\\frac{1}{n})\\\\setminus\\\\{a\\\\}$, such that $a_n\\\\in E_k$.\\n\\nBut $a_n\\\\in E_k$ means that $\\\\lim_{x\\\\to a_n}f(x)$ exists with $\\\\lim_{x\\\\to a_n}|f(x)-f(a_n)|>1/k$, there will be a $b_n\\\\in B(a_n,1/n)$ with\\n\\n$$\\n|f(b_n)-f(a_n)| >\\\\frac{1}{k}.\\n$$\\n\\nSince both $a_n, b_n\\\\to a\\\\in E_k$, as $\\\\lim_{x\\\\to a}f(x)$ exists, we have $0\\\\ge \\\\frac{1}{k}$, which is absurd.\\n\\n</proof>\\n\\n#### Reference\\n\\n- <a>https://www.facebook.com/photo?fbid=10223732659676781&set=gm.10159856304916489</a>\\n","title":"Countability of a kind of Discontinuity","date":"2022-04-04T00:00:00.000Z","id":"math004","tag":"math","intro":"Record a mathematical problem from facebook.","toc":false}]')},,,,,,,,,,,,,,,,,,function(n,e,t){},function(n,e,t){},function(n,e,t){},,function(n,e,t){},,,,,function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},,function(n,e,t){},function(n,e,t){},function(n,e,t){},,,function(n,e){},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,function(n,e,t){},function(n,e,t){var a={"./001-tkinter.md":765,"./002-wpf.md":766,"./003-react-native.md":767,"./004-emotion_vgg.md":768,"./005-yolo-custom-training.md":769,"./006-qt-project-starter.md":770,"./007-electron-next.md":771,"./008-defectGAN.md":772,"./009-text-detection-and-erase.md":773,"./010-classifier-by-means-of-detection.md":774,"./011-translate-custom-model.md":775,"./012-libtorch_deployment.md":776,"./013-timesheet.md":777,"./014-mobileapp.md":778};function i(n){var e=o(n);return t(e)}function o(n){if(!t.o(a,n)){var e=new Error("Cannot find module '"+n+"'");throw e.code="MODULE_NOT_FOUND",e}return a[n]}i.keys=function(){return Object.keys(a)},i.resolve=o,n.exports=i,i.id=764},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/001-tkinter.9c673438.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/002-wpf.7d39e7bc.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/003-react-native.92650200.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/004-emotion_vgg.ecc2fc86.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/005-yolo-custom-training.7fee5f8f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/006-qt-project-starter.8d76ecd9.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/007-electron-next.683dd32b.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/008-defectGAN.0aa0eb8c.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/009-text-detection-and-erase.61fd3e02.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/010-classifier-by-means-of-detection.23223aca.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/011-translate-custom-model.e959957c.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/012-libtorch_deployment.6a362faa.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/013-timesheet.59a3dedb.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/014-mobileapp.ce052003.md"},function(n,e,t){},function(n,e,t){},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){},function(n,e,t){var a={"./math/001-distrbution-example.md":851,"./math/002-closure-of-interior.md":852,"./math/003-lift-problem.md":853,"./math/004-countability-discontinuity.md":854,"./old-math/001-Study-Notes-on-Distribution/Study-Notes-on-Distribution.md":855,"./old-math/004-Review-of-Basic-Statistics/Review-of-Basic-Statistics.md":856,"./old-math/012-modulus-of-continuity/modulus-of-continuity.md":857,"./old-math/013-convex-analaysis/convex-analysis.md":858,"./old-math/014-subgradients/subgradients.md":859,"./tech/002-Lazy-React-Router.md":860,"./tech/003-On-Looping-all-Files-in-Frontend.md":861,"./tech/005-conda-useful-commands.md":862,"./tech/007-redux-saga.md":863,"./tech/009-algorithms.md":864,"./tech/010-algo-exercises.md":865,"./tech/011-typescript-typetrick.md":866,"./tech/015-disquz.md":867,"./tech/016-spa-seo.md":868,"./tech/017-pytorch-study-notes.md":869,"./tech/018-tensorflow-revisit.md":870,"./tech/019-colab.md":871,"./tech/020-react-icons.md":872,"./tech/021-callbacks.md":873,"./tech/022-transferred-learning.md":874,"./tech/023-dataset-pipeline-with-augmentation.md":875,"./tech/024-python-scrapping.md":876,"./tech/025-cpp_01.md":877,"./tech/026-cpp_02.md":878,"./tech/027-cpp_03.md":879,"./tech/028-texcommand.md":880,"./tech/029-live2d.md":881,"./tech/030-chrome_automation.md":882,"./tech/031-automation_winapp.md":883,"./tech/032-python-import.md":884,"./tech/033-automation_winapp.md":885,"./tech/034-debugger.md":886,"./tech/035-decompose-a-class.md":887,"./tech/036-threading.md":888,"./tech/037-datetime.md":889,"./tech/038-debug.md":890,"./tech/039-nextjs-debug.md":891,"./tech/040-nextjs-electron.md":892,"./tech/041-child-process.md":893,"./tech/042-img-compression.md":894,"./tech/043-python-generator.md":895,"./tech/044-dc-gan-in-python.md":896,"./tech/045-mongo.md":897,"./tech/046-custom-logger.md":898,"./tech/047-configure-s3-send-sqs.md":899,"./tech/048-makeStyles-in-next-js.md":900,"./tech/049-mAP.md":901,"./tech/050-tensorflow-shapes.md":902,"./tech/051-desc-dot-ts.md":903,"./tech/052-excels-in-python.md":904,"./tech/053-cycle-gan-tf.md":905,"./tech/054-mongo-engine-py.md":906,"./tech/055-scroll.md":907,"./tech/056-color-gan.md":908,"./tech/057-begin-rust.md":909,"./tech/058-json-in-python.md":910,"./tech/059-ts-debug-config.md":911,"./tech/060-ssh-download-file.md":912,"./tech/061-rust-beginner-summary.md":913,"./tech/062-DCT.md":914,"./tech/063-fadein.md":915,"./tech/064-residue-block.md":916,"./tech/065-pandoc.md":917,"./tech/065-yolo-v3-deep-dive.md":918,"./tech/066-WGAN.md":919,"./tech/068-coco.md":920,"./tech/069-PCA.md":921,"./tech/070-SVD.md":922,"./tech/072-DefectGAN.md":923,"./tech/073-install-tensorflow-gpu.md":924,"./tech/075-shell-script.md":925,"./tech/076-pytorch-summary.md":926,"./tech/077-continuous-logging.md":927,"./tech/079progress-bar.md":928,"./tech/080-mask-in-numpy.md":929,"./tech/081-cv2-drawing.md":930,"./tech/082-albumentation.md":931,"./tech/084-faster-rcnn.md":932,"./tech/085-augmentation_in_obj_detection.md":933,"./tech/086-feature_extractor.md":934,"./tech/087-retrain-nan.md":935,"./tech/088-transformer.md":936,"./tech/089-transformer-2.md":937,"./tech/090-angular-trial.md":938,"./tech/091-ViT.md":939,"./tech/092-mesh-grid-trick.md":940,"./tech/093-mesh-grid-trick.md":941,"./tech/094-faster-rcnn-swin.md":942,"./tech/095-gradient-clipping.md":943,"./tech/097-SEAttention.md":944,"./tech/098-Cascade-RCNN.md":945,"./tech/099-CPP.md":946,"./tech/100-deformable-detr.md":947,"./tech/102-collate_fn.md":948,"./tech/103-albumentation-in-pytorch.md":949,"./tech/104-retina-face.md":950,"./tech/105-onnx-model.md":951,"./tech/106-config-organization-in-frontend.md":952,"./tech/107-redux-toolkit-quick-setup.md":953,"./tech/108-mui-latest-css.md":954,"./tech/109-visual-studio-setup.md":955,"./tech/110-copy-constructor.md":956,"./tech/111-precompiled-header.md":957,"./tech/112-DSR-ELE.md":958,"./tech/113-Array.md":959,"./tech/114-Iterator.md":960,"./tech/115-print.md":961,"./tech/116-TCPServer.md":962,"./tech/117-dark-matter.md":963,"./tech/118-CmakeFile.md":964,"./tech/119-SocketProgramming.md":965,"./tech/120-pytorchcpp.md":966,"./tech/121-cpp-configurtation.md":967,"./tech/122-cpp-utils.md":968,"./tech/123-com_two_threads.md":969,"./tech/124-diffusion_models.md":970,"./tech/125-opencvinstall.md":971,"./tech/126-cxxstandard.md":972,"./tech/127-precompiled-header.md":973,"./tech/128-libtorch-study.md":974,"./tech/129-std_variant.md":975,"./tech/130-url-scheme.md":976,"./tech/131-mongo-aggregation.md":977,"./tech/132-redux-thunk.md":978,"./tech/133-Review-FileUpload.md":979,"./tech/134-Data-Scrapping.md":980,"./tech/136-mui-css-animation.md":981,"./tech/137-jsonpipeline-in-java.md":982,"./tech/138-redux-slice-template.md":983,"./tech/139-lookup-from-array-of-objects.md":984,"./tech/141-two-kinds-of-lookup.md":985,"./tech/142-dayjs.md":986,"./tech/143-scrollbar.md":987,"./tech/144-radio-button-groups.md":988,"./tech/145-persist-data.md":989,"./tech/147-docker-study.md":990,"./tech/148-tsconfig.md":991,"./tech/149-s3-deployment-ci.md":992,"./tech/150-remote-debuggeri.md":993,"./tech/151-normalizr.md":994,"./tech/152-LazyLoading.md":995,"./tech/153-S3Deployment.md":996,"./tech/154-uwsgi-flask.md":997,"./tech/155-gmail-pubsub.md":998,"./tech/156-serverless.md":999,"./tech/157-SSE.md":1e3,"./tech/158-algolia.md":1001,"./tech/159-clickoutside.md":1002,"./tech/160-multiselections.md":1003,"./tech/161-third-party.md":1004,"./tech/162-snackbar-utils.md":1005,"./tech/163-path-params.md":1006,"./tech/164-google-login.md":1007,"./tech/165-forwardRef.md":1008,"./tech/166-Rerender-component.md":1009,"./tech/167-gov-analyst-programmer.md":1010,"./tech/168-boxshadow.md":1011,"./tech/169-ruststudynotes.md":1012,"./tech/170-scroll.md":1013,"./tech/171-download-files.md":1014,"./tech/172-restrict-CORS.md":1015,"./tech/173-website-route-schema.md":1016,"./tech/174-childcomponent.md":1017,"./tech/175-rust-operator-overloading.md":1018,"./tech/176-rust-fmt.md":1019,"./tech/177-env.md":1020,"./tech/178-socketio.md":1021,"./tech/179-jest.md":1022,"./tech/180-tokio-chat.md":1023,"./tech/181-scale-socketio.md":1024,"./tech/182-left-join-mongoose.md":1025,"./tech/183-voice-chat.md":1026,"./tech/184-mongo-auto-increment.md":1027,"./tech/185-reanimated.md":1028,"./tech/186-expo-cli.md":1029,"./tech/187-custom-toast.md":1030,"./tech/188-custom-bottom-sheet.md":1031,"./tech/189-aws-fargate.md":1032,"./tech/190-mobile.md":1033,"./tech/191-Audio-recording-in-react-native.md":1034,"./tech/192-pin-to-zoom-camera.md":1035,"./tech/193-terra-form-create.md":1036,"./tech/194-remote-states.md":1037,"./tech/195-conditions-and-loops.md":1038,"./tech/196-variables-and-remote-execution.md":1039,"./tech/197-terraform-modules.md":1040,"./tech/198-background-timer.md":1041,"./tech/199-excel-generation.md":1042,"./tech/200-bototm-sheet.md":1043,"./tech/201-SSE-revist.md":1044,"./tech/202-docker-compose-postgre.md":1045,"./tech/203-golang-sql.md":1046,"./tech/204-go-chi.md":1047,"./tech/205-pgsql.md":1048,"./tech/206-logger.md":1049,"./tech/207-toOid.md":1050,"./tech/208-docker-nodejs.md":1051,"./tech/210-sample-pangesture.md":1052,"./tech/211-transactio-in-plain-SQL.md":1053,"./tech/212-SQLStatements-Workflow.md":1054,"./tech/213-gmail.md":1055,"./tech/214-complete-go.md":1056,"./tech/215-docker-run-infinitiy.md":1057,"./tech/216-docker-weight.md":1058,"./tech/217-env-from-aws.md":1059,"./tech/218-record-migration-script.md":1060,"./tech/219-gatling.md":1061,"./tech/220-frontend-refresh-token.md":1062,"./tech/221-prisma-kysely.md":1063,"./tech/222-springboot.md":1064,"./tech/223-sql-command-migrate.md":1065,"./tech/224-jooq.md":1066,"./tech/225-tss.md":1067,"./tech/226-kysely_in_sql.md":1068,"./tech/227-dead-delayed-queues.md":1069,"./tech/230-paste-image.md":1070,"./tech/231-minimal-prisma copy.md":1071,"./tech/232-go-jet.md":1072,"./tech/233-github-actions.md":1073,"./tech/234-algolia-revisit.md":1074,"./tech/235-github-actinos-fargate copy.md":1075,"./tech/236-organize-js-rabbitmq.md":1076,"./tech/237-auto-hot-key-for-vscode.md":1077,"./tech/238-redis-code-organization.md":1078,"./tech/239-forcefully-migrate-prisma.md":1079,"./tech/240-ULID-in-pgsql.md":1080,"./tech/241-timestamp-in-pgsql.md":1081,"./tech/242-as-with-clause.md":1082,"./tech/243-push-notification.md":1083,"./tech/244-sql-function-to-generate-date.md":1084,"./tech/245-custom-node-script copy.md":1085,"./tech/246-in-app-notification.md":1086,"./tech/247-custom-hashmap.md":1087,"./tech/248-wait-group.md":1088,"./tech/249-mutex.md":1089,"./tech/250-Debezium.md":1090,"./tech/251-dev-container.md":1091,"./tech/252-AWS-practitioner-study-notes.md":1092,"./tech/253-Lambda-deep-dive.md":1093,"./tech/254-create-lambda-layer.md":1094,"./tech/255-aws-sdk.md":1095,"./tech/256-dynamodb-crud.md":1096,"./tech/257-eas-update copy.md":1097,"./tech/258-debezium.md":1098,"./tech/259-k8s.md":1099,"./tech/260-self-reflection-on-db.md":1100};function i(n){var e=o(n);return t(e)}function o(n){if(!t.o(a,n)){var e=new Error("Cannot find module '"+n+"'");throw e.code="MODULE_NOT_FOUND",e}return a[n]}i.keys=function(){return Object.keys(a)},i.resolve=o,n.exports=i,i.id=850},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/001-distrbution-example.e65e1f05.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/002-closure-of-interior.334dec1d.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/003-lift-problem.d8ee066a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/004-countability-discontinuity.540aae57.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/Study-Notes-on-Distribution.e8358576.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/Review-of-Basic-Statistics.98054008.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/modulus-of-continuity.abb86c66.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/convex-analysis.9ba537a9.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/subgradients.166f61de.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/002-Lazy-React-Router.08885309.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/003-On-Looping-all-Files-in-Frontend.a6a707f6.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/005-conda-useful-commands.355d3b7e.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/007-redux-saga.edeccaf3.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/009-algorithms.db490160.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/010-algo-exercises.ec198c55.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/011-typescript-typetrick.68b8d4fb.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/015-disquz.33075066.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/016-spa-seo.8de40b3b.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/017-pytorch-study-notes.85ad2f4d.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/018-tensorflow-revisit.f9ca5fee.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/019-colab.683a9c29.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/020-react-icons.cac92527.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/021-callbacks.305c4603.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/022-transferred-learning.d93b083b.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/023-dataset-pipeline-with-augmentation.6a4a47cf.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/024-python-scrapping.b7a66fc6.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/025-cpp_01.e59ffca4.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/026-cpp_02.1bd1487d.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/027-cpp_03.4bb4fb3c.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/028-texcommand.500f97fd.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/029-live2d.13970fb6.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/030-chrome_automation.812d6284.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/031-automation_winapp.3988b97f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/032-python-import.dd2fc549.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/033-automation_winapp.6244494b.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/034-debugger.1e4a266a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/035-decompose-a-class.487b13da.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/036-threading.0104e95a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/037-datetime.79186632.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/038-debug.a3f69dea.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/039-nextjs-debug.b68bed84.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/040-nextjs-electron.0b58a36a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/041-child-process.a919a6fa.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/042-img-compression.e75782c4.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/043-python-generator.4d0faac7.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/044-dc-gan-in-python.e2c44a38.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/045-mongo.bc91a64f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/046-custom-logger.a7f00153.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/047-configure-s3-send-sqs.d3974f2c.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/048-makeStyles-in-next-js.c715ff59.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/049-mAP.d6a2460f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/050-tensorflow-shapes.c833a850.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/051-desc-dot-ts.3046d9a8.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/052-excels-in-python.05b2734c.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/053-cycle-gan-tf.9e64c3ea.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/054-mongo-engine-py.833b1782.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/055-scroll.f8d1e1bc.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/056-color-gan.f894cbc3.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/057-begin-rust.279f5c43.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/058-json-in-python.f4b6af38.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/059-ts-debug-config.1cef68c3.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/060-ssh-download-file.9713c370.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/061-rust-beginner-summary.c01d3619.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/062-DCT.d88aa49e.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/063-fadein.c5f5a80f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/064-residue-block.b4c570e9.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/065-pandoc.65e59ae9.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/065-yolo-v3-deep-dive.51a0abb4.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/066-WGAN.9e4a12ea.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/068-coco.8f819ee0.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/069-PCA.661c5a30.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/070-SVD.20b4697a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/072-DefectGAN.19e3437a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/073-install-tensorflow-gpu.4b648761.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/075-shell-script.bbcb64fb.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/076-pytorch-summary.265f3f74.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/077-continuous-logging.a753ead8.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/079progress-bar.7f6c52c6.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/080-mask-in-numpy.227fc325.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/081-cv2-drawing.18fbd268.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/082-albumentation.fc45a174.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/084-faster-rcnn.b124b5e7.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/085-augmentation_in_obj_detection.4b0704c5.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/086-feature_extractor.a2a86d12.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/087-retrain-nan.2df6a9fd.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/088-transformer.9ca3c8f5.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/089-transformer-2.8cb2fb66.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/090-angular-trial.bfc47191.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/091-ViT.b0355ad3.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/092-mesh-grid-trick.e251b450.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/093-mesh-grid-trick.4eff09de.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/094-faster-rcnn-swin.d216c822.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/095-gradient-clipping.c3801dd6.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/097-SEAttention.5785e140.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/098-Cascade-RCNN.1325fb38.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/099-CPP.2fead5a0.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/100-deformable-detr.3ae0a06d.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/102-collate_fn.da48325d.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/103-albumentation-in-pytorch.474b39e7.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/104-retina-face.ba186d0d.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/105-onnx-model.4fab43c5.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/106-config-organization-in-frontend.ab473995.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/107-redux-toolkit-quick-setup.ffd792b6.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/108-mui-latest-css.d858eed7.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/109-visual-studio-setup.f479c724.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/110-copy-constructor.1f5cfd29.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/111-precompiled-header.a9a72c4b.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/112-DSR-ELE.902396f4.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/113-Array.ea42e061.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/114-Iterator.0c2f4b80.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/115-print.cd7fbc03.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/116-TCPServer.995308d5.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/117-dark-matter.8a437402.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/118-CmakeFile.2351f983.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/119-SocketProgramming.46988e6d.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/120-pytorchcpp.c9e979b4.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/121-cpp-configurtation.8e8b8a7a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/122-cpp-utils.0899069b.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/123-com_two_threads.3c34ecf2.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/124-diffusion_models.d8ba8eb4.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/125-opencvinstall.9e47e751.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/126-cxxstandard.f895846b.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/127-precompiled-header.e9d36764.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/128-libtorch-study.bde16a76.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/129-std_variant.1dddce95.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/130-url-scheme.365d4964.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/131-mongo-aggregation.671c9652.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/132-redux-thunk.9540bdac.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/133-Review-FileUpload.49aa1a12.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/134-Data-Scrapping.69ad618c.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/136-mui-css-animation.52541200.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/137-jsonpipeline-in-java.aad53006.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/138-redux-slice-template.9772cf46.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/139-lookup-from-array-of-objects.9a4c3358.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/141-two-kinds-of-lookup.72c71604.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/142-dayjs.d0f06511.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/143-scrollbar.f1d7012a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/144-radio-button-groups.001b374c.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/145-persist-data.ae0385d8.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/147-docker-study.b0cd8337.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/148-tsconfig.0791850a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/149-s3-deployment-ci.439f6062.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/150-remote-debuggeri.77c96726.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/151-normalizr.e31f0622.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/152-LazyLoading.d0ecbf79.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/153-S3Deployment.9993336f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/154-uwsgi-flask.324ff8e5.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/155-gmail-pubsub.29b4f035.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/156-serverless.5ca27c58.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/157-SSE.5d29d2f0.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/158-algolia.324bc5a1.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/159-clickoutside.0ef16d71.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/160-multiselections.bd84d998.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/161-third-party.cc036ea9.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/162-snackbar-utils.cf3408b5.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/163-path-params.e1d0addb.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/164-google-login.b3ed32aa.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/165-forwardRef.89b7c760.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/166-Rerender-component.6ccced95.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/167-gov-analyst-programmer.e70d405a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/168-boxshadow.b2e5c53d.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/169-ruststudynotes.5ff30337.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/170-scroll.3a3a58d9.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/171-download-files.9e2e719f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/172-restrict-CORS.1098f441.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/173-website-route-schema.cf8bba12.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/174-childcomponent.7408ecf5.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/175-rust-operator-overloading.88f16ca4.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/176-rust-fmt.377cc1fd.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/177-env.28b9e94e.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/178-socketio.fd666900.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/179-jest.e75b56bd.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/180-tokio-chat.58f3e868.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/181-scale-socketio.8e08e487.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/182-left-join-mongoose.ed3e62e5.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/183-voice-chat.35010b1e.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/184-mongo-auto-increment.808ec54c.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/185-reanimated.501f1ed7.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/186-expo-cli.2a583e1c.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/187-custom-toast.3461e32b.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/188-custom-bottom-sheet.e7585bb3.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/189-aws-fargate.07cac871.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/190-mobile.abc7af18.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/191-Audio-recording-in-react-native.321f3995.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/192-pin-to-zoom-camera.5af0e7fe.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/193-terra-form-create.59ed0c51.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/194-remote-states.388573c1.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/195-conditions-and-loops.5ee421d8.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/196-variables-and-remote-execution.cb564688.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/197-terraform-modules.b76a8d5d.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/198-background-timer.a196ea73.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/199-excel-generation.83de2134.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/200-bototm-sheet.fb21890f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/201-SSE-revist.b48192d3.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/202-docker-compose-postgre.6b834cfe.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/203-golang-sql.9642973f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/204-go-chi.684e9e63.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/205-pgsql.1e6950ae.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/206-logger.2ce77a88.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/207-toOid.aa6ce3bb.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/208-docker-nodejs.890e13f0.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/210-sample-pangesture.00fce445.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/211-transactio-in-plain-SQL.63273653.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/212-SQLStatements-Workflow.0e1c4c75.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/213-gmail.75ed5c15.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/214-complete-go.212b2b8c.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/215-docker-run-infinitiy.bb856219.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/216-docker-weight.ece106b6.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/217-env-from-aws.88db8c3f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/218-record-migration-script.0dc8f754.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/219-gatling.cb6a681a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/220-frontend-refresh-token.595fa10f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/221-prisma-kysely.af8ab496.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/222-springboot.d318f916.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/223-sql-command-migrate.0b929f91.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/224-jooq.d675a719.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/225-tss.ad830ef6.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/226-kysely_in_sql.41e3009f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/227-dead-delayed-queues.35cbc22e.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/230-paste-image.61118371.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/231-minimal-prisma copy.6553c65a.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/232-go-jet.71dda99f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/233-github-actions.f8960758.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/234-algolia-revisit.355d8892.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/235-github-actinos-fargate copy.9b1e20e2.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/236-organize-js-rabbitmq.9ff155a9.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/237-auto-hot-key-for-vscode.5d993067.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/238-redis-code-organization.6811f113.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/239-forcefully-migrate-prisma.337b08eb.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/240-ULID-in-pgsql.2e760b20.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/241-timestamp-in-pgsql.4681200f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/242-as-with-clause.cc0a46c8.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/243-push-notification.88fcc900.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/244-sql-function-to-generate-date.37d763e4.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/245-custom-node-script copy.12931400.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/246-in-app-notification.67c3a86e.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/247-custom-hashmap.5742cca9.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/248-wait-group.3a13e614.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/249-mutex.345052aa.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/250-Debezium.20b9a512.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/251-dev-container.de850948.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/252-AWS-practitioner-study-notes.d91c28d0.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/253-Lambda-deep-dive.57b60a85.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/254-create-lambda-layer.d39d997e.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/255-aws-sdk.46ad0e99.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/256-dynamodb-crud.c1e5cc97.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/257-eas-update copy.bbd0397e.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/258-debezium.b846682f.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/259-k8s.04f7221e.md"},function(n,e,t){"use strict";t.r(e),e.default=t.p+"static/media/260-self-reflection-on-db.ea587c32.md"},function(n,e,t){},,,function(n,e,t){"use strict";t.r(e);var a,i=t(2),o=t.n(i),s=t(13),r=t.n(s),c=(t(310),t(311),t(10)),l=(t(312),0),d=0,p=40,u=600,m=t(1168);!function(n){n.NONE="",n.ROOT="/",n.ABOUT="/",n.SKILLS="/skills",n.PORTFOLIO="/portfolio",n.ARTWORKS="/artworks",n.EXPERIENCE="/experience",n.MATH_MATERIAL="/math-material",n.CONTACT="/contact",n.BLOG="/blog",n.BLOG_WILDCARD="/blog/*",n.LANG_STUDY="/lang-study",n.LANG_STIDY_FF14="/lang-study-jp"}(a||(a={}));var h={ROOT:{path:a.ROOT,title:"About"},ABOUT:{path:a.ABOUT,title:"About"},SKILLS:{path:a.SKILLS,title:"Skills"},PORTFOLIO:{path:a.PORTFOLIO,title:"Portfolio"},ARTWORKS:{path:a.ARTWORKS,title:"Artworks"},EXPERIENCE:{path:a.EXPERIENCE,title:"Experience"},MATH_MATERIAL:{path:a.MATH_MATERIAL,title:"Math Material"},CONTACT:{path:a.CONTACT,title:"Contact"},BLOG:{path:a.BLOG,title:"Blog"},BLOG_WILDCARD:{path:a.BLOG_WILDCARD,title:"Blog"},LANG_STUDY:{path:a.LANG_STUDY,title:"Language Study"},LANG_STIDY_FF14:{path:a.LANG_STIDY_FF14,title:"Language Study (JP)"}},g=function(n){return"/blog/article/".concat(n)},f=h,_=t(9),b=t(31),y=t(1),w=["height","width","inline"],v=function(n){var e=n.height,t=void 0===e?20:e,a=n.width,i=void 0===a?5:a,o=n.inline,s=void 0!==o&&o,r=Object(b.a)(n,w);return Object(y.jsx)("div",Object(_.a)({style:{height:t,width:i,minHeight:t,userSelect:"none",display:s?"inline-block":"block"}},r))},x=t(70),k=t(11),S=t.n(k),T=(t(314),t(1169)),C=t(97),E=function(){return Object(C.b)()},A=C.c;function j(){return Object(y.jsx)("div",{className:"site-top-menu",style:{height:p},children:Object(y.jsx)(m.a,{children:Object(y.jsxs)("div",{className:"button-row",children:[Object(y.jsx)(R,{navRoute:f.ABOUT.path,title:"About Me"}),Object(y.jsx)(R,{navRoute:f.PORTFOLIO.path,title:"Portfolio"}),Object(y.jsx)(R,{navRoute:f.BLOG.path,title:"Blog"}),Object(y.jsx)(R,{navRoute:f.ARTWORKS.path,title:"Artworks"}),Object(y.jsx)(R,{navRoute:f.MATH_MATERIAL.path,title:"Math Material"})]})})})}var R=function(n){var e=n.navRoute,t=n.title,a=n.onClick,i=Object(c.f)(),o=A((function(n){return n.app.activeNavRoute}))===e?" active":"";return Object(y.jsx)("a",{href:e,onClick:function(n){if(n.preventDefault(),1===n.button||n.ctrlKey&&0===n.button)window.open(e,"_blank");else setTimeout((function(){i.push(e),a&&"function"==typeof a&&a()}),50)},children:Object(y.jsx)(T.a,{className:"menu-button"+o,children:Object(y.jsx)("div",{className:"title-container",children:t})})})},I=t(1150),O=Object(I.a)((function(n){return{higherContainer:{}}})),N=function(n){return/(\/blog\/)|(\/blog$)/g.test(n)},D=t(12),P=t(1152),L=t(1153),$=(t(319),t(1151)),M=(t(320),["show","className"]),F=function(n){var e=n.show,t=n.className,a=Object(b.a)(n,M),i=e?"":"collapsed";return Object(y.jsx)(T.a,Object(_.a)(Object(_.a)({className:S()(t,"custom-collapse-button",i)},a),{},{children:Object(y.jsx)($.a,{})}))},U=["children","title","innerRef","className","enableCollapse"],B=function(n){var e=n.children,t=n.title,a=void 0===t?"":t,o=n.innerRef,s=n.className,r=void 0===s?"":s,c=n.enableCollapse,l=void 0!==c&&c,d=Object(b.a)(n,U),p=Object(i.useState)(!1),u=Object(D.a)(p,2),m=u[0],h=u[1],g=Object(i.useState)(!0),f=Object(D.a)(g,2),w=f[0],v=f[1];Object(i.useEffect)((function(){h(!0)}),[]);var x=w?"":"message-collapsed";return Object(y.jsx)(P.a,{in:m,children:Object(y.jsxs)("div",Object(_.a)(Object(_.a)({className:S()("message-block-container",r,x),ref:o},d),{},{children:[a&&Object(y.jsxs)("div",{className:"title-row",children:[w&&Object(y.jsx)("div",{className:"title",children:a}),l&&Object(y.jsx)(F,{show:w,onClick:function(){v((function(n){return!n}))}})]}),Object(y.jsx)(L.a,{in:w,children:Object(y.jsx)("div",{style:{background:l?"white":"inherit"},children:Object(y.jsx)("div",{style:{display:"flex",width:"inherit !important"},children:Object(y.jsx)("div",{className:"box",style:{borderTopLeftRadius:a?0:10,borderTopRightRadius:a?0:10},children:Object(y.jsx)("div",{className:"message-block",children:e})})})})})]}))})},z=t(1154),q=t(1155),G=t(1158),W=t(1160),H=t(1161),V=t(1156),K=t(1159),X=t(1157),Y=(t(321),t(72)),Q="#E5E5E5",J={display:"inline-block",borderRadius:4};function Z(n){var e=n.skills,t=function(n){var e=n.style;return Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(v,{width:2,inline:!0})," ",Object(y.jsx)(Y.a,{size:7,style:Object(_.a)({color:Q},e)}),Object(y.jsx)(v,{width:5,inline:!0})]})};return Object(y.jsx)("div",{style:{marginTop:10,marginBottom:20,borderRadius:4,padding:20,paddingTop:10,paddingBottom:10},children:Object(y.jsxs)("div",{style:{marginTop:8,marginBottom:8,display:"flex",flexWrap:"wrap"},children:[Object(y.jsx)("div",{style:{fontWeight:600,fontSize:14},children:Object(y.jsx)("div",{style:{color:"white",background:"#93A9C3",paddingLeft:10,paddingRight:10,borderRadius:4},children:"Skills Needed"})}),Object(y.jsx)("div",{style:{marginLeft:10,flex:1},children:e.map((function(n,e){var a=n.techSkills,i=n.category,o=0===e;return Object(y.jsx)("div",{children:Object(y.jsxs)("div",{style:{display:"inline-block",fontSize:14,paddingLeft:10,paddingRight:10,borderRadius:4,paddingBottom:1},children:[i&&Object(y.jsxs)("div",{children:[Object(y.jsxs)("div",{style:{marginTop:o?0:8,display:"flex",alignItems:"center"},children:[Object(y.jsx)(t,{style:{marginLeft:2,marginRight:6}}),Object(y.jsx)("span",{style:{fontWeight:600},children:i})]}),a.map((function(n,e){var t=e===a.length-1;return Object(y.jsxs)("div",{style:{marginTop:8,display:"flex",alignItems:"center"},children:[Object(y.jsx)("div",{style:{color:"#E5E5E5",marginRight:10,transform:"scale(0.7)",display:"flex"},children:t?Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)("div",{style:{marginTop:-9,transform:"scaleY(1.5)",display:"inline-block"},children:"|"}),Object(y.jsx)("div",{style:{display:"inline-block",whiteSpace:"nowrap"},children:"\u2500\u2500\u2500 "}),Object(y.jsx)("div",{style:{display:"inline-block",marginLeft:4,transform:"translateY(-1px)"}})]}):Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)("div",{style:{transform:"scaleY(4)",display:"inline-block"},children:"|"}),Object(y.jsx)("div",{style:{display:"inline-block",whiteSpace:"nowrap"},children:"\u2500\u2500\u2500 "}),Object(y.jsx)("div",{style:{display:"inline-block",marginLeft:4,transform:"translateY(-1px)"}})]})}),Object(y.jsx)("div",{style:J,children:n})]})}))]}),!i&&Object(y.jsx)("div",{children:a.map((function(n,e){var a=0===e;return Object(y.jsxs)("div",{style:{marginTop:a?0:8,display:"flex",alignItems:"center"},children:[Object(y.jsx)(t,{style:{marginRight:4,marginLeft:2}}),Object(y.jsx)("div",{style:J,children:n})]})}))})]})})}))})]})})}var nn,en=t(132),tn="rgba(0, 0, 0, 0.1) 0px 0px 5px 0px,rgba(0, 0, 0, 0.1) 0px 0px 1px 0px",an="rgba(14, 30, 37, 0.12) 0px 2px 4px 0px,rgba(14, 30, 37, 0.32) 0px 2px 16px 0px",on="rgba(17, 17, 26, 0.1) 0px 4px 16px,rgba(17, 17, 26, 0.1) 0px 8px 24px,rgba(17, 17, 26, 0.1) 0px 16px 56px",sn="rgba(0, 0, 0, 0.15) 0px 2px 8px",rn="rgba(0, 0, 0, 0.05) 0px 0px 0px 1px,rgb(209, 213, 219) 0px 0px 0px 1px inset",cn="rgba(9, 30, 66, 0.25) 0px 4px 8px -2px,rgba(9, 30, 66, 0.08) 0px 0px 0px 1px",ln=function(n){var e=n.items,t=n.alignment,a=void 0===t?"left":t,i=n.dotVarient,o=n.useCollapseGlobally,s=void 0!==o&&o,r=n.expandedIndex,c=void 0===r?-1:r,l=n.setExpandIndex,d=void 0===l?function(){}:l;return e?e&&Object(y.jsx)(z.a,{align:a,className:"custom-timeline",children:e.map((function(n,e){var t=n.date,a=n.title,o=n.company,r=n.description,l=n.icon,p=n.link,u=(n.openByDefault,n.useCollapseLocally),m=void 0!==u&&u,h=n.skills,g=s&&m,f=c===e,_=Object(y.jsxs)(y.Fragment,{children:[r&&Object(y.jsxs)("div",{className:"description2",children:[r," "]}),p&&Object(y.jsxs)("div",{className:"url-link",children:["link: ",Object(y.jsx)("a",{href:p,target:"_blank",children:p})]})]}),b=dn({color:f?"#8a9aad":"#2a2e36"});return Object(y.jsxs)(q.a,{children:[Object(y.jsx)(V.a,{children:Object(y.jsx)("div",{className:"date",children:Object(y.jsx)(X.a,{variant:"body2",color:"textSecondary",children:t})})}),Object(y.jsxs)(G.a,{children:[Object(y.jsx)(K.a,{variant:i,children:l}),Object(y.jsx)(W.a,{})]}),Object(y.jsx)(H.a,{children:Object(y.jsx)("div",{className:"timeline-card",children:Object(y.jsxs)("div",{children:[Object(y.jsx)("div",{className:"title",children:a}),Object(y.jsx)(v,{height:5}),Object(y.jsx)("div",{className:"description-row",children:Object(y.jsxs)("div",{className:"description",children:[o," "]})}),h&&Object(y.jsx)(Z,{skills:h}),g&&Object(y.jsx)("div",{className:"description-row",children:Object(y.jsxs)(T.a,{className:S()("detail-button",b.detailButton),onClick:function(){!function(n){d(c!==n?n:-1)}(e)},children:[Object(y.jsx)(en.b,{size:20})," ",Object(y.jsx)(v,{width:2})," Detail  ",Object(y.jsx)($.a,{className:f?"expanded":"collapsed"})]})}),g&&Object(y.jsxs)(L.a,{in:f,children:[_,Object(y.jsx)(v,{height:5})]}),!g&&_]})})})]},"".concat(a,"_").concat(e))}))}):Object(y.jsx)(y.Fragment,{})},dn=Object(I.a)({detailButton:{"&.MuiButton-root":{"& *":{color:function(n){return n.color||"#212529 !important"}},boxShadow:tn,padding:"2px 15px 2px 20px"}}}),pn=Object(I.a)({customAnchor:{"& a":{color:"#989fc3 !important"}}},{name:"custom-anchor"}),un=function(n){var e=n.href,t=n.title,a=pn();return Object(y.jsx)("span",{className:a.customAnchor,children:Object(y.jsx)("a",{href:e,target:"_blank",children:t||e})})},mn=["thickness","style"],hn=function(n){var e=n.thickness,t=void 0===e?1:e,a=n.style,i=Object(b.a)(n,mn);return Object(y.jsx)("div",Object(_.a)({style:Object(_.a)({borderTop:"".concat(t,"px solid rgba(0,0,0,0.1)")},a)},i))},gn=(t(322),function(n){var e=n.items;return Object(y.jsx)("div",{className:"skills-custom-list-row",children:e.map((function(n,e){var t=n.flex,a=n.category,i=n.skills;return Object(y.jsxs)("div",{className:"category-row",children:[Object(y.jsx)("div",{className:"title",children:a}),Object(y.jsx)("div",{className:"list",children:Object(y.jsxs)("div",{className:"list-container",children:[a&&Object(y.jsx)("ul",{className:t?"flex":"",children:i.map((function(n,e){return Object(y.jsx)("li",{className:"custom-list-item",children:n},e)}))},"".concat(a,"_").concat(e)),!a&&Object(y.jsx)("div",{style:{marginLeft:20},children:Object(y.jsx)("ul",{className:t?"flex":"",children:i.map((function(n){return Object(y.jsx)("li",{className:"custom-list-item",children:n})}))},"".concat(a,"_").concat(e))})]})})]},"".concat(a,"_").concat(e))}))})}),fn=function(){return Object(y.jsxs)("div",{children:[Object(y.jsx)(v,{}),Object(y.jsx)("h4",{children:"Programming Skills"}),Object(y.jsx)(hn,{}),Object(y.jsx)(v,{}),Object(y.jsx)(gn,{items:[{flex:!0,category:"Source Control",skills:["Git"]},{flex:!0,category:"Deployment",skills:["Docker","Basic CI/CD Pipeline in Gitlab","Github Actions"]},{flex:!0,category:"Cloud (AWS)",skills:["Security Group","Target Group","Load Balancer","ECS with EC2 or Fargate","Route53","S3","CloudFront","Lambda Functions","API-Gateway","RDS","SQS"]},{flex:!0,category:"Typescript",skills:["React","React-Native","Redux","Redux-Saga","Next.js","Electron.js","Electron with Next.js","Node.js","Express.js","Socket.io","Twilio.js","Knex.js","Mongoose.js","Prisma.js","Prisma-Kysely.js"]},{flex:!0,category:"Java",skills:["Spring Boot Project Maintenance","Hibernate","MyBatis","JOOQ"]},{category:"C#",flex:!0,skills:["Windows Presentation Foundation"]},{category:"C++",flex:!0,skills:["CMake Ecosystem","Libtorch","OpenCV","ImGui for Desktop App"]},{category:"Golang",flex:!0,skills:["Gin","Goose","Sqlc","Azure-sdk for Voice","Go-Jet"]},{flex:!0,category:"Python",skills:["Tensorflow v2","PyTorch","Pandas","Flask","ONNX","Openpyxl","Boto3"]},{flex:!0,category:"Rust",skills:[Object(y.jsxs)(y.Fragment,{children:["Elliptic Curve Digital Signature Algorithm, ",Object(y.jsx)(un,{title:"Code Implementation",href:"https://machingclee.github.io/blog/article/Elliptic-Curve-and-Operator-Overloading/"})]})]},{flex:!0,category:"Data Streaming",skills:["Debezium (Kafka)"]},{flex:!0,category:"Message Broker",skills:["RabbitMQ"]},{flex:!0,category:"Database Query & Management",skills:["PostgreSQL","MongoDB"]},{category:"Deep-Learning",skills:[Object(y.jsx)(y.Fragment,{children:"Embed custom trained pytorch model in ONNX format into frontend"}),Object(y.jsxs)(y.Fragment,{children:["Implementation from scratch and training experience with CNN such as",Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Object Detection"}),Object(y.jsx)("li",{children:"Text Detection with Rotation"}),Object(y.jsx)("li",{children:"Image Classification"}),Object(y.jsx)("li",{children:"Image Generation and Image In-painting"})]}),Object(y.jsx)(v,{height:10})]}),Object(y.jsxs)(y.Fragment,{children:["Implementation and training experience with LSTM Model and Transformer to do:",Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Summarization on a column of data"}),Object(y.jsx)("li",{children:"Translation of random date format into standard YYYY-MM-DD one"})]})]})]}]}),Object(y.jsx)(v,{}),Object(y.jsx)("h4",{children:"Art Skills"}),Object(y.jsx)(hn,{}),Object(y.jsx)(v,{}),Object(y.jsx)(gn,{items:[{flex:!0,category:"Tools",skills:["Photoshop CC","3DS Max"]},{flex:!0,category:"Thingies I Draw",skills:["2D Character","2D Environment"]}]})]})},_n=t(1165),bn=t(268),yn=t.p+"static/media/CV_Portfolio.ee6d8ae7.pdf",wn=t.p+"static/media/agent-classification.17931009.pdf",vn=t.p+"static/media/transformer-date-fomatter.18a2d3fc.pdf",xn=t(76);!function(n){n[n.BLOG_MENU_WIDTH=160]="BLOG_MENU_WIDTH"}(nn||(nn={}));var kn={experienceExpandedIndex:-1,experienceCollapseGlobally:!0,activeNavRoute:f.ABOUT.path,sortPostBy:"date",leftMenuIsOpen:!0,showLoading:!1,activeTag:"",articles:{archive:{open:!1},defaultTocCrossed:!0,activeBlogArticleId:"",activePortfolioTitle:"",portfolios:[],blogs:[],langStudies:[]}},Sn=Object(xn.b)({name:"app",initialState:kn,reducers:{setExperienceCollapseGlobally:function(n,e){n.experienceCollapseGlobally=e.payload},setActiveExpIndex:function(n,e){n.experienceExpandedIndex=e.payload},setSortPostBy:function(n,e){n.sortPostBy=e.payload},setActiveTag:function(n,e){n.activeTag=e.payload},setActiveRouteTitle:function(n,e){n.activeRouteTitle=e.payload.activeRouteTitle},setShowLoading:function(n,e){n.showLoading=e.payload.showLoading},setDefaultTocCrossed:function(n,e){n.articles.defaultTocCrossed=e.payload},updateArchive:function(n,e){var t=e.payload;Object.entries(t).forEach((function(e){var t=Object(D.a)(e,2),a=t[0],i=t[1],o=a;n.articles.archive[o]=i}))},changeActiveNavRoute:function(n,e){n.activeNavRoute=e.payload},updateArticles:function(n,e){var t=e.payload;Object.entries(t).forEach((function(e){var t=Object(D.a)(e,2),a=t[0],i=t[1],o=a;n.articles[o]=i}))},clearArticles:function(n,e){n.articles=kn.articles,n.activeTag=kn.activeTag},clearState:function(n){kn}}}),Tn=function(n){var e=n.href,t=n.text,a=n.style;return Object(y.jsxs)("a",{href:e,target:"_blank",style:Object(_.a)({boxShadow:rn,padding:"2px 6px 6px 10px",margin:"0px 6px",borderRadius:4,whiteSpace:"nowrap"},a),children:[Object(y.jsx)("span",{style:{color:"rgb(40, 97, 151) !important"},children:t}),Object(y.jsx)(bn.a,{size:20,style:{marginLeft:4}})]})},Cn=function(){var n=A((function(n){return n.app.experienceExpandedIndex})),e=A((function(n){return n.app.experienceCollapseGlobally})),t=E();return Object(y.jsxs)("div",{children:[Object(y.jsx)(v,{}),Object(y.jsxs)("div",{style:{display:"flex",alignItems:"center"},children:[Object(y.jsx)("h4",{children:"Experience "}),Object(y.jsx)(_n.a,{sx:{marginLeft:"20px",fontSize:"18px",borderRadius:"4px",padding:"4px 10px",fontWeight:600,border:"1px solid rgba(0,0,0,0.3)","& a":{color:"#394852 !important"}},children:Object(y.jsx)(un,{title:"CV in PDF",href:yn})})]}),Object(y.jsx)(v,{height:10}),Object(y.jsx)(hn,{}),Object(y.jsx)(v,{}),Object(y.jsx)(ln,{expandedIndex:n,setExpandIndex:function(n){return t(Sn.actions.setActiveExpIndex(n))},useCollapseGlobally:e,items:[{title:"Software Engineer",date:Object(y.jsxs)(y.Fragment,{children:[" ",Object(y.jsx)("div",{children:"15-May 2023 - "}),Object(y.jsx)("div",{children:"Present"})]}),useCollapseLocally:!0,company:"Wonderbricks Limited",skills:[{category:"Typescript / Frontend",techSkills:["React","Redux-Toolkit",Object(y.jsxs)(y.Fragment,{children:["React-Native  ",Object(y.jsx)(Tn,{href:"/blog/category/react-native",text:"Study notes on react-native"})]}),"React Native Reanimated","Expo-Cli (Development Build, ad-hoc Internal Distribution, etc)","Algolia (Search Engine)","Socket.io-client","Push Notification for iOS, Android via EXPO"]},{category:"Typescript / Backend",techSkills:["Express","Mongoose","Socket.io","Algolia (Search Engine)","Googleapis + Nodemailer (for Sending Emails)","Prisma (db migration only)",Object(y.jsxs)(y.Fragment,{children:["Prisma-Kysely ",Object(y.jsx)("span",{style:{opacity:.55},children:"(as type-safe query builder, a counterpart of JOOQ in java; and go-jet in golang)"})]}),"Expo Push Notification"]},{category:"iOS Deployment to AppStore with EXPO",techSkills:["Development Build for Native Modules",Object(y.jsxs)(y.Fragment,{children:["Internal Ad-Hoc  Distribution (",Object(y.jsx)("b",{children:"obsoleted"})," due to hassles such as QR-code scanning, device registration, debug mode, ..., etc)"]}),"Store Build for publishing to App Store Connect","Internal and External Test via TestFlight",Object(y.jsxs)(y.Fragment,{children:["App Submissions and ",Object(y.jsx)("i",{children:"Rejections"})]}),Object(y.jsxs)(y.Fragment,{children:["Managed everything above with 4 stages (DEV, UAT, POC, PROD) with different",Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"bundleIdentifier"}),Object(y.jsx)("li",{children:"ascAppId"}),Object(y.jsx)("li",{children:"version and buildNumber"}),Object(y.jsx)("li",{children:"env (EXPO_PUBLIC_ 's)"})]})]})]},{category:"Java / Backend",techSkills:[Object(y.jsxs)(y.Fragment,{children:[" Springboot Project ",Object(y.jsx)("b",{children:Object(y.jsx)("i",{children:"(maintenance only)"})})]}),"Spring-Data-MongoDB","Algolia (Search Engine)",Object(y.jsxs)("div",{children:["Server Sent Event (SSE) ",Object(y.jsx)("i",{children:Object(y.jsxs)("b",{children:[" by WebFlux"," "]})})]})]},{category:"Python / Backend",techSkills:["Flask","Boto3","Openpyxl"]},{category:"Database",techSkills:[Object(y.jsxs)(y.Fragment,{children:["mySQL/pgSQL ",Object(y.jsx)(Tn,{href:"/blog/category/sql",text:"Studies with different frameworks"})]}),Object(y.jsxs)(y.Fragment,{children:["MongoDB",Object(y.jsx)(Tn,{href:"/blog/category/mongo",text:"Studies on mongo"})]})]},{category:"Cloud (AWS)",techSkills:["Deploy Serverless Flask Backend for Excel Generation in Lambda","Deploy Serverless Node.js Backend for Google Authentication in Lambda","Deploy React App to S3 with Cloudfront","Deploy Docker Images to ECS Fargate","Manage Load-Balancer which points to various services and set inbound rules for each port"]},{category:"Kafka + Debezium + kafkajs",techSkills:["Create database change stream to support our monitoring service."]},{category:Object(y.jsxs)(y.Fragment,{children:["RabbitMQ ",Object(y.jsx)(Tn,{style:{fontWeight:400},text:"Message Queue Design for an Mobile App",href:"/blog/article/Code-Organization-for-RabbitMQ"})]}),techSkills:[Object(y.jsx)(y.Fragment,{children:"Delayed Queue by means of Dead-letter Queue, e.g., pin a message to the top and restore its position 1 week later"}),Object(y.jsx)(y.Fragment,{children:"Direct Exchange with prefetchCount for rate limit"})]},{category:Object(y.jsxs)(y.Fragment,{children:["CICD / Github Actions ",Object(y.jsx)(Tn,{style:{fontWeight:400},href:"/blog/category/github-actions",text:"Study Notes for Github Actions"})," "]}),techSkills:["Develop Workflow for Frontend Deployment","Develop Workflow for Backend Deployment via pushing image to ECR, updating ECS Task and updating ECS Service"]},{category:"Docker",techSkills:["Containerization of Python, Node.js and Java application into images for deployment onto cloud"]},{category:Object(y.jsx)(y.Fragment,{children:"System Design"}),techSkills:["LLM Summary Workflow using RabbitMQ for rate limiting",Object(y.jsxs)(y.Fragment,{children:["In-App Notification as a new table  storing user-specfic data, with careful synchronization to true data",Object(y.jsx)(Tn,{style:{fontWeight:400},href:"/blog/article/In-App-Notification",text:"Table Design"})]}),Object(y.jsxs)(y.Fragment,{children:["Caching system with the help of in-app notification table which successfully separates user-specific data from ",Object(y.jsx)("i",{children:Object(y.jsx)("b",{children:"general data"})})]}),"REST api based chat system with messages populated by socket.io"]},{category:"Documentation (Docusaurus Project)",techSkills:["Initiated this project and consistently make change/progress record for different projects","Implemented CICD to encourage contributions from staffs",Object(y.jsxs)(y.Fragment,{children:["Google Authenticated via backend served in ",Object(y.jsx)("b",{children:Object(y.jsx)("i",{children:"Lambda"})})]})]},{category:"Redis",techSkills:[Object(y.jsx)(y.Fragment,{children:"Implemented caching layer to offload query tasks from database"}),Object(y.jsxs)(y.Fragment,{children:["Clear understanding why we ",Object(y.jsx)("b",{children:"first"})," update db, ",Object(y.jsx)("b",{children:"then"})," clear cache, but not the vice versa"]}),Object(y.jsx)(y.Fragment,{children:"Used as Redis-Adapter to scale up node-js based socket-io servers"})]}],description:Object(y.jsx)(y.Fragment,{children:Object(y.jsxs)("ul",{children:[Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Web Related."}),Object(y.jsxs)("ul",{children:[Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Frontend."}),Object(y.jsxs)("ul",{children:[" ",Object(y.jsx)("li",{children:"Build, revamp and maintain React projects in Typescript."})]})]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Backend."}),Object(y.jsx)("ul",{children:Object(y.jsx)("li",{children:"Maintain SpringBoot project, build APIs using MyBatis and mongo-java-driver"})})]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Cloud."}),Object(y.jsxs)("ul",{children:[Object(y.jsxs)("li",{children:["Move part of the services in SpringBoot project  from pyinstaller-created executable  to a single ",Object(y.jsx)("b",{children:Object(y.jsx)("i",{children:"serverless Python Flask server"})}),", which are APIs to generate excel and pdf documents when receiving json data."]}),Object(y.jsxs)("li",{children:["Serverless part have been summarized ",Object(y.jsx)("a",{href:"/blog/article/Serverless-Flask-and-Serverless-Express-ts",target:"_blank",children:"here"})," for future reuse, additionally created a functioning template for nodejs express."]})]})]})]})]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Mobile Related."}),Object(y.jsxs)("ul",{children:[Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Frontend."}),Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Develop this realtime text-messaging and LLM based project from scratch by React-Native and EXPO"}),Object(y.jsx)("li",{children:"Create reusable components by proper props design or packaging into a class"}),Object(y.jsx)("li",{children:"Create custom interactive components by React-Native-Reanimated"})]})]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Backend."}),Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Develop entire backend application from scratch by nodejs, express, socketio, etc."}),Object(y.jsx)("li",{children:"Develop Lambda functions in python and nodejs for generating report in excel or word."}),Object(y.jsx)("li",{children:"Designed middleware to let user send text, audio, images via REST apis and get the messages in the room they joined via socket.io."}),Object(y.jsx)("li",{children:"Implement jwt authentication and refresh-token mechanism in both front and backend."}),Object(y.jsx)("li",{children:"Develop Push notifcation system for ios and android."}),Object(y.jsx)("li",{children:"Design tables for our backend to fulfll challenging requirements from product manager."}),Object(y.jsx)("li",{children:"In table design, make careful consideration on the data-type of primary keys. Replaced the purely random UUID by ULID to improve performance of insertion."}),Object(y.jsx)("li",{children:"Unify the structure of API response so that the frontend can handle errors gracefully."}),Object(y.jsx)("li",{children:"Migrate schema of PostgreSQL DB via Prisma. Have good experience of creating, resolving, and testing migration scripts in dev and non-dev environments."}),Object(y.jsxs)("li",{children:["Create instant audio uploading on speech ends and create data processing pipeline ",Object(y.jsx)("b",{children:"in stream"})," to transform",Object(y.jsxs)("ol",{children:[Object(y.jsx)("li",{children:" base64-encoded strings into bytes"}),Object(y.jsxs)("li",{children:[" that bytes from ",Object(y.jsx)("b",{children:".m4a"})," into ",Object(y.jsx)("b",{children:".mp3"})]}),Object(y.jsx)("li",{children:" upload resulting stream of bytes of mp3 to azure blob storage"})]})]})]})]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"System Design."}),Object(y.jsxs)("ul",{children:[Object(y.jsxs)("li",{children:["Develop message queue system using RabbitMQ to:",Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Rate limit api which has a limit of 100 concurrent calls (azure openai service)"}),Object(y.jsx)("li",{children:"Delay actions by means of DeadLetter Queue"})]})]}),Object(y.jsx)("li",{children:"Develop a class in the above message queue system to organize consumption logic, error-report logic and msg-publishing logic in a type-safe way"}),Object(y.jsx)("li",{children:"Develop a caching system (and a class) to GET and SET cached value in a managable way"}),Object(y.jsx)("li",{children:"Develop in-app notifcation to separate user-specifc data from our API, making those API cachable (userId independent)"})]})]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"iOS Deployment to AppStore with EXPO."}),Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Internal Ad-Hoc Distribution"}),Object(y.jsx)("li",{children:"Store Build for publishing to App Store Connect"}),Object(y.jsx)("li",{children:"Internal and External Test via TestFlight"}),Object(y.jsx)("li",{children:"App Submissions and Rejections"}),Object(y.jsx)("li",{children:"Managed everything above with 4 stages (DEV, UAT, POC, PROD) with diferent bundleIdentifer, ascAppId, version and buildNumber, env (EXPO_PUBLIC_'s)"})]})]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"DevOps / CICD."}),Object(y.jsx)("ul",{children:Object(y.jsxs)("li",{children:["Create automated deployment workfows for frontend and backend via github actions. Which includes:",Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Containerize the nodejs backend > push to image registry > update task defntion > instruct ECS Fargate to use updated task"}),Object(y.jsx)("li",{children:"Deployment of documentation project onto S3 and cache-invalidation of cloudfront"})]})]})})]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Other."}),Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Initiated a google\u2010authenticated documentation project by using Docusaurus to record detail that our developer should know"}),Object(y.jsx)("li",{children:"Lead, guide and nurture junior programmer by constantly sharing articles, my github projects, and related youtube videos when there are knowledge that they lack of"})]})]})]}),Object(y.jsx)("ul",{})]})]})})},{title:"AI Engineer",date:Object(y.jsxs)(y.Fragment,{children:[" ",Object(y.jsx)("div",{children:"15 Aug 2022 - "}),Object(y.jsx)("div",{children:"April 2023"})]}),company:"Eye Catching Limited",useCollapseLocally:!0,skills:[{category:"Python",techSkills:["PyTorch"]},{category:"Typescript",techSkills:["Tensorflow","Onnx"]},{category:"C++",techSkills:["Libtorch","Imgui",Object(y.jsxs)(y.Fragment,{children:["CMake Ecosystem",Object(y.jsx)(Tn,{href:"/blog/article/Simple-Introduction-to-CMake-Files",text:"studies on CMake"})]})]}],description:Object(y.jsx)(y.Fragment,{children:Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Maintain angular project."}),Object(y.jsx)("li",{children:"Study eye-tracing related algorithm."}),Object(y.jsx)("li",{children:"Study and deploy machine leanring algorihtm in web-app and desktop-app."}),Object(y.jsx)("li",{children:"Completely translate a BlazeFace model in python into the same model in libtorch of C++ for GUI application."}),Object(y.jsx)("li",{children:"Implement methods to ensure weights in pytorch model can be used in libtorch model."}),Object(y.jsx)("li",{children:"Study CMake and develop software to combine libtorch model and imgui application."})]})})},{title:"Senior Software Developer",date:Object(y.jsxs)(y.Fragment,{children:[" ",Object(y.jsx)("div",{children:"Sep 2021 - "}),Object(y.jsx)("div",{children:"July 2022"})]}),useCollapseLocally:!0,company:"RaSpect Intelligence Inspection Limited",skills:[{category:"Python",techSkills:["Selenium","Boto3 (AWS SDK)","Mongoengine","gRPC","PyTorch"]},{category:"Data Annotation Tool",techSkills:["CVAT","COCO-Annotator"]},{category:"AI-General",techSkills:["Data Annotation","DataLoader with Various Data Augmentations","Model Training for Rust Detection","Model Training for Crack Detection"]},{category:"AI-Models",techSkills:["GAN for Producing Synthetic Data","Faster RCNN from Scratch and its Modification",Object(y.jsxs)(y.Fragment,{children:["Single Stage Headless ",Object(y.jsx)("b",{children:"Face"})," Detector into ",Object(y.jsx)("b",{children:"Rust"})," Detector"]})]},{category:"Javascript (Legacy Project)",techSkills:["Next"]}],description:Object(y.jsx)(y.Fragment,{children:Object(y.jsxs)("ul",{children:[Object(y.jsxs)("li",{children:[" ",Object(y.jsx)("b",{children:"Web or Hardware Related."}),Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Maintain React Next project."}),Object(y.jsx)("li",{children:"Data scrapping for house transactions records."}),Object(y.jsx)("li",{children:"Automation on tracking latest rtk-data from official geodetic website and data-processing pipeline."}),Object(y.jsx)("li",{children:"Work with a local grpc based service in Python to interact with a compiled binary (.exe), messed with multi-threading stuff."})]})]}),Object(y.jsxs)("li",{children:[" ",Object(y.jsx)("b",{children:"Object Detection Related."}),Object(y.jsxs)("ul",{children:[Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Text Detection."})," Implemented text detection (EAST) to facilitate signboard defect classification."]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Crack Detection."})," Help implement detection model for crack and spalling."]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Rust Detection."})," Implemented two solutions: ",Object(y.jsx)("b",{children:"(i)"})," Text removal -> classifcation pipeline; and ",Object(y.jsx)("b",{children:"(ii)"})," direct object detection model using faster rcnn."]})]})]}),Object(y.jsxs)("li",{children:[" ",Object(y.jsx)("b",{children:"Image Generation/Inpainting."}),Object(y.jsxs)("ul",{children:[Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"DefectGAN."})," Implemented image-generation model following  ",Object(y.jsx)(un,{href:"https://arxiv.org/pdf/2103.15158.pdf",title:"this paper"})," that generate synthetic data on cracks and spallings for training defect detection models on facades."]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Text Removal."})," Experimented with existing algorithms like various GAN or image-inpainting method in pytorch. Finally I follow ",Object(y.jsx)(un,{href:"https://arxiv.org/pdf/2104.11493.pdf",title:"this paper"})," to obtain a text eraser with satisfactory performance. ",Object(y.jsx)("br",{}),"For an example of failed model, we refer the results to ",Object(y.jsx)(un,{href:"https://github.com/machingclee/2022-06-03-EnsNet-for-text-removal",title:"here"}),"."]})]})]})]})})},{title:"Software Engineer",date:Object(y.jsxs)(y.Fragment,{children:[" ",Object(y.jsx)("div",{children:"Aug 2020 - "}),Object(y.jsx)("div",{children:"Mid-Aug 2021"})]}),useCollapseLocally:!0,company:"EAB Systems (Hong Kong) Limited",skills:[{category:"Typescript",techSkills:["React","Redux","React-Native","Express","Mongoose","Sendgrid","Twilio"]},{category:"Networking",techSkills:["socket.io"]},{category:"Python",techSkills:["Pandas","Tensorflow v2"]}],description:Object(y.jsx)(y.Fragment,{children:Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Build video conferencing frontend and backend application in React, Twilio and express. "}),Object(y.jsx)("li",{children:"Build CMS system that lets users create their own single page application."}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Machine Learning Related."})," Build CSV Importer (together with an express layer and a Flask layer) that can parse a csv/excel file and perform:",Object(y.jsxs)("ul",{children:[Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Data Classification."}),Object(y.jsxs)("ul",{children:[Object(y.jsxs)("li",{children:[Object(y.jsx)("strong",{children:"Sample Code"}),": ",Object(y.jsx)("a",{href:wn,target:"_blank",children:"Link"})]}),Object(y.jsx)("li",{children:"Classify column based on existing data using LSTM model with two dense layers and finally a softmax."})]})]}),Object(y.jsxs)("li",{children:[Object(y.jsx)("b",{children:"Auto Date-reformatting."}),Object(y.jsxs)("ul",{children:[Object(y.jsxs)("li",{children:[Object(y.jsx)("strong",{children:"Sample Code"}),": ",Object(y.jsx)("a",{href:vn,target:"_blank",children:"Link"})]}),Object(y.jsx)("li",{children:" Based on machine learning model (transformer in NLP) that can translate all common form of date into YYYY-MM-DD format."})]}),Object(y.jsx)("br",{})]})]})]}),"Data Classification and Auto Date-refomatting are separately two models placed inside tensorflow-serving for deployment."]})})},{title:"Front-end Developer",date:"Sep 2019 - Aug 2020",useCollapseLocally:!0,company:"eLearningPro",skills:[{category:"Javascript",techSkills:["React"]},{category:"Python",techSkills:["tkinter","pyinstaller"]},{category:"Art",techSkills:["Adobe Premiere","Adobe Photoshop","Saola Animate (CSS Animation)"]}],description:Object(y.jsx)(y.Fragment,{children:Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:'Create HTML5 Coursewares, maintain web pages and construct React frontend application."'}),Object(y.jsxs)("li",{children:["Created a python GUI project for text extraction from an image (a work necessary to translate old fresh game into html5 game).",Object(y.jsxs)("p",{children:["Detail can be found ",Object(y.jsx)(un,{href:"".concat("","/portfolio/Desktop-App-to-Capture-Text-From-Images"),title:"here"}),"."]})]})]})})},{title:"2D Game Artist",date:"Jan 2018 - Dec 2018",useCollapseLocally:!0,company:"\u6df1\u5733\u75af\u70b9\u5b50\u79d1\u6280\u6709\u9650\u516c\u53f8",skills:[{category:"Software",techSkills:["Unity","Autodesk 3ds Max","Adobe Photoshop"]},{category:"Artistic Skills",techSkills:["2D Character Design","UI Assets Design","Low-poly 3D Model Construction"]}],description:Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)("div",{children:"Responsible for constructing 3d accessories, creating environment art and character design."}),Object(y.jsxs)("div",{children:["Game that I worked on: ",Object(y.jsx)("a",{href:"https://www.taptap.com/app/37681/topic",target:"_blank",children:"Link"})]})]})},{title:"Unemployed",date:"Oct 2015 to Dec 2017",skills:[{techSkills:["Thick Skin","Self-Discipline","Parental Support"]}],company:Object(y.jsx)(v,{height:5}),description:Object(y.jsxs)(y.Fragment,{children:[" ",Object(y.jsx)("p",{children:"I was striving to become an artist within two years. Of this time I was taking online courses and drawing on a daily basis. I aimed at being able to draw a variety of game assets (not just character)."}),"My outcome based on this period can be seen ",Object(y.jsx)(un,{href:"".concat("","/artworks"),title:"here"}),"."]})},{title:"Senior Research Assistant",date:"Mar 2015 - Oct 2015",company:"Department of Mathematics, Hong Kong Baptist University",description:"Study HJB equations arised in specific financial games."},{title:"Research Assistant",date:"Sep 2014 - Feb 2015",company:"Department of Mathematics, Hong Kong University of Science and Technology",description:"Study the property of viscosity solution of HJB equations."},{title:"Teaching Assistant",date:"Sep 2012 - June 2014",company:"Department of Mathematics, Hong Kong University of Science and Technology",description:Object(y.jsxs)("div",{children:[Object(y.jsx)("div",{children:"Fulfill teaching duty as required in obtaining studentship in the course of M.Phil study. Subjects include:"}),Object(y.jsx)("div",{children:Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:"Calculus II"}),Object(y.jsx)("li",{children:"Linear Algebra"}),Object(y.jsx)("li",{children:"Mathematical Analysis"}),Object(y.jsx)("li",{children:"Real Analysis"})]})}),Object(y.jsxs)("div",{children:["Teaching Material: ",Object(y.jsx)(un,{href:"https://sites.google.com/site/mathcclee/home",title:"Link"})]})]})}]})]})},En=t.p+"static/media/my_picture_6.33c544db.jpg",An=(t(324),function(n){var e=n.height,t=n.children;return Object(y.jsx)("div",{style:{position:"relative",width:e,height:e,borderRadius:"50%",overflow:"hidden",display:"flex",justifyContent:"center",alignItems:"center",boxShadow:an},className:"circular",children:t})}),jn=(t(325),t(269)),Rn=t.n(jn),In=t(270),On=t.n(In),Nn=t(272),Dn=t.n(Nn),Pn=t(271),Ln=t.n(Pn),$n=(t(326),Object(I.a)({aboutMeButton:{"& button":{display:"inline-block",padding:"6px 10px",minWidth:"unset","& svg":{color:"#5d5d86 !important"}}}},{name:"custom-href"})),Mn=function(n){var e=n.icon,t=n.href,a=n.disabled,i=void 0!==a&&a,o=$n();return Object(y.jsx)("a",{className:o.aboutMeButton,href:t,target:"_blank",style:{pointerEvents:i?"none":"auto"},children:Object(y.jsx)(T.a,{children:e})})},Fn=function(n){var e=n.icon,t=n.info;return Object(y.jsx)("div",{className:"contact-item",children:Object(y.jsx)("table",{children:Object(y.jsx)("tbody",{children:Object(y.jsxs)("tr",{children:[Object(y.jsx)("td",{className:"key",children:e}),Object(y.jsx)("td",{children:t})]})})})})},Un=function(){return Object(y.jsxs)("div",{className:"contact",children:[Object(y.jsxs)("div",{className:"linkedin-git",children:[Object(y.jsx)(Mn,{icon:Object(y.jsx)(Rn.a,{style:{width:22,marginLeft:1}}),href:"https://github.com/machingclee"}),Object(y.jsx)(Mn,{icon:Object(y.jsx)(On.a,{style:{fontSize:26}}),href:"https://www.linkedin.com/in/chingclee"})]}),Object(y.jsx)(v,{height:7}),Object(y.jsxs)("div",{children:[Object(y.jsx)(Fn,{icon:Object(y.jsx)(Ln.a,{}),info:Object(y.jsx)("a",{href:"https://machingclee.github.io/",target:"_blank",children:"https://machingclee.github.io"})}),Object(y.jsx)(Fn,{icon:Object(y.jsx)(Dn.a,{}),info:"machingclee@gmail.com"})]})]})},Bn=t(273),zn=t(74),qn=t.n(zn),Gn=t(133),Wn=function(n){var e,t=new Date(n);return{date:t.getDate(),month:(e=t.getMonth(),{0:"January",1:"February",2:"March",3:"April",4:"May",5:"June",6:"July",7:"August",8:"September",9:"October",10:"November",11:"December"}[String(e)]),year:t.getFullYear()}},Hn=function(n){return encodeURIComponent(n)},Vn=function(n){return decodeURIComponent(n)},Kn=function(n){return n.replace(/\W+/g,"-")},Xn=function(n){return n.replace(/\$/g,"!@!@!").replace(/\s/g,"-").replace(/\!@\!@\!/g,"$")},Yn=t(275),Qn=t.n(Yn);var Jn=function(n){return n.replace(/\d+/g,"")},Zn=function(n){return!("boolean"==typeof n?n:!!/(true|yes)/.test(n))},ne=function(){var n=Object(Gn.a)(qn.a.mark((function n(e){var t,a,i,o,s,r,c;return qn.a.wrap((function(n){for(;;)switch(n.prev=n.next){case 0:return t=e,t="https://raw.githubusercontent.com/machingclee/machingclee.github.io/main"+e,n.next=4,fetch(t);case 4:return a=n.sent,n.next=7,a.text();case 7:return i=n.sent,o=Qn()(i),s=o.content,r=o.data,c=r,n.abrupt("return",{content:s,data:c});case 11:case"end":return n.stop()}}),n)})));return function(e){return n.apply(this,arguments)}}();var ee=t(1162),te=t(64),ae=t.n(te),ie=Object(I.a)({returnButton:{textTransform:"capitalize",fontVariant:"small-caps",minWidth:"unset",padding:"2px 12px",paddingLeft:0,fontSize:"16px",color:"#394852","& svg":{color:"#394852",fontSize:"30px!important"},marginLeft:"-8px"}},{name:"return-button-modified"}),oe=function(n){var e=n.returnCallback,t=ie(),a=Object(c.f)();return Object(y.jsxs)(T.a,{onClick:function(){return e?e():a.goBack()},className:t.returnButton,children:[Object(y.jsx)(ae.a,{})," Return"]})},se=t(276),re=t.n(se),ce=t(277),le=t.n(ce),de=t(278),pe=t.n(de),ue=t(279),me=t.n(ue),he=(t(527),t(1167)),ge=t(284),fe=t(58),_e=t.n(fe),be={jax:["input/TeX","output/HTML-CSS"],tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]},"HTML-CSS":{availableFonts:["TeX"],linebreaks:{automatic:!0,width:"container"},minScaleAdjust:100},TeX:{}},ye=function(n){var e=n.tex,t=n.inline,a=void 0===t||t;return Object(y.jsx)(_e.a.Node,{formula:e,inline:a})},we=t(21),ve=t.n(we),xe=t(285),ke=t.n(xe),Se=["inline","className","children"],Te=Object(I.a)({example:{paddingLeft:"calc(2em - 3px)",marginTop:40,marginBottom:60,borderLeft:"3px solid rgba(123,185,209,0.3)"},proof:{"& > p:nth-child(1)":{textIndent:"calc(3px - 2em) !important"},paddingLeft:"calc(2em - 3px)",marginTop:20,marginBottom:20},codeBlock:{"& .react-syntax-highlighter-line-number":{minWidth:"2.5em !important"},"& pre":{paddingTop:"0.95em !important",background:"#F5F5F5!important",border:"1px solid #E0E0E0",borderRadius:2},"& .no-bg":{background:"transparent !important","& code":{background:"transparent !important"}}}},{name:"custom-code-style"}),Ce=function n(e){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"",a=arguments.length>2&&void 0!==arguments[2]&&arguments[2],i=t;return null===e||void 0===e||e.forEach((function(e){if("string"===typeof e)i+=a?"$".concat(e,"$"):"".concat(e);else{var t=e.props;if(null===t||void 0===t?void 0:t.children){var o,s=null===t||void 0===t||null===(o=t.className)||void 0===o?void 0:o.includes("math-inline");i=n(null===t||void 0===t?void 0:t.children,i,s)}}})),i},Ee={leftBlockClassname:""},Ae={leftBlockClassname:"h4-left-block"},je={leftBlockClassname:"h5-left-block"},Re={leftBlockClassname:"h6-left-block"},Ie=function(n){return function(e){var t=e.node,a=e.children,i=t.tagName,s=Ce(a,"");return Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)("a",{className:"md-title",id:s.replace(/\$/g,"!@!@!").replace(/\s/g,"-").replace(/\!@\!@\!/g,"$")}),o.a.createElement(i,{},Object(y.jsxs)(y.Fragment,{children:[n.leftBlockClassname&&Object(y.jsx)("div",{className:n.leftBlockClassname,style:{height:"100%",display:"inline"}}),a]}))]})}},Oe={code:function(n){var e=n.inline,t=n.className,a=n.children,i=Object(b.a)(n,Se),o=Te(),s=/language-(\w+)-*(\d+)*/.exec(t||""),r=ve.a.get(s,"[1]",""),c=ve.a.get(s,"[2]","");return!e&&s?Object(y.jsx)("div",{className:S()(o.codeBlock),children:Object(y.jsx)(he.a,Object(_.a)({style:Object(_.a)({},ge.a),className:"none"===r?"no-bg":"",showLineNumbers:!!c,lineNumberStyle:{opacity:.35},startingLineNumber:c?parseInt(c):void 0,language:r,children:String(a).replace(/\n$/,"")},i))}):Object(y.jsx)("code",Object(_.a)(Object(_.a)({className:t},i),{},{children:a}))},example:function(n){n.className;var e=n.children,t=Te();return Object(y.jsx)("div",{className:t.example,children:e})},proof:function(n){n.className;var e=n.children,t=Te();return Object(y.jsx)(y.Fragment,{children:Object(y.jsxs)("div",{className:t.proof,children:[e,Object(y.jsx)("div",{style:{display:"flex",justifyContent:"flex-end"},children:Object(y.jsx)(ke.a,{})})]})})},h1:Ie(Ee),h2:Ie(Ee),h3:Ie(Ee),h4:Ie(Ae),h5:Ie(je),h6:Ie(Re),span:function(n){var e=n.className,t=n.children;return/math\-inline/.exec(e||"")?Object(y.jsx)(ye,{tex:String(t).replace(/\n$/,""),inline:!0}):Object(y.jsx)(y.Fragment,{children:t})},div:function(n){var e=n.className,t=n.children;return/math\-display/.exec(e||"")?Object(y.jsx)("div",{className:"display-math",children:Object(y.jsx)(ye,{tex:String(t).replace(/\n$/,""),inline:!1})}):Object(y.jsx)(y.Fragment,{children:t})}};function Ne(n,e){return n.children===e.children}var De=Object(i.memo)((function(n){n.style;var e=n.children,t=n.clipDistance,a=(null===e||void 0===e?void 0:e.toString())||"",o=Object(c.g)().pathname;return Object(i.useEffect)((function(){var n=window.MathJax;n&&(n.Hub.Queue(["Typeset",n.Hub]),n.Hub.Config({TeX:{tags:"ams",Macros:{R:"{\\mathbb{R}}",KL:"D_{\\text{KL}}",P:"\\mathbb{P}",C:"\\mathbb{P}",N:"\\mathbb{N}",brac:["\\left(#1\\right)",1],cbrac:["\\left\\{#1\\right\\}",1],sbrac:["\\left[#1\\right]",1],Z:"\\mathbb{Z}",E:"{\\mathbb{E}}",var:"\\mathrm{Var}",qed:"\\tag*{$\\blacksquare$}",bf:"\\boldsymbol",argmax:"\\mathrm{argmax}",argmin:"\\mathrm{argmin}",Lip:"\\mathrm{Lip}",spt:"\\mathrm{spt}",pd:"\\partial",dis:"\\displaystyle",id:"\\mathrm{id}",inner:["\\left\\langle#1\\right\\rangle",1],ol:"\\overline",olw:["\\overline{#1}^{w}",1],dom:"\\mathrm{dom}\\,",epi:"\\mathrm{epi}\\,",limi:"\\mathop{\\underline{\\lim}}",lims:"\\mathop{\\overline{\\lim}}",w:"w\\,\\text{-}\\!",conv:"\\mathrm{conv}",aff:"\\mathrm{aff}",nn:"\\mathcal{N}",limn:"\\lim_{n\\to\\infty}",limsn:"\\lims_{n\\to\\infty}",limin:"\\limi_{n\\to\\infty}",wto:"\\rightharpoonup"},extensions:["AMSmath.js","boldsymbol.js ","AMSsymbols.js","extpfeil.js"]}}))}),[o]),Object(y.jsx)(re.a,{components:Oe,linkTarget:"_blank",transformLinkUri:function(n){return(null===n||void 0===n?void 0:n.startsWith("/"))?"".concat("").concat(n):n},transformImageUri:function(n){return(null===n||void 0===n?void 0:n.startsWith("/"))?"".concat("").concat(n):n},remarkPlugins:[me.a,le.a],rehypePlugins:[pe.a],children:void 0!==t?a.substring(0,t)+"...":a})}),Ne),Pe=t(30),Le=(t(763),Object(I.a)({"@keyframes fadein":{"0%":{opacity:0},"100%":{opacity:1}},customFadin:{"&.fade-in":{animation:"$fadein .3s ease-in-out"}}},{name:"fade-in",classNamePrefix:"fade-in"}));function $e(n){var e=n.children,t=n.dependencies,a=void 0===t?[]:t,o=n.style,s=void 0===o?{}:o,r=Object(i.useState)(!1),c=Object(D.a)(r,2),l=c[0],d=c[1],p=Le();return Object(i.useEffect)((function(){d(!1),setTimeout((function(){d(!0)}),1)}),Object(Pe.a)(a)),Object(y.jsx)("div",{style:Object(_.a)({opacity:l?1:0},s),children:Object(y.jsx)("div",{className:S()(p.customFadin,l?"fade-in":""),children:Object(y.jsx)(y.Fragment,{children:e})})})}var Me=t(134),Fe=t.n(Me),Ue=t(96),Be=t.n(Ue),ze=t(136),qe=Object(I.a)((function(n){return{thumbnailAnchor:{"& .image-preview":{transition:"all 0.4s ease-in-out",color:"black",fontSize:15,borderRadius:10,backgroundColor:"rgba(255,255,255,0.1)",backdropFilter:"blur(25px)",boxShadow:cn,opacity:.2,pointerEvents:"none",top:"50%",left:-80,transform:"translateY(-50%)",position:"absolute",padding:"5px 10px 15px 10px",width:260,zIndex:1e4,overflow:"hidden","& .img-container":{overflow:"hidden",position:"relative",width:"100%",opacity:0,transition:"all 0.4s ease-in-out",height:0,"& img":{opacity:.7,maxWidth:"100%",transition:"all 0.4s ease-in-out",position:"absolute",left:"50%",transform:"translateX(-50%)",borderRadius:4}}},"&:hover":{"& .image-preview":{zIndex:10001,height:"unset",opacity:1,"& .img-container":{opacity:1,height:function(n){return n.height}}}}},nextPrev:{opacity:.65,fontWeight:600,marginBottom:10,display:"flex",alignItems:"center","&.active":{opacity:.8}},projects:{"& img, & iframe":{boxShadow:"rgba(0, 0, 0, 0.1) 0px 0px 5px 0px, rgba(0, 0, 0, 0.1) 0px 0px 1px 0px"},"& a":{color:"#2a718d !important"},fontSize:"110%","& center":{marginTop:20,marginBottom:20}},thumbTitle:{padding:20,paddingTop:10,paddingBottom:10,textAlign:"center",width:"100%",pointerEvents:"none",fontSize:18,fontWeight:600,position:"absolute",zIndex:3,left:0,top:"50%",transform:"translateY(-50%)"},thumbnailBtn:{backdropFilter:"saturate(50%) blur(8px)",backgroundColor:"rgba(255, 255, 255, 0.95)",borderRadius:0,position:"absolute",top:0,left:0,width:"100%",height:"100%",zIndex:2},thumbnail:{border:"1px solid rgba(0,0,0,0.02)",background:"rgba(".concat(240,",").concat(240,",").concat(240,",1)"),position:"relative",height:180,overflow:"hidden","& .title-date":{fontSize:12,fontWeight:400,display:"flex",justifyContent:"flex-end"},"& .title-container":{transition:"backdropFilter 0.2s ease-in-out, transform 0.2s ease-in-out, opacity 0.2s ease-in-out",borderRadius:3,marginLeft:180,boxShadow:sn,background:"rgba(255,255,255, 0.8)",padding:15,backdropFilter:"blur(25px) brightness(120%)","& .title-inner":{color:"#2a2e36 !important"},"& .title-tech":{fontSize:15,fontWeight:400,"& .item":{display:"flex",justifyContent:"flex-start",alignItems:"center"}}},"&:hover":{"& .title-container":{backdropFilter:"blur(25px) brightness(120%)"},zIndex:4,"& .thumbnail-btn":{background:"rgba(255,255,255, 0.95)"},"& .thumb-title":{}},"& .thumb-title":{transition:"opacity 0.15s ease-in-out, backdropFilter 0.3s ease-in-out"},"& img":{objectFit:"cover"}},portfolio:{"& code":{background:"#F5F5F5!important",padding:"0.2em 0.4em",borderRadius:6,color:"inherit",fontSize:"0.85em !important",border:"1px solid rgba(0,0,0,0.03)"},"& pre":{margin:"1em 0","& code":{fontSize:"0.85em !important",border:"none"}},"& h1, & h2, & h4, & h5, & h6":{marginTop:30,marginBottom:20},maxWidth:740,"& img, & iframe":{borderRadius:4},"& .TOC-heading":{"& > ul":{marginBottom:20},"& ul > li":{marginBottom:10},"& ul>li>ul>li":{marginTop:10,marginBottom:-5},"& ul>li>ul>li:first-child":{marginTop:2}}}}}),{name:"PortfolioCard"}),Ge=function(){return function(n){var e=[];return n.keys().map((function(t,a){e.push(n(t))})),e}(t(764))},We=function(n){var e=n.portfolio,t=n.cardStyle,a=void 0===t?{borderRadius:4}:t,o=n.decorator,s=Object(c.f)(),r=Xn(e.title),l=qe({height:e.hoverImageHeight}),d="".concat(f.PORTFOLIO.path,"/").concat(r),p=new Date(e.date),u=p.getFullYear(),m=p.getMonth().toString().padStart(2,"0"),h=p.getDate().toString().padStart(2,"0"),g="".concat(u,"-").concat(m,"-").concat(h),_=Object(i.useState)(null),b=Object(D.a)(_,2),w=b[0],x=b[1];Boolean(w);return Object(y.jsx)(y.Fragment,{children:Object(y.jsxs)("a",{className:l.thumbnailAnchor,style:{position:"relative"},href:d,onClick:function(n){null===n||void 0===n||n.preventDefault(),n&&(1===n.button||n.ctrlKey&&0===n.button?window.open(d,"_blank"):s.push(d))},onMouseEnter:function(n){x(n.currentTarget)},onMouseLeave:function(n){x(null)},children:[Object(y.jsxs)("div",{className:l.thumbnail,style:a,children:[Object(y.jsxs)("div",{className:S()(l.thumbTitle,"thumb-title"),children:[o&&Object(y.jsx)("div",{style:{top:"50%",left:40,position:"absolute",zIndex:4,transform:"translateY(-50%)",pointerEvents:"none"},children:o}),Object(y.jsx)("div",{className:"title-container",children:Object(y.jsxs)("div",{className:"title-inner",children:[Object(y.jsx)("div",{children:e.title}),Object(y.jsx)(v,{height:6}),Object(y.jsx)(hn,{thickness:1}),Object(y.jsx)(v,{height:15}),Object(y.jsx)("div",{className:"title-tech",children:Object(y.jsxs)(ee.a,{container:!0,children:[Object(y.jsx)(ee.a,{sm:2,item:!0,style:{textAlign:"end"},children:Object(y.jsx)("span",{style:{display:"inline-block",border:"1px solid #7bb9d1",color:"white",backgroundColor:"#7bb9d1",fontWeight:600,padding:2,fontSize:14,marginRight:4,borderRadius:5,paddingLeft:6,paddingRight:6},children:"Tech"})}),Object(y.jsx)(ee.a,{sm:5,item:!0,style:{paddingLeft:20},children:e.techs.map((function(n){return Object(y.jsxs)("div",{className:"item",children:[Object(y.jsx)(Fe.a,{fontSize:"small",style:{fontSize:6,marginRight:4}}),n]},n)}))}),Object(y.jsx)(ee.a,{sm:5,children:Object(y.jsx)("div",{className:"title-date",children:Object(y.jsxs)("span",{style:{borderRadius:4,padding:"2px 4px",border:"1px solid rgba(0,0,0,0.2)"},children:["Project Date ",Object(y.jsx)(ye,{tex:"\\approx\\,",inline:!0}),g]})})})]})})]})})]}),Object(y.jsxs)("div",{style:{position:"absolute",width:"100%",overflow:"hidden"},children:[Object(y.jsx)("div",{style:{width:"100%",height:"100%",objectFit:"cover",position:"absolute",top:0,left:0,zIndex:2}}),Object(y.jsx)("img",{src:"".concat(e.thumbnail),style:{objectFit:"cover",width:"100%",transform:"translateY(".concat(e.thumbTransY,"px)")}})]}),Object(y.jsx)(T.a,{className:S()(l.thumbnailBtn,"thumbnail-btn")})]}),Object(y.jsxs)("div",{className:"image-preview",children:[Object(y.jsx)("div",{children:e.intro}),Object(y.jsx)(v,{height:10,className:"img-spacer"}),Object(y.jsx)("div",{style:{display:"flex",justifyContent:"center"},children:Object(y.jsx)("div",{className:"img-container",children:Object(y.jsx)("img",{src:e.thumbnail})})})]})]})})},He=function(){var n=A((function(n){return n.app.articles.portfolios})),e=Object(c.g)().pathname;n.length;return Object(y.jsx)($e,{dependencies:[e],children:Object(y.jsx)("div",{children:Object(we.cloneDeep)(n).sort((function(n,e){var t=parseInt(n.id.replace(/\D/g,""));return parseInt(e.id.replace(/\D/g,""))-t})).map((function(n,e){return Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(We,{portfolio:n},n.id),Object(y.jsx)(v,{height:12})]})}))})})},Ve=function(n){var e=n.portfolio,t=n.prevPortfolio,a=n.nextPortfolio,i=e.content,o=(e.id,e.intro,e.thumbnail,e.title),s=Object(c.g)().pathname,r=Object(c.f)(),l=qe({height:0});return Object(y.jsxs)($e,{dependencies:[s],children:[Object(y.jsxs)("div",{className:l.portfolio,children:[Object(y.jsx)("div",{children:Object(y.jsx)(oe,{returnCallback:function(){r.push(f.PORTFOLIO.path)}})}),Object(y.jsx)(v,{height:20}),Object(y.jsx)("h3",{style:{marginBottom:40},children:o}),Object(y.jsxs)("div",{className:"content-row",children:[Object(y.jsx)("h5",{children:" Contents "}),Object(y.jsx)(v,{height:10}),Object(y.jsx)(hn,{style:{marginTop:-12}}),Object(y.jsx)("p",{})]}),Object(y.jsx)("div",{className:"TOC-heading",children:Object(y.jsx)(ze.a,{markdown:(null===e||void 0===e?void 0:e.content)||"",hyperlink:!0})}),Object(y.jsx)(hn,{}),Object(y.jsx)(v,{}),Object(y.jsx)(De,{children:i||""})]}),Object(y.jsx)(v,{height:100}),Object(y.jsx)(v,{}),Object(y.jsxs)("div",{children:[Object(y.jsx)("div",{style:{position:"relative"},children:t&&Object(y.jsxs)(y.Fragment,{children:[Object(y.jsxs)("div",{className:l.nextPrev,children:[Object(y.jsx)(ae.a,{style:{marginBottom:-1}})," Newer"]}),Object(y.jsx)(We,{portfolio:t,decorator:Object(y.jsx)(ae.a,{style:{opacity:.2,transform:"scale(5)"}})})]})}),Object(y.jsx)(v,{}),Object(y.jsx)("div",{style:{position:"relative"},children:a&&Object(y.jsxs)(y.Fragment,{children:[Object(y.jsxs)("div",{className:l.nextPrev,children:["Older ",Object(y.jsx)(Be.a,{style:{marginBottom:-1}})]}),Object(y.jsx)(We,{portfolio:a,decorator:Object(y.jsx)(Be.a,{style:{opacity:.2,transform:"scale(5)"}})})]})})]})]})};var Ke,Xe=t.p+"static/media/WorkshopinAnalysis.402ecd08.pdf",Ye=t.p+"static/media/Combine_Notes_Linear_Algebra.6aed7a5f.pdf",Qe=t.p+"static/media/Question_and_Answer.028118b5.pdf",Je=function(n){var e=n.show,t=void 0!==e&&e,a=n.children;return Object(y.jsx)("div",{style:{transition:"opacity 0.35s ease-in-out",opacity:t?1:0},children:Object(y.jsx)(y.Fragment,{children:a})})},Ze=function(n){var e=n.stage,t=n.activeStage,a=n.children,i=t===e;return Object(y.jsx)(Je,{show:i,children:Object(y.jsx)(L.a,{in:i,timeout:500,children:Object(y.jsx)(y.Fragment,{children:a})})})},nt=t.p+"static/media/TN_01.c90e2df3.pdf",et=t.p+"static/media/TN_02.337c3b08.pdf",tt=t.p+"static/media/TN_03.9205b858.pdf",at=t.p+"static/media/TN_04.6dad924e.pdf",it=t.p+"static/media/TN_045.2068ad93.pdf",ot=t.p+"static/media/TN_05.cfa186c2.pdf",st=t.p+"static/media/TN_06.4f1cfb3b.pdf",rt=t.p+"static/media/TN_07.9edb624e.pdf",ct=t.p+"static/media/TN_08.2bd9ac95.pdf",lt=t.p+"static/media/TN_09.b1556ebc.pdf",dt=t.p+"static/media/TN_10.fce3e5cf.pdf",pt=t.p+"static/media/Midterm_Review_limsupinf.1e2193c7.pdf",ut=[{title:"Review of Linear Algebra and Multivariable Calculus ",link:nt},{title:Object(y.jsxs)(y.Fragment,{children:["Differentiability and ",Object(y.jsx)(ye,{tex:"C^1"})," Theorem"]}),link:et},{title:"Inverse and Implicit Function Theorem ",link:tt},{title:"Basic Concept and Properties of Limit Inferior and Limit Superior",link:at},{title:"More Examples of Limit Superior and Limit Inferior",link:it},{title:Object(y.jsxs)(y.Fragment,{children:["Midterm Review on ",Object(y.jsx)(ye,{tex:"\\limsup"})," and ",Object(y.jsx)(ye,{tex:"\\liminf"})," "]}),link:pt},{title:"Sequence and Series of Functions; Uniform and Nonuniform Convergence",link:ot},{title:"Consequence of Uniform Convergence",link:st},{title:Object(y.jsxs)(y.Fragment,{children:["Set Operations with Functions; Topology on ",Object(y.jsx)(ye,{tex:"\\mathbb{R}"})," Openness, Closedness and Compactness"]}),link:rt},{title:"Lebesgue Inner, Outer Measures and Lebesgue Measure",link:ct},{title:"Lebesgue Measurable Functions",link:lt},{title:"Lebesgue Integration and Convergence Theorems",link:dt}],mt=t.p+"static/media/TutorialNote01.6e54e0cb.pdf",ht=t.p+"static/media/TutorialNote02.b3893a4a.pdf",gt=t.p+"static/media/TutorialNote03.eb578e97.pdf",ft=t.p+"static/media/TutorialNote04.c4a50fec.pdf",_t=t.p+"static/media/TutorialNote05.4c1b94b1.pdf",bt=t.p+"static/media/TutorialNote06.f2953336.pdf",yt=t.p+"static/media/TutorialNote07.6adb08de.pdf",wt=t.p+"static/media/TutorialNote08.3e13cb05.pdf",vt=t.p+"static/media/TutorialNote09.c6fe4b65.pdf",xt=t.p+"static/media/TutorialNote10.2941efe7.pdf",kt=[{title:"Linear Systems; Echelon and Reduced Echelon Forms; Basic and Free Variables",link:mt},{title:"Vector Spaces, Linear Span and Linear Independence",link:ht},{title:"Linear Transformations",link:gt},{title:"Matrix Operations; Inverse, Transpose and Elementary Matrices; Determinants",link:ft},{title:"Vector Spaces Again, Bases and Dimensions",link:_t},{title:Object(y.jsxs)(y.Fragment,{children:["Important Subspaces of ",Object(y.jsx)(ye,{tex:"\\mathbb R^n"}),", Row, Column Spaces; Rank-Nullity Theorem"]}),link:bt},{title:"Coordinates and Matrix Representations of Linear Transformations; Generalized Nullity-Rank Theorem",link:yt},{title:"Eigenvalues and Eigenvectors for Square Matrices; Algebraic and Geometric Multiplicity of Eigenvalues; Diagonalization",link:wt},{title:'Inner Product, "Norm" (i.e., "Length"); Orthogonal and Orthonormal Sets, Orthogonal Complement; Gram-Schmidt Orthogonalization Process',link:vt},{title:"Orthogonal Projection onto Finite Dimensional Subspaces; Minimizer Obtained by Orthogonal Projections",link:xt}],St=t.p+"static/media/TN_00.51d1ba8e.pdf",Tt=t.p+"static/media/TN_01.716a26e2.pdf",Ct=t.p+"static/media/TN_02.009428e5.pdf",Et=t.p+"static/media/TN_03.479dda15.pdf",At=t.p+"static/media/TN_04.ac918127.pdf",jt=t.p+"static/media/TN_05.e6aeca59.pdf",Rt=t.p+"static/media/TN_06.83510eba.pdf",It=t.p+"static/media/TN_07.566a16bc.pdf",Ot=t.p+"static/media/TN_08.b7a801ee.pdf",Nt=t.p+"static/media/TN_09.f2722170.pdf",Dt=t.p+"static/media/TN_10.6d1f38b4.pdf",Pt=[{title:"Mathematical Induction and Inequalities",link:St},{title:"Informal Logic, Sets and Functions",link:Tt},{title:"Countability",link:Ct},{title:"Infimum and Supremum",link:Et},{title:"Sequences",link:At},{title:"Midterm-Review",link:t.p+"static/media/review.c9c071ab.pdf"},{title:"Continuous Functions",link:jt},{title:"Differentiation (Part I): L'Hopital's Rule & Mean-Value Theorem",link:Rt},{title:"Differentiation (Part II): Taylor Series (Skipped in this course)",link:It},{title:"Riemann Integral (Part I): Riemannn Integrability",link:Ot},{title:"Riemann Integral (Part II): Miscellaneous",link:Nt},{title:"Riemann Integral (Part III): Improper Integral (Skipped in this course)",link:Dt}];t(779);!function(n){n.LINEAR_ALGEBRA="Linear Algebra",n.MATHEMATICAL_ANALYSIS="Mathematical Analysis",n.REAL_ANALYSIS="Real Analysis"}(Ke||(Ke={}));var Lt=[{category:Ke.LINEAR_ALGEBRA,list:kt},{category:Ke.MATHEMATICAL_ANALYSIS,list:Pt},{category:Ke.REAL_ANALYSIS,list:ut}],$t=(t(780),t(286)),Mt=t(138),Ft=function(n){var e=n.split(","),t=Object(D.a)(e,3),a=t[0],i=t[1],o=t[2];return{src:a,width:Number(i),height:Number(o)}},Ut=(t(844),t(7)),Bt=t(289),zt=t.n(Bt),qt=t(290),Gt=t.n(qt),Wt=t(288),Ht=t.n(Wt),Vt=function(){return{scrollContainer:document.querySelector(".os-viewport")}},Kt=function(n){var e=n.id,t=void 0===e?"":e,a=n.activeArticle,i=n.className,o=void 0===i?"":i,s=n.ref,r=void 0===s?void 0:s;return Object(y.jsxs)(x.a,{ref:r,id:t,className:S()(o),options:{scrollbars:{autoHide:"leave",autoHideDelay:100}},children:[Object(y.jsxs)("div",{className:"content-row",children:[Object(y.jsx)("h5",{children:" Contents "}),Object(y.jsx)(v,{height:10}),Object(y.jsx)(hn,{style:{marginTop:-12}}),Object(y.jsx)("p",{})]}),Object(y.jsx)("div",{className:"TOC-heading",children:Object(y.jsx)(ze.a,{markdown:(null===a||void 0===a?void 0:a.content)||"",hyperlink:!0})}),Object(y.jsx)(hn,{}),Object(y.jsx)(v,{})]})};function Xt(n){var e=n.width,t=void 0===e?10:e;return Object(y.jsx)("div",{style:{width:t,minWidth:t,userSelect:"none"}})}var Yt=Object(I.a)((function(n){return{tagsRow:function(n){return{display:"flex",flexWrap:"wrap",listStyle:"none",paddingLeft:"0 !important",paddingRight:20,marginBottom:6,"& div":{"& code":{pointerEvents:n.clickable?"auto":"none",padding:"2px 5px",borderRadius:3,color:"white",backgroundColor:"#3b62914c !important",marginRight:10,transition:"opacity 0.1s ease-in-out",cursor:"pointer",userSelect:"none","&:hover":{opacity:.5}}}}}}}));function Qt(n){var e=n.tags,t=n.styles,a=void 0===t?{}:t,i=n.clickable,o=void 0===i||i,s=n.onClick,r=void 0===s?function(){}:s,l=Object(c.f)(),d=Yt({clickable:o});return Object(y.jsx)("div",{className:S()(d.tagsRow,"tags-container"),style:Object(_.a)({},a),children:Object(Pe.a)(e).sort((function(n,e){return n.localeCompare(e)})).map((function(n,e){return Object(y.jsx)("div",{onClick:function(){l.push("".concat(f.BLOG.path,"/category/").concat(n)),r()},children:Object(y.jsx)(De,{children:"`".concat(n,"`")})},e)}))})}t(845);var Jt=Object(I.a)({articleStyle:{"& .mtext, & .mjx-char":{fontSize:"1.15rem"},"& h4":{"& ~ h5":{marginTop:40}},"& h5":{"& ~ h6":{marginTop:40}},"& a":{color:"#2a718d !important"}},codeStyle:{"& code":{fontSize:"0.85em !important",background:"#F5F5F5!important",border:"1px solid rgba(0,0,0,0.03)",borderRadius:4},"& pre":{margin:"1em 0","& code":{fontSize:"0.85em !important",border:"none"}}},floatingTitle:{display:"block",paddingRight:10,borderRight:"6px solid transparent"},activeFloatingTitle:{borderRight:"6px solid #6fa8cd",transition:"border-color 0.2s ease-in-out"},inactiveFloatingTitle:{color:"inherit",borderRight:"6px solid transparent",backgroundColor:"inherit"},floatingTOC:Object(Ut.a)({"& .TOC-heading":{fontSize:16},paddingTop:0,"& .content-row":{position:"sticky",paddingTop:20,top:0,backdropFilter:"blur(8px)",background:"rgba(255,255,255,0.7)"},"& h5":{marginTop:0},overflowY:"scroll",maxHeight:650,transform:"translate(-500px,-30px)",borderRadius:6,padding:20,backgroundColor:"white",border:"1px solid rgba(0,0,0,0.1)",width:360,transition:"top 0.1s linear",position:"sticky",top:70,left:50,opacity:0,pointersEvent:"none","&.open":{opacity:1,transition:"opacity 0.1s linear",pointersEvent:"auto"}},"@media (max-width:1700px)",{opacity:"0 !important",pointersEvent:"none !important"})},{name:"custom-articles"}),Zt=function(n){return n.length>25?n.substring(0,25)+" ...":n},na=function(){var n=E(),e=A((function(n){return n.app.articles.archive.open}));return Object(y.jsxs)(T.a,{onClick:function(){n(Sn.actions.updateArchive({open:!e}))},style:{display:"flex",alignItems:"center"},children:[Object(y.jsx)(zt.a,{style:{marginRight:8},fontSize:"small"})," Archive"]})};function ea(n){var e=n.showComment,t=n.showTOC,a=n.articles,o=void 0===a?[]:a,s=n.activeArticle,r=n.navToArticle,l=(n.style,n.className),d=void 0===l?"":l,p=Jt(),u=E(),h=Object(c.g)().pathname,g=Object(c.f)(),_=Object(i.useRef)(null),b=Object(i.useRef)(""),w=A((function(n){return n.app.leftMenuIsOpen})),x=o.length,k=o.findIndex((function(n){return n.title===(null===s||void 0===s?void 0:s.title)})),C=o[Math.max(k-1,0)],j=o[Math.min(k+1,x-1)],R=null===s||void 0===s?void 0:s.date,I=Object(i.useState)(null),O=Object(D.a)(I,2),N=O[0],P=O[1],L=document.querySelector(".cross-sensor"),$=Object(i.useCallback)((function(){var n=document.querySelectorAll(".md-title"),e=n.length;if(e>0){var t=0;for(var a in Array.from(n)){if(!(n[parseInt(a)].getBoundingClientRect().top<140)){t=Math.max(parseInt(a)-1,0);break}parseInt(a)===e-1&&(t=e-1)}var i=Array.from(document.querySelectorAll(".".concat(p.floatingTOC," a")));n.forEach((function(n,e){var a=n.id,o=i.find((function(n){return decodeURI(n.href).includes("#".concat(a))}));if(o)if(e===t){var s=document.querySelector("#floating-toc .os-viewport"),r=o.offsetTop;null===s||void 0===s||s.scrollTo({left:0,top:r-400,behavior:"smooth"}),o.className=S()(p.floatingTitle,p.activeFloatingTitle)}else o.className=S()(p.floatingTitle,p.inactiveFloatingTitle)}))}}),[s]),M=Object(i.useCallback)(Object(we.debounce)((function(){L&&P(L.getBoundingClientRect().top),$()}),100),[L,s,h]),F=Vt().scrollContainer,U=A((function(n){return n.app.articles.defaultTocCrossed}));return Object(i.useEffect)((function(){b.current,null===s||void 0===s||s.id;s&&s.id}),[s]),Object(i.useEffect)((function(){return L?(M(),F.addEventListener("scroll",M)):P(300),function(){L&&F.removeEventListener("scroll",M)}}),[L]),Object(i.useEffect)((function(){null!=N&&u(N<300?Sn.actions.setDefaultTocCrossed(!0):Sn.actions.setDefaultTocCrossed(!1))}),[N]),s?Object(y.jsx)(_e.a.Provider,{options:be,children:Object(y.jsx)("div",{className:S()("articles",d,w?"":"expanded"),style:{position:"relative"},children:Object(y.jsx)("div",{className:"articles-container",children:Object(y.jsx)(m.a,{children:Object(y.jsx)("div",{style:{display:"flex",justifyContent:"center",width:"100%",margin:0},children:Object(y.jsxs)("div",{className:S()("content",p.codeStyle),children:[s&&s.hasToc&&Object(y.jsx)("div",{style:{position:"absolute",height:"100%"},children:Object(y.jsx)(Kt,{id:"floating-toc",className:S()(p.floatingTOC,U?"open":""),activeArticle:s})}),Object(y.jsx)("div",{className:S()("article",p.articleStyle),children:s&&Object(y.jsxs)($e,{dependencies:[h],children:[Object(y.jsxs)("div",{children:[" ",Object(y.jsx)(oe,{returnCallback:function(){localStorage.getItem("scroll")?g.goBack():g.push(f.BLOG.path)}})," "]}),Object(y.jsx)(v,{height:20}),Object(y.jsxs)("div",{ref:_,children:[Object(y.jsxs)("div",{className:"first-row",children:[Object(y.jsx)("div",{className:"date",children:"".concat(null===R||void 0===R?void 0:R.month," ").concat(null===R||void 0===R?void 0:R.date,", ").concat(null===R||void 0===R?void 0:R.year)}),Object(y.jsxs)("div",{className:"archive-button-row",children:[Object(y.jsx)(Ht.a,{documentTitle:s.title,pageStyle:function(){return"\n          @page {\n            margin: 30mm;\n    }\n          @media print {\n            html, body {\n            font-color: black;\n          font-size: 130%;\n          height: initial !important;\n          overflow: initial !important;\n      }\n          body .archive-button-row {\n            display: none;\n\n      }\n    }\n  "},trigger:function(){return Object(y.jsxs)(T.a,{children:[Object(y.jsx)(Gt.a,{style:{transform:"scale(0.9)"}}),Object(y.jsx)(Xt,{width:7}),"Print"]})},content:function(){return _.current}}),Object(y.jsx)(Xt,{width:5}),Object(y.jsx)(na,{})]})]}),Object(y.jsx)(v,{}),Object(y.jsx)(hn,{thickness:3}),Object(y.jsx)(v,{height:20}),Object(y.jsx)("h3",{style:{marginBottom:10},children:s.title}),Object(y.jsx)(Qt,{tags:s.tags,clickable:!1,styles:{marginBottom:s.hasToc?0:30}}),t&&s.hasToc&&Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(Kt,{activeArticle:s}),Object(y.jsx)("div",{className:"cross-sensor"})]}),Object(y.jsx)(De,{children:(null===s||void 0===s?void 0:s.content)||""}),Object(y.jsx)(v,{}),Object(y.jsx)(hn,{thickness:3}),Object(y.jsx)(v,{height:10})]})]})}),s&&s.id&&e&&Object(y.jsx)("div",{style:{userSelect:"none"}}),s&&Object(y.jsx)(y.Fragment,{children:Object(y.jsxs)("div",{className:"prev-next-article-buttons-row",children:[Object(y.jsx)("div",{children:k>0&&Object(y.jsxs)(T.a,{onClick:function(n){r&&r(C.articleRouteId,n)},title:C.title,children:[Object(y.jsx)(ae.a,{})," ",Zt(C.title)]})}),Object(y.jsx)("div",{children:k<o.length-1&&Object(y.jsxs)(T.a,{onClick:function(n){r&&r(j.articleRouteId,n)},title:j.title,children:[Zt(j.title)," ",Object(y.jsx)(Be.a,{})]})})]})})]})})})})})}):null}t(846),t(847);var ta=t(1166),aa=t(1171),ia=["open","anchor","container","children","drawerWidth"],oa=Object(I.a)((function(n){return Object(aa.a)({root:{display:function(n){return n.drawerWidth}},appBar:{transition:n.transitions.create(["margin","width"],{easing:n.transitions.easing.sharp,duration:n.transitions.duration.leavingScreen})},appBarShift:{width:function(n){return"calc(100% - ".concat(n.drawerWidth,"px)")},transition:n.transitions.create(["margin","width"],{easing:n.transitions.easing.easeOut,duration:n.transitions.duration.enteringScreen}),marginRight:function(n){return n.drawerWidth}},title:{flexGrow:1},hide:{display:"none"},drawer:{width:function(n){return n.drawerWidth},flexShrink:0},drawerPaper:{width:function(n){return n.drawerWidth}},drawerHeader:Object(_.a)(Object(_.a)({display:"flex",alignItems:"center",padding:n.spacing(0,1)},n.mixins.toolbar),{},{justifyContent:"flex-start"}),content:{flexGrow:1,padding:n.spacing(3),transition:n.transitions.create("margin",{easing:n.transitions.easing.sharp,duration:n.transitions.duration.leavingScreen}),marginRight:function(n){return-n.drawerWidth}},contentShift:{transition:n.transitions.create("margin",{easing:n.transitions.easing.easeOut,duration:n.transitions.duration.enteringScreen}),marginRight:0}})})),sa=function(n){var e=n.open,t=n.anchor,a=void 0===t?"right":t,i=(n.container,n.children),o=n.drawerWidth,s=void 0===o?400:o,r=Object(b.a)(n,ia),c=oa({drawerWidth:s});return Object(y.jsx)(ta.a,Object(_.a)(Object(_.a)({},r),{},{variant:"temporary",anchor:a,open:e,classes:{paper:c.drawerPaper},ModalProps:{keepMounted:!0},children:Object(y.jsx)("div",{children:i})}))};t(848);function ra(n){var e=n.open,t=n.toggleOpen,a=n.activeArticleId,i=n.articles,o=n.navToArticle;return Object(y.jsx)(sa,{onClose:t,open:e,drawerWidth:600,children:Object(y.jsxs)("div",{className:"achive-drawer",children:[Object(y.jsx)(v,{}),Object(y.jsx)("div",{children:i.map((function(n){var e=n.date,i=n.title,s=(n.intro,n.articleRouteId),r=s===a,c=e.date,l=e.month,d=e.year;return Object(y.jsxs)("div",{className:S()("toc-home-row",r?"active":""),children:[Object(y.jsx)("div",{className:"toc-home-title",children:Object(y.jsx)("span",{className:"toc-home-title-text",onClick:function(){o(s),t()},children:Object(y.jsx)(De,{children:i})})}),Object(y.jsx)(hn,{}),Object(y.jsx)("div",{className:"toc-home-date",children:Object(y.jsx)("i",{children:"".concat(l," ").concat(c,", ").concat(d)})})]},i)}))})]})})}var ca=function(){var n=A((function(n){return n.app.articles.archive.open})),e=Object(c.f)(),t=A((function(n){return n.app.articles.blogs})),a=E(),i=A((function(n){return n.app.articles.activeBlogArticleId}));return Object(y.jsx)(ra,{articles:t,open:n,toggleOpen:function(){a(Sn.actions.updateArchive({open:!n}))},activeArticleId:i,navToArticle:function(n){e.push(g(Hn(n)))}})},la=t(291),da=t.n(la),pa=(t(849),Object(I.a)({anchorContainer:{paddingTop:40},anchorWithoutAnchorBehaviour:{textDecoration:"none !important","&:hover":{color:"inherit !important"}},byTopicTOC:{"& .tag":{textTransform:"capitalize",fontVariant:"small-caps",fontSize:"1.4rem",fontWeight:550,color:"#394852"},"& .entity":{transition:"opacity 0.1s ease-in-out",marginBottom:15},"& .title":{"&:hover":{opacity:.5},display:"flex","& .bullet":{transform:"translateY(1px)",display:"flex",justifyContent:"center",alignItems:"center",width:30,"& svg":{fontSize:8}},"& .content":{fontWeight:500}},"& .intro":{marginLeft:30,fontSize:15,opacity:.85},"& p":{marginBottom:2}}},{name:"TOC-articles"}));function ua(n){var e=n.articles,t=n.sortPostBy,a=n.navToArticle,i=void 0===a?function(){}:a,s=(n.navToArticleNewTab,n.activeTag),r=void 0===s?"":s,c=pa(),l=function(){var n=new Set;return e.map((function(e){return e.tags.forEach((function(e){return n.add(e)}))})),Array.from(n).sort((function(n,e){return n.localeCompare(e)}))}();return Object(y.jsxs)("div",{className:"toc-home",children:["topic"===t&&Object(y.jsx)("div",{className:c.byTopicTOC,children:l.map((function(n){return Object(y.jsx)(o.a.Fragment,{children:Object(y.jsx)($e,{children:Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)("div",{className:c.anchorContainer,children:Object(y.jsx)("a",{id:n},"anchor-".concat(n))}),Object(y.jsxs)("div",{children:[Object(y.jsx)("div",{className:"tag",children:n}),Object(y.jsx)(v,{height:8}),Object(y.jsx)(hn,{}),Object(y.jsx)(v,{height:16}),Object(y.jsx)("div",{children:e.filter((function(e){return e.tags.indexOf(n)>-1})).map((function(n){return Object(y.jsxs)("div",{className:"entity",children:[Object(y.jsx)("a",{className:c.anchorWithoutAnchorBehaviour,href:g(n.articleRouteId),onClick:function(e){i(n.articleRouteId,e)},children:Object(y.jsxs)("div",{className:"title",children:[Object(y.jsx)("div",{className:"bullet",children:Object(y.jsx)(Fe.a,{})}),Object(y.jsx)("div",{className:"content",children:n.title})]})}),Object(y.jsx)("div",{className:"intro",children:Object(y.jsx)(De,{children:n.intro})})]},n.articleRouteId)}))})]},n)]})})},n)}))}),"date"===t&&Object(y.jsx)(y.Fragment,{children:e.map((function(n){var e=n.date,t=n.title,a=n.intro,o=n.articleRouteId,s=n.edited,l=(n.tags,e.date),d=e.month,p=e.year,u=function(){return""===r||!!n.tags.includes(r)};return Object(y.jsxs)(L.a,{in:u(),children:[Object(y.jsx)(v,{height:10}),Object(y.jsx)(P.a,{in:u(),children:Object(y.jsxs)("div",{className:"toc-home-row",children:[Object(y.jsxs)("div",{className:"toc-home-date",children:[Object(y.jsx)("div",{children:Object(y.jsxs)("i",{children:["".concat(d," ").concat(l),","]})}),Object(y.jsxs)("div",{children:[" ",Object(y.jsx)("i",{children:p})," "]}),s&&Object(y.jsxs)("div",{className:"edited",children:[Object(y.jsx)(v,{height:5}),Object(y.jsx)("div",{children:Object(y.jsx)("i",{children:"Last Edit:"})}),Object(y.jsx)("div",{children:Object(y.jsxs)("i",{children:[" `$",s.month," $",s.date,"`"]})}),Object(y.jsxs)("div",{children:[" ",Object(y.jsx)("i",{children:s.year})," "]})]})]}),Object(y.jsxs)("div",{className:"toc-home-title",children:[Object(y.jsx)("div",{className:"toc-home-title-text",children:Object(y.jsx)("div",{className:"toc-home-text-container",children:Object(y.jsx)("a",{className:c.anchorWithoutAnchorBehaviour,href:g(o),onClick:function(n){return i(o,n)},children:Object(y.jsx)(De,{children:t})})})}),Object(y.jsx)(v,{height:8}),Object(y.jsx)(Qt,{tags:n.tags}),Object(y.jsx)(v,{height:12}),Object(y.jsx)("div",{className:"toc-home-intro",children:Object(y.jsx)(De,{children:a})}),Object(y.jsx)(v,{height:5})]})]},t)})]},n.articleRouteId)}))})]})}var ma=t(294),ha=t.n(ma),ga=t(295),fa=t.n(ga),_a=t(135),ba=t(186),ya=t(296),wa=t(292),va=["iconContainerHeight","className","inputRef"],xa="#F3F3F3",ka="rgba(0,0,0,0)",Sa=Object(I.a)({searchInput:{backgroundColor:xa,outline:"none",padding:10,borderRadius:80,border:"1px solid ".concat(ka),borderLeft:"none","&:focus-visible":{}}});function Ta(n){var e=n.iconContainerHeight,t=void 0===e?35:e,a=n.className,i=n.inputRef,o=Object(b.a)(n,va),s=Sa();return Object(y.jsxs)("div",{style:{display:"flex"},children:[Object(y.jsxs)("div",{style:{maxHeight:t,padding:22,display:"flex",justifyContent:"center",alignItems:"center",backgroundColor:xa,borderTopLeftRadius:80,border:"1px solid ".concat(ka),borderRight:"none",borderBottomLeftRadius:80},children:[" ",Object(y.jsx)(Y.b,{})," "]}),Object(y.jsx)("input",Object(_.a)(Object(_.a)({ref:i},o),{},{style:{flex:1,borderTopLeftRadius:0,borderBottomLeftRadius:0},className:S()(a||"",s.searchInput)}))]})}var Ca=function(){var n=A((function(n){return n.app.articles.blogs})),e=A((function(n){return n.app.articles.activeBlogArticleId})),t=n.find((function(n){return n.articleRouteId===e})),a=function(){var e={};return n.forEach((function(n){n.tags.forEach((function(n){var t=ve.a.get(e,n,0)+1;ve.a.set(e,n,t)}))})),e}();return{articles:n,activeBlogArticleId:e,activeArticle:t,tagAndCounts:a}},Ea=t(1163),Aa=t(293),ja=t.n(Aa),Ra=Object(I.a)({searchItem:{"& .tags":{display:"flex",justifyContent:"flex-end","& div":{marginBottom:6,display:"flex",justifyContent:"flex-end","&:not(.tags-container)":{height:21}}},"& p":{marginBottom:0},"& .info":{padding:10},"& .info:hover":{backgroundColor:"rgba(220,220,220)"}}});function Ia(){var n=Object(i.useState)([]),e=Object(D.a)(n,2),t=e[0],a=e[1],o=Object(i.useState)(""),s=Object(D.a)(o,2),r=s[0],l=s[1],d=Object(i.useRef)(null),p=Object(c.f)(),m=Ra(),h=Object(i.useRef)(null),f=Object(i.useRef)({});Object(i.useState)((function(){h.current=ja()((function(){var n=this;this.field("tag"),this.field("tags"),this.field("title"),this.field("intro"),this.field("content"),console.log("indexing ..."),wa.forEach((function(e,t){var a=t.toString(),i=e.intro,o=e.tag,s=void 0===o?"":o,r=e.tags,c={intro:i,tag:s,tags:void 0===r?"":r,title:e.title,content:e.content};f.current[a]=c,n.add(Object(_.a)(Object(_.a)({},c),{},{id:a}))}))}))}));var b=Object(we.debounce)((function(n){l(n.target.value);var e=n.target.value;if(e){var t,i=null===h||void 0===h||null===(t=h.current)||void 0===t?void 0:t.search(e),o=(null===i||void 0===i?void 0:i.sort((function(n,e){return e.score-n.score})).map((function(n){var e,t=n.ref,a=(n.matchData,n.score,null===(e=f.current)||void 0===e?void 0:e[t]);return{intro:a.intro,tag:a.tag,tags:a.tags,title:a.title,content:a.content}})))||[];a(o)}else a([])}),300),w=Object(i.useRef)(null),k=Object(i.useRef)(null),S=Object(i.useState)(!0),T=Object(D.a)(S,2),C=T[0],E=T[1],A=Object(i.useRef)(null),j=function(n){var e=n.ref,t=Object(i.useState)(!0),a=Object(D.a)(t,2),o=a[0],s=a[1];return Object(i.useEffect)((function(){function n(n){e.current&&!e.current.contains(n.target)?s(!0):s(!1)}return document.addEventListener("mousedown",n),function(){document.removeEventListener("mousedown",n)}}),[]),{outsideClicked:o,setOutsideClicked:s}}({ref:k}),R=j.outsideClicked,I=j.setOutsideClicked,O=Ca().tagAndCounts,N=Object.keys(O).filter((function(n){return n.indexOf(r)>-1}));return Object(i.useEffect)((function(){R?setTimeout((function(){E(!1)}),300):E(!0)}),[R]),Object(y.jsxs)(y.Fragment,{children:[Object(y.jsxs)("div",{style:{position:"relative"},ref:k,children:[Object(y.jsx)(Ta,{placeholder:"Tag, title or content",onChange:b,inputRef:A}),!R&&Object(y.jsx)("div",{onClick:function(){a([]),l(""),A.current&&(I(!1),A.current.value="")},style:{position:"absolute",top:"50%",right:10,transform:"translateY(-50%)",cursor:"pointer"},children:Object(y.jsx)(Y.c,{style:{fontSize:26}})}),C&&Object(y.jsx)("div",{ref:d,children:Object(y.jsx)(Ea.a,{in:!R,style:{transformOrigin:"top center"},children:Object(y.jsx)(x.a,{ref:w,style:{transition:"height 0.3s ease-in-out",marginTop:5,borderRadius:4,maxHeight:u,position:"absolute",zIndex:1,width:"100%",overflowY:"scroll",backgroundColor:"white",padding:10,boxShadow:on},options:{scrollbars:{autoHide:"leave",autoHideDelay:100}},children:Object(y.jsxs)("div",{style:{margin:10,overflow:"hidden"},children:[N.length>0&&Object(y.jsx)(y.Fragment,{children:Object(y.jsxs)("div",{style:{marginLeft:20,display:"flex",opacity:.6},children:[Object(y.jsx)("div",{style:{width:144},children:" Searchable Tags "}),Object(y.jsx)("div",{style:{flex:1,borderLeft:"4px solid #3b629126",paddingLeft:20},children:N.sort((function(n,e){return n.localeCompare(e)})).join(", ")})]})}),0==t.length&&r&&Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(v,{}),Object(y.jsx)("div",{children:"No result was found."})]}),t.length>0&&t.map((function(n){var e=n.intro,t=n.title,a=n.tag,i=n.tags,o=(a||i).split(",").map((function(n){return n.trim()}));return Object(y.jsx)("div",{className:m.searchItem,style:{padding:"10px 10px",cursor:"pointer"},children:Object(y.jsxs)("div",{style:{display:"flex"},children:[Object(y.jsxs)("div",{style:{flex:1},className:"info",onClick:function(){p.push(g(Kn(t))),I(!0)},children:[Object(y.jsx)("div",{style:{color:"##394852",fontWeight:600,fontSize:"1.04rem"},children:Object(y.jsx)(De,{children:t})}),Object(y.jsx)("div",{style:{color:"##394852",fontSize:"0.86rem",opacity:.7},children:Object(y.jsx)(De,{children:e})})]}),Object(y.jsx)("div",{className:"tags",style:{width:180,paddingTop:10,paddingLeft:20,display:"flex",justifyContent:"flex-end"},children:Object(y.jsx)(Qt,{tags:o,onClick:function(){return I(!0)}})})]})},t)}))]})})})})]}),Object(y.jsx)(v,{height:20})]})}var Oa=Object(y.jsx)(Xt,{width:4}),Na={web:Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(ha.a,{style:{transform:"scale(0.8)",marginLeft:-5}}),Oa]}),math:Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(fa.a,{style:{transform:"scale(0.85)",marginLeft:-7}}),Oa]}),coding:Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(en.a,{style:{transform:"scale(1.2)"}}),Oa,Oa]}),"deep-learning":Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(_a.b,{style:{transform:"scale(1)",marginLeft:0}}),Oa,Oa]}),react:Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(_a.a,{}),Oa,Oa]}),python:Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(ba.b,{style:{transform:"scale(1.1)"}}),Oa,Oa]}),javascript:Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(ba.a,{}),Oa,Oa]}),all:Object(y.jsxs)(y.Fragment,{children:[Object(y.jsx)(ya.a,{})," ",Oa,Oa]})},Da=Object(I.a)({sortRow:{display:"flex","& button":{fontWeight:500,letterSpacing:"unset",fontSize:14,transition:"opacity 0.1s ease-in-out","&.active":{opacity:1,background:"#3b62918c",color:"white",borderColor:"transparent"},"&:hover":{opacity:.87},opacity:.5,border:"1px solid #31343a",fontVariant:"small-caps",width:140,padding:"3px 14px",textTransform:"none"}}},{name:"my-blog"}),Pa=function(n){return Object.keys(Na).includes(n)?Na[n]:Object(y.jsx)(y.Fragment,{})},La=function(){return function(n){var e=[];return n.keys().map((function(t,a){e.push(n(t))})),e}(t(850))};var $a=[{path:f.ROOT.path,component:function(){var n=Object(c.g)().pathname,e=A((function(n){return n.app.articles.defaultTocCrossed})),t=$a.find((function(n){return n.path===f.ROOT.path})),a=A((function(n){return n.app.leftMenuIsOpen})),o=A((function(n){return n.app.activeNavRoute})),s=Object(i.useRef)(null),r=O(),u=Object(i.useRef)(""),h=function(){return document.querySelector(".os-viewport").scrollTo(0,0)};return Object(i.useEffect)((function(){var e=N(u.current||""),t=N(n||""),a=/\/blog\/article\//g.test(n||"");e&&t?a&&h():h(),u.current=n}),[n]),Object(y.jsx)(y.Fragment,{children:Object(y.jsxs)("div",{style:{height:"100vh",overflowY:"hidden"},className:S()(r.higherContainer,e?"has-article":""),children:[Object(y.jsx)(j,{}),Object(y.jsx)(x.a,{id:"top-overlayscroll-container",ref:s,className:"main-content",style:{height:"calc(100vh - ".concat(o===f.LANG_STIDY_FF14.path?0:p,"px)"),width:"100%",overflow:"hidden"},children:Object(y.jsx)(m.a,{className:S()("total-container"),children:Object(y.jsxs)("div",{className:"right",style:{paddingLeft:a?l:d},children:[Object(y.jsx)(v,{height:40}),t&&t.routes&&Object(y.jsx)(c.c,{children:t.routes.map((function(n,e){return Object(y.jsx)(c.a,{path:n.path,component:n.component,exact:n.exact},n.path)}))}),Object(y.jsx)(v,{}),Object(y.jsx)(v,{}),Object(y.jsx)(v,{}),Object(y.jsx)(v,{}),Object(y.jsx)(v,{})]})})})]})})},title:"",routes:[{title:f.ABOUT.title,path:f.ABOUT.path,component:function(){var n=Object(Bn.a)({filename:"James-Lee-Portfolio.pdf",method:"open",page:{margin:7}}),e=(n.toPDF,n.targetRef);return Object(y.jsxs)("div",{className:"about-me",children:[Object(y.jsx)("div",{style:{display:"flex",justifyContent:"center"}}),Object(y.jsx)(v,{}),Object(y.jsx)(v,{}),Object(y.jsxs)(B,{children:[Object(y.jsxs)("div",{ref:e,children:[Object(y.jsx)("h4",{children:"About Me "}),Object(y.jsx)(hn,{}),Object(y.jsx)(v,{}),Object(y.jsxs)("div",{className:"self-intro",children:[Object(y.jsxs)("div",{className:"avatar-row left",children:[Object(y.jsx)("div",{style:{width:"100%",display:"flex",justifyContent:"center"},children:Object(y.jsx)(An,{height:160,children:Object(y.jsx)("img",{src:En})})}),Object(y.jsx)(v,{}),Object(y.jsx)("div",{className:"name-row",children:"Ching-Cheong Lee"}),Object(y.jsx)(v,{height:10}),Object(y.jsxs)("div",{className:"job-title-row",children:[Object(y.jsx)("span",{className:"software-engineer",children:"Software Engineer"})," in ",Object(y.jsx)("br",{})," Hong Kong"]}),Object(y.jsx)(v,{height:15}),Object(y.jsx)(Un,{})]}),Object(y.jsxs)("div",{className:"right custom-indent",style:{borderLeft:"1px solid rgba(0,0,0,0.1)",paddingLeft:40},children:[Object(y.jsx)("p",{children:"Graduated from HKUST with an M.Phil degree in Mathematics, worked in both academic and nonacademic fields with solid knowledge in frontend programming."}),Object(y.jsxs)("p",{children:["I used to have strong interest in deep learning in computer vision. ",Object(y.jsx)("b",{children:"Recently"})," my interest drifts towards software development, especially on ",Object(y.jsx)("i",{children:"backend and cloud architecture"})," such as:",Object(y.jsxs)("ul",{style:{marginTop:10},children:[Object(y.jsxs)("li",{style:{textIndent:0},children:["Choices of Databases",Object(y.jsxs)("ul",{style:{opacity:.5},children:[Object(y.jsx)("li",{children:"Mongo?"}),Object(y.jsx)("li",{children:"Rdbms?"}),Object(y.jsx)("li",{children:"Why not both? What else?"})]})]}),Object(y.jsxs)("li",{style:{textIndent:0},children:["Choices of Message Brokers",Object(y.jsxs)("ul",{style:{opacity:.5},children:[Object(y.jsx)("li",{children:"RabbitMQ?"}),Object(y.jsx)("li",{children:"SQS?"}),Object(y.jsx)("li",{children:"Kafka?"}),Object(y.jsx)("li",{children:"Why?"})]})]}),Object(y.jsxs)("li",{style:{textIndent:0},children:["Choices and Tradeoffs of Different Ways to Deploy a Service",Object(y.jsxs)("ul",{style:{opacity:.5},children:[Object(y.jsx)("li",{children:"Why EC2?"}),Object(y.jsx)("li",{children:"Why ECS?"}),Object(y.jsx)("li",{children:"Why EKS?"}),Object(y.jsx)("li",{children:"Why the heck someone holds a static react page using EC2? Burning 3x USD/month just for a static page?"})]})]}),Object(y.jsxs)("li",{style:{textIndent:0},children:["A ",Object(y.jsx)("b",{children:"combo"})," of services such as sqs, dynamodb, lambda functions, cloudwatch events for scheduled jobs, etc."]})]})]}),Object(y.jsx)("p",{children:"I have been a game artist as well in mainland China. My art portfolio can be found in"}),Object(y.jsx)("a",{href:"https://www.artstation.com/checkercc",target:"_blank",style:{borderRadius:8,padding:"2px 10px 5px 10px",boxShadow:rn},children:"https://www.artstation.com/checkercc"}),Object(y.jsx)("p",{style:{textIndent:0,marginTop:16},children:"Recent artworks are mostly related to Final Fantasy 14."})]})]}),Object(y.jsx)(v,{}),Object(y.jsx)(v,{}),Object(y.jsx)("h4",{children:"Education"}),Object(y.jsx)(hn,{}),Object(y.jsx)(v,{}),Object(y.jsx)(ln,{items:[{title:"MicroMaster in A.I. and Programming",date:"03/2019 - 06/2019",company:"Tecky Academy, Hong Kong"},{title:"M.Phil. in Mathematics",date:"09/2012 - 08/2014",company:"The Hong Kong University of Science and Technology, Hong Kong",description:Object(y.jsxs)("div",{children:["Thesis: ",Object(y.jsx)(un,{href:"https://lbezone.ust.hk/pdfviewer/web/viewer.php?file=aHR0cHM6Ly9sYmV6b25lLnVzdC5oay9vYmovMS9vL2IxMzM0NDg0L2IxMzM0NDg0LnBkZg==#page=1",title:"Interpolating Sequences and Extremal Problems"})]})},{title:"B.Sc. in Mathematics (First Class Honor)",date:"09/2009 - 06/2012",company:"The Hong Kong University of Science and Technology, Hong Kong"}]}),Object(y.jsx)(v,{}),Object(y.jsx)(Cn,{}),Object(y.jsx)(v,{}),Object(y.jsx)(fn,{})]}),Object(y.jsx)(v,{}),Object(y.jsx)(v,{}),Object(y.jsx)("h4",{children:"Computer Set-up for Deep-Learning"}),Object(y.jsx)(hn,{}),Object(y.jsx)(v,{}),Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:" OS: Windows 10 Professional"}),Object(y.jsx)("li",{children:"CPU: Intel(R) Core(TM) i9-10900 CPU @ 2.80GHz  2.81 GHz"}),Object(y.jsx)("li",{children:"Graphic Card: NVIDIA GeForce RTX 3090"}),Object(y.jsx)("li",{children:"RAM: 32.0GB"}),Object(y.jsx)("li",{children:" Many Others ..."})]})]}),Object(y.jsx)(v,{}),Object(y.jsx)(v,{}),Object(y.jsx)(v,{}),Object(y.jsx)(v,{})]})},exact:!0},{title:f.PORTFOLIO.title,path:f.PORTFOLIO.path,component:function(){var n=qe({height:0}),e=E(),t=Object(c.h)("".concat(f.PORTFOLIO.path,"/:matchedPortfolioTitle")),a=A((function(n){return n.app.articles.activePortfolioTitle})),o=A((function(n){return n.app.articles.portfolios})),s=o.findIndex((function(n){return Xn(n.title)===a})),r=o[s],l=Object(we.get)(o,"[".concat(s-1,"]"),null),d=Object(we.get)(o,"[".concat(s+1,"]"),null),p=((null===t||void 0===t?void 0:t.params)||{matchedPortfolioTitle:""}).matchedPortfolioTitle,u=function(n){e(Sn.actions.updateArticles({portfolios:n}))};return Object(i.useState)((function(){!function(n){for(var e=n.getArticles,t=n.setArticles,a=e(),i=[],o=[],s=0;s<a.length;s++)o.push(ne(a[s].default).then((function(n){var e=n.content,t=n.data;if(!t.id)throw new Error("id was not found on article: ".concat(t.title,"."));if(!t.wip){var a=t.tech;if(a){var o=null===a||void 0===a?void 0:a.split(",").map((function(n){return n.replace(/^\s*/g,"")})).map((function(n){return n.replace(/\s*$/g,"")}));t.techs=o}i.push(Object(_.a)({content:e},t))}})).catch((function(n){console.log(n)})));Promise.all(o).then((function(){var n=i.sort((function(n,e){var t=parseInt(n.id.replace(/\D+/g,""));return parseInt(e.id.replace(/\D+/g,""))-t}));t(n)}))}({getArticles:Ge,setArticles:u})})),Object(i.useEffect)((function(){e(Sn.actions.updateArticles({activePortfolioTitle:p}))}),[p]),Object(i.useEffect)((function(){return function(){e(Sn.actions.updateArticles({activePortfolioTitle:""}))}}),[]),Object(y.jsxs)(B,{className:n.projects,title:"Projects",children:[Object(y.jsx)(v,{height:20}),Object(y.jsx)("div",{style:{padding:50,paddingTop:10},children:Object(y.jsxs)(_e.a.Provider,{options:be,children:[""==a&&Object(y.jsx)(He,{}),""!==a&&r&&Object(y.jsx)(Ve,{portfolio:r,prevPortfolio:l,nextPortfolio:d})]})})]})},exact:!1},{title:f.ARTWORKS.title,path:f.ARTWORKS.path,component:function(){var n=["https://cdnb.artstation.com/p/assets/images/images/011/689/703/medium/ching-cheong-lee-6.jpg?1530874176,6,3","https://cdna.artstation.com/p/assets/images/images/010/280/502/medium/ching-cheong-lee-47-artstation.jpg?1523588913,1,1","https://cdna.artstation.com/p/assets/images/images/010/280/394/large/ching-cheong-lee-000000preview.jpg?1523565280,4,5","https://cdna.artstation.com/p/assets/images/images/014/349/930/medium/ching-cheong-lee-2.jpg?1543595516,4,4","https://cdna.artstation.com/p/assets/images/images/006/007/422/medium/ching-cheong-lee-10.jpg?1495381736,4,3","https://cdna.artstation.com/p/assets/images/images/033/246/420/medium/ching-cheong-lee-18.jpg?1608905010,1,1","https://cdna.artstation.com/p/assets/images/images/005/924/540/large/ching-cheong-lee-treepractice.jpg?1494750047,1,1","https://cdna.artstation.com/p/assets/images/images/011/579/872/large/ching-cheong-lee-27.jpg?1530305575,3,1","https://cdnb.artstation.com/p/assets/images/images/007/711/521/medium/ching-cheong-lee-7.jpg?1508008886,5,2","https://cdnb.artstation.com/p/assets/images/images/005/069/419/medium/ching-cheong-lee-22.jpg?1488275626,3,2","https://cdna.artstation.com/p/assets/images/images/005/069/204/large/ching-cheong-lee-12.jpg?1488274783,3,2"],e=Object(i.useState)(0),t=Object(D.a)(e,2),a=t[0],o=t[1],s=Object(i.useState)(!1),r=Object(D.a)(s,2),c=r[0],l=r[1],d=Object(i.useCallback)((function(n,e){e.photo;var t=e.index;o(t),l(!0)}),[]);return Object(y.jsx)("div",{className:"artworks",children:Object(y.jsxs)(B,{title:"Selective Artworks",children:[Object(y.jsxs)("p",{children:["A full list of my artworks can be found in my arstation page: ",Object(y.jsx)("a",{target:"_blank",href:"https://www.artstation.com/checkercc",children:"https://www.artstation.com/checkercc"}),"."]}),Object(y.jsx)(v,{}),Object(y.jsx)($t.a,{photos:n.map((function(n){return Ft(n)})),columns:2,onClick:d}),Object(y.jsx)(Mt.b,{children:c?Object(y.jsx)(Mt.a,{onClose:function(){o(0),l(!1)},children:Object(y.jsx)(Mt.c,{isFullscreen:!1,currentIndex:a,views:n.map((function(n){Ft(n).src;return{source:{regular:Ft(n).src}}}))})}):null})]})})},exact:!0},{title:f.MATH_MATERIAL.title,path:f.MATH_MATERIAL.path,component:function(){var n=Object(i.useState)(Ke.LINEAR_ALGEBRA),e=Object(D.a)(n,2),t=e[0],a=e[1];return Object(y.jsx)("div",{className:"math-material",children:Object(y.jsx)(B,{title:"Study Notes and Blog",children:Object(y.jsxs)(_e.a.Provider,{children:[Object(y.jsx)("h5",{children:"Math Study Notes"}),"The following are study notes I made when I was studying mathematics in my undergraduate career.",Object(y.jsx)(v,{height:8}),Object(y.jsxs)("ul",{children:[Object(y.jsx)("li",{children:Object(y.jsx)(un,{title:"Linear Algebra",href:Ye})},"linear"),Object(y.jsx)("li",{children:Object(y.jsx)(un,{title:"Real Analaysis (Specifically on Measure Theory)",href:Xe})},"real"),Object(y.jsx)("li",{children:Object(y.jsx)(un,{title:"Record of Solved Problems and Solutions ",href:Qe})},"problems")]}),Object(y.jsx)(v,{}),Object(y.jsx)("h5",{children:"Math Tutorial Notes"}),"A list of tutorial notes I made when I was a teaching assistant in HKUST.",Object(y.jsx)(v,{}),Object(y.jsxs)("div",{className:"notes",children:[Object(y.jsx)("div",{className:"button-column",children:Object.values(Ke).map((function(n){return Object(y.jsxs)(T.a,{className:S()("topic-button",t===n?"active":""),onClick:function(e){a(n)},children:[" ",n]},n)}))}),Object(y.jsx)("div",{className:"notes-column",children:Lt.map((function(n){return Object(y.jsx)(Ze,{activeStage:t,stage:n.category,children:Object(y.jsx)("div",{className:"tutorial-notes",children:n.list.map((function(n,e){return Object(y.jsx)("div",{className:"note",children:Object(y.jsx)("div",{children:Object(y.jsxs)("div",{className:"tutorial-title-row",children:[Object(y.jsx)("div",{className:"topic",children:"Note ".concat(String(e+1).padStart(2,"0"))}),Object(y.jsx)("div",{className:"title",children:Object(y.jsx)(un,{title:n.title,href:n.link})})]})})},n.link)}))})},n.category)}))})]}),Object(y.jsx)(v,{}),Object(y.jsx)(v,{}),Object(y.jsx)("h5",{children:"Old Blog"}),Object(y.jsx)("p",{children:Object(y.jsx)("a",{href:"https://checkerlee.blogspot.com/",target:"_blank",children:" https://checkerlee.blogspot.com/"})})]})})})},exact:!0},{title:f.CONTACT.title,path:f.CONTACT.path,component:Un,exact:!0},{title:f.BLOG.title,path:f.BLOG.path,component:function(){var n,e=Da(),t=Object(c.f)(),a=Object(c.g)().pathname,o=function(){var n=Object(i.useState)(0),e=Object(D.a)(n,2),t=e[0],a=e[1],o=Vt().scrollContainer;return Object(i.useEffect)((function(){var n=Object(we.debounce)((function(){var n=(null===o||void 0===o?void 0:o.offsetHeight)||0,e=(null===o||void 0===o?void 0:o.scrollTop)||0,t=Math.max(((null===o||void 0===o?void 0:o.scrollHeight)||1)-n,1),i=e;a(Math.min(1,i/t))}),100);return null===o||void 0===o||o.addEventListener("scroll",n),function(){null===o||void 0===o||o.removeEventListener("scroll",n)}}),[o]),{ScrollMeterButton:function(){return Object(y.jsxs)("div",{className:"scroll-top-meter",style:{position:"fixed",right:60,bottom:60,zIndex:2,backgroundColor:"rgba(0,0,0,0.5)",color:"white",padding:"5px 6px",borderRadius:3,fontSize:"0.8rem",userSelect:"none",width:55,height:30},children:[Object(y.jsx)("div",{style:{display:"flex",justifyContent:"center",transform:"scale(1.5, 0.75)",marginTop:-9,marginBottom:-10,opacity:t>.01?1:.2},children:Object(y.jsx)(da.a,{})}),Object(y.jsx)("div",{style:{display:"flex",justifyContent:"center"},children:Math.ceil(100*t)+"%"}),Object(y.jsx)(T.a,{style:{position:"absolute",top:0,left:0,width:"100%",height:"100%",minWidth:0},onClick:function(){return o.scrollTo({top:0})}})]})}}}().ScrollMeterButton,s=((null===(n=Object(c.h)("".concat(f.BLOG.path,"/category/:activeTag")))||void 0===n?void 0:n.params)||{activeTag:""}).activeTag,r=Object(c.h)("".concat(f.BLOG.path,"/article/:matchedArticleId")),l=Vn(((null===r||void 0===r?void 0:r.params)||{matchedArticleId:""}).matchedArticleId),d=A((function(n){return n.app.sortPostBy})),p=Ca(),u=p.activeArticle,m=p.activeBlogArticleId,h=p.articles,_=p.tagAndCounts,b=function(n,e){w.current=0;var a=g(Hn(n));e&&(e.preventDefault(),1===e.button||e.ctrlKey&&0===e.button?window.open(a,"_blank"):t.push(a))};Object(i.useEffect)((function(){k(Sn.actions.setActiveTag(s))}),[s]);var w=Object(i.useRef)(0),x=h.length,k=E(),C=function(n){k(Sn.actions.updateArticles({blogs:n}))};Object(i.useState)((function(){!function(n){var e=n.getArticles,t=n.setArticles,a=e(),i=[],o=[],s=E();s(Sn.actions.setShowLoading({showLoading:!0}));for(var r=0;r<a.length;r++)o.push(ne(a[r].default).then((function(n){var e=n.content,t=n.data,a=t.date,o=t.title,s=t.intro,r=t.tags,c=void 0===r?"":r,l=t.tag,d=void 0===l?"":l,p=t.edited,u=void 0===p?"":p,m=t.id,h=t.wip,g=void 0!==h&&h,f=t.toc,_=void 0===f||f,b=t.update,y=void 0===b?"":b;y&&console.log("typeof update",typeof y," value",y);var w=Wn(a),v=u?Wn(u):null,x=(d||c).split(",").map((function(n){return n.trim()}));if(!m)throw new Error("id was not found on article: ".concat(o,"."));Zn(g)&&i.push({date:w,edited:v,title:o,content:e.trim(),intro:s,articleRouteId:Kn(o),tags:x,id:m,hasToc:_})})).catch((function(n){return console.log(n)})));Promise.all(o).then((function(){var n=i.sort((function(n,e){var t="".concat(n.date.date," ").concat(n.date.month," ").concat(n.date.year),a="".concat(e.date.date," ").concat(e.date.month," ").concat(e.date.year),i=new Date(a).getTime()-new Date(t).getTime();return 0!==i?i:function(){var t=Jn(n.id),a=Jn(e.id);return t===a?parseInt(e.id.replace(a,""))-parseInt(n.id.replace(t,"")):1}()}));t(n)})).then((function(){setTimeout((function(){s(Sn.actions.setShowLoading({showLoading:!1}))}),100)}))}({getArticles:La,setArticles:C})}));var j=Object(i.useCallback)((function(n){var e=parseInt(localStorage.getItem("scroll")||"0"),t=n.currentTarget.location.pathname;if(t===f.BLOG.path||t==="".concat(f.BLOG.path,"/"))var a=setInterval((function(){var n=document.querySelector(".os-viewport");n&&(null===n||void 0===n||n.scrollTo({left:0,top:e}),clearInterval(a))}),10)}),[m]),R=Object(i.useCallback)(Object(we.debounce)((function(n){var e=document.querySelector(".os-viewport"),t=e.scrollTop;t!=w.current&&(w.current=t,!m&&e&&localStorage.setItem("scroll",e.scrollTop+""))}),100),[m]);Object(i.useEffect)((function(){-1===a.indexOf(f.BLOG.path)&&localStorage.setItem("scroll","0")}),[a]),Object(i.useEffect)((function(){var n=document.querySelector(".os-viewport");return window.addEventListener("popstate",j),null===n||void 0===n||n.addEventListener("scroll",R),function(){window.removeEventListener("popstate",j),null===n||void 0===n||n.removeEventListener("scroll",R)}}),[m]),Object(i.useEffect)((function(){l&&h.length>0?k(Sn.actions.updateArticles({activeBlogArticleId:l})):k(Sn.actions.updateArticles({activeBlogArticleId:""}))}),[l,h]);var I=function(n){t.push("".concat(f.BLOG.path,"/category/").concat(n))};return Object(i.useEffect)((function(){d&&k(Sn.actions.setActiveTag(""))}),[d]),Object(y.jsxs)("div",{className:"blog",children:[Object(y.jsx)(ca,{}),Object(y.jsx)(o,{}),Object(y.jsxs)(B,{title:l?void 0:Object(y.jsx)(y.Fragment,{children:"Articles"}),children:[!l&&Object(y.jsxs)("div",{className:e.sortRow,children:[Object(y.jsx)(T.a,{className:"date"===d?"active":"",onClick:function(){k(Sn.actions.setSortPostBy("date"))},children:"Sort by Date"}),Object(y.jsx)(Xt,{}),Object(y.jsx)(Xt,{}),Object(y.jsx)(T.a,{className:"topic"===d?"active":"",onClick:function(){k(Sn.actions.setSortPostBy("topic"))},children:"Sort by Topic"})]}),Object(y.jsx)(v,{}),Object(y.jsx)(Ia,{}),Object(y.jsx)(v,{}),!l&&function(){var n=Object(c.f)();return Object(y.jsxs)(y.Fragment,{children:[Object(y.jsxs)("div",{className:"category-row",children:[Object(y.jsxs)("div",{className:"top-row",children:[Object(y.jsx)("div",{className:"cat-left",children:" Categories"}),Object(y.jsxs)("div",{className:"cat-right",children:[" ",Object(y.jsx)(na,{})," "]})]}),Object(y.jsx)(v,{height:10}),Object(y.jsx)("div",{children:Object(y.jsx)("ul",{children:Object(y.jsxs)("li",{className:S()("cat-button",""===s?"active":""),children:["All",Object(y.jsx)("sup",{children:x}),Object(y.jsx)(T.a,{onClick:function(){return n.push(f.BLOG.path)},style:{position:"absolute",top:0,left:0,width:"100%",height:"100%",minWidth:0}})]})})}),Object(y.jsx)("div",{children:Object(y.jsx)("ul",{children:Object.entries(_).sort((function(n,e){var t=Object(D.a)(n,2),a=t[0],i=(t[1],Object(D.a)(e,2)),o=i[0];i[1];return a.localeCompare(o)})).map((function(n){var e=Object(D.a)(n,2),t=e[0],a=e[1],i=s===t;return Object(y.jsxs)("li",{className:S()("cat-button",i?"active":""),children:[Pa(t),t,Object(y.jsx)("sup",{children:a}),Object(y.jsx)(T.a,{onClick:function(){if("date"===d)I(t);else{var n=document.getElementById(t),e=document.querySelector(".os-viewport");console.log(n,e),n&&e&&e.scrollTo({left:0,top:n.offsetTop})}},style:{position:"absolute",top:0,left:0,width:"100%",height:"100%",minWidth:0}})]},t)}))})})]}),"date"===d&&Object(y.jsx)(v,{})]})}(),!u&&Object(y.jsx)(ua,{activeTag:s,sortPostBy:d,articles:h,navToArticle:b}),l&&Object(y.jsx)(ea,{className:"tech-blog",showComment:!0,showTOC:!0,articles:h,activeArticle:u,navToArticle:b})]})]})},exact:!1}]}],Ma=t(1164);t(1101);function Fa(){var n=A((function(n){return n.app.showLoading}));return Object(y.jsx)(y.Fragment,{children:n&&Object(y.jsx)("div",{className:"custom-loading-overlay",children:Object(y.jsx)(Ma.a,{className:"loading-svg",style:{color:"rgba(0,0,0,0.2)"}})})})}var Ua=function(){var n=E(),e=Object(c.g)().pathname,t=$a.find((function(n){return n.path===f.ROOT.path})),o=A((function(n){return n.app.articles.blogs})),s=A((function(n){return n.app.articles.activeBlogArticleId})),r=o.find((function(n){return n.articleRouteId===s})),l=A((function(n){return n.app.activeNavRoute}));return Object(i.useEffect)((function(){var i=t.routes.map((function(n){return n.path})).reduce((function(n,t){return new RegExp(t).test(e)?t:n}),a.NONE);i!==l&&(n(Sn.actions.changeActiveNavRoute(i)),i!==a.BLOG&&n(Sn.actions.updateArticles({activeBlogArticleId:"",defaultTocCrossed:!1})))}),[e,l]),Object(i.useEffect)((function(){var n=t.routes,a=n.reduce((function(n,t){return new RegExp(t.path).test(e)?t:n}),n[0]);document.title="CCLee / ".concat(null===a||void 0===a?void 0:a.title),s&&(document.title="CCLee / ".concat(null===a||void 0===a?void 0:a.title," / ").concat(null===r||void 0===r?void 0:r.title))}),[e,r]),Object(y.jsxs)("div",{className:"App",children:[Object(y.jsx)(Fa,{}),Object(y.jsx)("div",{className:"main",style:{backgroundRepeat:"no-repeat",backgroundSize:"cover",backgroundImage:'url("'.concat("",'/assets/images/background_img.jpg")')},children:Object(y.jsx)(c.c,{children:$a.map((function(n){return Object(y.jsx)(c.a,{path:n.path,component:n.component,exact:n.exact},n.path)}))})})]})},Ba=function(n){n&&n instanceof Function&&t.e(5).then(t.bind(null,1339)).then((function(e){var t=e.getCLS,a=e.getFID,i=e.getFCP,o=e.getLCP,s=e.getTTFB;t(n),a(n),i(n),o(n),s(n)}))},za={selectedNoteId:"",selectedPageId:"",selectedVocabId:"",imageText:"",history:[],searchText:""},qa=Object(xn.b)({name:"dict",initialState:za,reducers:{setSearchText:function(n,e){n.searchText=e.payload},setSelectedNoteId:function(n,e){n.selectedNoteId=e.payload},setSelectedPageId:function(n,e){n.selectedPageId=e.payload},setSelectedVocabId:function(n,e){n.selectedVocabId=e.payload},clearState:function(n,e){Object.entries(za).forEach((function(e){var t=Object(D.a)(e,2),a=t[0];t[1];n[a]=za[a]}))}}}),Ga=Object(xn.a)({reducer:{dict:qa.reducer,app:Sn.reducer},middleware:function(n){return n({serializableCheck:!1})},devTools:!0}),Wa=(t(1102),t(1103),t(91));localStorage.setItem("scroll","0"),r.a.render(Object(y.jsx)(C.a,{store:Ga,children:Object(y.jsx)(Wa.a,{basename:"/",children:Object(y.jsx)(Ua,{})})}),document.getElementById("root")),Ba()}]),[[1104,1,2]]]);