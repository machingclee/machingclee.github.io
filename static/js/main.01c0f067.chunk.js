(this["webpackJsonpgithub.io.source"]=this["webpackJsonpgithub.io.source"]||[]).push([[0],Array(280).concat([function(e){e.exports=JSON.parse('[{"content":" ##### Routing in This Blog Page The routing of this react application is controlled by the following single router.ts file: typescript interface IRouter { path: string; component: () => JSX.Element; exact?: boolean; routes?: IRouter[]; } const routers: IRouter[] = [ { path: navRoutes.ROOT, component: MainContent, routes: [ { path: navRoutes.ABOUT, component: About, exact: true, }, { path: navRoutes.SKILLS, component: Skills, exact: true, }, ...{ path: ${navRoutes.BLOG}, component: Blog, exact: false, }, ], }, ]; Note that for a sub-routing of navRoutes.BLOG (/blog) to work, exact: false is important. MainContent is the largest container where we want to switch from one component to another. I used to add another Switch in Blog component to achive this sub-routing, but recently I find a much convenient way. I call this a lazy routing since it can really achive routing in an extremely lazy but cleaner way by using useRouteMatch. The Blog component now has the following structure: typescript export default () => { ... const match = useRouteMatch<{ matchedArticleTitle: string }>( ${navRoutes.BLOG}/:matchedArticleTitle ); const { matchedArticleTitle } = match?.params || { matchedArticleTitle: \\"\\" }; ... const activeBlogArticleTitle = useAppSelector( (state: TRootState) => state.app.blog.activeBlogArticleTitle ); const selectedArticle = articles.find( md => md.title === activeBlogArticleTitle ); ... useEffect(() => { dispatch(appActions.updateBlog( { activeBlogArticleTitle: matchedArticleTitle } )); }, [matchedArticleTitle]); return ( <div className=\\"blog\\"> ... </div > ) } useRouteMatch catches the param in exactly the way we want. And this is cleaner than using Route in the sense that you can observe which route, in which way, affects this component directly inside the code of this component. Now on matchedArticleTitle changed, our useEffect hook will update the state of activeBlogArticleTitle in our redux store. And we can change our content based on this unique article name (as selectedArticle is changed). ","title":"Lazy React Router","date":"2021-07-12T00:00:00.000Z","id":"blog0002","tag":"react","intro":"Inside a routed component, we introduce useRouteMatch on type annotation and the way to extract params."},{"content":" ##### Loop Through a Directory In node.js, specifically in backend, we can use fs.readdir to return a list of files and directories of our target directory. Though fs is not available in frontend, we record a function that achieve similar objective: javascript const getModules = () => { function importAll(r: any) { let files: any[] = []; r.keys().map((item: any, index: any) => { files.push(r(item)); }); return files; } //@ts-ignore const files: { default: string }[] = importAll( require.context(\\"./articles\\", true, /\\\\.md$/) ); return files; }; Forgive me to have //@ts-ignore here as I don\'t want the hassle of annotating types to the utility function that I wouldn\'t maintain. The most important part is the function: require.context, which takes (directoryName:string, useSubdirectories: boolean, regExp: RegExp) as its positioinal argument. This blog page is an example of using this util function. Whenever I add an md file, the function can loop through my target directory, add a new file path to the array I am going to map, and generate a new post automatically. ","title":"On Looping all Files in Frontend","date":"2021-07-13T00:00:00.000Z","id":"blog0003","tag":"react","intro":"In backend we can loop through the files inside a directory using fs.readdir, we introduce a function that can achive the same thing in frontend using webpack\'s require.context function."},{"content":" #### Basic Commands - Create an virtual environment: bash conda create --name ENV_NAME python=3.8 - Remove an virtual environment bash conda env remove -n ENV_NAME - Register ENV to jupyter by bash python -m ipykernel install --user --name=ENV_NAME - Remove ENV in jupyter by bash jupyter kernelspec uninstall ENV_NAME - List all existing virtual environments: bash conda env list - List all libraries inside your activated virtual environment: bash conda list - Create a requirements.txt bash conda list -e > requirements.txt # for conda pip freeze > requirements.txt # for pip - conda/pip install the packages in requirements.txt bash conda install --file requirements.txt pip install -r requirements.txt #### Common Packages bash conda install -c conda-forge jupyterlab conda install -c conda-forge tensorflow conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch ","title":"Useful Conda Commands","date":"2021-07-18T00:00:00.000Z","id":"blog0005","tag":"python","intro":"Record the common commands that is needed to create a new virtual environment with conda."},{"content":" ##### Folder Structure and Scripts <center> <img width=\\"220\\" src=\\"/assets/tech/01.png\\"/> </center> <br/> Let me take the action in this blog as an example. javascript // app.action.ts export const UPDATE_BLOG = \\"UPDATE_BLOG\\"; export type UpdateBlog = ReturnType<typeof updateBlog>; export const updateBlog = (update: Partial<TAppState[\\"blog\\"]>) => { return { type: UPDATE_BLOG, payload: update, }; }; We can do our api-related logic in the .saga below! Later whenever we want to modify our api, saga is the only place we need to look at: javascript // app.saga.ts import { takeEvery, fork } from \'redux-saga/effects\'; import * as appActions from \\"../actions/app.actions\\"; function* watchUpdateBlog() { yield takeEvery(appActions.UPDATE_BLOG, updateBlog); } function* updateBlog(action: appActions.UpdateBlog) { console.log(\\"I am saga\\"); yield put(your action, if any); } export default [ fork(watchUpdateBlog) ] javascript // root.saga.ts import { all } from \\"redux-saga/effects\\"; import appSaga from \\"./app.saga\\"; function* rootSaga() { yield all([...appSaga]); } export default rootSaga; javascript // app.reducer.ts import * as appActions from \\"../actions/app.actions\\"; export default (state: TAppState = initialState, action: AnyAction): TAppState => { switch (action.type) { case appActions.UPDATE_BLOG: { const update = (action as appActions.UpdateBlog).payload; return { ...state, blog: { ...state.blog, ...update } } default: return state; } } javascript // root.reducer.ts import { combineReducers } from \\"redux\\"; import appReudcer from \\"./app.reducer\\"; const rootReducer = combineReducers < any > { app: appReudcer, }; export default rootReducer; Finally: javascript // store.ts import { createStore, applyMiddleware } from \\"redux\\"; import { createLogger } from \\"redux-logger\\"; import createSagaMiddleware from \\"redux-saga\\"; import rootReducer from \\"../reducers/root.reducer\\"; import rootSaga from \\"../sagas/root.saga\\"; const logger = createLogger({ collapsed: true }); const sagaMiddleware = createSagaMiddleware(); const store = createStore(rootReducer, applyMiddleware(logger, sagaMiddleware)); sagaMiddleware.run(rootSaga); export default store; Now javascript import { Provider } from \\"react-redux\\"; ReactDOM.render( <Provider store={store}> <App /> </Provider>, document.getElementById(\\"root\\") ); and happy saga! <center> <img src=\\"/assets/tech/02.png\\"/> </center> ##### Intercept the Async Actions and Error Handling javascript import { put, call } from \\"redux-saga/effects\\"; export const safe = (showLoading: boolean, saga: any, ...args: any) => function* (action: any) { try { if (showLoading) { // yield put(loadingActions.setLoading(true)); } console.log(\\"intercepted\\"); yield call(saga, ...args, action); } catch (error) { switch (error.request.status) { case 400: console.log(${error.response.status}: Bad Request); break; case 401: break; case 404: console.log(${error.response.status}: Not Found); break; case 405: case 500: console.log(${error.response.status}: Internal Server Error); break; default: console.log(${error.response.status}: Service Unavailable); break; } const response = error.response.data; // yield put(showAlert([response.message], \'error\')); } finally { if (showLoading) { // yield delay(500); // yield put(loadingActions.setLoading(false)); } } }; In app.saga.ts now we can replace our controller updateBlog by javascript // app.saga.ts import { safe } from \\"./middlewares/safe\\"; function* watchUpdateBlog() { // if true, do whatever you want, like displaying loading popover yield takeEvery(appActions.UPDATE_BLOG, safe(true, updateBlog); } Result: <center> <img src=\\"/assets/tech/03.png\\"/> </center> ","title":"On Redux-Saga","date":"2021-07-19T00:00:00.000Z","id":"blog0007","tag":"react","intro":"Record a functional Redux-Saga setup."},{"content":" #### Algorithms ##### The Merge Sort python def merge_sort(arr): if len(arr) == 1: return arr mid_index = int(len(arr)/2) L = arr[0:mid_index] R = arr[mid_index:] L = merge_sort(L) R = merge_sort(R) - The algorithm is locked and looped here until a return is resolved. - When merge_sort(len-1 array)\'s return two sorted arrays to the previous call, L and R then merge into a len-2 array arr, and get returned. - Then the locked merge_sort(len-2 array)s\' call continue, L and R then merge into a sorted len-4 array arr. - Similarly, the locked merge_sort(len-4 array)s\' call return two sorted arrays separately, they then merge into a len-8 sorted array. python head_L, head_R, i = 0, 0, 0 while head_L < len(L) and head_R < len(R): if L[head_L] < R[head_R]: arr[i] = L[head_L] head_L += 1 else: arr[i] = R[head_R] head_R += 1 i += 1 while head_L < len(L): arr[i] = L[head_L] head_L += 1 i += 1 while head_R < len(R): arr[i] = R[head_R] head_R += 1 i += 1 print(f\\"sorting L:{L} and R:{R}, result:{arr}\\") return arr We try to sort the following and print log: python to_be_sorted = [2,10,5,100,66,24,27,30] merge_sort(to_be_sorted) which yields text sorting L:[2] and R:[10], result:[2, 10] sorting L:[5] and R:[100], result:[5, 100] sorting L:[2, 10] and R:[5, 100], result:[2, 5, 10, 100] sorting L:[66] and R:[24], result:[24, 66] sorting L:[27] and R:[30], result:[27, 30] sorting L:[24, 66] and R:[27, 30], result:[24, 27, 30, 66] sorting L:[2, 5, 10, 100] and R:[24, 27, 30, 66], result:[2, 5, 10, 24, 27, 30, 66, 100] ##### The Time Complexity <center> <img width=\\"380\\" src=\\"/assets/tech/merge-sort.png\\" /> </center> <br/> <center></center> The algorithm in plain text: - We are bisecting our length-$n$ long array by half until the resulting array has length $1$, which takes $\\\\log_2 n$ steps to complete. For each dividing steps, it takes exactly $n$ operations for cloning elements to the evenly divided pairs of arrays (L and R)\'s. At the end of bisections, we get at most $O(n\\\\log n)$ operations. - Starting from length-1 array, we merge pairs of adjacent arrays in the 3 while loops above, these triples of while loops in total take at most $O(n)$ operations. But it just takes $\\\\log n$ stages of steps to merge from length-1 array to length-$n$ array. Therefore by summing up, the time complexity of merge sort is $O(n\\\\log n)$. ","title":"Algorithm: Merge Sort and its Time Complexity","date":"2021-07-25T00:00:00.000Z","id":"blog0009","tag":"coding","intro":"Study the implementation of sorting algorithms"},{"content":" #### Exercises ##### Exercise 1 ($\\\\bf{h}$-index) > **Problem.** If a scholar has at least $h$ of their papers cited $h$ times, then the $h$-index of a scholar is defined to be $h$. Find $h$. python # Algorithm: def hIndex(citations): result = 0 length = len(citations) n_cites = [0] * (length+1) for c in citations: c_ = min(c, length) n_cites[c_] += 1 total = 0 for i in range(length, -1, -1): total += n_cites[i] if total == i: result = total break return result **Complexities.** - **Time.** The loop for c in citations takes $O(n)$ operations and the loop for i in range(length, -1, -1) also takes $O(n)$ operations, the algorithm in total has time complexity $O(n)$. - **Space.** The space complexity is $O(n)$ as well since we have constructed a new array n_cites. --- ##### Exercise 2 (Trie) This exercise presents us a well-known sequential data structure called **Trie**. It is beneficial for us to see it at least once to get motivated for tackling the next similar problems. > **Problem.** Given a list of words, for each word find the shortest unique prefix. You can assume a word will not be a substring of another word (i.e., play and playing won\'t be in the same words list). Example: - Input: [\'james\', \'john\', \'jack\', \'techlead\'] - Ouput: [\'jam\', \'jo\',\'jac\', \'t\'] We tackle it by defining our node by: python # each children is identified/pointed by a character class Node: def __init__(self, char): self.count = 0 self.char = char self.children = {} python class Trie: def __init__(self): self.root = Node(\\"\\") def insert(self, word): curr_node = self.root for c in word: if c not in curr_node.children: curr_node.children[c] = Node(c) curr_node = curr_node.children[c] curr_node.count += 1 def unique_prefix(self, word): curr_node = self.root prefix = \\"\\" for c in word: if curr_node.count == 1: return prefix else: curr_node = curr_node.children[c] prefix += curr_node.char return prefix Finally we define python def shortest_unique_prefix(words): trie = Trie() for word in words: trie.insert(word) unique_prefix = [] for word in words: unique_prefix.append(trie.unique_prefix(word)) return unique_prefix and python words = [\'james\', \'john\', \'jack\', \'techlead\'] shortest_unique_prefix(words) gives [\'jam\', \'jo\', \'jac\', \'t\']. **Complexities.** Denote $\\\\ell$ the maximum length among all $n$ words, i.e., $\\\\ell=$ max([len(w) for w in words]). - **Time.** For each of $n$ words we spend $O(1)$ operations to go from one node to another, it also takes $O(1)$ operation to check if the current node has count == 1, so in total our time complexity is $O(n\\\\ell)$. - **Space.** In the worst case we insert $n$ distinct words that starts from different characters, therefore the space complexity is $O(n\\\\ell)$. ","title":"Exercises on Algorithms","date":"2021-07-26T00:00:00.000Z","id":"blog0010","tag":"coding","intro":"A note to solved problems and calculate the related time complexities."},{"content":" #### Special Types ##### Infer T from T[] typescript type ArrayElement<ArrayType extends readonly unknown[]> = ArrayType extends readonly (infer ElementType)[] ? ElementType : never; ##### Combine T1[\\"fields\\"] = T2[] and S1[\\"fields\\"] = S2[] to get T with T[\\"fields\\"] = (T2 & S2)[] This is a record of real use case in my project: Both frontend and backend receive csvConfig with type CSVConfig. For code sharing purpose, this type is not repeatedly defined in both front- and back-end separately, rather it is defined in a custom npm-package installed from our local npm-registry. However, as business logic grows, there are **_additional_** properties in the ArrayType of CSVConfig[\\"fields\\"] (i.e., the S in S[]) that are redundant to the backend and make sense to the frontend only. We can of course add additional property in the type definition inside our npm-package project, but why don\'t we just augment our ArrayType of CSVConfig[\\"fields\\"] if these additional properties have nothing to do with the backend? This gets rid of the hessels of npm link and npm unlink --no-save to our npm-package locally (by the way, npm unlink without --no-save can be disastrous). Suppose that the type CSVConfig[\\"fields\\"] is T[]. We want to augment T by intersecting T with the ArrayType (again, that means the S in S[]) of typescript // not a valid syntax TDataProcessorConfig[\'fields\'] = { defaultValue?: string, processors?: IDataProcessor[] }[]. Heuristically we want our augmented fields to be like { [k: keyof T]: T[k], defaultValue?: string, processors?: IDataProcessor[] }[]. Now we can augment our CSVConfig by typescript type TCSVConfigFields = ArrayElement<CSVConfig[\\"fields\\"]>; type TDataProcessorField = ArrayElement<TDataProcessorConfig[\\"fields\\"]>; type TAugmentedCSVConfig = Omit<CSVConfig, \\"fields\\"> & { fields: (TCSVConfigField & TDataProcessorField)[]; }; By experiment Omit is necessary, otherwise for any variable that inherits type TAugmentedCSVConfig, its array element of fields property is still accessible to keys in TCSVConfigField only. It seems that property\'s type is not overridable by intersection at the moment. ","title":"Typescript Type Tricks","date":"2021-07-14T00:00:00.000Z","id":"blog0011","tag":"typescript","intro":"Useful custom type and last resort to get correct type that we may encounter in typescript."},{"content":" ##### Procedures Go to https://disqus.com/, click get started, and go along until you get your link to embed disqus\'s javascript into your site, which is something like javascript https://short-name.disqus.com/embed.js We can either embed the comment plug-in manually, or directly install a nice wrapper from <a href=\\"https://www.npmjs.com/package/disqus-react\\">here</a>. In either case, we need to reload the script in order to get a new thread connecting to disqus whenever we change our article, therefore we need the following: typescript const prevId = useRef<string>(\\"\\"); useEffect(() => { const sameAsPrevId = prevId.current === activeArticle?.id; if (sameAsPrevId) { return; } (Window as any).DISQUS?.reset({ reload: true, config: function () { this.page.identifier = process.env[\\"REACT_APP_WEB_HOST\\"] + activeArticle.id; this.page.url = process.env[\\"REACT_APP_WEB_HOST\\"] + activeArticle.id; }, })(function () { var d = document; var s = d.createElement(\\"script\\"); s.src = \\"https://c-c-lee-blog.disqus.com/embed.js\\"; s.async = true; s.setAttribute(\\"data-timestamp\\", new Date() + \\"\\"); (d.head || d.body).appendChild(s); })(); prevId.current = activeArticle.id; }, [activeArticle]); Part of the code is taken directly from the instruction of disqus when you get start. ##### Potential Traps _By experiment_, in (Window as any).DISQUS?.reset we need to change both - this.page.identifier and - this.page.url. These parameters are consistent with what we put into the props of DiscussionEmbed: typescript import { DiscussionEmbed } from \\"disqus-react\\"; <DiscussionEmbed shortname={\\"c-c-lee-blog\\"} config={{ url: process.env[\\"REACT_APP_WEB_HOST\\"] + activeArticle.id, identifier: process.env[\\"REACT_APP_WEB_HOST\\"] + activeArticle.id, title: activeArticle.title, language: \\"en_us\\", }} />; Just need to be careful about the number of times the component rerender, which may be a trap that courses troubles. If it happens that the component rendered twice, problem would occur when the \\"reload\\" action is first dispatched with the old identifier. Happy blogging. ##### References - https://stackoverflow.com/questions/8944287/disqus-loading-the-same-comments-for-dynamic-pages ","title":"Disqus Comment Plug-in in React","date":"2021-08-12T00:00:00.000Z","id":"blog0015","tag":"react","intro":"An introduction to disqus that plug a small comment box into our website."},{"content":" #### Solutions ##### What do we have? There are two solutions after googling for a while if one insists on using react: - Do **_server side rendering_** using framework like next.js. - **_Pre-render_** our react app into several pieces of rendered static htmls. I would like to talk about the second approach as it just requires a web host be able to serve html files for reader from outside (github is a good choice in this regard combined with <a href=\\"https://www.npmjs.com/package/gh-pages\\">gh-pages</a>) . ##### Use Case and Tradeoff The drawback is glaringly obvious, pre-rendering is an awful choice for highly dynamic pages like a forum. But our page wouldn\'t change frequently (as a personal website), our use case is perfectly fine. But even our page does change frequently, we can **_selectively_** choose not to pre-render this particular page and let react handle the content in its own way by error handling tricks, but at last such content cannot be easily scrapped. #### Pre-rendering Procedures ##### Step 1: Webpack Config via react-app-rewired and customize-cra We will make use of two npm packages, <a href=\\"https://www.npmjs.com/package/react-app-rewired\\">react-app-rewired </a> and <a href=\\"https://www.npmjs.com/package/customize-cra\\">customize-cra</a>: bash yarn add react-app-rewired customize-cra @types/customize-cra - react-app-rewired helps override our webpack-config **_without ejection_** and; - customize-cra providers us with helper functions to override webpack config via react-app-rewired. Next we need <a href=\\"https://www.npmjs.com/package/prerender-spa-plugin\\">prerender-spa-plugin</a> to decompose our react app into pre-rendered htmls: bash yarn add prerender-spa-plugin Create config-overrides.js in the root directory, where: javascript const { override, addWebpackPlugin } = require(\\"customize-cra\\"); const path = require(\\"path\\"); const PrerenderSPAPlugin = require(\\"prerender-spa-plugin\\"); const Renderer = PrerenderSPAPlugin.PuppeteerRenderer; const preRenderPlugin = new PrerenderSPAPlugin({ staticDir: path.join(__dirname, \\"build\\"), indexPath: path.join(__dirname, \\"build\\", \\"index.html\\"), routes: [ \\"/\\", \\"/about\\", \\"/skills\\", \\"/experience\\", \\"/portfolio\\", \\"/blog\\", \\"/lang-study\\", \\"/artworks\\", \\"/math-material\\", \\"/contact\\", ], renderer: new Renderer({ timeout: 0, maxConcurrentRoutes: 1, renderAfterTime: 5000, headless: false, }), }); module.exports = override(addWebpackPlugin(preRenderPlugin)); Here we use renderer to preview rendered pages for debug purpose. If you are sure your routes for decompoisition are correct, you may skip it by setting renderAfterTime to 0. Removing renderer courses error to me. In package.json we replace json \\"scripts\\": { ... \\"build\\": \\"react-scripts build\\", }, by json \\"scripts\\": { ... \\"build\\": \\"react-app-rewired build\\", }, ##### Step 2: Decompose React App by Routes In package.json we add/replace the value of homepage: json { ..., \\"homepage\\": \\"https://machingclee.github.io\\", } this will provide process.env.PUBLIC_URL a value \\"https://machingclee.github.io\\" when process.env.NODE_ENV === \\"production\\", and otherwise an empty string so that it does nothing in development mode. Next in our routing: typescript import { BrowserRouter } from \\"react-router-dom\\"; ReactDOM.render( <BrowserRouter basename={process.env.PUBLIC_URL + \\"/\\"}> <App /> </BrowserRouter>, document.getElementById(\\"root\\") ); Keep in mind that hash routing **_will fail_** in pre-rendering process. When we yarn build (which runs react-app-rewired build), we get the following structure in our build directory: <center> <a href=\\"/assets/tech/004.png\\"> <img width=\\"460\\" src=\\"/assets/tech/004.png\\"/> </a> </center> ##### Step 3: Selective Pre-rendering by Means of 404 Error Handling Notice that /blog and /lang-study have no sub-routings, it will cause 404 error once we navigate to, say, /blog/On-Redux-Saga since /blog/On-Redux-Saga.html does not exist. When 404 error occurs, github would first look for 404.html, we can manipulate this mechanism into rendering our dynamic contents by javascript. Just make an identical copy of index.html in our ./build and name it 404.html: json \\"scripts\\": { ..., \\"build\\": \\"react-app-rewired build && cp build/index.html build/404.html } And we are done! ##### Step 4: Generate Additional Subroutes (e.g., blog articles) In my case all my subroutes are based on content in my md file, I can get all routes dynamically by file paths: javascript import fs from \\"fs\\"; const getAllFiles = (dir: string) => { const mdFiles: string[] = []; const getFiles = (dir: string) => { const paths = fs.readdirSync(dir); paths.forEach((p) => { const newPath = path.join(${dir}/${p}); const pathStat = fs.statSync(newPath); if (pathStat.isDirectory()) { getFiles(newPath); } else { if (newPath.endsWith(\\".md\\")) { mdFiles.push(newPath); } } }); }; getFiles(dir); return mdFiles; }; You can generate your routes in your own case, and concat your additional routes inside config-overrides. In case you cannot use import statement, at the same level of compilerOptions try to look at tsconfig.json and try to add: json \\"ts-node\\": { \\"compilerOptions\\": { \\"module\\": \\"CommonJS\\" } }, My final addtional routes file is like this, which gets imported from config-overrides for concatenation: json { \\"routes\\": [ \\"/blog/Lazy-React-Router\\", \\"/blog/On-Looping-all-Files-in-Frontend\\", \\"/blog/Useful-Conda-Commands\\", \\"/blog/Web-Scrapping-with-Selenium-and-Beautifulsoup-on-Chrome\\", \\"/blog/On-Redux-Saga\\", \\"/blog/Algorithm-Merge-Sort-and-its-Time-Complexity\\", \\"/blog/Exercises-on-Algorithms\\", \\"/blog/Typescript-Type-Tricks\\", \\"/blog/Disqus-Comment-Plug-in-in-React\\", \\"/blog/Make-your-React-App-Scrapable-by-Google-Search-Engine\\", \\"/lang-study/Asmongold-Reacts-to-Preach-Quitting-WoW\\", \\"/lang-study/Asmongold-Reacts-to-Is-FFXIV-Winning-the-MMO-War-By-Zepla\\", \\"/lang-study/Is-WoW-2-Blizzard-s-only-option-in-2021-Asmongold-Reacts-to-Bellular\\", \\"/lang-study/Asmongold-reacts-to-fan-made-memes-Reddit-Recap-33-FFXIV-Special-\\", \\"/lang-study/Asmongold-Reacts-to-WillE-Quitting-WoW-Full-Time-Content-Creation\\", \\"/lang-study/The-Best-MMO-Asmongold-Summit1g-Talk-FFXIV-vs-WoW\\", \\"/lang-study/Asmongold-reacts-to-fan-made-memes-Reddit-Recap-34-FFXIV-Special-\\", \\"/lang-study/Asmongold-Reacts-to-Stoopzz-Quitting-WoW-after-15-Years\\", \\"/lang-study/Asmongold-Reacts-to-FFXIV-s-10-Most-Prestigious-Things-to-FLEX-with-By-Zepla\\", \\"/lang-study/Asmongold-on-Blizzard-Firing-Diablo-4-Game-Director-More\\", \\"/lang-study/Asmongold-Reacts-to-Diablo-2-Resurrected-NEW-CINEMATICS\\", \\"/lang-study/Asmongold-Reacts-to-Most-Popular-Games-2004-2020-\\", \\"/lang-study/Asmongold-Reacts-to-The-Awful-Side-of-FFXIV-\\" ] } #### Results: Evidence Proving Pre-rendering Ocurred ##### Show Contents by Pre-rendered Html Navigation to https://machingclee.github.io/skills: <center> <a href=\\"/assets/tech/005.png\\"><img width=\\"460\\" src=\\"/assets/tech/005.png\\"/></a> </center> <br/> which is exactly <a href=\\"https://github.com/machingclee/machingclee.github.io/blob/master/skills/index.html\\"><strong><i>this file</i></strong></a>. Moreover, it becomes a standard react app when user starts to navigate anywhere in the app (no html will be downloaded any more). It can be thought of as having 10 entry points rather than 1 in the past.<br/><br/> ##### Show Contents by Javascript Rendering Navigation to https://localhost:3000/skills: <center> <a href=\\"/assets/tech/006.png\\"><img width=\\"460\\" src=\\"/assets/tech/006.png\\"/></a> </center> <br/> which is our index.html in build. ","title":"Make your React App Scrapable by Google Search Engine","date":"2021-08-14T00:00:00.000Z","id":"blog0016","tags":"react","intro":"SPAs (Single Page App) are known to be unfriendly to search engines and we try to deal with this problem for react app."},{"content":" #### The Entire GAN and DCGAN Script <details> <summary> Simple GAN </summary> python import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.datasets as datasets import torchvision.transforms as transforms from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter class Discriminator (nn.Module): def __init__(self, img_dim): super().__init__() self.disc = nn.Sequential( nn.Linear(img_dim, 128), nn.LeakyReLU(0.1), nn.Linear(128, 1), nn.Sigmoid() ) def forward(self, x): return self.disc(x) class Generator(nn.Module): def __init__(self, z_dim, img_dim): super().__init__() self.gen = nn.Sequential( nn.Linear(z_dim, 256), nn.LeakyReLU(0.1), nn.Linear(256, img_dim), nn.Tanh() ) def forward(self, x): return self.gen(x) device = \\"cuda\\" if torch.cuda.is_available() else \\"cpu\\" lr = 3e-4 z_dim = 64 img_dim = 28*28*1 batch_size = 32 num_epochs = 50 disc = Discriminator(img_dim).to(device) gen = Generator(z_dim, img_dim).to(device) fixed_noise = torch.randn(batch_size, z_dim).to(device) transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize( (0.5,), (0.5,) ) ]) dataset = datasets.MNIST( root=\\"./dataset/\\", transform=transform, download=True ) loader= DataLoader(dataset, batch_size, shuffle=True) opt_disc = optim.Adam(disc.parameters(), lr=lr) opt_gen= optim.Adam(gen.parameters(), lr=lr) criterion = nn.BCELoss() writer_fake = SummaryWriter(f\\"./runs/GAN_MNIST/fake\\") writer_real = SummaryWriter(f\\"./runs/GAN_MNIST/real\\") step = 0 for epoch in range(num_epochs): for batch_idx, (real, _) in enumerate(loader): real = real.view(-1, 784).to(device) batch_size = real.shape[0] noise = torch.randn(batch_size, z_dim).to(device) fake = gen(noise) disc_real = disc(real).view(-1) lossD_real = criterion(disc_real, torch.ones_like(disc_real)) # don\'t want opt_disc.step() update fake, so create a detached version fake at this point # also the gradient that is used to update disc has nothing to do with gen # as the graph of fake involve gen, we have to detach fake to avoid affecting gen itself disc_fake = disc(fake.detach()).view(-1) lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) lossD = (lossD_real + lossD_fake)/2 disc.zero_grad() lossD.backward() opt_disc.step() output = disc(fake).view(-1) lossG = criterion(output, torch.ones_like(output)) gen.zero_grad() lossG.backward() opt_gen.step() if batch_idx == 0: print( f\\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\\ Loss D: {lossD:.4f}, loss G: {lossG:.4f}\\" ) with torch.no_grad(): fake = gen(fixed_noise).reshape(-1, 1, 28, 28) data = real.reshape(-1, 1, 28, 28) img_grid_fake = torchvision.utils.make_grid(fake, normalize=True) img_grid_real = torchvision.utils.make_grid(data, normalize=True) writer_fake.add_image( \\"Mnist Fake Images\\", img_grid_fake, global_step=step ) writer_real.add_image( \\"Mnist Real Images\\", img_grid_real, global_step=step ) step += 1 </details> <details> <summary>DCGAN</summary> python import torch import torch.nn as nn import torch.optim as optim import torchvision import torchvision.datasets as datasets import torchvision.transforms as transforms from torch.utils.data import DataLoader from torch.utils.tensorboard import SummaryWriter class Discriminator(nn.Module): def __init__(self, channels_img, features_d): super(Discriminator, self).__init__() #Input: N x img_channels x 64 x 64 self.disc = nn.Sequential( nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1), # 32 x 32 nn.LeakyReLU(0.2), self._block(features_d, features_d*2, 4, 2, 1), # 16x16 self._block(features_d*2, features_d*4, 4, 2, 1), # 8x8 self._block(features_d*4, features_d*8, 4, 2, 1), # 4x4 nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0), # 1x1 nn.Sigmoid() ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.Conv2d( in_channels, out_channels, kernel_size, stride, padding, bias=False ), nn.BatchNorm2d(out_channels), nn.LeakyReLU(0.2) ) def forward(self, x): return self.disc(x) class Generator(nn.Module): def __init__(self, z_dim, channels_img, features_g): super(Generator, self).__init__() # Input: N x z_dim x 1 x 1 self.gen = nn.Sequential( self._block(z_dim, features_g*16, 4, 1, 0), # N x f_g*16 x 4 x 4 self._block(features_g*16, features_g * 8, 4, 2, 1), # 8 x 8 self._block(features_g*8, features_g * 4, 4, 2, 1), # 16 x 16 self._block(features_g*4, features_g * 2, 4, 2, 1), # 32 x 32 nn.ConvTranspose2d( features_g*2, channels_img, 4,2,1 ), # 64 x 64 nn.Tanh() ) def _block(self, in_channels, out_channels, kernel_size, stride, padding): return nn.Sequential( nn.ConvTranspose2d( in_channels, out_channels, kernel_size, stride, padding, bias=False ), nn.BatchNorm2d(out_channels), nn.ReLU() ) def forward(self, x): return self.gen(x) def initialize_weights(model): for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)): nn.init.normal_(m.weight.data, 0.0, 0.02) device = torch.device(\\"cuda\\") if torch.cuda.is_available() else \\"cpu\\" LEARNING_RATE=2e-4 BATCH_SIZE = 128 IMAGE_SIZE = 64 CHANNELS_IMG = 1 Z_DIM = 100 NUM_EPOCHS = 5 FEATURES_DISC = 64 FEATURES_GEN = 64 transform = transforms.Compose( [ transforms.Resize(IMAGE_SIZE), transforms.ToTensor(), transforms.Normalize( [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)] ) ] ) dataset=datasets.MNIST( root=\\"./dataset/\\", train=True, transform=transform, download=True ) loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True) gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device) disc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device) initialize_weights(gen) initialize_weights(disc) opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5,0.999)) opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5,0.999)) criterion = nn.BCELoss() fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device) writer_real = SummaryWriter(f\\"./logs/real\\") writer_fake = SummaryWriter(f\\"./logs/fake\\") step = 0 gen.train() disc.train() for epoch in range(NUM_EPOCHS): for batch_idx, (real, _) in enumerate(loader): real = real.to(device) noise = torch.randn((BATCH_SIZE, Z_DIM, 1, 1)).to(device) fake = gen(noise) disc_real = disc(real).reshape(-1) disc_fake = disc(fake.detach()).reshape(-1) loss_disc_real = criterion(disc_real, torch.ones_like(disc_real)) loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) loss_disc = (loss_disc_fake + loss_disc_real)/2 disc.zero_grad() loss_disc.backward() opt_disc.step() output = disc(fake).reshape(-1) loss_gen = criterion(output, torch.ones_like(output)) gen.zero_grad() loss_gen.backward() opt_gen.step() if batch_idx % 100 == 0: print( f\\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\\ Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\\" ) with torch.no_grad(): fake = gen(fixed_noise) img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True) img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True) writer_fake.add_image( \\"Mnist Fake Images\\", img_grid_fake, global_step=step ) writer_real.add_image( \\"Mnist Real Images\\", img_grid_real, global_step=step ) step += 1 </details> <br/> #### What Happens in the Train Loop ##### fake.detach() In the train loop of our simple GAN: python disc = Discriminator(img_dim).to(device) gen = Generator(z_dim, img_dim).to(device) opt_disc = optim.Adam(disc.parameters(), lr=lr) opt_gen = optim.Adam(gen.parameters(), lr=lr) criterion = nn.BCELoss() for epoch in range(num_epoch): for batch_index, (real, _) in enumerate(loader): real = real.reivew(-1, 784).to(device) batch_size = real.shape[0] noise = torch.randn(batch_size, z_dim).to(device) fake = gen(noise) disc_real = disc(real).view(-1) lossD_real = criterion(disc_real, torch.ones_like(disc_real)) disc_fake = disc(fake.detach()).view(-1) lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) lossD = (lossD_real + lossD_fake)/2 - Note that we have used fake.detach() here. After detaching our parameter/tensor fake will not have .grad and requires_grad=False. Also note that fake.detach() is a new variable, it will not mutate fake. - The main reason for detach() is that when opt_disc.step() get executed, it will update every parameter in the graph for which requires_grad=True. As fake is going to be **_reused_**, plus the gradient that is used to update disc has nothing to do with gen, we therefore create a detached version of fake at this point to avoid affecting gen itself. ##### disc.zero_grad(), lossD.backward(), opt_disc.step() And we want to discuss the following 3 lines: python disc.zero_grad() lossD.backward() opt_disc.step() - disc.zero_grad(): It sets the .grad of all disc.parameters() to 0. - lossD.backward(): when backward() is executed, every tensor (variable/parameter) that is involved in the calculation of lossD will be assigned a computed derivative. Since lossD is at the top of the computation graph, lossD.grad will be None. What\'s more, lossD_real.grad and lossD_fake.grad are the derivatives $$ d\\\\,\\\\text{lossD_real}=\\\\frac{\\\\partial \\\\,\\\\text{lossD}}{\\\\partial\\\\,{\\\\text{lossD_real}}}\\\\quad \\\\text{and} \\\\quad d \\\\,\\\\text{lossD_fake}=\\\\frac{\\\\partial\\\\, \\\\text{lossD} }{\\\\partial \\\\,\\\\text{lossD_fake}} $$ respectively. Similarly for a parameter/tensor, param_1, involved in lossD_real will have an assigned value param_1.grad which is $d\\\\,\\\\text{param_1}=\\\\partial \\\\, \\\\text{lossD}/\\\\partial \\\\, \\\\text{param_1}$ and so on and so forth. - opt_disc.step(): When this line is executed, the parameters\' .grad attribute will be used to update the parameter in disc. Recall that since opt_disc = optim.Adam(disc.parameters(), lr=lr), opt_disc possses the references to those parameters. #### Stackoverflow\'s Comments on module.zero_grad(), loss.backward() and optimizer.step() - Post 1 is a numerical explanation of the what\'re happening in .backward() and .step(). <details> <summary>details</summary> <br/> Some answers explained well, but I\'d like to give a specific example to explain the mechanism. Suppose we have a function $z = 3 x^2 + y^3$. The updating gradient formula of $z$ w.r.t $x$ and $y$ is: $$ \\\\begin{aligned} x&:=x-\\\\alpha \\\\frac{\\\\partial z}{\\\\partial x} = x-\\\\alpha \\\\cdot 6x\\\\\\\\ y&:=y-\\\\alpha \\\\frac{\\\\partial z}{\\\\partial y} = y-\\\\alpha \\\\cdot 3y^2 \\\\end{aligned} $$ Initial values are $x=1$ and $y=2$. python x = torch.tensor([1.0], requires_grad=True) y = torch.tensor([2.0], requires_grad=True) z = 3*x**2+y**3 print(\\"x.grad: \\", x.grad) print(\\"y.grad: \\", y.grad) print(\\"z.grad: \\", z.grad) # print result should be: x.grad: None y.grad: None z.grad: None Then calculating the gradient of $x$ and $y$ in current value ($x=1$, $y=2$) $$ \\\\begin{aligned} \\\\frac{\\\\partial z}{\\\\partial x}&=6x=6\\\\\\\\ \\\\frac{\\\\partial z}{\\\\partial y}&= 3y^2 = 12 \\\\end{aligned} $$ python # calculate the gradient z.backward() print(\\"x.grad: \\", x.grad) print(\\"y.grad: \\", y.grad) print(\\"z.grad: \\", z.grad) # print result should be: x.grad: tensor([6.]) y.grad: tensor([12.]) z.grad: None Finally, using SGD optimizer to update the value of x and y according the formula: $$ \\\\begin{aligned} x& := x-\\\\alpha \\\\frac{\\\\partial z}{\\\\partial x} = x-0.1\\\\times 6x = 1- 0.1\\\\times 6 = 0.4\\\\\\\\ y& := y-\\\\alpha \\\\frac{\\\\partial z}{\\\\partial y} = y - 0.1\\\\times 3y^2 = 2-0.1 \\\\times 12 = 0.8 \\\\end{aligned} $$ python # create an optimizer, pass x,y as the paramaters to be update, setting the learning rate lr=0.1 optimizer = optim.SGD([x, y], lr=0.1) # executing an update step optimizer.step() # print the updated values of x and y print(\\"x:\\", x) print(\\"y:\\", y) # print result should be: x: tensor([0.4000], requires_grad=True) y: tensor([0.8000], requires_grad=True) </details> <br/> - Post 2 discusses how to avoid updating parameter by using .detach(). <details> <summary>details</summary> <br/> Let\'s say we defined a model: model, and loss function: criterion and we have the following sequence of steps: python pred = model(input) loss = criterion(pred, true_labels) loss.backward() pred will have an grad_fn attribute, that references a function that created it, and ties it back to the model. Therefore, loss.backward() will have information about the model it is working with. Try removing grad_fn attribute, for example with: python pred = pred.clone().detach() Then the model gradients will be None and consequently weights will not get updated. And the optimizer is tied to the model because we pass model.parameters() when we create the optimizer. </details> <br/> #### Tensorboard ##### Command to Look at logs Directory bash tensorboard --logdir ./logs ##### Where do logs Come From? We have imported SummaryWriter: python from torch.utils.tensorboard import SummaryWriter We have also defined separate writers: python writer_real = SummaryWriter(f\\"./logs/real\\") writer_fake = SummaryWriter(f\\"./logs/fake\\") We have run add_image method when batch_idx % 100 == 0: python for batch_idx, (real, _) in enumerate(loader): ... if batch_idx % 100 == 0: fake = gen(fixed_noise) with torch.no_grad(): img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True) img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True) writer_fake.add_image( \\"Mnist Fake Images\\", img_grid_fake, global_step=step ) writer_real.add_image( \\"Mnist Real Images\\", img_grid_real, global_step=step ) step += 1 Result: <center> <a href=\\"/assets/tech/007.png\\"> <img width=\\"420\\" src=\\"/assets/tech/007.png\\"/> </a> </center> <br/> #### Using 1.65 GB Celebrities Dataset of 202,599 Images We can download the dataset from kaggle: https://www.kaggle.com/dataset/504743cb487a5aed565ce14238c6343b7d650ffd28c071f03f2fd9b25819e6c9 Now replace our MNIST dataset by: python # dataset=datasets.MNIST(root=\\"./dataset/\\",train=True,transform=transform,download=True) dataset = datasets.ImageFolder(root=\\"dataset/celeb_dataset/\\", transform=transform) loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True) and set CHANNELS_IMG = 3. #### References - https://stackoverflow.com/questions/53975717/pytorch-connection-between-loss-backward-and-optimizer-step - https://ithelp.ithome.com.tw/articles/10216440 - https://www.youtube.com/watch?v=OljTVUVzPpM - https://www.youtube.com/watch?v=IZtv9s_Wx9I ","title":"Understand Pytorch via GAN","date":"2021-08-20T00:00:00.000Z","id":"blog0017","tags":"pytorch","intro":"Understand Pytorch by implementing a simple version of GAN and also a DCGAN."},{"content":" #### tf.data.Dataset Storing a set of images require a lot of memory, but saving a list of strings does not. In tensorflow we can base our pipeline on saving image paths. ##### tf.data.Dataset.list_files We take the horse2zebra dataset as an example. python dataset = tf.data.Dataset.list_files(\\"horse2zebra/*/**\\") We can use take to get a specific amount of images from dataset generator: bash for d in dataset.take(1): print(d) # output: tf.Tensor(b\'horse2zebra\\\\\\\\testB\\\\\\\\n02391049_1020.jpg\', shape=(), dtype=string) ##### Data Processing with tf.data.Dataset: .map(), .filter(), .cache(), .shuffle(), .batch() Most of the time our dataset is structured as dataset/class1/a.jpg, dataset/class2/b.jpg, ..., so it is much more helpful to transform the list of images into img, label format. We will construct (generated) batches of tensors (this constitutes our dataset) in the following procedures $$ \\\\begin{aligned} &\\\\quad \\\\,\\\\,\\\\,\\\\, \\\\text{image_paths}\\\\\\\\ &\\\\to \\\\text{image_path, label tensor}\\\\\\\\ &\\\\to \\\\text{image array tensor, label tensor} \\\\end{aligned} $$ These will be done with the help of .map(), which will also be used to handle data processing. For this we define: python def get_label(file_path): return tf.strings.split(file_path, os.path.sep)[-2] def path_to_imgLabel(file_path): label = get_label(file_path) img = tf.io.read_file(file_path) img = tf.image.decode_jpeg(img, channels=3) return img, label def normalize_img(img): img = tf.cast(img, dtype=tf.float32) # Map values in the range [-1, 1] return (img / 127.5) - 1.0 def preprocess_train_image(img): img = tf.image.random_flip_left_right(img) img = tf.image.resize(img, [*orig_img_size]) img = tf.image.random_crop(img, size=[*input_img_size]) img = normalize_img(img) return img Now we can chain our processing as follows: python dataset = tf.data.Dataset.list_files(\\"horse2zebra/*/**\\").map(path_to_imgLabel) # at this point our dataset generates (img, label)\'s train_horses = dataset \\\\ .filter(lambda _, label : label == \\"trainA\\") \\\\ .map(lambda img, label: img) \\\\ # this line is specific to this file, usually we keep the label .map(preprocess_train_image, num_parallel_calls=autotune) \\\\ .cache() \\\\ .shuffle(buffer_size) \\\\ .batch(batch_size) Now we can run python for d in train_zebras.take(1): print(d) to check if the batch of data suits our training purpose. #### Graph Plotting by matplotlib.pyplot We mainly use ax (array of axes) python import matplotlib.pyplot as plt _, ax = plt.subplots(4, 2, figsize=(10, 15)) for i, samples in enumerate(zip(train_horses.take(4), train_zebras.take(4))): horse = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8) zebra = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8) ax[i, 0].imshow(horse) ax[i, 1].imshow(zebra) plt.show() Result: <br/> <img width=\\"420\\" src=\\"/assets/tech/008.png\\"/> <br/><br/> #### ReflectionPadding2D This is a built-in layer in pyTorch, but in tensorflow we need to built it manually: python class ReflectionPadding2D(layers.Layer): def __init__(self, padding=(1, 1), **kwargs): self.padding = tuple(padding) super(ReflectionPadding2D, self).__init__(**kwargs) def call(self, input_tensor, mask=None): padding_width, padding_height = self.padding # no padding for batch_size and channel axis padding_tensor = [ [0, 0], [padding_height, padding_height], [padding_width, padding_width], [0, 0], ] return tf.pad(input_tensor, padding_tensor, mode=\\"REFLECT\\") #### Residual Blocks There are many version of residue blocks. In cycleGAN their blocks keep the number of filters and also the spatial dimension. In some other cases the middle Conv2D layer shrinks the filter depth and finally restored it for the addition operation in skip connection. python def residual_block( x, activation, kernel_initializer=kernel_init, kernel_size=(3, 3), strides=(1, 1), padding=\\"valid\\", gamma_initializer=gamma_init, use_bias=False, ): dim = x.shape[-1] input_tensor = x x = ReflectionPadding2D()(input_tensor) x = layers.Conv2D( dim, kernel_size, strides=strides, kernel_initializer=kernel_initializer, padding=padding, use_bias=use_bias, )(x) x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x) x = activation(x) x = ReflectionPadding2D()(x) x = layers.Conv2D( dim, kernel_size, strides=strides, kernel_initializer=kernel_initializer, padding=padding, use_bias=use_bias, )(x) x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x) x = layers.add([input_tensor, x]) return x Though the kenrnel_size is (3,3), but there will be a ReflectionPadding2D and therefore the finally the width $$ [\\\\text{width} -3 + 1] + 2 = \\\\text{width} $$ remains unchanged ($+2$ comes from padding), and so does the height. #### Model.compile() and Model.train_step() In GANs most of the last training step must be implemented manually. We record the one in cycleGAN for reference. The technique will apply to all other custom training. Note the analogy with PyTorch\'s .backward() and .step(). python class CycleGan(keras.Model): def __init__( self, generator_G, generator_F, discriminator_X, discriminator_Y, lambda_cycle=10.0, lambda_identity=0.5, ): super(CycleGan, self).__init__() self.gen_G = generator_G self.gen_F = generator_F self.disc_X = discriminator_X self.disc_Y = discriminator_Y self.lambda_cycle = lambda_cycle self.lambda_identity = lambda_identity def compile( self, gen_G_optimizer, gen_F_optimizer, disc_X_optimizer, disc_Y_optimizer, gen_loss_fn, disc_loss_fn, ): super(CycleGan, self).compile() self.gen_G_optimizer = gen_G_optimizer self.gen_F_optimizer = gen_F_optimizer self.disc_X_optimizer = disc_X_optimizer self.disc_Y_optimizer = disc_Y_optimizer self.bv = gen_loss_fn self.discriminator_loss_fn = disc_loss_fn self.cycle_loss_fn = keras.losses.MeanAbsoluteError() self.identity_loss_fn = keras.losses.MeanAbsoluteError() def train_step(self, batch_data): # x is Horse and y is zebra real_x, real_y = batch_data # For CycleGAN, we need to calculate different # kinds of losses for the generators and discriminators. # We will perform the following steps here: # # 1. Pass real images through the generators and get the generated images # 2. Pass the generated images back to the generators to check if we # we can predict the original image from the generated image. # 3. Do an identity mapping of the real images using the generators. # 4. Pass the generated images in 1) to the corresponding discriminators. # 5. Calculate the generators total loss (adverserial + cycle + identity) # 6. Calculate the discriminators loss # 7. Update the weights of the generators # 8. Update the weights of the discriminators # 9. Return the losses in a dictionary with tf.GradientTape(persistent=True) as tape: # Horse to fake zebra fake_y = self.gen_G(real_x, training=True) # Zebra to fake horse -> y2x fake_x = self.gen_F(real_y, training=True) # Cycle (Horse to fake zebra to fake horse): x -> y -> x cycled_x = self.gen_F(fake_y, training=True) # Cycle (Zebra to fake horse to fake zebra) y -> x -> y cycled_y = self.gen_G(fake_x, training=True) # Identity mapping # expect/hope that G|_{zebras} = id and F|_{horses} = idm # i.e., almost no change same_x = self.gen_F(real_x, training=True) same_y = self.gen_G(real_y, training=True) # Discriminator output disc_real_x = self.disc_X(real_x, training=True) disc_fake_x = self.disc_X(fake_x, training=True) disc_real_y = self.disc_Y(real_y, training=True) disc_fake_y = self.disc_Y(fake_y, training=True) # Generator adverserial loss gen_G_loss = self.generator_loss_fn(disc_fake_y) gen_F_loss = self.generator_loss_fn(disc_fake_x) # Generator cycle loss cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle # Generator identity loss id_loss_G = ( self.identity_loss_fn(real_y, same_y) * self.lambda_cycle * self.lambda_identity ) id_loss_F = ( self.identity_loss_fn(real_x, same_x) * self.lambda_cycle * self.lambda_identity ) # Total generator loss total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F # Discriminator loss disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x) disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y) # Get the gradients for the generators # loss.backward() as in pyTorch grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables) grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables) # Get the gradients for the discriminators disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables) disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables) # Update the weights of the generators # optimizer.step() as in pyTorch self.gen_G_optimizer.apply_gradients( zip(grads_G, self.gen_G.trainable_variables) ) self.gen_F_optimizer.apply_gradients( zip(grads_F, self.gen_F.trainable_variables) ) # Update the weights of the discriminators self.disc_X_optimizer.apply_gradients( zip(disc_X_grads, self.disc_X.trainable_variables) ) self.disc_Y_optimizer.apply_gradients( zip(disc_Y_grads, self.disc_Y.trainable_variables) ) # Conclusion: the only difference to pytorch is that we need to # wrap the calculation of loss to get the calculation graph # i.e., wrap the stuff whose weight needs to be updated. # which usually starts from the beginning of getting batch_data return { \\"G_loss\\": total_loss_G, \\"F_loss\\": total_loss_F, \\"D_X_loss\\": disc_X_loss, \\"D_Y_loss\\": disc_Y_loss, } #### Monitor python class GANMonitor(keras.callbacks.Callback): \\"\\"\\"A callback to generate and save images after each epoch\\"\\"\\" def __init__(self, num_img=4): self.num_img = num_img def on_epoch_end(self, epoch, logs=None): _, ax = plt.subplots(4, 2, figsize=(12, 12)) for i, img in enumerate(test_horses.take(self.num_img)): prediction = self.model.gen_G(img)[0].numpy() prediction = (prediction * 127.5 + 127.5).astype(np.uint8) img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8) ax[i, 0].imshow(img) ax[i, 1].imshow(prediction) ax[i, 0].set_title(\\"Input image\\") ax[i, 1].set_title(\\"Translated image\\") ax[i, 0].axis(\\"off\\") ax[i, 1].axis(\\"off\\") prediction = keras.preprocessing.image.array_to_img(prediction) prediction.save( \\"generated_img_{i}_{epoch}.png\\".format(i=i, epoch=epoch + 1) ) plt.show() plt.close() #### Start the Training Process Our gen_G and gen_F only need optimizer, we calculate the loss on our own. We also pass tensorflow built-in loss functions for convenience (which will turns out to be one of the summands in our total loss). python # Loss function for evaluating adversarial loss adv_loss_fn = keras.losses.MeanSquaredError() # Define the loss function for the generators def generator_loss_fn(fake): fake_loss = adv_loss_fn(tf.ones_like(fake), fake) return fake_loss # Define the loss function for the discriminators def discriminator_loss_fn(real, fake): real_loss = adv_loss_fn(tf.ones_like(real), real) fake_loss = adv_loss_fn(tf.zeros_like(fake), fake) return (real_loss + fake_loss) * 0.5 # Create cycle gan model cycle_gan_model = CycleGan( generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y ) # Compile the model cycle_gan_model.compile( gen_G_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5), gen_F_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5), disc_X_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5), disc_Y_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5), gen_loss_fn=generator_loss_fn, disc_loss_fn=discriminator_loss_fn, ) # Callbacks plotter = GANMonitor() checkpoint_filepath = \\"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}\\" model_checkpoint_callback = keras.callbacks.ModelCheckpoint( filepath=checkpoint_filepath ) # Here we will train the model for just one epoch as each epoch takes around # 7 minutes on a single P100 backed machine. cycle_gan_model.fit( tf.data.Dataset.zip((train_horses, train_zebras)), epochs=1, callbacks=[plotter, model_checkpoint_callback], ) #### References - https://keras.io/examples/generative/cyclegan/ ","title":"Dataset Pipeline and Custom Training in Tensorflow","date":"2021-08-26T00:00:00.000Z","id":"blog0018","tags":"tensorflow","intro":"Revist Tensorflow with a complete dataset pipeline using tf.data.Dataset and custom training via cycle-GAN."},{"content":" #### Google Drive Mounting python from google.colab import drive drive.mount(\'/content/drive\') Now we can work on colab as in we are working in local environment. #### Unzip Files Now you can cd into your working directory. For me: python ! cd \'/content/drive/My Drive/Colab Notebooks/cycleGAN-tensorflow\' Inside cycleGAN-tensorflow I have a compressed file called horse2zebra.zip. We can now unzip it by python !unzip horse2zebra.zip which will create a new folder called horse2zebra and decompress your files into it. ","title":"Colab Setting","date":"2021-08-27T00:00:00.000Z","id":"blog0019","tags":"coding, deep-learning, python","intro":"Basic command needed to mount a google drive and also to unzip compressed large dataset."},{"content":" This is simply horrible, just go to https://react-icons.github.io/react-icons and install it. ","title":"Comprehensive List of SVG Icon Available in React","date":"2021-08-27T00:00:00.000Z","id":"blog0020","tags":"react","intro":"Record a useful package that contains a comprehensive list of svg icons.","toc":false},{"content":" #### Callbacks Both callbacks below accept a startAt and step as keyword arguments. I will adjust when I start to adjust the training process (step=2 when I adjust the learning the first time, startAt will be the epoch that I want to base on). ##### EpochCheckpoint This is to save the model on every end of epoch (modulo some constant, say 5). python from tensorflow.keras.callbacks import Callback class EpochCheckpoint(Callback): def __init__(self, output_dir, step=1 , every=1, startAt=0, model_title=\\"prediction-model\\"): super(Callback, self).__init__() self.step = step self.output_dir = output_dir self.every = every self.intEpoch = startAt self.model_title = model_title def on_epoch_end(self, epoch, logs={}): if (self.intEpoch + 1) % self.every == 0: p = os.path.sep.join([self.output_dir, self.model_title + \\"epoch-{}-{}.hdf5\\".format(self.step, self.intEpoch + 1)]) self.model.save(p, overwrite=True) self.intEpoch += 1 ##### TrainingMonitorCallback tensorboard would be more helpful, but one cannot use tensorboard if the training process is held in colab. For me I would generalize the approach by using this monitor callback. Two functionalities for this callback: - In this callback we will save a figure for each epoch to track how acc, val_acc, loss and val_loss differs when epoch increases. - A complete log of this values will be stored in a json file stored in jsonPath. python from tensorflow.keras.callbacks import BaseLogger import matplotlib.pyplot as plt import numpy as np import json import os import tensorflow.keras.backend as K class TrainingMonitorCallback(BaseLogger): def __init__(self, fig_dir, step=1, jsonPath=None, startAt=0): super(TrainingMonitorCallback, self).__init__() self.fig_dir = fig_dir self.jsonPath = jsonPath self.startAt = startAt self.step=step def on_train_begin(self, logs={}): self.H = {} if self.jsonPath is not None: if os.path.exists(self.jsonPath): self.H = json.loads(open(self.jsonPath).read()) if self.startAt > 0: for k in self.H.keys(): self.H[k] = self.H[k][:self.startAt] def on_epoch_end(self, epoch, logs={}): print(\\"[INFO] learning rate: {}\\".format(K.get_value(self.model.optimizer.lr))) for (k, v) in logs.items(): l = self.H.get(k, []) l.append(float(v)) self.H[k] = l if self.jsonPath is not None: f = open(self.jsonPath, \\"w\\") f.write(json.dumps(self.H, indent=4)) f.close() if len(self.H[\\"loss\\"]) > 0: epoch = len(self.H[\\"loss\\"]) N = np.arange(0, len(self.H[\\"loss\\"])) plt.style.use(\\"ggplot\\") plt.figure() _, ax1 = plt.subplots() l1, = ax1.plot(N, self.H[\\"loss\\"], label=\\"train_loss\\", color=\\"olive\\") l2, = ax1.plot(N, self.H[\\"val_loss\\"], label=\\"val_loss\\", color=\\"red\\") ax2 = ax1.twinx() l3, = ax2.plot(N, self.H[\\"accuracy\\"], label=\\"train_acc\\", color=\\"royalblue\\") l4, = ax2.plot(N, self.H[\\"val_accuracy\\"], label=\\"val_acc\\", color=\\"midnightblue\\") plt.title(\\"Training Loss and Accuracy [Epoch {}]\\".format(len(self.H[\\"loss\\"]))) plt.xlabel(\\"Epoch #\\") plt.ylabel(\\"Loss/Accuracy\\") plt.legend([l1,l2,l3,l4], [\\"train_loss\\", \\"val_loss\\", \\"train_acc\\", \\"val_acc\\"], loc=\'lower left\') # save the figure plt.savefig(f\\"{self.fig_dir}/epoch-{self.step}-{epoch}.png\\") plt.close() ##### Usage We define python callbacks = [ EpochCheckpoint(output_dir=\\"./checkpoints\\"), TrainingMonitorCallback( fig_dir=\\"./checkpoints-figure\\", jsonPath=\\"./checkpoints-json/loss.json\\" ) ] and plug it in the model.fit\'s keyward argument, callbacks. For example: python model.fit( train_dataset, steps_per_epoch=len(train_dataset), epochs=50, validation_data=val_dataset, validation_steps=len(val_dataset), callbacks=callbacks ) Result on every end of epoches: <img width=\\"420\\" src=\\"/assets/tech/009.png\\"/> <br /><br /> #### Restart Training Based on Past Epoch When monitoring our training process, at some point the loss and val_loss will diverge or both do not decrease. Then we can interrupt the process by control + c. We load our model/model_weight using a specific path (that is stored by using our checkpoint callback). We also adjust the learning rate when we find our training becomes stagnant. Both mentioned callbacks accept step and startAt as their kwargs in the constructor. For example, my prev_model_path below means (manual adjustment) step=3 and epoch=35 accumulatively (this is the total number of epoches in the training, never reset to 0 for new step). So when our next epoch is completed, the new saved model will be 4-36.hdf5. python from tensorflow.keras.models import load_model prev_model_path = \\"./checkpoints/prediction-modelepoch-3-35.hdf5\\" start_at_epoch = 35 new_lr = 1e-4 step = 4 model = load_model(prev_model_path) print(\\"[INFO] step: {}, start at epoch: {}\\".format(step, start_at_epoch)) print(\\"[INFO] old learning rate: {}\\".format(K.get_value(model.optimizer.lr))) K.set_value(model.optimizer.lr, new_lr) print(\\"[INFO] new learning rate: {}\\".format(K.get_value(model.optimizer.lr))) callbacks = [ EpochCheckpoint(output_dir=\\"./checkpoints\\", step=step, startAt=start_at_epoch), TrainingMonitorCallback( fig_dir=\\"./checkpoints-figure\\", jsonPath=\\"./checkpoints-json/loss.json\\", startAt=start_at_epoch, step=step ) ] model.fit( train_dataset, steps_per_epoch=len(train_dataset), epochs=50, validation_data=val_dataset, validation_steps=len(val_dataset), callbacks=callbacks ) ","title":"Tensorflow Callbacks and Restart Training Process Based on Past Epoches","date":"2021-08-28T00:00:00.000Z","id":"blog0021","tags":"tensorflow","intro":"Introduce useful callbacks that I use in monitoring training process. Also introduce how to retrain the model from a specific epoach."},{"content":" #### Preprocessing As a usual practice every classifier starts with pre-processing the dataset for normalization so that - the model needs not to learn the distribution of specific feature among the dataset and; - focus on just learning the features. To make use of VGG-16 we need to carry out exactly the same data pre-processing: python from tensorflow.keras.applications.vgg16 import preprocess_input we also import the following as a routine: python import tensorflow as tf from tensorflow.keras import optimizers as optim from tensorflow.keras import losses from tensorflow.keras import metrics from tensorflow.keras.applications import VGG16 from tensorflow.keras import Input from tensorflow.keras import layers from tensorflow.keras.models import Model #### Dataset Pipeline with VGG\'s preprocess_input In the sequel our label will be an nonnegative integer. As we have no incentive to construct one-hot vectors as labels, we will be using sparse-categorical-entropy loss. python def get_label(file_path): return tf.strings.split(file_path, os.path.sep)[-2] def path_to_imgLabel(file_path): label = tf.cast(tf.strings.to_number(get_label(file_path)), dtype=tf.int32) img = tf.io.read_file(file_path) img = tf.image.decode_jpeg(img, channels=3) return img, label def preprocess_train_image(img, label): img = tf.image.resize(img, (224, 224)) img = preprocess_input(img) return img, label train_dataset = tf.data.Dataset.list_files(\\"./trainingset/*/**\\") \\\\ .map(path_to_imgLabel) \\\\ .map(preprocess_train_image, num_parallel_calls=autotune) \\\\ .cache() \\\\ .shuffle(buffer_size) \\\\ .batch(batch_size) val_dataset = tf.data.Dataset.list_files(\\"./validationset/*/**\\") \\\\ .map(path_to_imgLabel) \\\\ .map(preprocess_train_image, num_parallel_calls=autotune) \\\\ .cache() \\\\ .batch(batch_size) #### Network Surgery ##### Objective Suppose that I need to classify a dataset into 544 labels, then I want to make use of the features extracted by VGG-16, flatten and classify these features by our fully connected layers. ##### Implementation We start off by constructing VGG16 model without head: python vgg_feature_model = VGG16( weights=\\"imagenet\\", include_top=False, input_tensor=Input((224,224,3)) ) As inspected from vgg_feature_model.summary() the VGG model accepts inputs of shape (None, 224, 224, 3). This is why we define vgg_feature_model this way. vgg_feature_model in fact accepts a input tensor of flexible shapes in its input_tensor arguement. Sometimes we may also want python vgg_feature_model = VGG16(..., input_tensor=Input((256,256,3))) because (256, 256) is a common image size in datasets, as long as the image size is not too far from (224, 224) we are fine. Next we define our feed-forward network for classification: python def forward_to_head(feature_model): feature = feature_model.output feature = layers.Flatten()(feature) head = layers.Dense(1024)(feature) head = layers.Dropout(0.5)(head) head = layers.Dense(544)(head) head = layers.Softmax()(head) return head output = forward_to_head(vgg_feature_model) model = Model(vgg_feature_model.input, output) ##### Start the Training Since our feed-forward network has no trained weights on any image data, there is a huge imbalance between the performance of VGG-16 and that of our network. For better result, we first freeze the training parameters: python for layer in vgg_feature_model.layers: layer.trainable = False and warm-up our dense network: python model.compile( optimizer=optim.Adam(learning_rate=1e-2, global_clipnorm=1), loss=\\"sparse_categorical_crossentropy\\", metrics=[\\"accuracy\\"] ) model.fit( train_dataset, steps_per_epoch=len(train_dataset), epochs=50, validation_data=val_dataset, validation_steps=len(val_dataset), callbacks=callbacks ) We would not expect very good result at this point, undesired phenomenon would arise such as increasing losses or stagnant accuracies. After a few epoches (the \\"few\\" is also a hyper-parameter for us to figure out). We can stop it by control + c and: python for layer in vgg_feature_model.layers: layer.trainable = True model.fit( train_dataset, steps_per_epoch=len(train_dataset), epochs=50, validation_data=val_dataset, validation_steps=len(val_dataset), callbacks=callbacks ) ","title":"Transfer Learning Based on VGG-16","date":"2021-08-28T00:00:00.000Z","id":"blog0022","tags":"deep-learning, tensorflow","intro":"In classification tasks there are already state-of-the-art models trained from a myriad of images. We try to make a network surgery on one of them (VGG-16 this time) to quickly classifiy our custom dataset with good result."},{"content":" #### Define Our Mapping Functions to Tensorflow Dataset ##### Image path to (img, label) format We start from pre-processing functions that handle the validation dataset and also arbitrary images with which we will feed into our model: python from tensorflow.keras.applications.vgg16 import preprocess_input autotune = tf.data.experimental.AUTOTUNE def get_label(file_path): return tf.strings.split(file_path, os.path.sep)[-2] def path_to_imgLabel(file_path): label = tf.cast(tf.strings.to_number(get_label(file_path)), dtype=tf.int32) img = tf.io.read_file(file_path) img = tf.image.decode_jpeg(img, channels=3) return img, label def preprocess_train_image(img, label): img = tf.image.resize(img, (vgg_img_size, vgg_img_size)) img = preprocess_input(img) return img, label ##### Image Augmentations I - The Basic Pipeline Since tf.imaga can just provide us a limited amounts of augmentations to our input data, like tf.image.random_flip_left_right, for a richer resouce of augmentation we try to import albumentations library: python from albumentations import ( Compose, RandomBrightnessContrast, ImageCompression, HueSaturationValue, HorizontalFlip, Rotate ) and define our transforms that will be applied to images when we take a batch of images from generator: python transforms = Compose([ Rotate(limit=40), RandomBrightnessContrast(p=0.5), ImageCompression(quality_lower=85, quality_upper=100, p=0.5), HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5), HorizontalFlip() ]) We define the data augmentation function (the last line, image_augmentation) that will take img, label as arguments: python from functools import partial def aug_fn(image, img_size): data = {\\"image\\": image} aug_data = transforms(**data) aug_img = aug_data[\\"image\\"] aug_img = tf.image.resize(aug_img, size=[img_size, img_size]) return aug_img def image_augmentation(image, label, img_size): aug_img = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32) img_shape=(img_size, img_size, 3) aug_img.set_shape(img_shape) return aug_img, label image_augmentation = partial(image_augmentation, img_size=vgg_img_size) ##### Image Augmentations II - Same Augmentations to a pair of Images In image segmentations our image augmentations have to be carried out equally to both the images and masks. Suppose our image training dataset, ./cars, has a associated mask dataset, ./mask, we pair them as follows: python dataset = tf.data.Dataset.list_files(\\"./cars/*\\") def path_to_imgLabel(file_path): mask_filepath = tf.strings.regex_replace(file_path, \\"cars\\", \\"masks\\" ) mask_filepath = tf.strings.regex_replace(mask_filepath, \\".jpg\\", \\"_mask.gif\\" ) img = tf.io.read_file(file_path) img = tf.image.decode_jpeg(img) img = tf.image.resize(img, (img_size, img_size)) mask = tf.io.read_file(mask_filepath) mask = tf.image.decode_gif(mask) mask = tf.image.resize(mask, (img_size, img_size)) mask = tf.reshape(mask, (img_size, img_size, 3)) mask = tf.reduce_mean(mask, axis=-1, keepdims=True) return img, mask train_data = dataset.map(path_to_imgLabel) Next we apply color-invariant augmentations to the pair (img, mask)\'s and color-related augmentation to img\'s, for that we define two transformations: python transforms_general = Compose( [ Rotate(limit=40), HorizontalFlip(), ], additional_targets={\'mask0\': \'image\'} ) transform_color = Compose( [ RandomBrightnessContrast(p=0.5), ImageCompression(quality_lower=85, quality_upper=100, p=0.5), HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5), ] ) they kwarg additional_targets={\'mask0\': \'image\'} means that mask0 will share the same set of transformation with image, and after the transformation we can retrieve the transformed mask by the key mask0. **_Note the tricky part here_**. It might be tempting to define additional_targets={\'mask\': \'image\'}, but unfortunately there are 4 reserved keywords that cannot be used as a key in the dictionary for additional_targets. From <a href=\\"https://albumentations.ai/docs/examples/example_multi_target/\\">documentation</a> these are image, mask, bboxes and keypoints. Now we define the next pair of functions which bring the transformations above into play: python def aug_fn(image, mask): transformed = transforms_general(image=image, mask0=mask) image = transformed[\'image\'] mask = transformed[\'mask0\'] aug_data = transform_color(image=image) image = aug_data[\'image\'] return image, mask def image_augmentation(image, mask): transformed = tf.numpy_function(func=aug_fn, inp=[image, mask], Tout=[tf.float32, tf.float32]) aug_img = transformed[0] aug_mask = transformed[1] aug_img.set_shape((img_size, img_size, 3)) aug_mask.set_shape((img_size, img_size, 1)) return aug_img, aug_mask Beware of the argument Tout above, it has to be a list that specifies the data type of the output. Otherwise OperatorNotAllowedInGraphError would occur since the function has no idea what is the data type that tf.numpy_function will return. #### Finish the Pipeline The strategy of our complete dataset pipeline is to: 1. take path to img, label; 2. then map the images by functions taking img, label to img, label multiple times, which depends on the number of mapping we need. For example, in the chain of .map(image_augmentation).map(preprocess_train_image) below we are free to remove the augmentation part and leave preprocess_train_image alone (which is what it is initially when there is no augmentation implemented). python train_dataset = tf.data.Dataset.list_files(\\"./train_dataset/*/**\\") \\\\ .map(path_to_imgLabel) \\\\ .map(image_augmentation, num_parallel_calls=autotune).prefetch(autotune) \\\\ .map(preprocess_train_image, num_parallel_calls=autotune) \\\\ .cache() \\\\ .shuffle(buffer_size) \\\\ .batch(batch_size) val_dataset = tf.data.Dataset.list_files(\\"./val_dataset/*/**\\") \\\\ .map(path_to_imgLabel) \\\\ .map(preprocess_train_image, num_parallel_calls=autotune) \\\\ .cache() \\\\ .batch(batch_size) #### Read the Result of Image Augmentation As usual we import: python import matplotlib.pyplot as plt and define: python def view_image(ds): image, label = next(iter(ds)) # extract 1 batch from the dataset image = image.numpy().astype(\'uint8\') label = label.numpy().astype(\'uint8\') fig = plt.figure(figsize=(22, 22)) for i in range(20): ax = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[]) ax.imshow(image[i]) ax.set_title(f\\"Label: {label[i]}\\") Finally we run: python view_image(train_dataset) Result: <br/> <br/> <center> <a href=\\"/assets/tech/010.png\\"> <img width=\\"420\\" src=\\"/assets/tech/010.png\\"/> </a> </center> <br/> #### References - https://www.youtube.com/watch?v=rAdLwKJBvPM - https://albumentations.ai/docs/examples/example_multi_target/ ","title":"Image Augmentation with Custom Dataset Pipeline","date":"2021-08-29T00:00:00.000Z","id":"blog0023","tags":"tensorflow","intro":"We introduce <a href=\\"https://albumentations.ai/docs/getting_started/installation/\\">albumentations</a> for image augmentation that helps generalize our model to unknown data."},{"content":" #### Introduction I: Scrap Elements by Tagname and ClassName We start off by importing three libraries: python from selenium import webdriver from bs4 import BeautifulSoup import time We will browse our webpage by webdriver.Chrome. We will then use BeautifulSoup to extract data based on HTML file structure such as tag name, such as class, etc. We use open rice as a data source as an example: python url = \\"https://www.openrice.com/zh/hongkong/restaurants?what=%E5%8F%B0%E9%A2%A8\\" We sleep for 10 seconds to make sure all datas have been loaded before getting page_source: python browser = webdriver.Chrome(\\"C:/Users/user/Repos/Python/2021-07-20-TrySelenium/chromedriver.exe\\") browser.get(url) time.sleep(10) html = browser.page_source Here html is a string of HTML source. We can now parse it using BeautifulSoup as follows python soup = BeautifulSoup(html, \\"html.parser\\") We now scrap all div elements that bear the classname content-cell-wrapper: python print(len(soup.find_all(\\"div\\", {\\"class\\": \\"content-cell-wrapper\\"}))) And we get 8. If we further study the HTML structure, we can distill the data by using regular expression. #### Introduction II: Automatic Images Scrapping Through Google ##### Discussion and Pattern We note that searching name in google images is the same as having GET-request to the https://www.google.com/search?q=name&tbm=isch&hl=en&sa=X, therefore we can combine selenium and BeautifulSoup to scrap all img elements. We also note that each img grasps the src attribute to detemine the image source. However, not every src is of the form http://....jpg, .png, .gif, etc which provides a direct GET-request to the image. Indeed the frontend may create an url(which is to be specified in src) using URL.createObjectURL(). This url is not a direct link to the image anymore, instead this will become a binary data represented by base-64 encoded string. For example, an image may require access tokens in cookie/local storage, for which the frontend developer cannot put an image link to src as it will create a get request **_without_** header. ##### Code Implementation We divide the steps by functions from top to bottom. The method scrap accepes labels, which is used to - create GET-requests for image scrapping and; - create a folder ./download_dir/{label} which stores all the scrapped images. python # download ChromeDriver first according to your chrome version # we need the path for .exe file. from selenium import webdriver from bs4 import BeautifulSoup import time import requests import re import base64 import os import sys class GoogleScrapper: def __init__( self, query_url=\\"https://www.google.com/search?q={}&tbm=isch&hl=en&sa=X\\", download_dir=\\"./scrapped_birds\\", chromedriver_path=\\"C:/Users/jameslcc/Desktop/chromedriver.exe\\" ): self.query_url = query_url self.download_dir = download_dir self.chromedriver_path = chromedriver_path def scrap_img_els(self, label, wait_before_scrapping=0): url = self.query_url.format(label) browser = webdriver.Chrome(self.chromedriver_path) browser.get(url) time.sleep(wait_before_scrapping) html = browser.page_source browser.close() soup = BeautifulSoup(html, \\"html.parser\\") return soup.find_all(\\"img\\") def download_imgs(self, img_els, label): class_dir = f\\"{self.download_dir}/{label}\\" if not os.path.exists(class_dir): os.mkdir(class_dir) for i, img_el in enumerate(img_els): img_src = img_el.get(\'src\', \'\') try: # if src refers to external link if img_src.startswith(\\"http\\"): r = requests.get(img_src) if r.status_code == 200: contype_type = r.headers.get(\'content-type\') # image/png, image/jpeg, etc if contype_type.startswith(\\"image/\\"): file_ext = re.sub(\\"image/\\", \\"\\",contype_type) file_path = f\\"{class_dir}/{label}-{i}.\\" + file_ext with open(file_path, \'wb\') as f: f.write(r.content) print(f\\"{file_path} has been saved\\") # if src refers to internal link if img_src.startswith(\\"data:\\"): # example: data:image/jpeg;base64,/9j/4AAQSkZJRgABAQ ... head, data = img_src.split(\',\', 1) file_ext = head.split(\';\')[0].split(\'/\')[1] plain_data = base64.b64decode(data) with open(f\\"{class_dir}/{label}-{i}.\\" + file_ext, \'wb\') as f: f.write(plain_data) print(f\\"{class_dir}/{label}-{i}.\\" + file_ext+ \\" has been saved\\") except: error_msg = sys.exc_info()[1] # raise Exception(error_msg) print(error_msg) def scrap(self, labels, wait_before_scrapping = 0): # catter for the case when labels is simply a string, we want a list of labels if isinstance(labels, str): labels = [labels] for label in labels: img_els = self.scrap_img_els(label, wait_before_scrapping) self.download_imgs(img_els, label) scrapper = GoogleScrapper() ","title":"Scrapping Images with Selenium and Beautifulsoup on Chrome","date":"2021-08-30T00:00:00.000Z","id":"blog0024","tag":"python","intro":"Record a flow of data-scrapping."},{"content":" This is an exercise from udemy course in which I need to create a function apply_all which accepts parameters int* arr_1, size_t size_1, int* arr_2, size_t size_2 such that arr_1 of size size_1 and arr_2 of size size_2 are multiplied to generate an array of size $\\\\text{size_1}\\\\times \\\\text{size_2}$. For example, array_1=[1, 2, 3] and array_2=[10, 20] are multiplied to produce [10, 20, 30, 20, 40, 60]. We first include the following standard library: cpp #include <iostream> using namespace std; Next our apply_all will produce a pointer pointing to an array allocated in **_heap memory_**: cpp int* apply_all(int* arr_1, size_t size_1, int* arr_2, size_t size_2) { \\tsize_t total_size{ size_1 * size_2 }; \\t// when using new, we are allocating new memory in heap to store the integer array. \\tint* new_arr = new int[total_size]; \\tint index{ 0 }; \\tfor (size_t j{ 0 }; j < size_2; j++) { \\t\\tfor (size_t i{ 0 }; i < size_1; i++) { \\t\\t\\t*(new_arr + index) = arr_2[j] * arr_1[i]; \\t\\t\\tindex++; \\t\\t} \\t} \\treturn new_arr; } **Warning to myself.** We cannot create a local variable inside apply_all and simply return the address of that variable. Since local variable are saved in **_stack memory_**, memory allocated in the function will be poped out/deallocated once the result is returned. The resulting address will point to garbage data. Next we define a simple function to print arrays: cpp void print(int* const result, int const size) { \\tcout << \\"Result: \\"; \\tcout << \\"[ \\"; \\tfor (int i{ 0 }; i < size; i++) { \\t\\tcout << *(result + i) << \\" \\"; \\t} \\tcout << \\"]\\" << endl; } Finally we combine the above result: cpp int main() { \\tint array_1[5]{ 1,2,3,4,5 }; \\tint array_2[3]{ 10,20,30 }; \\tint* result = apply_all(array_1, 5, array_2, 3); \\tcout << \\"Array1: \\"; \\tprint(array_1, 5); \\tcout << \\"Array2: \\"; \\tprint(array_2, 3); \\tprint(result, 15); \\t// raw pointer, deallocate memory: \\tdelete[] result; } the output in console is: text Array1: Result: [ 1 2 3 4 5 ] Array2: Result: [ 10 20 30 ] Result: [ 10 20 30 40 50 20 40 60 80 100 30 60 90 120 150 ] **Remark.** The variable name of an raw array in C++ in fact stores an address to the first element of that array. Therefore the variable name of an array is interchangeable with pointers in many use case. ","title":"C++ Beginner Notes 01 - Stack and Heap","date":"2021-09-08T00:00:00.000Z","id":"blog0025","tag":"C++","intro":"We write simple functions to understand cpp syntax, memory in stack and memory in heaps by using raw pointer.","toc":false},{"content":" #### Shallow Copy and Trouble We first define a class of objects for which we will do a shallow copy: cpp class Shallow { private: \\tint* data; public: \\tvoid set_data(int d) { \\t\\t*data = d; \\t}; \\tint get_data() { \\t\\treturn *data; \\t}; \\tShallow(int d); \\tShallow(const Shallow& source); \\t~Shallow(); }; Shallow::Shallow(int d) { \\tdata = new int; \\t*data = d; \\tcout << \\"1-arg constructor called\\" << endl; } Shallow::Shallow(const Shallow& source) : \\tdata{ source.data } { \\tcout << \\"copy constructor called\\" << endl; }; Shallow::~Shallow() { \\tdelete data; \\tcout << \\"destructor called to free memory\\"; }; We will be doing a shallow copy using copy constructor above. The destructor defined above is legit, but then a problem will be raised when any instance of Shallow is out of scope of some function, say: cpp void display_object(Shallow s) { \\t// error still occurs even the following line is commented out: \\tcout << s.get_data() << endl; } The reason is that our copy constructor just copy the address of the object\'s data just passed in. But when a destructor of a shallow copy is called, the original object will have data pointing to an invalid data, compiler will then throw an error. Let\'s simulate this problem below: cpp int main() { \\tShallow obj_1{ 100 }; \\tShallow obj_2{ obj_1 }; \\t// since pointing to the same data (originally 100) in heap, \\t// both obj_1.data and obj_2.data become 1000: \\tobj_2.set_data(1000); \\t// when obj_2 is out of scope, destructor is called: \\tdisplay_object(obj_2); \\t// obj_1.data points to invalid data, error occurs and we fail to return status code 0: \\treturn 0; } <center> <img src=\\"/assets/tech/012.png\\"/> </center> <br/> #### Deep Copying Instead Since pointing to the same allocated memory is a trouble. All data of any new instance of a class through copy constructor should point to the new allocated memory instead. This is easily adjustable by modifying the copy constructor as below: cpp Shallow::Shallow(const Shallow& source) : \\tShallow{ *source.data } { \\tcout << \\"copy constructor called\\" << endl; }; In other words, we delegate our copy constructor to our 1-argument constructor which is designed originally to allocate new memory in heap. Now with exactly the same code, our main can run faultlessly. #### Further Discussion with l-value As another observation: cpp int main() { \\tint x{ 100 }; \\t// or int y = x; \\tint y{ x }; \\ty = 1000; \\tcout << x << \' \' << y << endl; \\treturn 0; } We can observe that the output is text 100 1000 Therefore the copy constructor of int is indeed a deep copy (always allocate a new memory). But then how to make a new variable y that really makes a reference to x? We will need a concept called l-value reference, which is done by calling cpp int main() { \\tint x{ 100 }; \\tint& y = x; \\ty = 1000; \\tcout << x << \' \' << y << endl; \\treturn 0; } and this time the output is text 1000 1000 The l-value concept is usually introduced before we witness the definition of function prototypes like int func(type &variable). Yes the & operator here indeed indicates we want a l-value variable to be passed in. #### Move-Semantics ##### Use Case The opposite of l-value is r-value and this kind of reference is indicated by &&. The concept of r-value becomes very useful when it comes to move-semantics. Loosely speaking the most easily understandable use cases are: - When a data is to be constructed and passed into an l-value. For example, we may assign a large object as a member of a class, and this object is not going to be reused **_anywhere else_**. - When we want to pass a large object into a function which is not going to be reused **_anywhere else_**. The main problem to solve is > Creating an temp object and copying it for another object/function is inefficient. We would like to implement a move contructor which can reduce the number of times calling the copy constructor. The move constructor usually accepts an r-value for initiallization. There are two ways to construct r-value: - Return value of a function without assigning it to any l-value. - Object initialization without variable name. ##### Code Implementation of Move Constructor We rename the Shallow class above to Data as we do not focus on shallow copy anymore. Now the whole implementation of the Data class is (we haved just added additionally one constrctor below and changed the logging in destructor): cpp #include <iostream> using namespace std; class Data { private: \\tint* data; public: \\tint get_data() { \\t\\treturn *data; \\t}; \\tData(int d); \\tData(const Data& source); \\tData(Data&& source); \\t~Data(); }; Data::Data(int d) { \\tdata = new int; \\t*data = d; \\tcout << \\"1-arg constructor called\\" << endl; } Data::Data(const Data& source) : \\tData{ *source.data } { \\tcout << \\"copy constructor called\\" << endl; }; Data::Data(Data&& source) { \\tdata = source.data; \\tsource.data = nullptr; \\tcout << \\"move constructor called\\" << endl; } Data::~Data() { \\tif (data != nullptr) { \\t\\tcout << \\"destructor called to free memory\\" << endl; \\t} \\tdelete data; }; void display_object(Data s) { \\tcout << \\"Shallow.data: \\" << s.get_data() << endl; } Data create_data() { \\tData new_shallow{ 100 }; \\treturn new_shallow; } int main() { \\t// This call the move constructor: \\tdisplay_object(create_data()); \\t// This does not: \\t// display_object(Data{ 123 }); \\t// This does: \\t// display_object(std::move(Data{ 123 })); } We can observe that the **_unreused_** (we assume this is the case) variable returned from create_data() does not trigger the copy constructor, for which the copy constructor is supposed to allocate new memory to store our temporary object. Now the function call will be way more efficient if the object to pass into the function is very large. #### References - https://www.youtube.com/watch?v=IOkgBrXCtfo ","title":"C++ Beginner Notes 02 - Shallow Copy, Deep Copy and Move Semantics","date":"2021-09-09T00:00:00.000Z","id":"blog0026","tag":"C++","wip":false,"intro":"We list some potential problem of using shallow copy and how to avoid them by deep copy. We can also improve computation efficiency when a variable is never reused but needed to be passed into a function/class attribute. We achieve this by using move constructor."},{"content":" #### Basic Article in xeLaTeX latex \\\\documentclass[12pt,a4paper]{article} \\\\usepackage{amsmath} \\\\usepackage{amsfonts} \\\\usepackage{amssymb} \\\\usepackage[dvipsnames]{xcolor} \\\\usepackage{graphicx} \\\\usepackage[left=2.4cm,right=2.4cm,top=2cm]{geometry} \\\\usepackage{hyperref} \\\\usepackage{array} \\\\usepackage{enumitem} \\\\usepackage{graphbox} \\\\usepackage{float} \\\\usepackage[AutoFakeSlant=0.1, AutoFakeBold=true]{xeCJK} \\\\setCJKmainfont{SimSun} \\\\usepackage{multicol} \\\\linespread{1.4} \\\\thispagestyle{empty} \\\\begin{document} \\\\end{document} #### Math in Times with Adjusted BigOp Symbols ","title":"$\\\\LaTeX$ template","date":"2021-09-10T00:00:00.000Z","id":"blog0028","tag":"latex","intro":"Record some starting template that I made in the past.","toc":false},{"content":" #### \u539f\u756b\u8655\u7406 \u770b\u8655\u7406\u6d41\u7a0b\uff0c\u53ef\u4ee5\u9810\u5148\u628a\u7dda\u7a3f\u62c6\u958b\u518d\u5c40\u90e8\u5730\u628a\u4f5c\u54c1\u5b8c\u6210\u3002\u5982\u679c\u50cf\u6211\u4e00\u6a23\u6700\u7d42\u5f69\u5716\u7684\u4e0a\u8272\u5716\u5c64\u53ea\u6709\u4e00\u5c64\u7684\u8a71\uff0c\u4fbf\u8981\u518d\u7d30\u5fc3\u628a\u6700\u7d42\u6210\u54c1\u62c6\u958b\u3002 \u56e0\u70ba\u4fee\u5716\u57fa\u672c\u4e0a\u5fc5\u9808\uff0c\u4f5c\u756b\u4e5f\u4e0d\u7528\u5c0d\u7dda\u7a3f\u592a\u57f7\u7740\u3002\u4f5c\u54c1\u6574\u9ad4\u9060\u6bd4\u6700\u7d42\u7684\u7dda\u689d\u7cbe\u7dfb\u6027\u91cd\u8981\u3002\u6700\u5f8c\u6211\u5011\u9084\u5f97\u7528\u539a\u5857\u7684\u65b9\u5f0f\u628a\u5c0d live2D \u4f86\u628a\u4e0d\u5b8c\u6574\u7684\u90e8\u5206\u88dc\u5b8c\uff0c\u90a3\u6642\u5019\u518d\u628a\u7dda\u78e8\u7cbe\u7d30\u4e00\u9ede\u4e5f\u4e0d\u665a\u3002 <p>\u4f8b\u5982\u9019\u5f35\u89d2\u8272\u539f\u672c\u9577\u9019\u6a23\uff1a</p> <img width=\\"600\\" src=\\"/assets/tech/019.png\\"/> <p/> <center></center> \u5728\u5b8c\u6210 live2D \u5f8c\u9577\u9019\u6a23\uff1a <img width=\\"600\\" src=\\"/assets/tech/018.png\\"/> <p/> <center></center> \u56e0\u70ba\u7b2c\u4e00\u6b21\u6703\u8e29\u4e0d\u5c11\u5751\uff0c\u800c\u4e14\u5728\u88fd\u4f5c\u904e\u7a0b\u4e0d\u53ef\u80fd\u907f\u514d\u56de\u982d\u53bb\u4fee\u539f\u672c\u7684 psd \u6a94\u6848\uff0c\u56e0\u6b64\u5fc5\u9808\u61c2\u5f97\u5982\u4f55\u5728 live2D \u5c0e\u5165\u66f4\u6539\u904e\u7684 psd \u6a94\u6848\uff1a #### \u66f4\u6539\u904e\u7684 psd \u6a94\u6848\u5c0e\u5165 \u4f8b\u5982\u6211\u7684 .cmo3 \u6a94 (live2D \u5132\u5b58\u6a94\u6848\u7684\u683c\u5f0f) \u5f9e\u4e00\u958b\u59cb\u5c31\u4f7f\u7528\u4e86\u540d\u70ba 14.psd \u7684\u6a94\u6848\u3002\u7576\u6211\u5011\u4fee\u6539 14.psd \u5f8c\uff0c\u53ef\u4ee5\u628a psd \u6a94\u76f4\u63a5\u62c9\u5230 live2D \u7684\u5de5\u4f5c\u8996\u7a97\u9032\u884c\u66f4\u65b0\u3002 <img width=\\"600\\" src=\\"/assets/tech/020.png\\"/> <p/><p/> <img src=\\"/assets/tech/021.png\\"/> <p/> <img src=\\"/assets/tech/022.png\\"/> <br/><br/> #### Live2D \u5c40\u90e8\u7684\u6ce8\u610f\u4e8b\u9805 \u4e94\u5b98\u7684\u8655\u7406\u624b\u6cd5\uff0cAngle X Angle Y Angle Z \u7684\u89d2\u5ea6\u8655\u7406\uff0c\u7b49\u7b49\u7b49\u7b49\uff0c\u90fd\u975e\u5e38\u63a8\u85a6\u770b\u300c\u53c3\u8003\u300d\u90e8\u5206\u7684 **Live2D \u8d85\u5165\u9580\u8b1b\u5ea7\u7cfb\u5217**\u3002 \u4e94\u5b98\u4e2d\u6709\u4e9b\u90e8\u5206\u4e0d\u5bb9\u6613\u6709\u5751\u53ef\u4ee5\u8e29\uff0c\u50cf\u982d\u9aee\uff0c\u8033\u554a\uff0c\u9f3b\u4e4b\u985e\u7684\u90fd\u662f\u975e\u5e38\u5b89\u5168\u7684\u90e8\u5206\u3002\u4ee5\u4e0b\u7d00\u9304\u6211\u89ba\u5f97\u6709\u5fc5\u8981\u8a18\u4f4f\u7684\u5730\u65b9\uff1a ##### \u982d\u9aee \u9664\u975e\u8ddf\u6211\u7684\u4f8b\u5b50\u4e00\u6a23\u89d2\u8272\u5e36\u6709\u5e3d\u5b50\uff0c\u4e0d\u7136\u6b63\u5e38\u60c5\u6cc1\u4e0b\u982d\u9aee\u9700\u5206\u70ba - \u8907\u6578 (\u6216\u4e00) \u500b\u524d\u9aee - \u8033\u6735\u524d\u7684\u5169\u689d\u5074\u9aee - \u653e\u5728\u6700\u5f8c\u9762\u7684**\u5f8c\u9aee** \u982d\u90e8\u5206\u6700\u7406\u60f3\u662f\u5b8c\u6574\u7684\u79bf\u982d\u982d\u578b\u3002\u4f46\u4e0d\u4e00\u5b9a\u662f\u79bf\u982d\uff0c\u96d6\u7136\u5f88\u591a\u6642\u770b\u4e0d\u5230\u4f46\u53ef\u4ee5\u628a**\u9aee\u6839\u7d66\u756b\u51fa\u4f86**\u3002 ##### \u773c \u65b0\u624b\u7684\u7b2c\u4e00\u500b\u5751\u3002\u901a\u5e38 CG \u5c11\u5973\u90fd\u5e36\u6709 - \u776b\u6bdb\uff1b - \u5b64\u5ea6\u5411\u4e0a\u7684\u4e0a\u534a\u90e8\uff1b - \u5b64\u5ea6\u5411\u4e0b\u7684\u4e0b\u534a\u90e8\uff1b - \u773c\u73e0 (\u8acb\u4fdd\u7559\u5b8c\u6574\u7684\u6a62\u5713\u578b)\u3002 \u9019\u4e9b\u5728 live2D \u88fd\u4f5c\u904e\u7a0b\u90fd\u5fc5\u9808\u88ab\u5206\u62c6\u51fa\u4f86\u3002 <img src=\\"/assets/tech/023.png\\"/> ##### \u53e3 \u53e3\u5206\u70ba\u4e0a\u5507\u8ddf\u4e0b\u5507\uff0c\u770b\u98a8\u683c\u53ef\u4f7f\u7528\u5169\u6392\u7259\u9f52\uff1a <img src=\\"/assets/tech/024.png\\"/> <p/> <center></center> \u6211\u5011\u5229\u7528\u4e0a\u5507\u8ddf\u4e0b\u5507\u7684\u5f62\u72c0\u4f86\u505a\u51fa\u4e0d\u540c\u7684\u60c5\u611f\u3002 \u53e6\u5916\u56e0\u70ba\u53e3\u90e8\u8b8a\u578b\u5c24\u70ba\u8907\u96dc\uff0c\u5916\u52a0\u4e0a\u6211\u5011\u6709\u9700\u8981\u6642\u523b\u66f4\u6539 mesh \u7684\u5f62\u72c0\u4f86\u906e\u6a94\u6709\u6a5f\u6703\u9732\u51fa\u7684\u53e3\u8154\u5167\u90e8\u3002\u56e0\u6b64\u63a8\u85a6\u9019\u7a2e mesh \u7684\u5206\u4f48\u65b9\u6cd5\uff1a <img width=\\"600\\" src=\\"/assets/tech/025.png\\"/> <img width=\\"600\\" src=\\"/assets/tech/026.png\\"/> <p/><p/> <center></center> \u597d\u8655\uff1a > \u5728 live2D \u88cf mesh \u4ee5\u5916\u7684\u5340\u57df\u6703\u88ab\u81ea\u52d5\u522a\u9664\u4e26\u4e0d\u986f\u793a\u5728\u756b\u9762\u4e0a\u3002\u7576\u6211\u5011\u903c\u4e0d\u5f97\u5df2\u9700\u8981 \u589e\u52a0 / \u522a\u6e1b \u4f5c\u70ba\u906e\u64cb\u7684\u8272\u584a\u5340\u57df\u7684\u6642\u5019\uff0c\u6211\u5011\u53ef\u4ee5\u8abf\u6574\u6700\u5916\u5468\u7684\u7bc0\u9ede\u800c\u4e0d\u5f71\u97ff\u6700\u5167\u5c64\u7d93\u7531 deformer \u9020\u6210\u7684\u8b8a\u5316\u3002 \x3c!-- #### \u907f\u514d\u4f7f\u7528\u81ea\u5b9a\u7fa9\u53c3\u6578\uff0c\u7406\u89e3\u9810\u8a2d\u53c3\u6578\u4f7f\u7528\u65b9\u5f0f #### Deformers ##### Grid-based Surface ##### Rotation --\x3e #### \u6210\u54c1 <center> <video controls width=\\"500\\"> <source src=\\"/assets/videos/001.mp4\\" type=\\"video/mp4\\"> Sorry, your browser doesn\'t support embedded videos. <video/> </center> <p/><p/> #### \u53c3\u8003/\u6559\u5b78 - Live2D \u8d85\u5165\u9580\u8b1b\u5ea7\u7cfb\u5217\uff1a https://www.youtube.com/watch?v=rl3XFoLf3XA&list=PL_B-UPbBHi7TY2K-Wah40rlX3FvD7Expp - <a href=\\"https://www.youtube.com/watch?v=LXV4Q4e1RbU\\"> \u7528Live2D Cubism & FaceRig\u6210\u70baVTuber\u6216\u9059\u8ddd\u4e0a\u5b78\u5427! </a> \u4e00\u4e9b facerig \u8ddf obs \u7684\u57fa\u672c\u89c0\u5ff5\uff0c\u91cd\u9ede\u662f\u6b64\u5f71\u7247 description \u4e2d\u7684\u9023\u7d50\uff1a - <a href=\\"https://www.cg-method.com/live2d-facerig-parameters/\\"> live2D \u88cf\u54ea\u4e9b\u9810\u8a2d\u53c3\u6578\u53ef\u76f4\u63a5\u5728 facerig \u4e2d\u4f7f\u7528 </a> \u5982\u679c\u4e0d\u5e78\u4f7f\u7528\u4e86\u81ea\u5b9a\u7fa9\u53c3\u6578\u9020\u51fa\u5f88\u6f02\u4eae\u7684\u6548\u679c\uff0c\u90a3\u5c0d\u4e0d\u8d77\u4e86\u4f60\u8981\u5b78\u7fd2\u5982\u4f55\u628a\u81ea\u5b9a\u7fa9\u53c3\u6578\u53d6\u4ee3 facerig \u9ed8\u8a8d\u7684\u53c3\u6578 (\u6709\u597d\u5e7e\u500b\u5730\u65b9\u6211\u56e0\u6b64\u8981\u780d\u6389\u91cd\u505a)\uff1a - <a href=\\"https://www.cg-method.com/facerig-custom-parameter/\\"> \u30ab\u30b9\u30bf\u30e0\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\uff06Live2D \u306e\u30a2\u30cb\u30e1\u30fc\u30b7\u30e7\u30f3\uff08\u30e2\u30fc\u30b7\u30e7\u30f3\uff09\u306e\u8a2d\u5b9a\u65b9\u6cd5\u307e\u3068\u3081 </a> \u9019\u4e9b\u9ed8\u8a8d\u53c3\u6578\u4e0d\u4f46\u9650\u5236\u4e86\u53ef\u4f7f\u7528\u7684\u53c3\u6578\u540d\u7a31\uff0c\u9084\u9650\u5236\u4e86\u4ed6\u5011\u53ef\u4f7f\u7528\u7684**_\u503c\u57df_**\u3002\u4f8b\u5982\u81ea\u5b9a\u7fa9\u53c3\u6578\u9001\u7d66\u4f60 -30 \u5230 30 \u9019\u500b\u5340\u57df\uff0c\u7d93\u904e\u52aa\u529b\u641c\u5c0b\u73fe\u5728\u9084\u6c92\u6709\u65b9\u6cd5\u628a\u5b83 scale \u6210 0 \u5230 1 \u9593\u7684\u53c3\u6578\uff0c\u5fc5\u9808\u91cd\u505a\uff01\u6240\u4ee5\u9084\u662f\u4e56\u4e56\u7528\u9810\u8a2d\u53c3\u6578\u5427\uff01 ","title":"Live2D \u5fc3\u5f97","date":"2021-09-18T00:00:00.000Z","id":"blog0029","tag":"live2D","intro":"\u7d00\u9304\u4e00\u4e9b\u5751\u8ddf\u9084\u6709\u8a18\u61b6\u7684\u6b65\u9a5f"},{"content":" #### Chrome Options and Driver A ChromOptions object is instantiated as follows: python from selenium import webdriver options = webdriver.ChromeOptions() Available methods of ChromeOption object can be found in the <a href=\\"https://selenium-python.readthedocs.io/api.html?highlight=option#selenium.webdriver.chrome.options.Options.add_experimental_option\\">documentation of Selenium </a>. Common usages: - for .add_argument, for a list of arguments we refer to <a href=\\"https://chromium.googlesource.com/chromium/src/+/refs/heads/main/chrome/common/chrome_switches.cc\\">here</a>. For example, if we want to maximize the window on chrome launched, then write option.add_argument(\\"--start-maximized\\"). - for preference, we use .add_experimental_option, for a list of preferences we refer to <a href=\\"https://chromium.googlesource.com/chromium/src/+/refs/heads/main/chrome/common/pref_names.cc\\">here</a>. I am not sure whether .set_preference would override the default preference. In our case we want to specify the default download directory (which cannot be changed after the Driver object is instantisated): python options = webdriver.ChromeOptions() prefs = {\\"download.default_directory\\" : f\\"{os.getcwd()}/download\\"} options.add_experimental_option(\\"prefs\\", prefs) Now we build our driver using our options. python browser = webdriver.Chrome(executable_path=DRIVER_PATH, chrome_options=options) #### Implicit Wait From the <a href=\\"https://selenium-python.readthedocs.io/waits.html\\">documentation of Selenium</a> implicit-wait is: > An implicit wait tells WebDriver to poll the DOM for a certain amount of time when trying to find any element (or elements) not immediately available. The default setting is 0 (zero). Once set, the implicit wait is set for the life of the WebDriver object. We will discuss **_explicit-wait_** later in the discussion of downloading files. In my situation I choose to wait for 10 seconds as dom elements may take time to render: python browser.implicitly_wait(10) #### Select Element and Click In case an automation task can be done where every desired dom element can be selected by id attribute, we define the following util function: python def get_el_by_id(id): el = browser.find_element_by_id(id) assert el is not None, f\\"Element of id: {id} cannot be found\\" return el A list of methods to find dom element(s) can be found <a href=\\"https://selenium-python-zh.readthedocs.io/en/latest/locating-elements.html\\">here</a>. ##### Buttons Selecting a button and click is as simple as: python submit_btn = get_el_by_id(\\"butSubmit\\") submit_btn.click() ##### Dropdown List and Selection We can select an element in dropdown list by using their value attribute: python from selenium.webdriver.support.select import Select # dropdown_id: the id of the dropdown dom element # target_value: the value attribute of our target selection_list = Select(get_el_by_id(dropdown_id)) selection_list.select_by_value(target_value) #### Download Files and Explicit Wait As usual we select an element that would trigger download action and click it: python confirm_btn = get_el_by_id(\\"downloadBtn\\") confirm_btn.click() Next we look at the download directory: python import time def every_downloads_chrome(browser): if not browser.current_url.startswith(\\"chrome://downloads\\"): browser.get(\\"chrome://downloads/\\") return browser.execute_script(\\"\\"\\" var items = document.querySelector(\'downloads-manager\') .shadowRoot.getElementById(\'downloadsList\').items; if (items.length == 0) { return [\\"no_download\\"] } if (items.every(e => e.state === \\"COMPLETE\\")) { return items.map(e => e.fileUrl || e.file_url); } \\"\\"\\") # waits for all the files to be completed and returns the paths time.sleep(4) # timeout for 2 minutes paths = WebDriverWait(browser, 120, 1).until(every_downloads_chrome) assert len(paths) >= 1 and paths[-1] != \\"no_download\\", \\"No file is being downloaded\\" # get the name of lastly downloaded file: latest_download_filename = paths[-1].split(\\"/\\")[-1] # print the downloaded file for filepath in paths: print(f\\"Files donwloaded: {filepath}\\") Points to note: - Since website may have a delay after clicking the download button, we wait for 4 seconds to make sure there is a file to be downloaded. - We also handle the case that the server has an internal error that ruins our download process. In this case, we return [\\"no_download\\"] when items.length == 0. Note that the return value [] wouldn\'t stop the waiting process by experiment. Documentation for WebDriverWait: https://selenium-python.readthedocs.io/api.html?highlight=WebDriverWait. #### References - <a href=\\"https://stackoverflow.com/questions/38335671/where-can-i-find-a-list-of-all-available-chromeoption-arguments\\">Stackoverflow - List of possible options for chrome driver</a> - <a href=\\"https://stackoverflow.com/questions/48263317/selenium-python-waiting-for-a-download-process-to-complete-using-chrome-web/48267887\\">Stackoverflow - Explit-wait for downloading files</a> ","title":"Automation Task for Chrome","date":"2021-10-02T00:00:00.000Z","id":"blog030","tag":"python, selenium","intro":"Simple click and download (and wait for its completion!) tasks that are achieved in python."},{"content":" When performing unit tests we usually import modules in project-directory/src. For this to work, we have to make sure sys.path contains our project-directory when running pytest, so that python from src.utils.module_name import SomeClass becomes possible. It is as simple as executing (instead of pytest -vv test/...) text python -m pytest -vv test/... and make sure we have cded into project directory when running this script. ","title":"How to Include Project Directory in sys.path when Running pytest","date":"2021-10-20T00:00:00.000Z","id":"blog032","tag":"python","intro":"As titled.","toc":false},{"content":" #### Modifier Keys - ^ Control Key - # Windows Key - + Shift - ! Alt #### My Usages autohotkey KeyWait, Shift KeyWait, LWin KeyWait, RWin #NoEnv #SingleInstance #MaxHotkeysPerInterval 3000 ;Process, Priority, , H SendMode Input ; Show scroll velocity as a tooltip while scrolling. 1 or 0. tooltips := 0 ; The length of a scrolling session. ; Keep scrolling within this time to accumulate boost. ; Default: 500. Recommended between 400 and 1000. timeout := 500 ; If you scroll a long distance in one session, apply additional boost factor. ; The higher the value, the longer it takes to activate, and the slower it accumulates. ; Set to zero to disable completely. Default: 30. boost := 30 ; Spamming applications with hundreds of individual scroll events can slow them down. ; This sets the maximum number of scrolls sent per click, i.e. max velocity. Default: 60. limit := 60 ; Runtime variables. Do not modify. distance := 0 vmax := 1 ; Key bindings WheelUp:: Goto Scroll WheelDown:: Goto Scroll #WheelUp:: Suspend #WheelDown:: Goto Quit Scroll: t := A_TimeSincePriorHotkey if (A_PriorHotkey = A_ThisHotkey && t < timeout) { ; Remember how many times we\'ve scrolled in the current direction distance++ ; Calculate acceleration factor using a 1/x curve v := (t < 80 && t > 1) ? (150.0 / t) - 1 : 1 ; Apply boost if (boost > 1 && distance > boost) { ; Hold onto the highest speed we\'ve achieved during this boost if (v > vmax) vmax := v else v := vmax v *= distance / boost } ; Validate v := (v > 1) ? ((v > limit) ? limit : Floor(v)) : 1 if (v > 1 && tooltips) QuickToolTip(\\"\xd7\\"v, timeout) MouseClick, %A_ThisHotkey%, , , v } else { ; Combo broken, so reset session variables distance := 0 vmax := 1 MouseClick %A_ThisHotkey% } return Quit: QuickToolTip(\\"Exiting Accelerated Scrolling...\\", 1000) Sleep 1000 ExitApp QuickToolTip(text, delay) { ToolTip, %text% SetTimer ToolTipOff, %delay% return ToolTipOff: SetTimer ToolTipOff, Off ToolTip return } ^space::#space +WheelDown::WheelLeft +WheelUp::WheelRight #Up:: Send {LWin down}{Tab down} KeyWait, Up Send {LWin up}{Tab up} return ^#A:: Winset, Alwaysontop, , A #5:: Send {ctrl down}{shift down}{alt down}{5} KeyWait, 5 Send {ctrl up}{shift up}{alt up} Return #4:: Send {ctrl down}{shift down}{alt down}{4} KeyWait, 4 Send {ctrl up}{shift up}{alt up} Return #3:: Send {ctrl down}{shift down}{alt down}{3} KeyWait, 3 Send {ctrl up}{shift up}{alt up} Return #2:: Send {ctrl down}{shift down}{alt down}{2} KeyWait, 2 Send {ctrl up}{shift up}{alt up} Return #1:: Send {ctrl down}{shift down}{alt down}{1} KeyWait, 1 Send {ctrl up}{shift up}{alt up} Return #0:: Send {ctrl down}{shift down}{alt down}{0} KeyWait, 0 Send {ctrl up}{shift up}{alt up} Return #If !(WinActive(\\"ahk_exe ffxiv_dx11.exe\\") or WinActive(\\"ahk_exe Photoshop.exe\\") or WinActive(\\"ahk_exe chrome.exe\\") or WinActive(\\"ahk_exe Code.exe\\") or WinActive(\\"TwitchUI.exe\\")) ^w:: Send !{Space}n Return #If #IfWinActive ahk_exe Unity.exe !MButton:: !LButton Return #ifWinActive #IfWinActive ahk_exe Substance Painter.exe #!LButton:: Send {Ctrl down}{RButton down} KeyWait, LButton Send {Ctrl up}{RButton up} Return MButton:: Send {Ctrl down}{Alt down}{Lbutton down} KeyWait, MButton Send {Ctrl up}{Alt up}{Lbutton up} Return !MButton:: Send {Alt down}{Lbutton down} KeyWait, MButton Send {Alt up}{Lbutton up} Return WheelUp::Send {WheelDown} WheelDown::Send {WheelUp} Return #ifWinActive #IfWinActive ahk_exe sai2.exe #!LButton::^!LButton Esc::Send +{Home} ^LButton:: Send {Shift down}{Ctrl down}{LButton down} KeyWait, LButton Send {Shift up}{Ctrl up}{LButton up} Return MButton:: Send {Space down}{LButton down} KeyWait, MButton Send {Space up}{LButton up} Return #ifWinActive #IfWinActive ahk_exe ZBrush.exe MButton:: Loop{ GetKeyState, state, Alt If state = D Break else Send {Alt down}{Rbutton down} KeyWait, MButton Send {RButton Up} Sleep, 1 Send {alt up} Return } Return !MButton:: Send {Rbutton down} KeyWait, MButton Send {RButton Up} Return #ifWinActive #IfWinActive ahk_exe maya.exe Mbutton::!MButton !Mbutton:: !LButton Return #IfWinActive #IfWinActive ahk_exe Photoshop.exe #!LButton::#!RButton !^RButton::!^LButton MButton:: While GetKeyState(\\"MButton\\", \\"P\\") { Send {Space down}{LButton} } Return $^z:: while(GetKeyState(\\"CTRL\\", \\"p\\") && GetKeyState(\\"z\\", \\"p\\")) { If (A_TimeSinceThisHotkey < 250) { Sendinput ^z sleep, 250 } else { SendInput ^z Sleep, 100 } } Return $^y:: While (GetKeyState(\\"CTRL\\", \\"p\\") && GetKeyState(\\"y\\", \\"p\\")) { If (A_TimeSinceThisHotkey < 250) { Sendinput ^y sleep, 250 } else { SendInput ^y Sleep, 100 } } Return #ifWinActive #IfWinActive ahk_exe InDesign.exe MButton:: Send {Space down}{LButton down} KeyWait, MButton Send {Space up}{LButton up} Return #ifWinActive #IfWinActive ahk_exe ffxiv_dx11.exe NumpadEnd::l NumpadAdd:: Send {CtrlDown}{F1} KeyWait, NumpadAdd Send {CtrlUp} Return +NumpadAdd:: Send {CtrlDown}{F4} KeyWait, NumpadAdd Send {CtrlUp} Return NumpadEnter:: Send {CtrlDown}{F2} KeyWait, NumpadEnter Send {CtrlUp} Return NumLock::Return NumpadSub:: Send {CtrlDown}{F3} KeyWait, NumpadSub Send {CtrlUp} Return +NumpadSub:: Send {CtrlDown}{F5} KeyWait, NumpadSub Send {CtrlUp} Return NumpadMult:: Send {CtrlDown}{F6} KeyWait, NumpadMult Send {CtrlUp} Return Numpad0:: Send {CtrlDown}{F7} KeyWait, Numpad0 Send {CtrlUp} Return NumpadDot:: Send {CtrlDown}{F8} KeyWait, NumpadDot Send {CtrlUp} Return #G:: Send {AltDown}{F12} KeyWait, G Send {AltUp} Return NumpadDel:: SendInput, {Shift Down}={Shift Up} Return #ifWinActive ","title":"Auto Hotkey Record","date":"2021-10-17T00:00:00.000Z","id":"blog033","tag":"coding, autohotkey","intro":"A record of my latest autohotkey setup for different applications.","toc":false},{"content":" #### Debuggable Test File We have discussed how to include project directory in sys.path when running pytest. Sometimes it is much more convenient to debug a utility function rather than testing it. In order to make statements like from src.utils.abc import Abc **_always_** possible, our sys.path has to include the directory path that contains the folder src. Suppose I want to record a concrete usage of a utility class in a test test/unit/test_abc.py, where: python # test/unit/test_abc.py from src.utils.abc import Abc def test_abc(): abc = Abc() abc.start() if __name__ == \\"__main__\\": test_abc() Now test_abc.py is not only a testable file, it is also debuggable because we can trigger debug action in vscode as if we are running the script directly (so that __name__ == \\"__main__\\"). An appropriate configuration for debugger has to be made in order to make src folder accessible. #### Debugger Configuration: .vscode/launch.json json // .vscode/launch.json { \\"version\\": \\"0.2.0\\", \\"configurations\\": [ { \\"name\\": \\"Python: Current File\\", \\"type\\": \\"python\\", \\"request\\": \\"launch\\", \\"program\\": \\"${file}\\", \\"console\\": \\"integratedTerminal\\", \\"env\\": { \\"PYTHONPATH\\": \\"${workspaceRoot}\\" } } ] } Now happy debugging (just press F5). ","title":"Python Debugger with Project Directory Included in sys.path","date":"2021-10-25T00:00:00.000Z","id":"blog034","tag":"python","intro":"Record a structure that makes a test_sth.py debuggable and make sure project directory path is included in sys.path when debug mode is enabled."},{"content":" #### Folder Structure We start from the structure, sqs_client folder contains a utility class SqsClient in __init__.py which I have written in the past. <center> <img src=\\"/assets/tech/027.png\\"/> </center> <p/> Originally all functions are written within a class defined in __init__.py. It causes mental fatigue when there is a bunch of functions but to only one of them we want to focus. It also violates the **_separation of concerns_** principle for each single .py file (as a module) when all functionality are written in just a single file. You may also notice I have an underscore in _listen.py. As in other languages this notation has something to do with privateness. However: #### Naming Convention --- Can we Make Class Methods Private? In python there is **_no concept_** of private methods, the best you can do is to use **_name mangling_** as in: python class A: def __some_method(self): pass But users can still get access to it by a = A() and a._A__some_method(). Worse still, functions named with double underscored as prefix cannot be imported by any other files. This makes our approach of decomposing a class impossible (as we shall see). A single underscore _ is enough to indicate a function should just be used internally (you can inspect it form other built-in modules in python like os). In IDEs, these methods will be sorted at the bottom in auto-completion. #### Implementation of Code Separation As usual our __init__.py will be the entry point of our class and: python # __init__.py class SqsClient: def __init__(self,...): self.sqs_client = ... # we remove the following function and import it from send_message.py # def send_message(self, message=\\"n/a\\"): # self.sqs_client.send_message(MessageBody=message) from ._listen import _listen from .send_message import send_message from .send_test_message import send_test_message from .start_listening_test_message import start_listening_test_message from .add_receive_message_handler import add_receive_message_handler from .start_listening import start_listening And we take send_message.py as an example: python # send_message.py def send_message(self, message=\\"n/a\\"): self.sqs_client.send_message(MessageBody=message) As we can see: - We can copy and paste the methods written in __init__.py directly to another file without any modification of code (this make any refactoring rather simple!). - In separated file we can still get access to attributes assigned to self. When we init an instance by sqs_client = SqsClient(), we get access to all methods! We can run something like python sqs_client.send_message(message=\\"123\\") sqs_client.start_listening() # ... etc and our __init__.py now just plays the role as an entry point. ","title":"Decompose a Class into Separate Files","date":"2021-10-26T00:00:00.000Z","id":"blog035","tag":"python","intro":"When a class grows to 500 to 1000 lines of codes it becomes hard and tedious to maintain because there starts to be many class methods, some are short, some are huge. It is a good starting point to separate particularly long functions from the class with ***very very minimal effort***, but how?"},{"content":" #### Datetime Object Construction ##### From Constructor python from datetime import datetime datetime(year, month, day, hour) datetime(year, month, day, hour, minute) datetime(year, month, day, hour, minute, secons) ... are all valid method overloading for datetime object construction. ##### From String ###### From Local Time String to Local Datetime python from datetime import datetime hk_time = \\"2021-11-11 10:32:34.126377\\" hk_time_datetime = datetime.strptime(hk_time, \'%Y-%m-%d %H:%M:%S.%f\') ###### From Local Time String to UTC Datetime python from datetime import datetime, timezone local_time = \\"2021-11-11 10:32:34.126377\\" local_time_datetime = datetime.strptime(local_time, \'%Y-%m-%d %H:%M:%S.%f\') # local datetime object utc_time_datetime = local_time_datetime.astimezone(timezone.utc) print(local_time_datetime.hour, utc_time_datetime.hour) # output: 10, 2 ###### From UTC Time String to Local Datetime python from datetime import datetime, timezone from dateutil import tz utc_time = \\"2021-11-11 10:32:34.126377\\" utc_time = datetime.strptime(utc_time, \'%Y-%m-%d %H:%M:%S.%f\') # This is still a local datetime utc_time_datetime = utc_time.replace(tzinfo=timezone.utc) # correct time-zone info local_zone = tz.tzlocal() hk_time_datetime = utc_time_datetime.astimezone(local_zone) ##### From Timestamp No matter which timezone we use, we still get the unique timestamp as below: python local = local_time_datetime.timestamp utc = utc_time_datetime.timestamp print(local, utc) # result: 1636597954.126377, 1636597954.126377 Therefore timestamp is also a good data normalization of time, and the following script will do: python local_datetime_obj = datetime.fromtimestamp(timestamp) Also note that we have the following conversion as timestamp may be generated from frontend which use javascript: python js_timestamp = int(py_timestamp * 1000) py_timestamp = js_timestamp / 1000 #### Datetime Object Manipulation ##### Timedelta as Summand Sometimes we want to **_calculate_** the offset time of a given time. For example in a GPS track recording project, a trip may start at datetime start_time and end at datetime end_time. We need GNSS base stations data that are within two hours before start_time and 1 hour after end_time, then we do the following operations: python from datetime import timedelta offset_start_time = start_time - timedelta(hours=2) offset_end_time = end_time + timedelta(hours=1) the resulting object is still a datetime object. Now we can download the base stations data from Geodetic Survey of Hong Kong (GSHK) by converting them to utc+0 format (we can use .astimezone(timezone.utc) to generate utc datetime) and getting the corresponding hours. ##### Timedelta from Subtraction Since base stations data are not immediately available at the time we stop our trip. We need to check whether the current time (the time we download the differential data) is far enough from datetime object end_time, say 15 minutes. python from datetime import datetime difference = datetime.now() - end_time # This is a Timedelta object diff_in_min = difference.total_seconds() / 60 # difference.seconds is not always what we want if diff_in_min >= 15: # download data pass ","title":"Datetime Object in Python","date":"2021-11-20T00:00:00.000Z","id":"blog037","tag":"python","intro":"Record a list of useful utility functions in handling date-time object in python."},{"content":" #### Lunch.json The following configuration can enable us to run whatever file in debug mode. json { \\"version\\": \\"0.2.0\\", \\"configurations\\": [ { \\"type\\": \\"node\\", \\"request\\": \\"launch\\", \\"name\\": \\"Debug File\\", \\"program\\": \\"${file}\\", \\"env\\": { \\"NODE_ENV\\": \\"some-stage\\", \\"BDSONS_DB_PWD\\": \\"some-password\\" }, \\"cwd\\": \\"${workspaceFolder}\\" } ] } It is worth noting that some test cases also require specific environment variable. But how do we actually run a test file using library like ava (which is very similar to pytest)? For that we need an analog of if __name__ == \\"__main__\\": in python, which turns out to be js if (require.main === module) { // our test function } By using these we can design a test file that can be both executed by test library and also by developer individually. ","title":"Debug Javascript For Individual File","date":"2021-12-01T00:00:00.000Z","id":"blog038","tag":"javascript","intro":"Sometimes it is convenient to directly execute a test file to understand how the project work. In the past we discussed how to make a runnable test file in python, in javascript we can use exactly the same approach for debugging."},{"content":" #### The Boilerplate For Next.js and Electron there is an official boilerplate for new comers to work on: <a href=\\"https://github.com/vercel/next.js/tree/canary/examples/with-electron-typescript?fbclid=IwAR2DFf6dHAIEpaTp16FMfq-cUwIfwBXhgLuzscU6wkB_NCF4Bz-fRtS21W4\\">Link for Next.js, Electron and Typescript</a> Unforturnately there is no documentation on how to set up a debugger for this kind of \\"side product\\". #### Debugger Configuation in Vscode ##### The launch.json File Having struggled for a while I finally come up with the following configuration: json // <project-dir>/.vscode/launch.json { \\"version\\": \\"0.2.0\\", \\"configurations\\": [ { \\"type\\": \\"node\\", \\"request\\": \\"launch\\", \\"name\\": \\"Next: Node\\", \\"runtimeExecutable\\": \\"npm\\", \\"runtimeArgs\\": [\\"run\\", \\"dev\\"], \\"port\\": 9229, \\"console\\": \\"integratedTerminal\\" } ] } ##### The next.config.js File Next in the root directory create a file named next.config.js. Inside it we add: js // <project-dir>/next.config.js module.exports = { webpack: (config) => { config.output = config.output || {}; config.output.devtoolModuleFilenameTemplate = function (info) { return \\"file:///\\" + encodeURI(info.absoluteResourcePath); }; return config; }, }; Now we can press F5 to start npm run dev in debug mode. It works in frontend as well but in vscode I mostly debug backend script since chrome dev tool is much more convenient for debugging frontend code. ","title":"Debug Nextjs with Typescript in VSCode","date":"2021-12-02T00:00:00.000Z","id":"blog039","tag":"javascript, react, nextjs","intro":"I am working on a desktop application by using Electron + Next.js in typescript. Since I cannot live without debugger, I have spent time searching debug config in the internet and finally come into a functioning configuration!"},{"content":" #### Result for the Moment The following is a demo of the application I have made so far by Next.js and Electron.js. <center> <video controls width=\\"500\\"> <source src=\\"/assets/videos/002.mp4\\" type=\\"video/mp4\\"> Sorry, your browser doesn\'t support embedded videos. <video/> </center> <p/> This is essentially the remake of an app I made using WPF, with which to achive this stage of result it took me almost 2 month to learn and work it out. But with next.js and electron it just took me about a week, in which I made this app when I am off my work. <p/> #### Starting Template Next.js\'s creator, Vercel, has a starting template <a href=\\"https://github.com/vercel/next.js/tree/canary/examples/with-electron-typescript?fbclid=IwAR07uNHSsGiBtukkMq8aXdv6maFhmRahu6_pKbxbjD9W14ielfZt6EN_Aw0\\"> <b>here</b></a> to begin with. #### Redux in Electron The usual routine of adding redux stores also applies to Electron application with Next (the Render part). To add debugger for redux, we need: text yarn add electron-devtools-installer and add javascript if (process.env.NODE_ENV === \\"development\\") { app.whenReady().then(async () => { const installer = await import(\\"electron-devtools-installer\\"); const REDUX_DEVTOOLS = installer.REDUX_DEVTOOLS; const installExtension = installer.default; installExtension(REDUX_DEVTOOLS) .then((name) => console.log(Added Extension: ${name})) .catch((err) => console.log(\\"An error occurred: \\", err)); }); } right before app.on(\'ready\', ...). #### Lessons I learn ##### Should we use pages/api? **_No_**. The resulting production build is nothing more than a static page. Of course in development we can call api to communicate with operating system, but afterwards there is no way to deploy your application to anywhere else. Instead, please keep using frontend\'s event emitter --- the ipcRenderer and the backend event receiver, the ipcMain. In my application I have wrapped up these two components into two functions in order to just focus on the data flow: js // ipc-renderer-util.ts const emit = <S, T>(eventKey: string, data: S) => { return ( new Promise() < T > ((resolve, _reject) => { global.ipcRenderer.once(eventKey, (_event, data: T) => { resolve(data); }); global.ipcRenderer.send(eventKey, data); }) ); }; export default { emit, }; js // ipc-main.util.ts import { ipcMain } from \\"electron\\"; import { IpcMainEvent } from \\"electron/main\\"; const listen = <S, T>(eventKey: string, callback: (data: S) => Promise<T>) => { console.log(\\"eventKey\\", eventKey); ipcMain.on(eventKey, (event: IpcMainEvent, data: S) => { callback(data).then((result) => { event.sender.send(eventKey, result); }); }); }; export default { listen, }; Usage in \\"frontend\\" (the web view): js const addNote = async (note: Note) => { const res = await ipcRendererUtil.emit<Note, CustomApiResponse>( \\"ADD_NOTE\\", note ) return res; } Usage in \\"backend\\" (the file running Electron): js ipcMainUtil.listen < Note, CustomApiResponse > (\\"ADD_NOTE\\", async (note) => { const result = (await db.notes.insert) < Note > note; return { success: true, result }; }); ##### Images in Next.js can just be accessed in ./public folder. How to get rid of this Restriction? At some point we may want the location to store images to be more controllable. We can achieve this by the following config: js // next.config.js const withImages = require(\\"next-images\\"); module.exports = withImages({ webpack: (config) => { config.output = config.output || {}; config.output.devtoolModuleFilenameTemplate = function (info) { return \\"file:///\\" + encodeURI(info.absoluteResourcePath); }; return config; }, images: { disableStaticImages: true, }, }); ##### How to get standard directory path such as Pictures, Documents, etc? We use ~/Documents as an example: js import { app } from \\"electron\\"; app.on(\\"ready\\", async () => { app.getPath(\\"documents\\"); // path for documents }); A complete list of possible paths can be found in this <a href=\\"https://www.electronjs.org/docs/latest/api/app#appgetpathname\\">documentation</a>. ##### How to convert imagepath into base64 encoded data? As I am doing it so often, I have summarized it in the following function. I make use of the conversion between img and canvas element: js const getBase64DataUrlFromImagePath = (imgPath: string) => { return ( new Promise() < { base64DataURL: string, width: number, height: number } > ((resolve, reject) => { const img = new Image(); img.src = imgPath; img.onload = () => { const _canvas = document.createElement(\\"canvas\\"); _canvas.width = img.width; _canvas.height = img.height; const _ctx = _canvas.getContext(\\"2d\\"); if (_ctx) { _ctx.drawImage(img, 0, 0, img.width, img.height); const dataURL = _canvas.toDataURL(); resolve({ base64DataURL: dataURL, width: img.width, height: img.height, }); } }; }) ); }; export default getBase64DataUrlFromImagePath; If one wants a base64 encoed string instead of a dataUrl, the following conversion js const base64Data = base64DataURL.replace(/^data:image\\\\/png;base64,/, \\"\\"); will do. ##### How to effectively make a \'click outside\' handler in React? We create the following hook: js import { MutableRefObject } from \\"hoist-non-react-statics/node_modules/@types/react\\"; import { useEffect } from \\"react\\"; export default <T = any>({ onClickOutside, targetRef, }: { onClickOutside: (e: MouseEvent) => void, targetRef: MutableRefObject<T | null>, }) => { const handleClickOutside = (event: any) => { // @ts-ignore if (targetRef.current && !targetRef.current.contains(event.target)) { onClickOutside(event); } }; useEffect(() => { document.addEventListener(\\"click\\", handleClickOutside, true); return () => { document.removeEventListener(\\"click\\", handleClickOutside, true); }; }); }; If we click outside the targetRef.current, our onClickOutside should take effect. ##### How to make right click context menu? The react-contextmenu package is very simple to use, highly suggested! https://www.npmjs.com/package/react-contextmenu ##### How to do persistent data storage? Any other choices apart from sqlite? In the past I am just used to sqlite. A deeper consideration has been made since this is my second time to make a desktop application rigorously. Locally what I really need is no more than a few collections of json data, and in this direction I come into nedb, which is a mongo based local storage, each collection is saved in a *.db file with exactly the same query as we learn from mongoose (we can even borrow types in mongoose library when using nedb). In fact, each *.db file is nothing more than rows of json data, which can be read directly in any text editor, easy to read and debug! Since in the original nedb library the only way to get data is by callback, we can switch to another highly related library https://www.npmjs.com/package/nedb-promises which has promisified all the functions in nedb for us. ##### How to crop an image and get the base64DataUrl using canvas? This will be quite a long story to get correct coordinates. Given that I have x, y (the upper-left vertix) and width, height prepared from a convas, then our whole workflow: js const getCroppedImageAndExtractText = async (pageId: string) => { if (!canvasRef.current) { return; } const x = Math.floor(coorRef.current.X_0_scaled); const y = Math.floor(coorRef.current.Y_0_scaled); const width = Math.floor(coorRef.current.width_scaled); const height = Math.floor(coorRef.current.height_scaled); console.log(x, y, width, height); const _canvas = document.createElement(\\"canvas\\"); _canvas.width = width; _canvas.height = height; const _ctx = _canvas.getContext(\\"2d\\"); if (!_ctx) { return; } _ctx.drawImage(canvasRef.current, x, y, width, height, 0, 0, width, height); const base64DataURL = _canvas.toDataURL(); }; Here coordinates stored in coorRef get updated by our mousemove eventHandler. ##### How to use the vision-api from google to do text extraction? From the previous step we have base64DataURL, we can then convert it to base64Data, convert it to Buffer object and call vision-api library: js import vision from \\"@google-cloud/vision\\"; export async function textExtraction(base64Data: string) { try { const client = new vision.ImageAnnotatorClient(option); const buffer = Buffer.from(base64Data, \\"base64\\"); const [textDetections] = await client.textDetection(buffer); //@ts-ignore const [annotation] = textDetections.textAnnotations; return annotation.description; } catch (err) { console.log(err); return null; } } ##### How to make event listener to enable \'widen a div by dragging the edge\'? **Stategy.** - Record the current width of the div in a ref variable and fix it. - When mouse down, we start to record the change in width. - On mouse move, we change the state of width by calculating the increment in width, and add it to ref, then setState. - When mouse up, we update the current width of the div in ref. Note that in this process there will be intermediate event listeners that should be being deleted, otherwise they will get accumulated when we repeat the drag and move process. js const mousedownHandler = useCallback((e) => { if (!mouseIsDownRef.current) { window.addEventListener(\\"mousemove\\", mousemoveHandler) originalVocabListClientRectLeft.current = vocabListRef.current?.offsetWidth || 0 mouseIsDownRef.current = true; oldClientX.current = e.clientX; listenerCacheRef.current.push({ eventKey: \\"mousemove\\", listener: mousemoveHandler }); dragbarWasHit.current = resizeBarRef.current?.contains(e.target) as boolean; } else { mouseIsDownRef.current = false; } }, []) const mousemoveHandler = useCallback((e: globalThis.MouseEvent) => { // @ts-ignore if (resizeBarRef.current && dragbarWasHit.current && mouseIsDownRef.current) { const increment = oldClientX.current - e.clientX; setVocablistWidth((originalVocabListClientRectLeft.current || 0) + increment); } }, []) const mouseupHandler = useCallback((e) => { setDragBarColorOpacity(0); mouseIsDownRef.current = false; mouseIsMovingRef.current = false; dragbarWasHit.current = false; originalVocabListClientRectLeft.current = vocabListRef.current?.offsetWidth || 0 if (listenerCacheRef.current) { const numOfListeners = listenerCacheRef.current.length; for (let i = 0; i < numOfListeners; i++) { const pop = listenerCacheRef.current.pop(); if (pop) { const { eventKey, listener } = pop; window.removeEventListener(eventKey, listener) } } } }, []) useEffect(() => { window.addEventListener(\\"mousedown\\", mousedownHandler) window.addEventListener(\\"mouseup\\", mouseupHandler) }, []) ##### Usage of debounce in lodash debounce is used when there will be several similar calls within a short interval, and we just want to trigger the last call. Usages: search field, or text field that gets updated automatically. The usual strategy is to make these fields an uncontrolled component, we just look at the onChange event, and add a debounce function (that triggers update) in this onChange function. The event-targets (like buttons, textfields, etc) are itself internally stateful, we will use a stateless function to make reference to a stateful uncontrolled component, get updated information and trigger our debounced event handler. Therefore our event handler needs not to update for triggering new function, this makes debounce possible. **Example 1.** js ... const searchDounce = useCallback(debounce((e: ChangeEvent<HTMLInputElement>) => { FuncStore const value = searchRef.current?.value; if (value === \\"\\" || value) { dispatch(dictSlice.actions.setSearchText(value)); } }, 800), []) const searchHandler = (e: ChangeEvent<HTMLInputElement>) => { searchDounce(e); } ... return ( ... <input placeholder={\\"Search\\"} ref={searchRef} onChange={searchHandler} /> ... ) **Example 2.** In this example we have api-call after the text editing in a textarea is complete. Extremely good use case of debounce: js // logic that should not be triggered repeatedly is wrapped in a debounce: const saveTextDebounce = useCallback( debounce(async () => { if (selectedPageId) { const textAreaValue = textAreaRef.current?.value || \\"\\"; const res = await apiUtil.updatePage(selectedPageId, { $set: { imageText: textAreaValue.trim() }, }); const { success, message } = res; dispatch( uiSlice.actions.updateNotification({ open: true, message: success ? \\"message saved succesfully\\" : JSON.stringify(message), severity: success ? \\"success\\" : \\"error\\", }) ); } }, 800), [selectedPageId] ); const changeHandler = (e: ChangeEvent<HTMLTextAreaElement>) => { if (selectedPage) { const text = e.target.value; dispatch( dictSlice.actions.setSelectedPageImageText({ selectedPageId, text }) ); saveTextDebounce(); } }; ##### How to register a global shortcut? In my case I register ctrl+shift+n as a shortcut to my new page button. Right below my mainWindow.load(url): js // electorn-src/index.ts mainWindow.loadURL(url); globalShortcut.register(\\"CommandOrControl+Shift+N\\", () => { mainWindow.webContents.send(\\"GLOBAL_NEW_PAGE\\"); }); In our frontend: js export default function Pages() { const { pathname } = useRouter(); useEffect(() => { global.ipcRenderer.on(\\"GLOBAL_NEW_PAGE\\", () => { const newPageBtnEl = document.querySelector(\\"button#new-page-button\\"); if (newPageBtnEl) { (newPageBtnEl as HTMLButtonElement).click(); } }) }, []) ... ","title":"Nextjs with Electron","date":"2021-12-06T00:00:00.000Z","id":"blog040","tag":"react, nextjs, google-cloud, electron","intro":"Since FF14 has a new patch of huge update, in order to take this oppurtunity to learn Japanese I decided to write a desktop application again."},{"content":" #### Situation I Encountered As I am used to image processing in python than in nodejs, therefore I wrote a script in jupyter lab and wish to run it in my Electron-Nextjs application. The script is very simple, it is used to compress all the images inside a folder: py # compress_image.py for image_name in os.listdir(IMG_DIR): if image_name.endswith(\\".jpg\\"): print(\\"processing\\", image_name) pic = Image.open(os.path.join(IMG_DIR, image_name)).convert(\'RGB\') temp_img_dir = os.path.abspath(os.path.join(IMG_DIR, \\"..\\", \\"_screenshots\\")) if not os.path.exists(temp_img_dir): os.makedirs(temp_img_dir) tar_image_path = os.path.abspath(os.path.join(temp_img_dir, image_name)) pic.save(tar_image_path, optimize=True, quality=60) print(tar_image_path, \\"saved\\") shutil.rmtree(IMG_DIR) os.rename(temp_img_dir, IMG_DIR) #### Call this Python Script in Nodejs and Read any Log from the print A function that takes cmd: string as input and output exitCode: number is created: js // exec-util.js import { exec } from \\"child_process\\"; const execUtil = async (cmd: string) => { return ( (await new Promise()) < number > ((resolve, _) => { console.log(\\"spwanning process with command: \\", cmd); const child = exec(cmd); child.stdout?.setEncoding(\\"utf8\\"); child.stderr?.setEncoding(\\"utf8\\"); child.stdout?.on(\\"data\\", function (data) { console.log(\\"stdout: \\" + data); }); child.stderr?.on(\\"data\\", function (data) { console.log(\\"stderr: \\" + data); }); child.on(\\"close\\", function (code) { console.log(\\"exit code: \\" + code); if (code === null) { resolve(1); } else { resolve(code); } }); }) ); }; Now in my nodejs program (in fact, in Electron) I call the following: js const cmd = conda activate <env> && python <some/location/compress_image.py>; const exitCode = await execUtil(cmd); Which gives me the output coming from my print command in my python script (apart from exit code: 0): text stdout: C:\\\\Users\\\\user\\\\OneDrive\\\\Documents\\\\SCDictionary\\\\web-assets\\\\_screenshots\\\\1638900870035.jpg saved processing 1638900889189.jpg C:\\\\Users\\\\user\\\\OneDrive\\\\Documents\\\\SCDictionary\\\\web-assets\\\\_screenshots\\\\1638900889189.jpg saved processing 1638900915323.jpg C:\\\\Users\\\\user\\\\OneDrive\\\\Documents\\\\SCDictionary\\\\web-assets\\\\_screenshots\\\\1638900915323.jpg saved processing 1638900919493.jpg C:\\\\Users\\\\user\\\\OneDrive\\\\Documents\\\\SCDictionary\\\\web-assets\\\\_screenshots\\\\1638900919493.jpg saved processing 1638900925208.jpg ... exit code: 0 ","title":"Call Python Script in Node.js","date":"2021-12-11T00:00:00.000Z","id":"blog041","tag":"javascript","intro":"Sometimes we would like to call python script inside our node js program. We discuss how to execute a command, how to read the log and error in node-js and how to wait until this process is done."},{"content":" #### Why Compression In Electron saving a jpeg image with decent quality can produce a file of size ranged from 500kb to 2mb, which is way too big for storing and display in web. Like my page \\"Japanese Study\\" each \\"note\\" has at least 50 images. Therefore before saving any images, data compression is necessary in backend and we will use an npm library called sharp, it can compress and save an image even it is a buffer, a usual format for data transimission in nodejs backend. #### Implementations We create the following utility function: js import sharp from \\"sharp\\"; const compressImageAndSave = (imageBuffer: Buffer, imagePath: string) => { return ( new Promise() < { err: Error, info: sharp.OutputInfo } > ((resolve, _) => { sharp(imageBuffer) .jpeg({ mozjpeg: true, quality: 80 }) .toFile(imagePath, (err, info) => { resolve({ err, info }); }); }) ); }; export default compressImageAndSave; sharp can actually take many formats of data to create an sharp object. Instead of Buffer, you may also want string, uint8-array, etc. You can also reduce quality of png, webp, etc. Now an image that takes 2mb (saved from raw buffer and base64-encoded image) can now be reduced to 100kb with an unvisible downgrade of quality. ","title":"Nodejs Image Compression in Backend with sharp","date":"2021-12-16T00:00:00.000Z","id":"blog042","tag":"javascript","intro":"We discuss how to compress images effectively in nodejs backend."},{"content":" #### Problem of Looping a Generator Using for Loop Given a generator gen we can loop through the element that it generates by writing python for g in gen: # do something pass This kind of interations is fine if we do simple scripting/small tasks. I want to point out the downside of using for loop: ##### No Control on When to Stop However, looping in this way means we have no control on when to stop our for loop until the generator get exhausted (which usually results from breaking the while loop in that generator). Especially when we do such kind of iteration in another thread, we wish to stop it in our will. ##### Why break is not always Working You may think well we can add a conditional break inside the for loop, but what if gen itself is blocking once certain condition is met and never ends? In this case any logic to break the for loop will never run. For example, gen can be a generator that read the tail part of a file, i.e., it yields the latest line of the file whenever there is a new line written to that file. If the implementation has no stopping mechanism (like tailer.follow in <a href=\\"https://pypi.org/project/tailer/\\">tailer</a>), how do we stop looping a generator manaully? Changing the source code is not the best way as very likely we have to share the code within our team. #### If Interupting a Loop of Generator is What you Need Instead we create a while loop to consume the output of the generator: python gen = my_gen() stop_event = Event() while True and not stop_event.is_set(): try: line = next(gen) # do something except StopIteration: print(\\"stopped generator\\") break Be careful we have to handle the StopInteration exception that is raised when the while loop in gen breaks. Now we can stop the iteration whenever we trigger stop_event.set(). #### Concret Example of Generator Consider the following generator: python def my_gen(): count = 0 start = time.time() while True: now = time.time() time_diff = now - start if 2 <= time_diff and time_diff < 6 and count <= 4 : count += 1 yield(\\"start\\") if time_diff >= 6: break Now if we run python print(\\"start generator\\") for line in my_gen(): print(line) print(\\"stopped generator\\") we get python start generator # at 0th second start # at 2th second start # at 2th second start # at 2th second start # at 2th second start # at 2th second stopped generator # at 6th second - Basically the generator is blocking (as we yield nothing) from 0 to 2nd second; - print 5 start\'s at the 2nd second, blocking until 6th second; - and the code continue to print stopped generator when it gets out of the for loop. ","title":"Python Generator","date":"2022-01-04T00:00:00.000Z","id":"blog043","tag":"python","intro":"A simple remark on generator in python."},{"content":" #### GAN ##### Import python %matplotlib inline from numpy.random import randint from tensorflow.keras.datasets import mnist from tensorflow.keras.optimizers import Adam from tensorflow.keras.losses import BinaryCrossentropy from tensorflow.keras.layers import Input, Conv2D, Dense, Reshape, Flatten from tensorflow.keras.models import Model import numpy as np import cv2 import tensorflow as tf import matplotlib.pyplot as plt import os ##### For GPU Training python os.environ[\'TF_XLA_FLAGS\'] = \'--tf_xla_enable_xla_devices\' config = tf.compat.v1.ConfigProto( gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8) ) config.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config) tf.compat.v1.keras.backend.set_session(session) ##### Constants python batch_size = 128 img_shape = (28, 28, 1) ##### Models ###### Generator python def get_generator(img_shape=img_shape, z_dim=100): input = Input(shape=(z_dim,)) x = Dense(128)(input) x = LeakyReLU(alpha=0.01)(x) x = Dense(28*28, activation=\\"tanh\\")(x) x = Reshape(img_shape)(x) model = Model(input, x) return model ###### Discriminator python def build_discriminator(img_shape=(28,28,1)): input = Input(shape=img_shape) x = Flatten()(input) x = Dense(128)(x) x = LeakyReLU(alpha=0.01)(x) x = Dense(1, activation=\\"sigmoid\\")(x) model = Model(input, x) return model ##### Dataset Generator Using tf.data.Dataset python def get_dataset_gen(batch_size = batch_size): (x_train, _), (_, _) = mnist.load_data() x_train = x_train/127.5 - 1 x_train = np.expand_dims(x_train, axis=-1) dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(1000).batch(batch_size) return iter(dataset) **Remark.** Sometimes we want dataset to be images mapped by image filepaths. Apart from tf.data.Dataset.list_files(target/dir/*.jpg), we can also save the list of imagepaths in notebook and apply .from_tensor_slices() as above, we then use .map to get the images. ##### Training ###### On Gradient Tapes Since generator and discriminator are two separate models, they are not trained at the same time. Namely, the update on weights inside generator and update on that in discriminator are independent of each other. Here we use two tf.GradientTape\'s to record calculations, and do the differentation separately. ###### On Model.fit We can still train a GAN by tricky use of Model.fit (by setting discrimnator.trainable to False before the compilation of the model). In my opinion using gradient tape is much more straight-foward once we get the idea. ###### Start the Training Loop python gen_opt = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999) disc_opt = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999) fig = plt.figure() fig.set_figheight(20) fig.set_figwidth(20) for epoch in range(10): dataset_gen = get_dataset_gen() for batch_num, batch_of_num_imgs in enumerate(dataset_gen): batch_of_num_imgs_tensor = tf.convert_to_tensor(batch_of_num_imgs) noise = tf.random.normal((batch_size, 100), mean=0.0, stddev=1.0) with tf.GradientTape(persistent=True) as tape: faked_imgs = gen(noise) critic_on_reals = disc(batch_of_num_imgs_tensor) critic_on_fakeds = disc(faked_imgs) disc_loss_on_real_imgs = bcentropy(tf.ones_like(critic_on_reals), critic_on_reals) disc_loss_on_faked_imgs = bcentropy(tf.zeros_like(critic_on_fakeds), critic_on_fakeds) disc_loss = 0.5*disc_loss_on_real_imgs + 0.5*disc_loss_on_faked_imgs gen_loss = bcentropy(tf.ones_like(critic_on_fakeds), critic_on_fakeds) grad_disc = tape.gradient(disc_loss, disc.trainable_variables) grad_gen = tape.gradient(gen_loss, gen.trainable_variables) gen_opt.apply_gradients(zip(grad_gen, gen.trainable_variables)) disc_opt.apply_gradients(zip(grad_disc, disc.trainable_variables)) print(f\\"epoch{epoch}, processing batch {batch_num}\\", end=\\"\\\\r\\") if batch_num % 10 == 0: noise = np.random.normal(0, 1.0, (batch_size, 100)) imgs = gen.predict(noise) for i in range(0,4): for j in range(0, 4): index = 4*i + j img = imgs[index] plt.subplot(4, 4, 1 + 4*i + j) plt.imshow(img, cmap=\\"gray\\") plt.savefig(f\\"epoch-{str(epoch).zfill(3)}_batch-{str(batch_num).zfill(3)}\\", dpi=80, bbox_inches=\\"tight\\") ##### Results at 5-th Epoch and 380-th Batch, batch_size = 128 Each epoch has 460 batches, that means we have run through $460 * 4 + 380 = 2220$ batches. <center> <a href=\\"/assets/tech/039.png\\"> <img src=\\"/assets/tech/039.png\\" width=\\"550\\"/> </a> </center> #### DCGAN ##### Import python %matplotlib inline import matplotlib.pyplot as plt import numpy as np import tensorflow as tf from tensorflow.keras.datasets import mnist from tensorflow.keras.layers import BatchNormalization, Dense, Flatten, Reshape from tensorflow.keras.layers import LeakyReLU from tensorflow.keras.layers import Conv2D, Conv2DTranspose from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adam ##### Constants python img_rows = 28 img_cols = 28 channels = 1 img_shape = (img_rows, img_cols, channels) z_dim = 100 ##### Models ###### Generator python def build_generator(): model = tf.keras.Sequential() model.add(Dense(7*7*256, use_bias=False, input_shape=(100,))) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Reshape((7, 7, 256))) assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding=\'same\', use_bias=False)) assert model.output_shape == (None, 7, 7, 128) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\'same\', use_bias=False)) assert model.output_shape == (None, 14, 14, 64) model.add(BatchNormalization()) model.add(LeakyReLU()) model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding=\'same\', use_bias=False, activation=\'tanh\')) assert model.output_shape == (None, 28, 28, 1) return model ###### Discriminator python def build_discriminator(): model = Sequential() model.add( Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\'same\') ) model.add(LeakyReLU(alpha=0.01)) model.add( Conv2D(64, kernel_size=3, strides=2, input_shape=img_shape, padding=\'same\')) model.add(BatchNormalization()) model.add(LeakyReLU(alpha=0.01)) model.add( Conv2D(128, kernel_size=3, strides=2, input_shape=img_shape, padding=\'same\')) model.add(BatchNormalization()) model.add(LeakyReLU(alpha=0.01)) model.add(Flatten()) model.add(Dense(1, activation=\'sigmoid\')) return model ##### Auxilary Function: sample_images python def sample_images(generator, image_grid_rows=4, image_grid_columns=4): z = np.random.normal(0, 1, (image_grid_rows * image_grid_columns, z_dim)) gen_imgs = generator.predict(z) gen_imgs = 0.5*gen_imgs + 0.5 fig, axs = plt.subplots(image_grid_rows, image_grid_columns, figsize=(4, 4), sharey=True, sharex=True ) count = 0 for i in range(image_grid_rows): for j in range(image_grid_columns): axs[i, j].imshow(gen_imgs[count, :, :, 0], cmap=\\"gray\\") axs[i, j].axis(\\"off\\") count += 1 plt.show() ##### Define Custom Training using Gradient Tape and Start ##### Start the Training python generator = build_generator() discriminator = build_discriminator() losses = [] accuracies = [] iteration_checkpoints = [] cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True) def train(iterations, batch_size, sample_interval): (x_train, _), (_, _) = mnist.load_data() x_train = x_train/127.5 - 1.0 \\"\\"\\" np.shape(x_train) = (60000, 28, 28) for conv2D we need the channel dimension at the last axis \\"\\"\\" x_train = np.expand_dims(x_train, axis=3) real = np.ones((batch_size, 1)) fake = np.zeros((batch_size, 1)) gen_opt = Adam(1e-4) disc_opt = Adam(1e-4) for i in range(iterations): print(f\\"iteration: {i}\\", end = \\"\\\\r\\") idxs = np.random.randint(0, x_train.shape[0], batch_size) imgs = x_train[idxs] \\"\\"\\" We define tf.Variable(n) outside gradient tape, in gradient tape we record the calculation done on our training input (in tensor). When we use tape.gradient outside the tape, we are doing differentiation evaluated at the variable n. \\"\\"\\" z = tf.random.normal((batch_size, z_dim), 0, 1) with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: gen_imgs = generator(z, training=True) decisions_for_gen = discriminator(gen_imgs, training=True) decisions_for_real = discriminator(imgs, training=True) gen_loss = cross_entropy(real, decisions_for_gen) disc_fake_loss = cross_entropy(fake, decisions_for_gen) disc_real_loss = cross_entropy(real, decisions_for_real) disc_loss = 0.5*(disc_fake_loss + disc_real_loss) grad_of_gen = gen_tape.gradient(gen_loss, generator.trainable_variables) grad_of_disc = disc_tape.gradient(disc_loss, discriminator.trainable_variables) gen_opt.apply_gradients(zip(grad_of_gen, generator.trainable_variables)) disc_opt.apply_gradients(zip(grad_of_disc, discriminator.trainable_variables)) if (i+1) % sample_interval == 0: iteration_checkpoints.append(i+1) print(\\"%d [D loss: %f] [G loss: %f]\\" % (i + 1, disc_loss, gen_loss)) sample_images(generator) Finally we start the training python iterations = 20000 batch_size = 128 sample_interval = 1000 train(iterations, batch_size, sample_interval) ##### Results at the 3000-th Batch, batch_size = 128 Compared to the result of basic GAN with 2220 batches, the result of DCGAN at 3000-th batch is much better! <center> <a href=\\"/assets/tech/040.png\\"> <img src=\\"/assets/tech/040.png\\"/> </a> </center> ","title":"GAN and DCGAN in Tensorflow","date":"2022-02-15T00:00:00.000Z","id":"blog044","tag":"tensorflow, deep-learning","intro":"In the past I have learnt the most basic GAN and DCGAN using pytorch, but I am more familiar with tensorflow. I attempt to understand gradient tape using by using these two GAN again with mnist dataset."},{"content":" #### Start from Docker Hub We don\'t need to install anything from mongo website, we just need to pull the mongo image from docker hub. To have authentication, a super/root user called admin must be created in the first place. For that, instead of just running the image, we pass the environment variables - MONGO_INITDB_ROOT_USERNAME and - MONGO_INITDB_ROOT_USERNAME in text docker run -d --name bddms-mongo \\\\ -p 27017:27017 \\\\ -e MONGO_INITDB_ROOT_USERNAME=admin \\\\ -e MONGO_INITDB_ROOT_PASSWORD=123 \\\\ -v \\"/c/Users/user/OneDrive/Documents/db_backup/docker_bddms\\":/data/db mongo Here my local directory /c/Users/user/OneDrive/Documents/db_backup/docker_bddms is mount as a volume to the container\'s directory /data/db. #### Create User in Mongo Shell When this image is run in the detached mode, run text docker exec -it cb39731a97ff sh to get into the cli of the container running our mongo instance with id cb39731a97ff. We are ready to init the mongo shell and add user. For this, run text mongo -u admin -p 123 For sheer purpose of fulfilling the hard-coded config in the project, I need: - A user called bdsons with pwd bdsonspass; - authSource=admin for authentication, i.e., create a user bdsons **_inside_** the database admin; - bdsons to have access right to the database bdsons. Therefore: text use admin db.createUser({ \\\\ user: \\"bdsons\\", pwd: \\"bdsonspass\\", \\\\ roles:[{db: \\"admin\\", role:\\"readWrite\\"}, {db:\\"bdsons\\", role:\\"readWrite\\"}] \\\\ }) Then by running db.getUsers() we have text [ ..., { \\"_id\\" : \\"admin.bdsons\\", \\"userId\\" : UUID(\\"815c8f70-a65f-452d-a877-864d2d69fa00\\"), \\"user\\" : \\"bdsons\\", \\"db\\" : \\"admin\\", \\"roles\\" : [ { \\"role\\" : \\"readWrite\\", \\"db\\" : \\"bdsons\\" }, { \\"role\\" : \\"readWrite\\", \\"db\\" : \\"admin\\" } ], \\"mechanisms\\" : [ \\"SCRAM-SHA-1\\", \\"SCRAM-SHA-256\\" ] } ] authSource in the connection string to mongodb will look at user.db for authentication (trap: not the \\"db\\" in the roles). #### Connect to the mongoDb with Auth Now I can connect to the database named bdsons by using the string: text mongodb://bdsons:bdsonspass@localhost:27017/bdsons?authSource=admin ","title":"Set up Local Environment for Mongo using Docker","date":"2022-02-25T00:00:00.000Z","id":"blog045","tag":"coding, mongo","intro":"Since I have docker already installed, just record a few steps to get the local development ready using mongo db with authentication."},{"content":" #### In Python This python logger not only log the message in the console, it also pipes all the message inside a file which we can review as long as this file is saved in a volume. python import logging import datetime import os from utils.get_config import get_config def create_logger(log_dir,logging_level=logging.INFO): if not os.path.exists(log_dir): os.makedirs(log_dir) logger = logging.getLogger() logger.setLevel(logging_level) formatter = logging.Formatter( \'[%(levelname)1.1s %(asctime)s %(module)s:%(lineno)d] %(message)s\', datefmt=\'%Y%m%d %H:%M:%S\') sh = logging.StreamHandler() sh.setLevel(logging_level) sh.setFormatter(formatter) log_filename = datetime.datetime.now().strftime(\\"%Y-%m-%d_%H_%M_%S.log\\") fh = logging.FileHandler(os.path.sep.join([log_dir, log_filename])) fh.setLevel(logging_level) fh.setFormatter(formatter) logger.addHandler(sh) logger.addHandler(fh) return logger logger_dir = get_config(\\"log_dir\\", \\"sbg-logs\\") logger = create_logger(logger_dir) #### In javascript Unlike python, we will use npm package tracer to customize the logger as follows: javascript import tracer from \\"tracer\\"; import fs from \\"fs\\"; import path from \\"path\\"; const date = new Date(); const year = date.getFullYear(); const month = date.getMonth(); const day = date.getDate(); const hour = date.getHours(); const mins = date.getMinutes(); const seconds = date.getSeconds(); const dateString = ${year}-${month}-${day}_${hour}-${mins}-${seconds}; const logFileDir = \\"./logs\\"; const logFileLocation = path.join(logFileDir, ${dateString}.log); if (!fs.existsSync(logFileDir)) { fs.mkdirSync(logFileDir); } if (!fs.existsSync(logFileLocation)) { fs.open(logFileLocation, \\"w\\", () => {}); } const logger = tracer.colorConsole({ transport: function (data) { fs.createWriteStream(logFileLocation, { flags: \\"a\\", encoding: \\"utf-8\\", mode: 0o666, }).write(data.rawoutput + \\"\\\\n\\"); }, format: [ \\"[{{timestamp}}-<{{title}}>-{{file}}:{{line}}] {{message}}\\", //default format { error: \\"[{{timestamp}}-<{{title}}>-{{file}}:{{line}}] {{message}} \\\\nCall Stack:\\\\n{{stack}}\\", // error format }, ], dateformat: \\"HH:MM:ss.L\\", preprocess: function (data) { data.title = data.title.toUpperCase(); }, }); export default logger; ","title":"Customized Logger in Python and Javascript","date":"2022-03-07T00:00:00.000Z","id":"blog046","tag":"python, javascript","intro":"Logging is undoubtedly the most important part of an application. We study how to create a logger that tells us a message comes from which file and which line."},{"content":" #### IBM Documentation https://www.ibm.com/docs/it/dsm?topic=logs-create-sqs-queue-configure-s3-objectcreated-notifications ","title":"How to Configure S3 to Send SQS Message","date":"2022-03-10T00:00:00.000Z","id":"blog047","tag":"aws","intro":"Resource that explains how to construct correct policy on sqs-queue to allow s3 to send message to that queue."},{"content":" #### Create pages/\\\\_document.js Since we will not be going to edit this file, even our project is written in typescript, it is no harm to add this file and never touch it. Now js // pages/_document.js import React from \\"react\\"; import Document, { Html, Head, Main, NextScript } from \\"next/document\\"; import { ServerStyleSheets } from \\"@material-ui/core/styles\\"; class MyDocument extends Document { static async getInitialProps(ctx) { const sheets = new ServerStyleSheets(); const originalRenderPage = ctx.renderPage; ctx.renderPage = () => originalRenderPage({ enhanceApp: (App) => (props) => sheets.collect(<App {...props} />), }); const initialProps = await Document.getInitialProps(ctx); return { ...initialProps, // Styles fragment is rendered after the app and page rendering finish. styles: [ ...React.Children.toArray(initialProps.styles), sheets.getStyleElement(), ], }; } render() { return ( <Html lang=\\"en\\"> <Head /> <body> <Main /> <NextScript /> </body> </Html> ); } } export default MyDocument; #### Edit \\\\_app.tsx Finally js // pages/_app.tsx useEffect(() => { const jssStyles = document.querySelector(\\"#jss-server-side\\"); if (jssStyles) { jssStyles.parentElement?.removeChild(jssStyles); } }, []); #### Reference - <a href=\\"https://stackoverflow.com/questions/66089290/materialui-makestyles-undoes-custom-css-upon-refresh-in-nextjs\\">MaterialUI makeStyles undoes custom css upon refresh in NextJS</a> ","title":"Additional Configuration for makeStyles in Next js","date":"2022-03-13T00:00:00.000Z","id":"blog048","tag":"react, nextjs","intro":"Without additional setup, due to the natural of serverside rendering, some styles would become undefined and we study how to avoid them."},{"content":" #### Summary from Research In conclusion, when we are concerned with the performance of the model under different/specific IoU, i.e., when bounding box accuracy is our concern, then we use mAP. However, some model is well-trained for annotating target object already, but it fails to spot diversified target that cause missing detection, then IoU is not our concern, we can simply use **_F1- score_**. #### F1-Score Define $$ \\\\texttt{Precision} := \\\\frac{TP}{TP+FP}\\\\quad \\\\text{and}\\\\quad \\\\texttt{Recall} := \\\\frac{TP}{TP+FN}, $$ then we define F1-score to be $$ \\\\texttt{F1-score} := \\\\frac{2}{\\\\displaystyle\\\\frac{1}{ \\\\texttt{Precision}} + \\\\frac{1}{\\\\texttt{Recall}}} = \\\\frac{2\\\\times \\\\texttt{Precision}\\\\times \\\\texttt{Recall}}{ \\\\texttt{Precision}+\\\\texttt{Recall}}. $$ <p></p> I refer to **[ZL]** for more detail on F1-score and how it behaves compared to arithmetic mean and geometric mean. In short summary, $\\\\texttt{F1-score}$ penalizes **_unbalanced_** $\\\\texttt{Precision}$ and $\\\\texttt{Recall}$. #### mAP ##### Precision and Recall Given that we have fixed a label $\\\\mathcal L$ to work with, say $\\\\mathcal L = \\\\texttt{damaged_display}$, we need the following table with $$ \\\\text{Recall} = \\\\frac{\\\\text{acc. }TP}{\\\\text{Total number of signboard of class $\\\\mathcal L$}}. $$ Note that the denominator is actually the total number of ground truths from **_all_** images, not just from a single image. By sorting the results using objectiveness/confidence score, we get a table like: <center> <a href=\\"/assets/tech/028.jpg\\" target=\\"_blank\\"> <img src=\\"/assets/tech/028.jpg\\" style=\\"width:90%\\"/> </a> </center> <p/> ##### Why Recall Precision $\\\\displaystyle \\\\frac{TP}{TP+FP}$ (where $FP$ is the number of boxes identified as $\\\\mathcal L$ in correctly) is not enough as it doesn\'t count undetected signboard. ##### Calculation of mAP from the Graph By using the precision and recall columns we should be able to graph it as follows: <center> <a href=\\"/assets/tech/029.png\\" target=\\"_blank\\"> <img src=\\"/assets/tech/029.png\\" /> </a> </center> <p/> Note that the area under the path now means exactly the average precision (that\'s why recall is a fraction, it helps normalize the number of calls). Programmatically we don\'t calculate that area directly, we smooth the graph out for approximated average precision in the following way: <center> <a href=\\"/assets/tech/029.png\\" target=\\"_blank\\"> <img src=\\"/assets/tech/030.png\\"/> </a> </center> <p/> The new dots is exactly the same as the value $p(r)=\\\\max_{r\'\\\\ge r}p(r\')$, where $r$ is the recall, our approximated precision is $$ \\\\frac{1}{\\\\text{number of rows in table}} \\\\times \\\\sum_{r\\\\ge 0} p(r) $$ which is, for example, $$ \\\\frac{1}{11}(1+0.6666+0.4285+0.4285+0.4285+0+0+0+0+0+0) = 26.84\\\\% $$ in the figure above. #### References - **[ZL]** Zeya LT, <a href=\\"https://towardsdatascience.com/essential-things-you-need-to-know-about-f1-score-dbd973bf1a3\\">Essential Things You Need to Know About F1-Score</a> ","title":"Simple Introduction to mAP and F1-Score","date":"2022-03-21T00:00:00.000Z","id":"blog049","tag":"deep-learning","intro":"Explain why and how it can provides a performance index to a localization algorithm."},{"content":" #### Sample Code for Shape Experimental Calculation Suppose we have the up-sampling part in part of the U-Net: python u = UpSampling2D(size=2)(layer_input) u = Conv2D(filters, kernel_size=4, strides=1, padding=\\"same\\", activation=\\"relu\\")(u) What is the resulting shape of the output? For strides=1 and padding=\\"same\\" we can memorize the output shape are always unchanged. But what if kernel_size = 3 and strides=2, padding=\\"valid\\"? There is no point to memorize the formula for output shape as we can always experiment it out as follows: python x = tf.random.normal([1, 28,28,3]) x = Conv2D(32, kernel_size=3, strides=2, padding=\\"valid\\", activation=\\"relu\\")(x) print(tf.shape(x)) # output: tf.Tensor([ 1 13 13 32], shape=(4,), dtype=int32) #### Rigorous Proof to Formula of Shapes Let $s\\\\in \\\\mathbb N$, for $\\\\texttt{strides=}s$ and $\\\\texttt{padding=\\"same\\"}$ we can prove the following: > $\\\\displaystyle \\\\texttt{output_width} = \\\\left\\\\lfloor\\\\frac{\\\\text{input_width}-1}{s}\\\\right\\\\rfloor + 1 = \\\\left\\\\lceil\\\\frac{\\\\text{input_width}}{s}\\\\right\\\\rceil$ This is due to the following simple fact: > **Fact.** Let $w$ and $s$ be positive integers, there holds > > $$ > \\\\left\\\\lfloor\\\\frac{w-1}{s}\\\\right\\\\rfloor + 1 = \\\\left\\\\lceil\\\\frac{w}{s}\\\\right\\\\rceil. > $$ <proof> **Proof.** We do case by case study. If $w=ks$ for some positive $k\\\\in \\\\mathbb N$, then $$ \\\\text{LHS} = \\\\left\\\\lfloor k - \\\\frac{1}{s}\\\\right\\\\rfloor +1 = (k-1)+1=k = \\\\lceil k\\\\rceil = \\\\text{RHS}. $$ When $w=ks+j$, for some $k\\\\in \\\\mathbb N$ and $j\\\\in \\\\mathbb N\\\\cap (0,s)$, then $$ \\\\text{LHS} = \\\\left\\\\lfloor k+\\\\frac{j-1}{s}\\\\right\\\\rfloor + 1 = k+1 = \\\\left\\\\lceil k+\\\\frac{j}{s}\\\\right\\\\rceil = \\\\left\\\\lceil \\\\frac{ks+j}{s}\\\\right\\\\rceil = \\\\left\\\\lceil\\\\frac{w}{s}\\\\right\\\\rceil=\\\\text{RHS}. $$ </proof> ","title":"Resulting Shapes of Conv-net by Direct Experiment","date":"2022-03-23T00:00:00.000Z","id":"blog050","tag":"deep-learning","intro":"From time to time it is easy to forget the formula to calculate the output shape of Conv2D, MaxPooling2D, etc, layers. Record some sample code to test shapes easily."},{"content":" Include the *.d.ts file in tsconfig.json with the following content: javascript declare module \\"react-images\\" { export var Modal; export var ModalGateway; export default Carousel; } declare module \'*.pdf\' { const src: string; export default src; } declare module \'*.md\' { const md: string; export default md; } declare module \\"json-to-pretty-yaml\\" { export default { stringify(json: Object): string } } ","title":"Type Annotation Record in Typescript for 3rd-party Library and Usual Import","date":"2022-03-23T00:00:00.000Z","id":"blog051","tag":"typescript","toc":false,"intro":"Record some type annotation used for third party untyped library or those needed in importing files (like pdf)."},{"content":" #### Description of an Unusual Excel File Assume that we are going to process the following \\"unstructured\\" (or structured in an unusual way) excel file: <a href=\\"/assets/tech/031.png\\"> <img src=\\"/assets/tech/031.png\\" width=\\"100%\\"> </a> <p/> The image displays an excel organized: - with some row being categories, - with some row being items of categories right above, and regarded as the true header of the table, - with some row meaning True and False (colored or not). - with some column even being empty. It cannot be regarded as a usual table, we have to carefully handle it case by case. That means we have to be very familiar with package that handle the excel files. #### Pandas ##### Read Lines for Pandas python import pandas as pd excel_file = \\"Program_Details.xlsx\\" df = pd.read_excel(excel_file, engine=\\"openpyxl\\", sheet_name=\\"Raw Data\\") symp_row = df.iloc[2, 6:] print(symp_row) text Unnamed: 6 Anxiety Unnamed: 7 Bereavement Unnamed: 8 Cool Emotions Unnamed: 9 Debility \u865b\u5f31 \\\\n\u865b\u5f31 Unnamed: 10 Depression ... Unnamed: 220 US$ Unnamed: 221 HK$\\\\n(US$1=HK$8) Unnamed: 222 Discount /\\\\nShipping /\\\\nSurplus Unnamed: 223 price per drop Unnamed: 224 NaN - Regardless of the behaviour of how pandas handle empty rows, we can experiment on python df.iloc[i, 6:] to test which row index i we should start with. - Empty cells have value nan when we cast to string. - To iterate over the row, we use python # generator that gives (col_name, value): df.iloc[i, 6:].items() - we can also use python datas_df = df.iloc[0:, 1:4] for index, value in datas_df.iterrows(): col1, col2, col3 = value to unpack the values. #### openpyxl However, there is no way for pandas to get the color of a cell, we resort to openpyxl which pandas base on in order to read .xlsx (another extension for excel file). ##### Read Lines for openpyxl python import openpyxl from openpyxl import load_workbook excel_file = \\"Program_Details.xlsx\\" wb = load_workbook(excel_file, data_only=True) sheet = wb[\'Raw Data\'] - The cell of the i-th row and j-th column is sheet[i][j], the values is read by python cell = sheet[i][j] cell.value - The background color of the cell can be obtained by python cell.fill.start_color.index #### Conclusion - If we just want to read an excel file, then openpyxl is enough. - If we want to manipulate the data in an excel file, then we use pandas as the syntax python df.iloc[i:j, h:k] is very handy compared to openpyxl. ","title":"Two Methods to Read Excel Files in Python","date":"2022-03-24T00:00:00.000Z","id":"blog052","tag":"python","intro":"Record two methods to read excel files in Python"},{"content":" #### Results <center> <a href=\\"/assets/tech/032.png\\"> <img src=\\"/assets/tech/032.png\\" width=\\"300\\" style=\\"margin-right:20px\\"/> </a> <a href=\\"/assets/tech/033.png\\"> <img src=\\"/assets/tech/033.png\\" width=\\"300\\"/> </a> </center> <p/> #### With Gradient Tape, We Don\'t Need Model.training = False I create this whole model following the video in reference 1) of Reference section, however, I never get the training work for any algorithm that involves train_on_batch (I am suspicious about that after certain version of tensorflow, the call Model.training = False before compiling models fails). Nevertheless it is a very good exercise for me to write down the formula and implement the update of parameter by gradient tape because I have to know what\'s happening behind the scene clearly. Hope you also enjoy writing down the explicit formula of the losses and updating the parameters to have the feeling of \\"training separately\\" (the reason for Model.training = False). #### Preparation ##### For Cuda In order to enable cuda for traininig using GPU some special script needs to be run at the beginning. python os.environ[\'TF_XLA_FLAGS\'] = \'--tf_xla_enable_xla_devices\' config = tf.compat.v1.ConfigProto( gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8 )) config.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config) tf.compat.v1.keras.backend.set_session(session) ##### Preliminary Import python %matplotlib inline from tensorflow.keras.models import Model import os import matplotlib.pyplot as plt import tensorflow as tf import numpy as np from tensorflow_addons.layers import InstanceNormalization from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, LeakyReLU, Activation, Concatenate from tensorflow.keras.preprocessing.image import img_to_array, load_img from tensorflow.keras.initializers import RandomNormal from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError from tensorflow.keras.optimizers import Adam from numpy.random import randint from numpy import load from numpy import zeros, ones, asarray from sklearn.utils import resample from random import random dataset_name = \\"horse2zebra\\" n_sample = 1000 ##### Source of Training Data https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/ #### Formulas and Explanation In cycle-GAN we have 4 models: - $g_{AB}:A \\\\to B$, a generator from domain $A$ to domain $B$ - $g_{BA}:B\\\\to A$, a generator from domain $B$ to domain $A$ - $d_A:A\\\\to [0,1]$, a discriminator model to discriminate images in domain $A$ - $d_B:B\\\\to [0,1]$, a discriminator model to discriminate images in domain $B$ Here by \\"domain\\" we mean a category/class of images. For each training loop we will be training each of the models separately. Denote $p_A$ the distribution of data in domain $A$, and $p_B$ that in $B$. In each loop mathematically we wish to minimize the loss: $$ \\\\begin{align*} \\\\mathcal L_{g_{AB}} &= \\\\mathbb E_{a\\\\sim p_A, b\\\\sim p_B} \\\\bigg( 5\\\\cdot \\\\mathcal L_{\\\\mathrm{id}} + 10\\\\cdot \\\\mathcal L_{\\\\mathrm{cycle}} + \\\\mathcal L_{g\\\\text{-}d} \\\\bigg)(a, b) \\\\\\\\ \\\\mathcal L_{d_A}(a,b) &= \\\\mathbb E_{a\\\\sim p_A, b\\\\sim p_B} \\\\frac{1}{2}\\\\bigg(\\\\|d_A(a) - 1\\\\|_2^2 + \\\\|d_A(g_{BA}(b)) - 0\\\\|_2^2\\\\bigg) \\\\end{align*} $$ Here $$ \\\\begin{align*} \\\\mathcal L_{\\\\mathrm{id}}(a, b) &= \\\\|b - g_{AB}(b)\\\\|_1\\\\\\\\ \\\\mathcal L_{\\\\mathrm{cycle}}(a,b) &= \\\\|a - g_{BA}(g_{AB}(a))\\\\|_1\\\\\\\\ \\\\mathcal L_{g\\\\text{-}d}(a, b)&= \\\\|d_B(g_{AB}(a)) - 1\\\\|_2^2 \\\\end{align*} $$ $\\\\mathcal L_\\\\mathrm{id}$ is called **_identitiy loss_**, $\\\\mathcal L_{\\\\mathrm{cycle}}$ is called **_cycle loss_** and $\\\\mathcal L_{g\\\\text{-}d}$ is called **generator\'s discriminator loss**. Unlike the simplest GANs, the result of discriminator is not a single value in $[0,1]$ any more, instead in this implementation we use a matrix of shape (None, 16, 16, 1). The negative logarithmic penality is replaced by mean-square loss (without taking root). $\\\\mathcal L_{g_{BA}}$ and $\\\\mathcal L_{d_B}(a,b)$ are similarly defined, the code implementation of the $\\\\mathcal L_{g_{AB}}$ can be found in implementation section of this article: python 5 * mae(X_realA, same_inA) + 10 * mae(X_realA, cycle_A) + g_BtoA_disc_loss #### Implementation ##### Helper Functions python def load_img_from_path(image_path): pixels = load_img(image_path, target_size=(256, 256)) pixels = img_to_array(pixels) pixels = pixels/127.5 - 1 return pixels def generate_real_samples(path, n_samples=n_sample, patch_shape=16): image_paths = np.array(get_image_paths(path)) indexes = randint(0, len(image_paths), n_samples) X = np.array([load_img_from_path(path) for path in image_paths[indexes]]) y = ones((n_samples, patch_shape, patch_shape, 1)) return X, y def generate_fake_samples(g_model, dataset, patch_shape=16): X = g_model.predict(dataset) y = zeros((len(X), patch_shape, patch_shape, 1)) return X, y def gen_dataset(n_sample=500): train_A_paths = get_image_paths(f\\"{dataset_name}/trainA\\") train_B_paths = get_image_paths(f\\"{dataset_name}/trainB\\") random_state = np.random.randint(0, 100) print(\\"random_state\\", random_state) shuffle_indexes = resample(range(len(train_A_paths)), replace=False, n_samples=500, random_state=random_state) shuffled_trainA_img = (load_img_from_path(train_A_paths[index]) for index in shuffle_indexes) shuffled_trainB_img = (load_img_from_path(train_B_paths[index]) for index in shuffle_indexes) dataset = zip(shuffled_trainA_img, shuffled_trainB_img) return dataset def update_fake_img_pool(pool, images, max_size=50): selected = [] for image in images: if len(pool) < max_size: pool.append(image) selected.append(image) elif random() < 0.5: selected.append(image) else: # take one from the pool and and update the pool # the pool may contain new image that we never index = randint(0, len(pool)) selected.append(pool[index]) pool[index] = image return asarray(selected) def check_dataset(): dataset_ = gen_dataset() for i in range(3): img_A, img_B = next(dataset_) img_A = (img_A + 1) * 127.5 img_B = (img_B + 1) * 127.5 plt.subplot(2, 3, 1+i) plt.axis(\\"off\\") plt.imshow(img_A.astype(\\"uint8\\")) plt.subplot(2, 3, 4+i) plt.axis(\\"off\\") plt.imshow(img_B.astype(\\"uint8\\")) ##### Generators python def resnet_block(n_filters, input_layer): init = RandomNormal(stddev=0.02) # as strides = 1, the shape is invariant g = Conv2D(n_filters, (3, 3), padding=\\"same\\", kernel_initializer=init)(input_layer) g = InstanceNormalization(axis=-1)(g) g = Activation(\\"relu\\")(g) g = Conv2D(n_filters, (3, 3), padding=\\"same\\", kernel_initializer=init)(g) g = InstanceNormalization(axis=-1)(g) g = Concatenate()([g, input_layer]) return g def define_generator(image_shape, n_resnet=9): init = RandomNormal(stddev=0.02) x = Input(shape=image_shape) g = Conv2D(64, (7, 7), padding=\\"same\\", kernel_initializer=init)(x) g = InstanceNormalization(axis=-1)(g) g = Activation(\\"relu\\")(g) g = Conv2D(128, (3, 3), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(g) g = InstanceNormalization(axis=-1)(g) g = Activation(\\"relu\\")(g) g = Conv2D(256, (3, 3), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(g) g = InstanceNormalization(axis=-1)(g) g = Activation(\\"relu\\")(g) for _ in range(n_resnet): g = resnet_block(256, g) g = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(g) g = InstanceNormalization(axis=-1)(g) g = Activation(\\"relu\\")(g) g = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(g) g = InstanceNormalization(axis=-1)(g) g = Activation(\\"relu\\")(g) g = Conv2D(3, (7, 7), padding=\\"same\\", kernel_initializer=init)(g) g = InstanceNormalization(axis=-1)(g) y = Activation(\\"tanh\\")(g) model = Model(x, y) return model ##### Discriminators python def define_discriminator(image_shape): init = RandomNormal(stddev=0.02) x = Input(shape=image_shape) d = Conv2D(64, (4, 4), strides=2, padding=\\"same\\", kernel_initializer=init)(x) d = LeakyReLU(alpha=0.2)(d) d = Conv2D(128, (4, 4), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(d) d = InstanceNormalization(axis=-1)(d) d = LeakyReLU(alpha=0.2)(d) d = Conv2D(256, (4, 4), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(d) d = InstanceNormalization(axis=-1)(d) d = LeakyReLU(alpha=0.2)(d) d = Conv2D(512, (4, 4), strides=(2, 2), padding=\\"same\\", kernel_initializer=init)(d) d = InstanceNormalization(axis=-1)(d) d = LeakyReLU(alpha=0.2)(d) d = Conv2D(512, (4, 4), padding=\\"same\\", kernel_initializer=init)(d) d = InstanceNormalization(axis=-1)(d) d = LeakyReLU(alpha=0.2)(d) y = Conv2D(1, (4, 4), padding=\\"same\\", kernel_initializer=init)(d) model = Model(x, y) return model ##### Custom Training Loop with Gradient Tape ###### Create Models python def update_fake_image_pool(pool, images, max_size=50): selected = [] for image in images: if len(pool) < max_size: pool.append(image) selected.append(image) elif random() < 0.5: selected.append(image) else: # take one from the pool and and update the pool # the pool may contain new image that we never index = randint(0, len(pool)) selected.append(pool[index]) pool[index] = image return asarray(selected) g_model_AtoB = define_generator(image_shape) g_model_BtoA = define_generator(image_shape) d_model_A = define_discriminator(image_shape) d_model_B = define_discriminator(image_shape) n_epochs, n_batch, = 100, 1 n_patch = d_model_A.output_shape[1] batch_per_epoch = int(n_sample/n_epochs) n_steps = batch_per_epoch * n_epochs def show_result(step): dataset_ = gen_dataset() fig = plt.figure() fig.set_figheight(10) fig.set_figwidth(10) for i in range(3): img_A, _ = next(dataset_) fake_B = g_model_AtoB.predict(np.array([img_A])) cycle_A = g_model_BtoA.predict(fake_B) img_A = (img_A + 1) * 127.5 fake_B = (fake_B[0] + 1) * 127.5 cycle_A = (cycle_A[0] + 1) * 127.5 plt.subplot(3, 3, 1+i) plt.axis(\\"off\\") plt.imshow(img_A.astype(\\"uint8\\")) plt.subplot(3, 3, 4+i) plt.axis(\\"off\\") plt.imshow(fake_B.astype(\\"uint8\\")) plt.subplot(3, 3, 7+i) plt.axis(\\"off\\") plt.imshow(cycle_A.astype(\\"uint8\\")) plt.savefig(f\'result_{str(step).zfill(2)}.png\') plt.show() ###### Start Training Loop Note that in gradient tape we pass an option persistent=True because we need to calculate the gradient several times using the same tape (record of calculations). python mse = MeanSquaredError() mae = MeanAbsoluteError() g_AB_opt = Adam(learning_rate=0.0002, beta_1 = 0.5) g_BA_opt = Adam(learning_rate=0.0002, beta_1=0.5) d_A_opt = Adam(learning_rate = 0.0002, beta_1 = 0.5) d_B_opt = Adam(learning_rate=0.0002, beta_1=0.5) poolA = list() poolB = list() for i in range(2000): shfted_index = i + 4880 print(f\\"step {shfted_index}\\", end=\\"\\\\r\\") if shfted_index % 10 == 0: show_result(shfted_index) X_realA, y_realA = generate_real_samples(f\\"./{dataset_name}/trainA\\", n_batch, n_patch) X_realB, y_realB = generate_real_samples(f\\"./{dataset_name}/trainB\\", n_batch, n_patch) X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch) X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch) X_fakeA = update_fake_img_pool(poolA, X_fakeA) X_fakeB = update_fake_img_pool(poolB, X_fakeB) X_realA = tf.convert_to_tensor(X_realA) y_realA = tf.convert_to_tensor(y_realA) X_realB = tf.convert_to_tensor(X_realB) y_realB = tf.convert_to_tensor(y_realB) X_fakeA = tf.convert_to_tensor(X_fakeA) y_fakeA = tf.convert_to_tensor(y_fakeA) X_fakeB = tf.convert_to_tensor(X_fakeB) y_fakeB = tf.convert_to_tensor(y_fakeB) with tf.GradientTape(persistent=True) as tape: fake_B = g_model_AtoB(X_realA, training=True) cycle_A = g_model_BtoA(fake_B, training=True) fake_A = g_model_BtoA(X_realB, training=True) cycle_B = g_model_AtoB(fake_A, training =True) same_inB = g_model_AtoB(X_realB) same_inA = g_model_BtoA(X_realA) disc_real_A = d_model_A(X_realA) disc_fake_A = d_model_A(fake_A) disc_real_B = d_model_B(X_realB) disc_fake_B = d_model_B(fake_B) g_AtoB_disc_loss = mse(y_realB, disc_fake_B) g_BtoA_disc_loss = mse(y_realA, disc_fake_A) total_g_AtoB_loss = 5 * mae(X_realB, same_inB) + 10 * mae(X_realB, cycle_B) + g_AtoB_disc_loss total_g_BtoA_loss = 5 * mae(X_realA, same_inA) + 10 * mae(X_realA, cycle_A) + g_BtoA_disc_loss # train discriminator and generator separately: disc_B_loss = 0.5 * (mse(y_realB, disc_real_B) + mse(y_fakeB, disc_fake_B)) disc_A_loss = 0.5 * (mse(y_realA, disc_real_A) + mse(y_fakeA, disc_fake_A)) # derivatives grad_d_A = tape.gradient(disc_A_loss, d_model_A.trainable_variables) grad_d_B = tape.gradient(disc_B_loss, d_model_B.trainable_variables) grad_g_AtoB = tape.gradient(total_g_AtoB_loss, g_model_AtoB.trainable_variables) grad_g_BtoA = tape.gradient(total_g_BtoA_loss, g_model_BtoA.trainable_variables) # back-propagate d_A_opt.apply_gradients(zip(grad_d_A, d_model_A.trainable_variables)) d_B_opt.apply_gradients(zip(grad_d_B, d_model_B.trainable_variables)) g_AB_opt.apply_gradients(zip(grad_g_AtoB, g_model_AtoB.trainable_variables)) g_BA_opt.apply_gradients(zip(grad_g_BtoA, g_model_BtoA.trainable_variables)) #### Reference - Unpaired image to image translation\u200b using cycleGAN in keras, <br/> https://www.youtube.com/watch?v=2MSGnkir9ew - Tensoflow\'s guide to CycleGAN, <br/> https://www.tensorflow.org/tutorials/generative/cyclegan - Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks, <br/> https://arxiv.org/abs/1703.10593 ","title":"Cycle-GAN in Tensorflow","date":"2022-03-25T00:00:00.000Z","id":"blog053","tag":"tensorflow, deep-learning","intro":"Implement custom training loop in tensorflow for Cycle-GAN without the use of *trick* that sets Model.training = False."},{"content":" #### Connection - **In mongoose.** javascript // getConnection.ts import mongoose from \\"mongoose\\" let connCache: typeof mongoose; export default getConnection = () => { if (!connCache) { const dbOptions = { user: config.get(\'db.user\'), pass: config.get(\'db.password\'), maxPoolSize: config.get(\'db.poolSize\'), ssl: config.get(\'db.ssl\'), replicaSet: config.get(\'db.replicaSet\'), authSource: config.get(\'db.authSource\'), useUnifiedTopology: true, useNewUrlParser: true, }; connCache = await mongoose.connect(config.get(\'db.uri\'), dbOptions); } return connCache } Example of corresponding mongo db config: json { \\"uri\\": \\"mongodb+srv://some.net/dbName?retryWrites=true&w=majority\\" \\"user\\": \\"user\\" \\"password\\": \\"password\\" \\"authSource\\": \\"admin\\" \\"ssl\\": true } With mongoose being a singleton, after monngoose.connect() we can freely import our Model objects and do our CRUD operations using these objects. **Use of the Connection Object.** Sometimes it is more convenient to get the model by just using the modelName , i.e., conn.model(modelName), and this is the reason we export the method to get conn. - **In mongoengine.** After pip install mongoengine in our virtual environment, we can start connecting to our db as follows: python from mongoengine import connect from config2.config import config # if our os.environ[\\"ENV\\"] == abc, make sure config/abc.json (or yaml) exists # with the field \\"db\\": { \\"name\\": ..., } db_config = config.get(\\"db\\"); connect( alias=db_config[\\"name\\"], host=db_config[\\"uri\\"], username=db_config[\\"user\\"], password=db_config[\\"pwd\\"], authentication_source=db_config[\'auth_source\'] ) #### Schema Examples and DataTypes - **In mongoose.** The datatypes used in a schema are simply. - String - Number - Boolean - [subSchema] - [String] - [{id: String, name: String}] Recall that String, Number and Boolean are default classes built-in in javascript. Note that a **_sub-schema_** (i.e., a schema of the sub-document/embedded-document) will have _id generated by default. If we just want an ordinary object, make sure we have _id: false as below: js const subSchema = new Schema<T>({...}, {_id: false, timestamps: true}) or otherwise just use an **_inline-schema_** as simple as {id: String, name:String}. Finally we raise an example of a schema: js export const OilModelName = \\"Oil\\"; const schema = new Schema() < TOil > ({ id: String, name: { english: String, latin: String, chinese: String, }, symptomToTreat: [String], type: String, isDeleted: Boolean, }, { timestamps: true, }); export const OilModel = model(OilModelName, schema); - **In mongoengine.** The class attributes inside a subclass of Document actually define the schema for that document. Generally a schema in mongoengine looks like: python from mongoengine import Document from mongoengine.fields import FloatField, DateTimeField from datetime import datetime class VibrationSensorMetrics(Document): meta = { \\"collection\\": \\"vibrationsensormetrics\\" } rmsVelocity = FloatField() createdAt = DateTimeField(default=datetime.utcnow()) Due to the value in meta, a new document will be saved in a collection named automatically as **_vibrationsensormetrics_** (lowered cases + \\"s\\") We have the following fields that define the type of each \\"key\\" in a document: - StringField - DictField - BooleanField - IntField - ListField - DateTimeField - ObjectIdField - FloatField - EmbeddedDocument - EmbeddedDocumentField - DictField Among the fields above, DictField is used only when we know it is an object but don\'t know the data structure. For known structure, we use EmbeddedDocumentField and we explain it in the next section. #### Complicated Example with Nested Objects in mongoengine We continue to complexify the schema of VibrationSensorMetrics above, we will use all of the datatypes defined in the previous section. python from mongoengine import Document from mongoengine.fields import ( StringField, BooleanField, IntField, ListField, DateTimeField, ObjectIdField, FloatField, EmbeddedDocument, EmbeddedDocumentField ) from datetime import datetime class FreqVelocity(EmbeddedDocument): frequency = FloatField() velocity = FloatField() class SpecificFreqVelocities(EmbeddedDocument): ft = EmbeddedDocumentField(FreqVelocity) rpm = EmbeddedDocumentField(FreqVelocity) bsf = EmbeddedDocumentField(FreqVelocity) bpfo = EmbeddedDocumentField(FreqVelocity) bpfi = EmbeddedDocumentField(FreqVelocity) class VibrationSensorMetrics(Document): meta = { \'collection\': \'vibrationsensormetrics\', \'indexes\': [ (\'sensorId\', \'measurableId\', \'workspaceId\'), (\'sensorId\', \'measurableId\') ] } def save(self, *args, **kwargs): if not self.createdAt: self.createdAt = datetime.utcnow() self.updatedAt = datetime.utcnow() return super(VibrationSensorMetrics, self).save(*args, **kwargs) _id = ObjectIdField() sensorId = ObjectIdField() measurableId = ObjectIdField() measurableComponentId = ObjectIdField() workspaceId = ObjectIdField() direction = StringField() s3ObjectKey = StringField() freq = ListField(FloatField()) acceleration = ListField(FloatField()) velocity = ListField(FloatField()) stage = IntField() rmsVelocity = FloatField() specificFreqVelocity = EmbeddedDocumentField(SpecificFreqVelocities) startTime = DateTimeField() endTime = DateTimeField() createdAt = DateTimeField() updatedAt = DateTimeField() archived = BooleanField() The EmbeddedDocumentField is used to embed a document (without ObjectId) into that key. An example of the above schema: json { \\"_id\\": { \\"$oid\\": \\"6243d2cfb6c5eb79cbded8dc\\" }, \\"sensorId\\": { \\"$oid\\": \\"623a8e98607fd5d3f2061412\\" }, \\"measurableId\\": { \\"$oid\\": \\"623bcb04dc39ae10e39c7857\\" }, \\"measurableComponentId\\": { \\"$oid\\": \\"623bcb04dc39ae10e39c7858\\" }, \\"workspaceId\\": { \\"$oid\\": \\"62219400a6533f774a0a8500\\" }, \\"direction\\": \\"radial\\", \\"s3ObjectKey\\": \\"ICC_Vibration/Erbessd_Phantom/189280107/20220304/0504_Ch1.txt\\", \\"freq\\": [], \\"acceleration\\": [], \\"velocity\\": [], \\"stage\\": 0, \\"rmsVelocity\\": 0.7, \\"specificFreqVelocity\\": { \\"ft\\": { \\"frequency\\": 6.346813346174303, \\"velocity\\": 0.6951668919677223 }, \\"rpm\\": { \\"frequency\\": 14.166666666666666, \\"velocity\\": 0.3500187046119141 }, \\"bsf\\": { \\"frequency\\": 56.86395840490943, \\"velocity\\": 0.05195638432785299 }, \\"bpfo\\": { \\"frequency\\": 139.62989361583465, \\"velocity\\": 0.01651277478295155 }, \\"bpfi\\": { \\"frequency\\": 172.036773050832, \\"velocity\\": 0.007293205675754226 } }, \\"startTime\\": { \\"$date\\": \\"2022-03-03T21:04:00.000Z\\" }, \\"endTime\\": { \\"$date\\": \\"2022-03-03T21:04:05.000Z\\" }, \\"timestamp\\": { \\"$date\\": \\"2022-03-30T03:47:25.852Z\\" } } #### Operations Basically in mongoose we rely on the Model instance and in mongoengine we rely on the Document instance. ##### Init Models/Documents, Create and Save Documents, Collection Names - **In mongoose.** We create/init a Model object by either js import { Schema, model } from \\"mongoose\\"; type TCustom = { id: String, name: String, isDeleted: Boolean }; const schema = new Schema() < T > ({ id: String, name: String, isDeleted: Boolean }, { timestamps: true }); export const SymCatModel = model(SymCatModelName, schema); or js // create connection const conn = await mongoose.connect(config.get(\\"db.uri\\"), dbOptions); // init model conn.model(\\"TheModel\\", theModelSchma); // get the model model = conn.model(\\"TheModel\\"); and use the async methods: - Model.create(object: TCustom) or - Model.insertMany(objects: TCustom[]) to create documents (don\'t need to call .save() method). - **In mongoengine.** We create/init a Document instance, the instance is a callable function that can create a document: python vsm_doc = VibrationSensorMetrics() new_doc = vsm_doc( rmsVelocity = 0.1 ) new_doc.save() A new document will be saved in a collection named vibrationsensormetrics. ##### Query for Documents - **In mongoose.** We use: - Model.find(filterQuery) - **In mongoengine.** We can either pass a **_keyword argument_** or a **_raw dictionary_** as a filterQuery to a Document instance: - Document.objects.get(_id=some_id) - Document.objects(_id=some_id).order_by(\\"-timestamp\\").first() - Document.objects(__raw__={ \\"_id\\": some_id }) For example, we can assign vsm_doc = VibrationSensorMetrics() with vsm_doc in place of Document in the above methods. ##### Update Documents - **In mongoose.** js // or updateMany const result = await OilModel.updateOne( { id: oilUpdate.id }, { $set: oilUpdate }, { upsert: false } ); - **In mongoengine.** python vsm_doc = VibrationSensorMetrics() vsm_doc.objects(_id=some_id).update(__raw__={ \\"$set\\": { f\'measurableComponent.{cpn_index}.velocity.value\': highest_velocity, f\'measurableComponent.{cpn_index}.velocity.latest\': time }, \\"unset\\": { \\"measurableComponents\\": \\"\\" } }, upsert=False ) ##### Delete Documents For the sake of completeness we also mention how to delete documents. Although in practice we prefer assigning a boolean to **_soft-delete_** a document instead. - **In mongoose**. js Model.deleteMany(filterQuery); - **In mongoengine**. js Document.objects((__raw__ = filterQuery)).delete(); ","title":"Mongoose in Nodejs and Mongoengine in Python","date":"2022-03-29T00:00:00.000Z","id":"blog054","tag":"python, mongo","intro":"Record everything we should know about mongo with mongoose in nodejs and mongoengine in python."},{"content":" #### Make Button able to be Ctrl-Clicked and Middle-Clicked If we simply make a button with history.push() registered as a click event, then this button cannot be ctrl-clicked and middle-clicked. Or even if we implement window.open(href, \\"_blank\\") when only these two operations are performed, we cannot manage to keep the browser to stay in the current tab. The only possible way is to use anchor, so let\'s record the following function that keep ordinary anchor behaviour and use history.push for simple left-click! The key boolean we need is: js const middleClickedOrCtrlPressed = event.button === 1 || event.ctrlKey; This indicates either - the middle-button is clicked or - the control key is pressed, with that: js const navToArticle = (articleId: string) => (event?: MouseEvent<HTMLElement>) => { if (event) { event.preventDefault(); // middleClick or ctrlkey pressed const middleClickedOrCtrlPressed = (event.button === 1) || event.ctrlKey if (!middleClickedOrCtrlPressed && event.button === 0) { // when not middle-clicked, or not ctrl-pressed and when it is a simple left-click history.push(urlByArticleId(articleId)) return; } } } return ( ... <a href={urlByArticleId(articleId)} onClick={navToArticle(articleId)} > <Button> ... </Button> </a> ) Sometimes we use <a/> to wrap <button/> element that may have animation effect, we may need to setTimeout for history.push (with 50ms, say) for smoother experience. #### Scroll to Target Element Recently I have implemented a floating TOC (as shown in the picture or LHS of this article), this will pop up when the browser scroll down and go across some threshold depending on the bottom of the top TOC: <center> <a href=\\"/assets/tech/034.png\\"> <img src=\\"/assets/tech/034.png\\" width=\\"600px\\"/> </a> </center> <p/> <center></center> But when there are too many titles, for example: in <a href=\\"https://machingclee.github.io/blog/article/Nextjs-with-Electron\\">this article</a>, then it is natural to wish the floating TOC can highlight and **_scroll to that title_** automatically. For highlighting title, behind the scene we calculate at which position we have scrolled to and assign the corresponding active anchor in the floating TOC an active className. We now focus on how to get the **_relative position_** of our _active anchor_ from the top of its parent --- the floating TOC. Suppose as in the picture we want to highlight the _active anchor_: **_How to convert imagepath into base64 encoded data_**, which is stored in the variable titleAnchor, then: - The **relative position** of titleAnchor to its parent is calculated by js const scrollDistance = (titleAnchor as HTMLAnchorElement).offsetTop; - We get the parent by the assigned id js const floatingToc = document.querySelector(\\"#floating-toc\\"); - Then we scroll to the specfic position by js floatingToc?.scrollTo({ left: 0, top: scrollDistance, behavior: \\"smooth\\", }); ","title":"Ctrl- and Middle-Clickable Button; Method to Scroll to Target Element","date":"2022-03-30T00:00:00.000Z","id":"blog055","tag":"react","intro":"Record how to make button left- and middle-clickable; Record how to scroll to desired HTML element vertically."},{"content":" #### About this Article This article is an experiment inspired by this tutorial: <a href=\\"https://towardsdatascience.com/colorizing-black-white-images-with-u-net-and-conditional-gan-a-tutorial-81b2df111cd8\\">Colorizing black & white images with U-Net and conditional GAN \u2014 A Tutorial</a>. Since I am not used to pytorch which the tutorial bases on, the following things will be rewritten in tensorflow: - The models - The data processing pipeline - The generator of dataset - The train loop (including the update of training weights) - The visualization of our results Enjoy! #### Results Since the training datasets are just animated characters. The only common characteristic are the color of skins, therefore the model is not able to paint clothes, hair in a colorful way (as it has no idea how to learn). Original image: <center> <img src=\\"/assets/tech/035.jpg\\"/> <p/> </center> Transferred to gray scale and let the GAN color it: <center> <img src=\\"/assets/tech/036.png\\" width=\\"600\\"/> <p/> </center> #### Preliminary Import ##### Usual Packages python from numpy.random import randint from tensorflow.keras.optimizers import Adam from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError from tensorflow.keras.initializers import HeNormal from tensorflow.keras.preprocessing.image import img_to_array, load_img from tensorflow.keras.layers import ( Input, Conv2D, Conv2DTranspose, LeakyReLU, Activation, Concatenate, BatchNormalization, ZeroPadding2D ) import numpy as np import cv2 import tensorflow as tf import matplotlib.pyplot as plt from glob import glob import os from tensorflow.keras.models import Model from tqdm.notebook import tqdm from skimage.color import rgb2lab, lab2rgb import os %matplotlib inline ##### For Using GPU python os.environ[\'TF_XLA_FLAGS\'] = \'--tf_xla_enable_xla_devices\' config = tf.compat.v1.ConfigProto( gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8) ) config.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config) tf.compat.v1.keras.backend.set_session(session) #### RGB Color Space and Lab Color Space (WIP) To be added. #### Dataset Generator We will again make use of the tf.data.Dataset.list_files as a starting point to create a tf.data.Dataset object. The Dataset object is very handy because of the following useful methods: - .map() - .filter() - .shuffle(buffer_size) - .batch(batch_size) - .cache() ###### Preprocessing Functions python SIZE = 256 def path_to_img(file_path): img = tf.io.read_file(file_path) img = tf.image.decode_jpeg(img, channels=3) img = tf.image.resize(img, (SIZE, SIZE)) return img def rgb_normalize_to_0_1(img): img = tf.cast(img, dtype=tf.float32) return img/255 def rgb_denormalize_from_0_1(img): return (img + 1)*127.5 def lab_normalize_to_minus1_to_1(img): L = img[:, :, 0] / 50. - 1 ab = img[:, :, [1, 2]]/110. return L[..., np.newaxis], ab def lab_denormalize_from_minus1_to_1(img): L = (img[:, :, [0]] + 1) * 50 ab = img[:, :, [1, 2]] * 110 return np.concatenate([L, ab], axis=-1) def process_img(img): # tf.numpy_function(func=lambda x: print(x), inp=[img], Tout=tf.float32) img = img/255. img = tf.image.random_flip_left_right(img) img_lab = tf.numpy_function(func=lambda x: rgb2lab(x).astype(\\"float32\\"), inp=[img], Tout=tf.float32) L, ab = tf.numpy_function(func=lab_normalize_to_minus1_to_1, inp=[img_lab], Tout=[tf.float32, tf.float32]) return L, ab ###### Chaining Preprocessing Functions Therefore we just need to take care of how to preprocess data from individual file path. Dataset\'s api will handle the rest. Among the above, we use .map to chain our data processing pipeline: python def get_data_generator(): buffer_size = 100 batch_size = 16 dataset = tf.data.Dataset.list_files(f\\"{dataset_name}/*.jpg\\")\\\\ .map(path_to_img)\\\\ .map(process_img)\\\\ .shuffle(buffer_size)\\\\ .batch(batch_size) return (data for data in iter(dataset)) #### Implementation ##### Generator by UNet Structure python def conv_block(n_filters, input, kernel_initialization=None): if kernel_initialization: y = Conv2D(n_filters, (3, 3), strides=(2, 2), padding=\\"same\\", use_bias=False, kernel_initializer=kernel_initialization)(input) else: y = Conv2D(n_filters, (3, 3), strides=(2, 2), padding=\\"same\\", use_bias=False)(input) y = BatchNormalization()(y) y = LeakyReLU(0.2)(y) return y def upconv_block(n_filters, input, skip_connection): u = Conv2DTranspose(n_filters, (3, 3), strides=(2, 2), padding=\\"same\\")(input) u = Concatenate(axis=-1)([u, skip_connection]) u = Conv2D(n_filters, (3, 3), strides=1, padding=\\"same\\", activation=\\"relu\\")(u) u = Conv2D(n_filters, (3, 3), strides=1, padding=\\"same\\", activation=\\"relu\\")(u) return u def get_generator(): init = HeNormal() x = Input(shape=(SIZE, SIZE, 1)) d1 = conv_block(64, x, kernel_initialization=init) d2 = conv_block(128, d1) d3 = conv_block(256, d2) d4 = conv_block(512, d3) u3 = upconv_block(256, d4, d3) u2 = upconv_block(128, u3, d2) u1 = upconv_block(64, u2, d1) final = upconv_block(2, u1, x) final = Activation(\\"tanh\\")(final) return Model(x, final) ##### PatchGAN Discriminator by Repeated Conv Blocks python def add_padding(padding=(1,1)): return ZeroPadding2D(padding=padding) def get_discriminator(): input = Input(shape=(256,256,2)) x = add_padding()(input) x = Conv2D(64, (4, 4), strides=2, padding=\\"same\\", use_bias=False)(x) x = LeakyReLU(0.2)(x) x = add_padding()(x) x = Conv2D(128, (4, 4), strides=2, padding=\\"same\\", use_bias=False)(x) x = BatchNormalization()(x) x = LeakyReLU(0.2)(x) x = add_padding()(x) x = Conv2D(256, (4, 4), strides=2, padding=\\"same\\", use_bias=False)(x) x = BatchNormalization()(x) x = LeakyReLU(0.2)(x) x = add_padding()(x) x = Conv2D(512, (4, 4), strides=1, padding=\\"same\\", use_bias=False)(x) x = BatchNormalization()(x) x = LeakyReLU(0.2)(x) x = add_padding()(x) x = Conv2D(1, (4, 4), strides=1, padding=\\"same\\")(x) return Model(input, x) #### Training ##### Functions to Visualize Intermediate Performance python def get_gray_image_from_path(img_path): im_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) im_gray_3ch = np.concatenate([im_gray[...,np.newaxis] for _ in range(3)], axis=-1) return im_gray_3ch def visualize_result(epoch=0, step=0): random_index = np.random.randint(0, len(imgs_paths)) img_path = imgs_paths[random_index] im_gray_3ch = get_gray_image_from_path(img_path) filename = os.path.basename(img_path) original_size = im_gray_3ch.shape[0:2][::-1] img_ = cv2.resize(im_gray_3ch, dsize=(SIZE, SIZE), interpolation=cv2.INTER_CUBIC) img_ = img_/255. img_lab = rgb2lab(img_).astype(\\"float32\\") L, _ = lab_normalize_to_minus1_to_1(img_lab) faked_coloring = gen.predict(np.array([L]))[0] colored_img_in_lab_in_minus1_to_1 = np.concatenate([L, faked_coloring], axis=-1) colored_img_in_lab = lab_denormalize_from_minus1_to_1(colored_img_in_lab_in_minus1_to_1) faked_colored_image = (lab2rgb(colored_img_in_lab) * 255).astype(\\"uint8\\") faked_colored_image = cv2.resize(faked_colored_image, dsize=original_size, interpolation=cv2.INTER_CUBIC) plt.figure(figsize=(18, 30)) plt.subplot(1, 2, 1) plt.axis(\\"off\\") plt.imshow(im_gray_3ch.astype(\\"uint8\\")) plt.subplot(1, 2, 2) plt.axis(\\"off\\") plt.imshow(faked_colored_image) taget_folder = \\"./epoch_{}\\".format(str(epoch).zfill(2)) if not os.path.exists(taget_folder): os.makedirs(taget_folder) plt.savefig(\\"./epoch_{}/result_{}_from_{}.png\\".format(str(epoch).zfill(2), str(step).zfill(3), filename), dpi=80, bbox_inches=\\"tight\\") ##### Start Training ###### Training Without Noise Now we implement our custom training loop for 10 epochs. We use get_data_generator to get new dataset for each epoch. We start our model/data initilization and training loop in separate code block: python gen_opt = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999) disc_opt = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999) gen_L1_loss_lambda = 100 mse = MeanSquaredError() mae = MeanAbsoluteError() and start: python for epoch in range(0, 10): epoch = epoch + 1 batch = 0 data_generator = get_data_generator() while True: try: batch += 1 print(f\\"{batch}-th batch\\", end=\\"\\\\r\\") Ls, abs = next(data_generator) real_images = tf.concat([Ls, abs], axis=-1) with tf.GradientTape(persistent=True) as tape: faked_coloring = gen(Ls) true_coloring = abs critic_on_faked_colorings = disc(faked_coloring) critic_on_true_coloring = disc(true_coloring) gen_loss = mse(tf.ones_like(critic_on_faked_colorings), critic_on_faked_colorings)\\\\ + 100 * mae(abs, faked_coloring) disc_loss = 0.5 * mse(tf.zeros_like(critic_on_faked_colorings), critic_on_faked_colorings)\\\\ + 0.5 * mse(tf.ones_like(critic_on_true_coloring), critic_on_true_coloring) grad_gen = tape.gradient(gen_loss, gen.trainable_variables) grad_disc = tape.gradient(disc_loss, disc.trainable_variables) gen_opt.apply_gradients(zip(grad_gen, gen.trainable_variables)) disc_opt.apply_gradients(zip(grad_disc, disc.trainable_variables)) if batch % 10 == 0: visualize_result(epoch, int(batch/10)) except StopIteration: print(f\\"Epoch {epoch} Ended\\") break except Exception as err: print(err) break ###### Training With Noise (WIP) ###### Training With Pretrained VGG-16 as a Backbone (WIP) #### References - <a href=\\"https://towardsdatascience.com/colorizing-black-white-images-with-u-net-and-conditional-gan-a-tutorial-81b2df111cd8\\"> Colorizing black & white images with U-Net and conditional GAN \u2014 A Tutorial </a> - <a href=\\"https://arxiv.org/abs/1603.08511\\"> Richard Zhang, Phillip Isola, Alexei A. Efros, Colorful Image Colorization </a> - <a href=\\"https://arxiv.org/abs/1611.07004\\"> Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros, Image-to-Image Translation with Conditional Adversarial Networks </a> ","title":"Color-GAN with Auto-Coloring on Animated Gray Images","date":"2022-04-01T00:00:00.000Z","id":"blog056","tag":"tensorflow, deep-learning","intro":"We study one of the GANs that can perform auto coloring to gray-images."},{"content":" #### pathlib - pathlib.Path object can be fed into open(). - Path.touch(exist_ok=True) creates a new file if file_path does not exist, otherwise do nothing. - Example: python from pathlib import Path file_path = \'batches.json\' json_filepath = Path(file_path) json_filepath.touch(exist_ok=True) with open(json_filepath, \\"w+\\") as json_io: pass #### json - We use json.load(IODevice) to load json data into a dictionary. - We use json.loads(String) to convert json string into a python object. - We use json.dumps(Object) to convert python object into a string. - Examples: python with open(json_file, \\"w+\\") as json_io: data = json.load(f) python json_file = Path(\'batches.json\') json_file.touch(exist_ok=True) with open(json_file, \\"w+\\") as json_io: json_io.write(json.dumps(trip_batches, indent=4)) ","title":"pathlib and json in Python","date":"2022-04-07T00:00:00.000Z","id":"blog058","tag":"python","intro":"Record simple usages of pathlib and json in python"},{"content":" json { \\"version\\": \\"0.2.0\\", \\"configurations\\": [ { \\"type\\": \\"node-terminal\\", \\"name\\": \\"Debug Current TS File (ts-node)\\", \\"request\\": \\"launch\\", \\"command\\": \\"yarn start\\", \\"cwd\\": \\"${workspaceRoot}\\" } ] } ","title":"Typescript Debugger Config","date":"2022-04-11T00:00:00.000Z","id":"blog059","tag":"typescript","intro":"Set up debugger for node project in typescript with minimal config.","toc":false},{"content":" #### SSH Into a Computer Given that I have following following config in ~/.ssh/config text Host gp HostName 12.34.567.89 User cclee Port 1314 Then I can ssh into this computer by text ssh gp and inputting the password. #### SSH into an EC2 Instance With EC2 we additionally need a private key. In both Windows (with git-bash) and unix-based system, we can use that key in .pem format. text Host uat HostName ec2-some-number.ap-northeast-1.compute.amazonaws.com User ubuntu Port 22 PreferredAuthentications publickey IdentityFile C:\\\\keys\\\\wbkeypair-tokyo-james.pem Host prd HostName ec2-some-other-number.ap-northeast-1.compute.amazonaws.com User ubuntu Port 22 PreferredAuthentications publickey IdentityFile C:\\\\keys\\\\wbkeypair-tokyo-james.pem #### Download a File in SSH Client ##### With ~/.ssh/config Suppose that we have identified the filepath to download, we can text scp gp:/the/file/path /local/file/path and we will be done by typing the password in console. ##### Without ~/.ssh/config text scp -P port-number user@domain:/destination source-file/directory ","title":"SSH Config and Download File in SSH Client","date":"2022-04-12T00:00:00.000Z","id":"blog060","tag":"coding","intro":"Record script to download files using SSH client."},{"content":" #### Lessons from the Implementation We record the implementation of the ThreadPool in the next section. For this section, we record several encountered errors and methods to get around them. ##### Box, Rc\\\\<T\\\\> | Weak\\\\<T\\\\>, RefCell\\\\<T\\\\>, Mutex\\\\<T\\\\>, Arc\\\\<t\\\\> - Box<dyn T> - **_Single thread only_** - Help save data on heap instead of memory - Checked in compile time - Rc<T> | Weak<T> - **_Reference Count Type_** - Checked in compile time - Used in **_single thread only_** - Created by let a = Rc:new(data) - Cloned by Rc::clone(&a) - a above is immutable - Used when multiple reference is needed - **_Mainly_** used as immutable reference - Combined with RefCell<T>, we can implement **_interior mutability_** - Rc<T>.downgrade() and Rc<T>.upgrade() can switch a reference to weak and strong respectively - (cont\'d) Used when there is a recursive relation that has parent-children relationship - weak_reference_count wouldn\'t affect the release of resource as long as the strong_reference_count of a variable goes to 0 - RefCell<T> - **_Reference Cell Type_** - Only checked in runtime - Used in **_single thread only_** - Can be used in mutable and immutable reference - Let let a = Rc::new(RefCell(4)), then a is immutable - But a.borrow_mut() can create a mutable reference - (cont\'d) After destructured, *a.borrow_mut() += 10 becomes valid - **Example (Interior Mutability).** rust // a should have been immutable, let a = Rc::new(RefCell::new(123)); // but with RefCell::new, the data become mutable by using .borrow_mut() let b = Rc::new(&a); *b.borrow_mut() += 1000; print!(\\"{:?}, {:?}\\", a, b); // print: // RefCell { value: 1123 }, RefCell { value: 1123 } - Some comments in the internet suggest avoiding interior mutability when possible - An immutable ereference can be created by a.borrow() - Mutex<T> - Used in **_multi-threaded scenario_** - Used when internal state of the object of type T can change in different thread, this is to avoid data race - Need Mutex<T>.lock.unwrap() to get the lock and gain right to access and mutate the value - Arc<T> - Used in **_multi-threaded scenario_** - **_Thread-safe_** and **_multi-threaded version_** of Rc<T>, can be referenced by many threads - A stands for **atomic** - **Example.** Suppose that a single receiver: Arc<Mutex<mpsc::Receiver<Message>>> is passed to 10 threads: rust let message: Message = receiver.lock().unwrap().recv().unwrap(); We will be using module mpsc (multi-producer, single-consumer) to get Sender and Receiver ##### Access Struct\'s Field but Encounter: does not implement the Copy trait Error We consider the following exmaple: rust struct Foo { bar: Bar, } impl Foo { fn foo(&self) { self.bar.bar(); } } struct Bar {} impl Bar { fn bar(self) {} } fn main() { let foo = Foo { bar: Bar {} }; foo.foo(); } By cargo check we get text 93 | let bar = self.bar; | ^^^^^^^^ | | | move occurs because self.bar has type Bar, which does not implement the Copy trait | help: consider borrowing here: &self.bar ###### Reason of the Problem The problem is that fn Bar.bar takes the ownership of the Bar instance (the self.bar). If self.bar.bar() were executable, then self.bar will be release **_accidentally_** after the execution of that function is finished, which is disastrous and should be forbiddened. <center></center> ###### Solution: Universal Trick by using Option\\\\<T\\\\> In other words, self.bar must be taken away **_intentionally_** in order to be fed into .bar method. The universal trick is to make self.bar be of type Option<T>, then rust (self.bar as Option<T>).take() can take away **_both_** the ownership and the value of self.bar and set self.bar to None: rust ... struct Foo { bar: Option<Bar>, } impl Foo { fn foo(&mut self) { if let Some(bar) = self.bar.take() { bar.bar(); } } } ... Note that because self.bar is mutated (as self.bar becomes Option::None), we need foo(&mut self) instead of foo(&self). <center></center> **Possible Scenario.** The problem discusses above occur occasionally. For example, if our struct contains a field named thread: thread::JoinHandle<()>, and if we want to execute thread.join(), then because the signature of .join is: rust pub fn join(self) -> Result<T> we must take out the thread intentionally by chaning the type of the field thread from type thread::JoinHandle<()> to Option<thread::JoinHandle<()>>. ","title":"Summarize Rust Beginning Tutorial by a Simplified Multithreading Web Server","date":"2022-04-15T00:00:00.000Z","id":"blog061","tag":"rust","intro":"Completed the Rust official tutorial from its <a href=\\"https://doc.rust-lang.org/book/title-page.html\\">online book</a> and concluded the 20 chapters by a multithreading simple web server. Try to record the subtitle detail for future reference."},{"content":" #### Introduction: Why Care DCT? We have been using **_Discrete Cosine Transform_** (DCT) without possibly awaring of it: DCT is actually the JPEG compression standard! In audio analysis, DCT is used in computing MFCC coefficients to extract feature vector (for more on MFCCs, see **[KC]** in reference section), which makes deep learning on sound possible by - Treating the sequence of MFCC coefficients as a time sequence of data (at each \\"window\\" of suitable \\"hop length\\", we get a float vector of fixed size) or; - Stacking the MFCC coefficients and treat it as an image. In the former case the use of sequence model such as LSTM or Transformer becomes possible. The later case makes it possible to use convolution layer such as Conv1D in Keras. #### Basic Knowledge to Work With ##### Definitions In what follows for $i,j=0,1,2,\\\\dots, N-1$ we will denote $$ u_j^i = \\\\alpha_j\\\\cos \\\\frac{(2i+1)j\\\\pi}{2N}\\\\quad\\\\text{and} \\\\quad \\\\alpha_j =\\\\begin{cases} \\\\displaystyle\\\\frac{1}{\\\\sqrt{N}}& \\\\text{if } j = 0,\\\\\\\\ \\\\displaystyle\\\\sqrt{\\\\frac{2}{N}}& \\\\text{if } j \\\\neq 0. \\\\end{cases} $$ For each fixed $j$ we will consider $u_j(x):= u_j^x =\\\\alpha_j\\\\cos \\\\frac{(2x+1)j\\\\pi}{2N} $ as a function on a discrete domain. The vectors $$ \\\\left\\\\{u_j^{:} := \\\\left[\\\\left.u_j^i= \\\\alpha_j\\\\cos \\\\frac{(2i+1)j\\\\pi}{2N}\\\\quad \\\\text{ for }0\\\\leq i<N\\\\right]\\\\in \\\\mathbb R^N \\\\right| j=0,1,\\\\dots,N-1\\\\right\\\\} $$ form an orthonormal basis in $\\\\mathbb R^N$, by stacking all them together column by column $$ U := \\\\begin{bmatrix} | &|& &|\\\\\\\\ u_0^{:}& u_1^{:}&\\\\cdots &u_{N-1}^{:} \\\\\\\\ |&|& &| \\\\end{bmatrix}, $$ then $U$ is orthonormal: $U^TU = I$. The orthogonality of $U$ and the construction of the basis $u_j^:$ will be explained in _Mathematics behind DCT_ section. For now let\'s enjoy the coding and see the results. ##### Computations We make explicit calculations in order to make readers comfortable with the definition above. First, given a gray-scale image $f:[0, N-1]\\\\times[0, N-1]\\\\to \\\\mathbb R$ there are always unique coefficients $F:[0, N-1]\\\\times[0, N-1]\\\\to \\\\mathbb R$ such that $$ f(p,q) =\\\\sum_{i=0}^{N-1}\\\\sum_{j=0}^{N-1}\\\\alpha_i \\\\alpha_j F(i, j) \\\\cos \\\\bigg(\\\\frac{(2p+1)i\\\\pi}{2N}\\\\bigg)\\\\cos\\\\bigg( \\\\frac{(2q+1)j\\\\pi}{2N}\\\\bigg).\\\\tag*{$(*)$} $$ In fact by using $U$ defined in the previous section. Denote also $f, F$ as the matrix with entry $f(p,q), F(p,q)$ at $p$-th row and $q$-th column, if we define $F = U^T f U$, then $$ f=UFU^T. $$ You can expand the RHS ($UFU^T$) to convince yourself that eventually you get the same expression as in $(*)$. On the other hand, by using this $U$ we have the reverse: $$ F(x,y) = \\\\sum_{h=0}^{N-1}\\\\sum_{k=0}^{N-1} f(h,k) \\\\alpha_x\\\\alpha_y \\\\cos \\\\bigg( \\\\frac{(2h+1)x\\\\pi}{2N}\\\\bigg)\\\\cos \\\\bigg(\\\\frac{(2k+1)y\\\\pi}{2N}\\\\bigg), $$ as this is not a summation of $\\\\cos$\'s of fixed frequencies, that\'s why we don\'t define $f=U^TfU$. Our coding will strictly follow the notation and calculation in this section. #### Implementation of JPEG Compression in Python For readers who are more familiar with matlab, you may also follow the youtube video listed in **[ET]**. Most of the content is a translation from matlab to python based on my understanding. Notation and definition may be different from the video. ##### Basic Import python import numpy as np import matplotlib.pyplot as plt from tensorflow.keras.preprocessing.image import img_to_array, load_img %matplotlib inline ##### Constants python N = 8 IMAGE_PATH = \\"512_512_image.jpg\\" ##### Basis of Cosine Transform python def range2D(n): indexes = [] for i in range(0, n): for j in range(0, n): indexes.append((i, j)) return indexes def alpha(p): return 1/np.sqrt(N) if p == 0 else np.sqrt(2/N) def cos_basis(i,j): def cal_result(a): x=a[0] y=a[1] return alpha(i) * alpha(j) * np.cos((2*x+1) * i * np.pi/(2*N)) * np.cos((2*y+1) * j * np.pi/(2*N)) return cal_result ##### Plot of the 2D Cosine Basis Functions python fig = plt.figure() fig.set_figheight(12) fig.set_figwidth(12) xy_range = range2D(N) xy_plane_NxN = (np.array(xy_range).reshape(N,N,2)) for i in range(0, N): for j in range(0, N): xy_result = np.apply_along_axis(cos_basis(i,j), -1, xy_plane_NxN) plt.subplot(N, N, 1 + N*i + j) plt.imshow(xy_result, cmap=\\"gray\\") <center> <a href=\\"/assets/tech/037.png\\" target=\\"_blank\\"> <img src=\\"/assets/tech/037.png\\"/ width=\\"500\\"> </a> </center> ##### Compute Coefficients for Cosine Transform Let\'s prepare the following $8\\\\times 8$ matrix that convert the original pixels into compacted energy distribution: python U = np.zeros((N,N)) for i in range(0, N): for j in range(0, N): U[i, j] = alpha(j) * np.cos((2*i+1)*j*np.pi/(2*N)) U_t = U.transpose() You may check that U is indeed orthonormal by: python np.matmul(U, U_t) # Result: # array([[ 1.00000000e+00, -1.01506949e-16, -3.78598224e-17, # -1.03614850e-17, -1.33831237e-16, 7.88118312e-17, # 2.00856868e-16, -1.70562493e-16], # [-1.01506949e-16, 1.00000000e+00, 1.70298111e-17, # -1.13980777e-17, 1.37840552e-16, -2.88419699e-16, # -1.24636773e-16, 2.51800326e-17], # [-3.78598224e-17, 1.70298111e-17, 1.00000000e+00, # -1.61419161e-16, -2.81922100e-17, -1.25319880e-16, # -1.83252893e-16, -2.79432802e-17], # [-1.03614850e-17, -1.13980777e-17, -1.61419161e-16, # 1.00000000e+00, -1.92659768e-17, 2.75108042e-16, # 2.02487213e-16, 2.22263103e-18], # [-1.33831237e-16, 1.37840552e-16, -2.81922100e-17, # -1.92659768e-17, 1.00000000e+00, -8.54966705e-17, # -2.36183557e-17, -3.71508058e-16], # [ 7.88118312e-17, -2.88419699e-16, -1.25319880e-16, # 2.75108042e-16, -8.54966705e-17, 1.00000000e+00, # 1.21575873e-16, 8.41357861e-17], # [ 2.00856868e-16, -1.24636773e-16, -1.83252893e-16, # 2.02487213e-16, -2.36183557e-17, 1.21575873e-16, # 1.00000000e+00, 3.92417644e-16], # [-1.70562493e-16, 2.51800326e-17, -2.79432802e-17, # 2.22263103e-18, -3.71508058e-16, 8.41357861e-17, # 3.92417644e-16, 1.00000000e+00]]) ##### Mask or Truncate the Energy Distribution Here we use a naive approach, we screen out the fourier coefficents by simply retaining those from the upper-left corner. For that, we create a mask: python def create_mask(closure): mask = np.zeros((N, N)) for i in range(N): for j in range(N): if closure(i,j): mask[i, j] = 1 return mask For example, by calling mask = create_mask(lambda i,j: 0<=i+j<=2) our mask will be like: text [[1. 1. 1. 0. 0. 0. 0. 0.] [1. 1. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.]] ##### Result: Apply Cosine Transform python original = load_img(IMAGE_PATH, target_size=(512, 512), color_mode=\\"grayscale\\") original = img_to_array(original).astype(\\"uint8\\") original = np.squeeze(original) original.shape mask = create_mask(lambda i,j: 0<=i+j<=2) print(mask) steps = int(original.shape[0]/N) # we choose 512x512 image, therefore steps = 64 print(steps) compressed = np.zeros_like(original) for x in range(steps): for y in range(steps): sub_pixels = original[N * y: N*(y+1), N * x: N*(x+1)] sub_pixels = sub_pixels - 127.5 fourier_coefficients = np.matmul(np.matmul(U_t, sub_pixels), U) fourier_coefficients = fourier_coefficients * mask reverted_pixels = np.matmul(np.matmul(U, fourier_coefficients), U_t) reverted_pixels = reverted_pixels + 127.5 compressed[N * y: N*(y+1), N * x: N*(x+1)] = reverted_pixels fig.set_figheight(20) fig.set_figwidth(20) plt.subplot(1, 2, 1) plt.imshow(original, cmap=\\"gray\\") plt.subplot(1, 2, 2) plt.imshow(compressed, cmap=\\"gray\\") plt.savefig(\\"DCT_result\\", dpi=200, bbox_inches=\\"tight\\") <center> <a href=\\"/assets/tech/038.png\\" target=\\"_blank\\"> <img src=\\"/assets/tech/038.png\\"/ width=\\"500\\"> </a> </center> #### Mathematics behind DCT **Notation.** Every vector will be considered as a **column vector** whenever the computation does not make sense when they are represented as a row. ##### From DFT Let $N\\\\ge 1 $ be an integer and denote $w = e^{2\\\\pi i/N}$, define $ v_k:= (1,w^k, w^{2k},\\\\dots, w^{(N-1)k}), $ then the matrix $$ V:=\\\\begin{bmatrix} |&|& & |\\\\\\\\ v_0&v_1&\\\\cdots & v_{N-1}\\\\\\\\ |&|& & | \\\\end{bmatrix} $$ is orthogonal simply because their inner product $$ (v_k,v_\\\\ell) = \\\\sum_{j=0}^{N-1}(w^k)^j (\\\\overline{w})^j = \\\\frac{(w^k \\\\overline{w}^\\\\ell )^{N} - 1}{\\\\underbrace{w^k\\\\overline{w}^\\\\ell-1}_{\\\\neq 0 \\\\text{ if $k\\\\neq \\\\ell$}}} $$ is $0$ when $k\\\\neq \\\\ell$, where $w^N=1$. Therefore the basis $\\\\mathcal V:=\\\\{v_0,v_1,\\\\dots,v_{N-1}\\\\}$ forms an orthogonal basis in $\\\\mathbb C^{N}$. But how to find a similar basis in $\\\\mathbb R^n$? It is not as simple as taking the real part of the linear combination $v = V[v]_{\\\\mathcal V} = \\\\sum a_i v_i$ since the coordinate $[v]_{\\\\mathcal V}\\\\in \\\\mathbb C^N$ even $v\\\\in \\\\mathbb R^N$. ##### DCT-2 ###### Strategy of Construction Consider the symmetric second difference matrix: $$ A = \\\\begin{bmatrix} \\\\otimes=1& \\\\otimes\'=-1 &0&0&&\\\\cdots&&0\\\\\\\\ -1&2&-1 &0&&\\\\cdots&&0\\\\\\\\ 0& -1&2&-1 &&\\\\cdots&&0\\\\\\\\ & &&&&&&\\\\\\\\ \\\\vdots& &&&\\\\ddots&&&\\\\vdots \\\\\\\\ & &&&&&&\\\\\\\\ &&&&&-1&2&-1\\\\\\\\ 0&0&0&0&\\\\cdots&0&\\\\boxtimes=-1&\\\\boxtimes\'=1 \\\\end{bmatrix} $$ which arises when a second derivate is approximated by the central second difference: $$ -f\'\'(x) = \\\\frac{1}{h^2}\\\\bigg(-f(x-h)+2f(x)-f(x+h)\\\\bigg) + o(h) $$ as $h\\\\to 0$. Let $u_{k,:}\\\\in \\\\mathbb R^N$ denote the values of a function evaluated on the discretized domain $[x_{0}, x_{1},\\\\dots,x_{N-1}]$ of $[0, \\\\pi-\\\\frac{1}{N}]$, i.e., $u_{k,:} = f_k(x_{:})$ for some $f_k:\\\\mathbb [0, \\\\pi-\\\\frac{1}{N}] \\\\to \\\\mathbb R$, where : means stacking values by running through all indexes at that position. Our cosine transform is based on decomposing a function into a linear combination of cosine functions (of different frequencies) **_in discrete case_**. To find them, we will choose $u_{k,:}$ such that $Au_{k,:}=\\\\lambda_k u_{k,:}$. By the fact that: > **Fact.** Eigenvectors associated to different eigenvalues are linearly independent. <center></center> we then obtain a set of eigenvectors (of different eigenvalues) solving the approximated problem $$ -\\\\frac{1}{N^2}f_k\'\'(x_{:})\\\\approx Au_k = \\\\lambda_k f_k(x_{:}). $$ Which function would solve the ODE $-f\'\'=N^2\\\\lambda f$? Which is the trigonometric function! ###### Why $(\\\\otimes, \\\\otimes\') = (1,-1)$ and $(\\\\boxtimes, \\\\boxtimes\')=(-1,1)$? These two assignments are determined by imposing boundary conditions. More specific, we extend the domain from $[0, \\\\pi-\\\\frac{1}{N}]$ to $[-\\\\frac{1}{N}, \\\\pi]$, imagine we are solving $\\\\tilde u_k \\\\in \\\\mathbb R^{N+2}$ (append one point at the beginning and the tail of $x_{:}\\\\in \\\\mathbb R^N$ respectively). We have not imposed any boundary condition to our solution $f_k$ yet. We will require our function be symmetric at $-\\\\frac{1}{2N}$ (or at $j=-\\\\frac{1}{2}$ if $j=-1,0,1,\\\\dots,N$, $x_j = \\\\frac{j\\\\pi}{N}$), this implies $f_k(x_{-1}) = f_k(x_0)$. At another boundary due to the shift above, we try to require $f_k\'(x_{N-\\\\frac{1}{2}})=0$, this implies $f_k(x_{N-1}) = f_k(x_N)$. $$ \\\\left\\\\{ \\\\begin{align*} u_{k,-1}&=f_k(x_{-1})=f_k(x_{0}) = u_{k,0}\\\\\\\\ u_{k,N-1}&=f_k(x_{N-1})= f_k(x_{N}) = u_{k,N} \\\\end{align*} \\\\right. $$ and plug this condition into the second difference formula to get: $$ \\\\left\\\\{ \\\\begin{align*} -u_{k,-1}&+2u_{k,0} -u_{k,1} = u_{k,0} -u_{k,1} \\\\\\\\ -u_{k,N-2}&+ 2 u_{k,N-1} -u_{k,N} = -u_{k,N-2}+ u_{k,N-1}. \\\\end{align*} \\\\right. $$ These become the necessary condition and thus have determined our $(\\\\otimes, \\\\otimes\') =(1,-1)$ and $(\\\\boxtimes, \\\\boxtimes\')=(-1,1)$, and therefore ensure our solution is a cosine function. Different values of $\\\\otimes$\'s and $\\\\boxtimes$\'s will correspond to different boundary condition imposed on $f_k$ for other combinations, see **[GS]** for a complete classification. The solution corresponding to $(\\\\otimes, \\\\otimes\') = (1,-1)$ and $(\\\\boxtimes, \\\\boxtimes\')=(-1,1)$ is usally called the basis of **_DCT-2_** (or simply **_DCT_**). ##### Derivation of the Basis of Discrete Cosine Transform We have spent many effort to determine the matrix $A$ to work with, let\'s start with computing the basis directly for our only candidate: Let $\\\\ell(x) = ax + b$ for some $a,b\\\\in \\\\mathbb R$, denote $$ u_{k,0:N} = (1, w^{\\\\ell(1)k},w^{\\\\ell(2)k},\\\\dots w^{\\\\ell(N-1)k}), $$ note that we have $\\\\sum_{k=0}^{N-1} u_{k,:} = \\\\sum_{i=0}^{N-1} (\\\\sum_{k=0}^{N-1} w^{\\\\ell(i)k}) e_i = \\\\frac{1-w^{N\\\\ell(i)}}{1-w^{\\\\ell(i)}} = 0$ whenever $\\\\ell(i)\\\\neq 0$ for integer $i$. As we will see this is indeed the case later for $\\\\ell (x) =\\\\frac{1}{2}x+ \\\\frac{1}{4}$. Since $\\\\ell(j\\\\pm 1) = \\\\ell(j) \\\\pm a$, then for $j=1,2,\\\\dots,N-2$, coordinate-wise: $$ \\\\begin{align*} [Au_{k,:}]_{j} & = -w^{\\\\ell(j-1)k} + 2w^{\\\\ell(j)k} - w^{\\\\ell(j+1)k} \\\\\\\\ &= \\\\big(2-(w^{ak} + w^{-ak}) \\\\big)w^{\\\\ell(j)k}\\\\\\\\ &= \\\\bigg(2-2\\\\cos \\\\frac{2ak\\\\pi }{N}\\\\bigg)u_{k,j}. \\\\end{align*} $$ Therefore $Au_{k,1:N-1} = (2-2\\\\cos \\\\frac{2ak\\\\pi i}{N}) u_{k,1:N-1}$ for whatever linear $\\\\ell$ we choose (here $x_{h:k} = [x_i \\\\quad \\\\text{ for }i\\\\text{ s.t. } h\\\\leq i <k]$). By taking real part on both sides, as $A$ is real, we have $$ A\\\\left[ \\\\sum_{1\\\\leq j< N-1} \\\\cos \\\\bigg(\\\\frac{2\\\\pi }{N}\\\\ell(j)k\\\\bigg)e_j\\\\right] = \\\\bigg(2-2\\\\cos \\\\frac{2ak\\\\pi }{N}\\\\bigg)\\\\sum_{1\\\\leq j< N-1} \\\\cos \\\\bigg(\\\\frac{2\\\\pi }{N}\\\\ell(j)k\\\\bigg)e_j, $$ here $e_j$ denotes the standard basis in $\\\\mathbb R^N$. It remains to require $u_j := \\\\cos \\\\frac{2k\\\\pi}{N}\\\\ell(j)$ satisfies $u_{-1}=u_0$ and $u_{N-1}=u_N$, from which we can determine $a,b$ and thus $\\\\ell$. The equation $u_1=u_0$ implies $\\\\cos \\\\big(\\\\frac{2k\\\\pi }{N}(-a+b)\\\\big) = \\\\cos \\\\big(\\\\frac{2k\\\\pi}{N}(b)\\\\big)$, since $\\\\cos$ is even, it is sufficient to require $$ a-b = b \\\\iff a = 2b. $$ The equation $u_{N-1}=u_N$ implies $$ \\\\cos \\\\bigg( 4kb\\\\pi -\\\\frac{2kb\\\\pi}{N}\\\\bigg) = \\\\cos \\\\bigg(4kb\\\\pi +\\\\frac{2kb\\\\pi}{N}\\\\bigg), $$ for this to hold, it is sufficient to require $4b = 1$ (as $\\\\cos$ is always symmetric about $x = k\\\\pi$ for every $k\\\\in \\\\mathbb Z$), altogether we have $a =\\\\frac{1}{2}$, and thus for $k=0,1,2,3,\\\\dots,N-1$, $$ \\\\frac{2\\\\pi}{N}\\\\ell(j)k = \\\\frac{2\\\\pi}{N}\\\\bigg(\\\\frac{1}{2}j + \\\\frac{1}{4}\\\\bigg) k = \\\\frac{(2j+1)k\\\\pi}{2N}. $$ <p></p> Finally, we have $$ z_k := \\\\sum_{0\\\\leq j< N} \\\\alpha_j\\\\cos \\\\bigg( \\\\frac{(2j+1)k\\\\pi}{2N}\\\\bigg)e_j,\\\\quad Az_k = \\\\bigg(2-2\\\\cos \\\\frac{k\\\\pi}{N}\\\\bigg) z_k $$ for suitably chosen $\\\\alpha_j$\'s, $j=0,1,2,\\\\dots,N-1$. #### Reference - **[KC]** Kartik Chaudhary, <a href=\\"https://towardsdatascience.com/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520\\"><i>Understanding Audio data, Fourier Transform, FFT and Spectrogram features for a Speech Recognition System</i></a>, - **[GS]** Gilbert Strang, <a href=\\"https://www.unioviedo.es/compnum/transversal_eng/DCT5.pdf\\"><i>The Discrete Cosine Transform</i></a> - **[ET]** Exploring Technologies, <a href=\\"https://www.youtube.com/watch?v=mUKPy3r0TTI\\"> <i>Discrete Cosine Transform (DCT) of Images and Image Compression</i></a> ","title":"Discrete Cosine Transform and JPEG Compression Implementation in Python","date":"2022-04-16T00:00:00.000Z","id":"blog062","tag":"math, python","intro":"Discrete Cosine Transform (DCT) is not only used in image processing, it is also used in signal processing of sound such as computing MFCC coefficients as a feature vector."},{"content":" js import React, { ReactNode, useEffect, useState } from \\"react\\"; import classnames from \\"classnames\\"; import { makeStyles } from \\"@material-ui/core\\"; const useStyles = makeStyles(( { \\"@keyframes fadein\\": { \\"0%\\": { opacity: 0, }, \\"100%\\": { opacity: 1 } }, customFadein: { \\"&.fade-in\\": { animation: \\"$fadein .3s ease-in-out\\" } } }, { name:\\"custom-fade-in\\" } ) export default function FadeIn({ children, dependencies = [] }: { children: ReactNode dependencies?: any[] }) { const [fadeIn, setFadeIn] = useState(false); const classes = useStyles(); useEffect(() => { setFadeIn(false); setTimeout(() => { setFadeIn(true); }, 1); }, [...dependencies]); return ( <div style={{ opacity: fadeIn ? 1 : 0 }}> <div className={classnames(classes.customFadein, fadeIn ? \\"fade-in\\" : \\"\\")} > <> {children} </> </div> </div> ); } Sometimes you may want the element to fade-in and out on state change, you can pass the arguments into FadeIn component as follows: js <FadeIn dependencies={[pathname]}> ... </Fadein> ","title":"Element that has Default Fade-in Transition on Mount","date":"2022-04-19T00:00:00.000Z","id":"blog063","tag":"react","intro":"Record an element that I made for default fade-in effect on component mount and state change.","toc":false},{"content":" #### Two Structures From the paper <a href=\\"https://arxiv.org/pdf/1512.03385.pdf\\"><i>Deep Residual Learning for Image Recognition</i></a> we can find two possible structure: <center> <a href=\\"/assets/tech/041.png\\"> <img src=\\"/assets/tech/041.png\\" width=\\"600\\"/> </a> </center> <p/> <center></center> The left is called a **_basic block structure_**, and the right is called a **_bottleneck structure_**. ##### Basic Block Structure Reference: <a href>https://keras.io/examples/generative/cyclegan/</a> python def residual_block( x, activation, kernel_initializer=kernel_init, kernel_size=(3, 3), strides=(1, 1), padding=\\"valid\\", gamma_initializer=gamma_init, use_bias=False, ): dim = x.shape[-1] input_tensor = x x = ReflectionPadding2D()(input_tensor) x = layers.Conv2D( dim, kernel_size, strides=strides, kernel_initializer=kernel_initializer, padding=padding, use_bias=use_bias, )(x) x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x) x = activation(x) x = ReflectionPadding2D()(x) x = layers.Conv2D( dim, kernel_size, strides=strides, kernel_initializer=kernel_initializer, padding=padding, use_bias=use_bias, )(x) x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x) x = layers.add([input_tensor, x]) return x ##### Bottleneck Structure For deeper network, we use: python def residual_block( x, filter_depth, strides=(1, 1), reduce_dim=False, reg=0.0001, bn_eps=2e-5, bn_mom=0.9 ): shortcut = x # we batch-normalize along the the channel axis, which is -1: bn = BatchNormalization(axis=-1, epsilon=bn_eps, momentum=bn_mom)(x) act = Activation(\\"relu\\")(bn) x = Conv2D( int(filter_depth * 0.25), (1, 1), strides=strides, use_bias=False, kernel_regularizer=l2(reg) )(act) x = BatchNormalization(axis=-1, epsilon=bn_eps, momentum=bn_mom)(x) x = Activation(\\"relu\\")(x) x = Conv2D( int(filter_depth * 0.25), (3, 3), strides=strides, padding=\\"same\\", use_bias=False, kernel_regularizer=l2(reg) )(x) x = BatchNormalization(axis=-1, epsilon=bn_eps, momentum=bn_mom)(x) x = Activation(\\"relu\\")(x) x = Conv2D( filter_depth, (1, 1), use_bias=False, kernel_regularizer=l2(reg) )(x) if reduce_dim: shortcut = Conv2D( filter_depth, (1, 1), strides=(2, 2), use_bias=False, kernel_regularizer=l2(reg) )(act) x = add([x, shortcut]) return x ","title":"Different Kinds of Residue Blocks","date":"2022-04-20T00:00:00.000Z","id":"blog064","tag":"deep-learning","intro":"Record different kinds of residue block that I have seen."},{"content":" #### About this Article The main objective of this article is to understand every function of YOLO-v3 implemented in tensorfow. We will understand the algorithm by examining the shape of every output. The sections below are named by module_name::{ function_1, function_2, ... }, where module_name means we are studying the file module_name.py. The link of **_source code_** are provided at the beginning of each of such sections. #### dataset::{ parse_annotation, bbox_iou } <a href=\\"https://github.com/machingclee/deep-learning-study/blob/main/2021-02-15-yolo-trials/2021-02-15-YOLOV3-head-detection/core/dataset.py\\"><i>Source Code</i></a> python def parse_annotation(self, annotation): line = annotation.split() image_path = line[0] if not os.path.exists(image_path): raise KeyError(\\"%s does not exist ... \\" % imagparse_annotation e_path) image = cv2.imread(image_path) bboxes = np.array([list(map(int, box.split(\',\'))) for box in line[1:]]) if self.data_aug: image, bboxes = self.random_horizontal_flip(np.copy(image), np.copy(bboxes)) image, bboxes = self.random_crop(np.copy(image), np.copy(bboxes)) image, bboxes = self.random_translate(np.copy(image), np.copy(bboxes)) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) image, bboxes = utils.image_preporcess(np.copy(image), [self.train_input_size, self.train_input_size], np.copy(bboxes)) return image, bboxes line = annotation.split() image_path = line[0] if not os.path.exists(image_path): raise KeyError(\\"%s does not exist ... \\" % image_path) image = cv2.imread(image_path) bboxes = np.array([list(map(int, box.split(\',\'))) for box in line[1:]]) if self.data_aug: image, bboxes = self.random_horizontal_flip(np.copy(image), np.copy(bboxes)) image, bboxes = self.random_crop(np.copy(image), np.copy(bboxes)) image, bboxes = self.random_translate(np.copy(image), np.copy(bboxes)) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) image, bboxes = utils.image_preporcess(np.copy(image), [self.train_input_size, self.train_input_size], np.copy(bboxes)) return image, bboxes This part is straightforward. $\\\\qed$ python-1 def bbox_iou(self, boxes1, boxes2): boxes1 = np.array(boxes1) boxes2 = np.array(boxes2) boxes1_area = boxes1[..., 2] * boxes1[..., 3] boxes2_area = boxes2[..., 2] * boxes2[..., 3] boxes1 = np.concatenate([boxes1[..., :2] - boxes1[..., 2:] * 0.5, boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1) boxes2 = np.concatenate([boxes2[..., :2] - boxes2[..., 2:] * 0.5, boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1) From the function we can deduce that $\\\\texttt{box}=(c_x,c_y,\\\\text{width},\\\\text{height})$, therefore $$ l \\\\texttt{boxes[...,:2]} - \\\\texttt{boxes[...,2:]}*0.5 = \\\\texttt{[all upper-left corners]} $$ and $$ \\\\texttt{boxes1[..., :2]} + \\\\texttt{boxes1[..., 2:]} * 0.5=\\\\texttt{[all lower-right corners]}, $$ while concatenating $(2,)$ np-array along the last axis simpliy means combing them into one $(4,)$ np-array. Numerically in training $\\\\texttt{boxes1}$ and $\\\\texttt{boxes2}$ are like: none [[0.59375 3.78125 0.5 0.625 ]] and none [[ 0.5 3.5 3.625 2.8125 ] [ 0.5 3.5 4.875 6.1875 ] [ 0.5 3.5 11.65625 10.1875 ]] respectively, python-13 left_up = np.maximum(boxes1[..., :2], boxes2[..., :2]) right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:]) Think of the above as entrywise comparisons that give an array of maximum, which yields the coordinates of intersection rectangle for each fixed $\\\\texttt{boxes1}$ to boxes in $\\\\texttt{boxes}$. python-15 inter_section = np.maximum(right_down - left_up, 0.0) The entries in $\\\\texttt{inter_section}$ are the $\\\\textit{width}$ and $\\\\textit{height}$ of the intersection, the (broadcasted) $\\\\texttt{np.maximum}$ is just a tricky way to handle empty intersection. python-16 inter_area = inter_section[..., 0] * inter_section[..., 1] union_area = boxes1_area + boxes2_area - inter_area return inter_area / union_area $\\\\qed$ python-1 def preprocess_true_boxes(self, bboxes): Here $\\\\texttt{bboxes}$ are the boxes from annotation file in which each line takes the form: $$ \\\\texttt{some/directory/hash.jpg 79,537,107,574,0 297,547,318,575,0} $$ python-2 label = [np.zeros((self.train_output_sizes[i], self.train_output_sizes[i], self.anchor_per_scale, 5 + self.num_classes)) for i in range(3)] bboxes_xywh = [np.zeros((self.max_bbox_per_scale, 4)) for _ in range(3)] bbox_count = np.zeros((3,)) for bbox in bboxes: bbox_coor = bbox[:4] bbox_class_ind = bbox[4] onehot = np.zeros(self.num_classes, dtype=np.float) onehot[bbox_class_ind] = 1.0 uniform_distribution = np.full(self.num_classes, 1.0 / self.num_classes) deta = 0.01 smooth_onehot = onehot * (1 - deta) + deta * uniform_distribution # bbox_xywh is ground truth bbox_xywh = np.concatenate([(bbox_coor[2:] + bbox_coor[:2]) * 0.5, bbox_coor[2:] - bbox_coor[:2]], axis=-1) # bbox_xywh_scaled is scaled ground truth relative to stride (13, 26, 52, as a unit) bbox_xywh_scaled = 1.0 * bbox_xywh[np.newaxis, :] / self.strides[:, np.newaxis] Note that $$ \\\\texttt{bbox_xywh[np.newaxis, :]} $$ is of shape $(1, 4)$ and $$ \\\\texttt{1/self.strides[:, np.newaxis]} $$ is of shape $(3, 1)$, their multiplication will be conducted by \\"broadcasting\'\' in numpy, which yields a $(3, 4)$ dimensional numpy array. The product $\\\\texttt{bbox_xywh_scaled}$ consists of $(c_x,c_y,w,h)$ which use \\"stride\\" as a unit, so 1 means \\"1 grid\\" (recall there are $13\\\\times 13$, $26\\\\times 26$, $52\\\\times 52$ grids predictions from Darknet backbone). python-22 iou = [] exist_positive = False for i in range(3): anchors_xywh = np.zeros((self.anchor_per_scale, 4)) anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5 anchors_xywh[:, 2:4] = self.anchors[i] $\\\\texttt{anchors_xywh}$ essentially moves centers of $\\\\texttt{bbox_xywh_scaled}$ to the middle of the grid where the original center lies in, then the anchor boxes\' width and height are assigned, replacing the original width, height of $\\\\texttt{bbox_xywh_scaled}$. python-28 iou_scale = self.bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh) The presence of $\\\\texttt{np.newaxis}$ is simply because multiplication between $(4,)$ and $(3,4)$ array does not make sense. The additional dimension expand $(4,)$ array into $(1,4)$ array, which is broadcasted and multiplied to $(3,4)$ array to give another $(3,4)$ array, and therefore, $\\\\texttt{iou_scale.shape} = (3, )$. python-29 iou.append(iou_scale) iou_mask = iou_scale > 0.3 # a boolean list of length 3 if np.any(iou_mask): # if one of them is True xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) label[i][yind, xind, iou_mask, :] = 0 label[i][yind, xind, iou_mask, 0:4] = bbox_xywh label[i][yind, xind, iou_mask, 4:5] = 1.0 label[i][yind, xind, iou_mask, 5:] = smooth_onehot $\\\\texttt{label[i]}$ is initialized at the beginning which is of size $$ \\\\texttt{train_output_sizes}\\\\times \\\\texttt{train_output_sizes}\\\\times 3\\\\times 85 $$ for each $\\\\texttt{i}$, where $\\\\texttt{train_output_sizes} = 13,26$ or $52$. python-38 bbox_ind = int(bbox_count[i] % self.max_bbox_per_scale) bboxes_xywh[i][bbox_ind, :4] = bbox_xywh $\\\\texttt{bboxes_xywh}$ is initialized (with zeros) at the beginning, $\\\\texttt{bboxes_xywh.shape} = (3,150,4)$. python-40 bbox_count[i] += 1 exist_positive = True if not exist_positive: best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1) # flatten and take max # best_detect belongs to which \\"i\\", namely, best \\"i\\" best_detect = int(best_anchor_ind / self.anchor_per_scale) # for this i, which index it is: best_anchor = int(best_anchor_ind % self.anchor_per_scale) # get the grid point in our 13x13, 26x26, 52x52 grid: xind, yind = np.floor(bbox_xywh_scaled[best_detect, 0:2]).astype(np.int32) label[best_detect][yind, xind, best_anchor, :] = 0 label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh label[best_detect][yind, xind, best_anchor, 4:5] = 1.0 label[best_detect][yind, xind, best_anchor, 5:] = smooth_onehot bbox_ind = int(bbox_count[best_detect] % self.max_bbox_per_scale) bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh # assign bbox_xywh into the list instead of append, # this is to make sure there are at most 150 boxes within all 3 scales. bbox_count[best_detect] += 1 label_sbbox, label_mbbox, label_lbbox = label sbboxes, mbboxes, lbboxes = bboxes_xywh return label_sbbox, label_mbbox, label_lbbox, sbboxes, mbboxes, lbboxes In short, $$ \\\\texttt{sbboxes, mbboxes, lbboxe} $$ are just ground truth bounding boxes (center, width and height), while $$ \\\\texttt{label_sbbox, label_mbbox, label_lbbox} $$ are ground truth bounding boxes with objectiveness and probabilities of **_each grid_**. $\\\\qed$ #### Model Structure of YOLOv3 In $\\\\texttt{darknet53}$ after a bunch of residue modules we get 3 branches $\\\\texttt{route_1}$, $\\\\texttt{route_2}$ and $\\\\texttt{cov}$, where route_1, route_2, conv = backbone.darknet53(input_layer) in YOLOv3 function. Moreover, $$ \\\\texttt{route_1.shape}= (-1,52,52,256) $$ $$ \\\\texttt{route_2.shape} = (-1,26,26, 512) $$ $$ \\\\texttt{conv.shape} =(-1,13,13,1024) $$ <center> <a href=\\"/assets/tech/042-yolo_structure.png\\"> <img src=\\"/assets/tech/042-yolo_structure.png\\" width=\\"100%\\"/> </a> <p></p> </center> Each branch then jumps into several stages of feature extractions, the whole process finally gives another 3 branches of undecoded/raw data of features, and they are endowed with the meaning of \\"grid-based detection\\" after reshaping into $$ (-1,\\\\texttt{output_size},\\\\texttt{output_size}, 3, 85) $$ dimensional array in decode function (see conv_output in the body). #### yolov3::{ YOLOv3, decode } <a href=\\"https://github.com/machingclee/deep-learning-study/blob/main/2021-02-15-yolo-trials/2021-02-15-YOLOV3-head-detection/core/yolov3.py\\"><i>Source Code</i></a> python-1 def YOLOv3(input_layer): route_1, route_2, conv = backbone.darknet53(input_layer) conv = common.convolutional(conv, (1, 1, 1024, 512)) conv = common.convolutional(conv, (3, 3, 512, 1024)) conv = common.convolutional(conv, (1, 1, 1024, 512)) conv = common.convolutional(conv, (3, 3, 512, 1024)) conv = common.convolutional(conv, (1, 1, 1024, 512)) As padding=\\"same\\" is being used along the chain of conv nets, there is no spatial dimension change. python-9 conv_lobj_branch = common.convolutional(conv, (3, 3, 512, 1024)) conv_lbbox = common.convolutional(conv_lobj_branch, (1, 1, 1024, 3*(NUM_CLASS + 5)), activate=False, bn=False) conv = common.convolutional(conv, (1, 1, 512, 256)) conv = common.upsample(conv) conv = tf.concat([conv, route_2], axis=-1) conv = common.convolutional(conv, (1, 1, 768, 256)) conv = common.convolutional(conv, (3, 3, 256, 512)) conv = common.convolutional(conv, (1, 1, 512, 256)) conv = common.convolutional(conv, (3, 3, 256, 512)) conv = common.convolutional(conv, (1, 1, 512, 256)) conv_mobj_branch = common.convolutional(conv, (3, 3, 256, 512)) conv_mbbox = common.convolutional(conv_mobj_branch, (1, 1, 512, 3*(NUM_CLASS + 5)), activate=False, bn=False) conv = common.convolutional(conv, (1, 1, 256, 128)) conv = common.upsample(conv) conv = tf.concat([conv, route_1], axis=-1) conv = common.convolutional(conv, (1, 1, 384, 128)) conv = common.convolutional(conv, (3, 3, 128, 256)) conv = common.convolutional(conv, (1, 1, 256, 128)) conv = common.convolutional(conv, (3, 3, 128, 256)) conv = common.convolutional(conv, (1, 1, 256, 128)) conv_sobj_branch = common.convolutional(conv, (3, 3, 128, 256)) conv_sbbox = common.convolutional(conv_sobj_branch, (1, 1, 256, 3*(NUM_CLASS +5)), activate=False, bn=False) return [conv_sbbox, conv_mbbox, conv_lbbox] $\\\\qed$ python-1 def decode(conv_output, i=0): \\"\\"\\" return tensor of shape [batch_size, output_size, output_size, anchor_per_scale, 5 + num_classes] contains (x, y, w, h, score, probability) \\"\\"\\" $\\\\texttt{conv_output}$ is the output of $\\\\texttt{YOLOv3}$ (i.e., $\\\\texttt{conv_sbbox}$, $\\\\texttt{conv_mbbox}$ or $\\\\texttt{conv_lbbox}$). python-6 conv_shape = tf.shape(conv_output) batch_size = conv_shape[0] output_size = conv_shape[1] conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS)) conv_raw_dxdy = conv_output[:, :, :, :, 0:2] conv_raw_dwdh = conv_output[:, :, :, :, 2:4] conv_raw_conf = conv_output[:, :, :, :, 4:5] conv_raw_prob = conv_output[:, :, :, :, 5: ] y = tf.tile(tf.range(output_size, dtype=tf.int32)[:, tf.newaxis], [1, output_size]) x = tf.tile(tf.range(output_size, dtype=tf.int32)[tf.newaxis, :], [output_size, 1]) For example, let\'s take $\\\\texttt{output_size}=13$, then $$ \\\\texttt{y = np.tile(np.arange(13)[:, np.newaxis], [1, 13])} $$ and $$ \\\\texttt{x = np.tile(np.arange(13)[np.newaxis, :], [13, 1])} $$ are respectively: none [[ 0 0 0 0 0 0 0 0 0 0 0 0 0] [[ 0 1 2 3 4 5 6 7 8 9 10 11 12] [ 1 1 1 1 1 1 1 1 1 1 1 1 1] [ 0 1 2 3 4 5 6 7 8 9 10 11 12] [ 2 2 2 2 2 2 2 2 2 2 2 2 2] [ 0 1 2 3 4 5 6 7 8 9 10 11 12] [ 3 3 3 3 3 3 3 3 3 3 3 3 3] [ 0 1 2 3 4 5 6 7 8 9 10 11 12] [ 4 4 4 4 4 4 4 4 4 4 4 4 4] [ 0 1 2 3 4 5 6 7 8 9 10 11 12] [ 5 5 5 5 5 5 5 5 5 5 5 5 5] [ 0 1 2 3 4 5 6 7 8 9 10 11 12] [ 6 6 6 6 6 6 6 6 6 6 6 6 6] [ 0 1 2 3 4 5 6 7 8 9 10 11 12] [ 7 7 7 7 7 7 7 7 7 7 7 7 7] [ 0 1 2 3 4 5 6 7 8 9 10 11 12] [ 8 8 8 8 8 8 8 8 8 8 8 8 8] [ 0 1 2 3 4 5 6 7 8 9 10 11 12] [ 9 9 9 9 9 9 9 9 9 9 9 9 9] [ 0 1 2 3 4 5 6 7 8 9 10 11 12] [10 10 10 10 10 10 10 10 10 10 10 10 10] [ 0 1 2 3 4 5 6 7 8 9 10 11 12] [11 11 11 11 11 11 11 11 11 11 11 11 11] [ 0 1 2 3 4 5 6 7 8 9 10 11 12] [12 12 12 12 12 12 12 12 12 12 12 12 12]] [ 0 1 2 3 4 5 6 7 8 9 10 11 12]] For $\\\\texttt{x}$ and $\\\\texttt{y}$ we expand dimension again along the last axis (break every single element into a bracketed element) before concatenation: python-19 xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1) At this point, $\\\\texttt{xy_grid}$ is $(13,13,2)$ dimensional. python-20 xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1]) xy_grid = tf.cast(xy_grid, tf.float32) Now $\\\\texttt{xy_grid}$ is $(\\\\texttt{batch_size}, 13, 13, 3, 2)$ dimensional. Recall that <center> <img src=\\"/assets/tech/042-decode_anchor.png\\" width=\\"360\\"/> </center> python-22 pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i] pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i] pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1) pred_conf = tf.sigmoid(conv_raw_conf) pred_prob = tf.sigmoid(conv_raw_prob) return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1) Bear in mind that decoded $\\\\texttt{x}$, $\\\\texttt{y}$ in $\\\\texttt{pred_xywh}$ denote the center of prediction rectangle, as is the output of the function $\\\\texttt{preprocess_true_boxes}$. $\\\\qed$ #### yolov3::{ compute_loss } <a href=\\"https://github.com/machingclee/deep-learning-study/blob/main/2021-02-15-yolo-trials/2021-02-15-YOLOV3-head-detection/core/yolov3.py\\"><i>Source Code</i></a> python def bbox_giou(boxes1, boxes2): boxes1 = tf.concat([boxes1[..., :2] - boxes1[..., 2:] * 0.5, boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1) boxes2 = tf.concat([boxes2[..., :2] - boxes2[..., 2:] * 0.5, boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1) boxes1 = tf.concat([tf.minimum(boxes1[..., :2], boxes1[..., 2:]), tf.maximum(boxes1[..., :2], boxes1[..., 2:])], axis=-1) boxes2 = tf.concat([tf.minimum(boxes2[..., :2], boxes2[..., 2:]), tf.maximum(boxes2[..., :2], boxes2[..., 2:])], axis=-1) boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1]) boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1]) left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2]) right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:]) inter_section = tf.maximum(right_down - left_up, 0.0) inter_area = inter_section[..., 0] * inter_section[..., 1] union_area = boxes1_area + boxes2_area - inter_area iou = inter_area / union_area enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2]) enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:]) enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0) enclose_area = enclose[..., 0] * enclose[..., 1] giou = iou - 1.0 * (enclose_area - union_area) / enclose_area return giou python-1 def compute_loss(pred, conv, label, bboxes, i=0): conv_shape = tf.shape(conv) batch_size = conv_shape[0] output_size = conv_shape[1] input_size = STRIDES[i] * output_size conv = tf.reshape(conv, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS)) conv_raw_conf = conv[:, :, :, :, 4:5] conv_raw_prob = conv[:, :, :, :, 5:] pred_xywh = pred[:, :, :, :, 0:4] pred_conf = pred[:, :, :, :, 4:5] label_xywh = label[:, :, :, :, 0:4] respond_bbox = label[:, :, :, :, 4:5] # objectiveness label_prob = label[:, :, :, :, 5:] giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1) input_size = tf.cast(input_size, tf.float32) bbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2) giou_loss = respond_bbox * bbox_loss_scale * (1 - giou) Note that for two sets $U,V\\\\in \\\\mathcal C$, where $\\\\mathcal C\\\\in 2^{\\\\mathbb R^2}$, the function $d(U,V) := 1-\\\\text{giou}(U,V)$ defines a metric, so $\\\\texttt{giou_loss}$ makes sense. python-24 iou = bbox_iou(pred_xywh[:, :, :, :, np.newaxis, :], bboxes[:, np.newaxis, np.newaxis, np.newaxis, :, :]) $\\\\texttt{bboxes}$ are batched inside Dataset(\\"train\\").__next__ before passing into $\\\\texttt{compute_loss}$ (in a while loop until image count reaches batch size). Therefore $\\\\texttt{bboxes.shape} = (16, 150, 4)$, where $150$ is the maximal number of anchors (most of them are zeros due to initialization), so we see 3 :\'s in $\\\\texttt{bboxes}$. Finally $$ \\\\texttt{pred_xywh.shape}=(16, 13,13,3,150,4)=\\\\texttt{bboxes.shape} $$ and $$ \\\\texttt{iou.shape} = (16,13,13,3,150) $$ where computation gets rid of the last dimension. $\\\\texttt{bboxes}$ is copied to every grid for computation because from the original paper of YOLOv3: > \\"the confidence prediction represents the IOU between the predicted box and any ground truth box\\" python-25 max_iou = tf.expand_dims(tf.reduce_max(iou, axis=-1), axis=-1) respond_bgd = (1.0 - respond_bbox) * tf.cast( max_iou < IOU_LOSS_THRESH, tf.float32 ) In the internet some people call $\\\\texttt{IOU_LOSS_THRESH}$ as $\\\\texttt{ignore_thresh}$. $\\\\texttt{respond_bgd}$ determines whether to penalize a prediction - that overlaps too few with ground truth anchors (i.e., detected wrong location) **_and_** - that makes false positive error. python-27 conf_focal = tf.pow(respond_bbox - pred_conf, 2) The concept of focal loss with $\\\\gamma = 2$ was introduced in **[TY]**, which down-weights the loss contributed by well-classificed (high confidence) examples. python-28 conf_loss = conf_focal * ( respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf) + respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf) ) Where $\\\\texttt{tf.nn.sigmoid_cross_entropy_with_logits(labels=z, logits=x)}$ is $$ \\\\texttt{z} * -\\\\log(\\\\texttt{sigmoid}(\\\\texttt{x})) + (1 - \\\\texttt{z}) * -\\\\log(1 - \\\\texttt{sigmoid}(\\\\texttt{x})), $$ therefore $\\\\texttt{x}$ has to be a raw prediction data (i.e., without signmoid). python-34 prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob) giou_loss = tf.reduce_mean(tf.reduce_sum(giou_loss, axis=[1,2,3,4])) conf_loss = tf.reduce_mean(tf.reduce_sum(conf_loss, axis=[1,2,3,4])) prob_loss = tf.reduce_mean(tf.reduce_sum(prob_loss, axis=[1,2,3,4])) return giou_loss, conf_loss, prob_loss $\\\\qed$ #### Training <a href=\\"https://github.com/machingclee/deep-learning-study/blob/main/2021-02-15-yolo-trials/2021-02-15-YOLOV3-head-detection/train.py\\"><i>Source Code</i></a> ###### Recap of Customized Training Loop with tf.GradientTape Apart from predefined loss functions (such as $\\\\texttt{categorical_crossentropy}$ for classfication, $\\\\texttt{mse}$ for regression, etc), it is ocassional to come across non-standard loss functions from other repository with the use of $\\\\texttt{tf.GradientTape}$. Such implementation usually involves 4 components: - **Component 1.** The model **architecture** - **Component 2.** The **loss function** used when computing the model loss - **Component 3.** The **optimizer** used to update the model weights - **Component 4.** The **step function** that encapsulates the forward and backward pass of the network Now the code below is self-explanatory: ###### Start Training python def train_step(image_data, target, epoch): # image_data = batch of images with tf.GradientTape() as tape: pred_result = model(image_data, training=True) giou_loss = conf_loss = prob_loss = 0 # optimizing process for i in range(3): conv, pred = pred_result[i*2], pred_result[i*2+1] batch_label, batch_bboxes = target[i] loss_items = compute_loss(pred, conv, batch_label, batch_bboxes, i) giou_loss += loss_items[0] conf_loss += loss_items[1] prob_loss += loss_items[2] total_loss = giou_loss + conf_loss + prob_loss gradients = tape.gradient(total_loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # update learning rate global_steps.assign_add(1) if global_steps < warmup_steps: lr = global_steps / warmup_steps * cfg.TRAIN.LR_INIT else: lr = cfg.TRAIN.LR_END + 0.5 * (cfg.TRAIN.LR_INIT - cfg.TRAIN.LR_END) * ( (1 + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi)) ) optimizer.lr.assign(lr.numpy()) # writing summary data with writer.as_default(): tf.summary.scalar(\\"lr\\", optimizer.lr, step=global_steps) tf.summary.scalar(\\"loss/total_loss\\", total_loss, step=global_steps) tf.summary.scalar(\\"loss/giou_loss\\", giou_loss, step=global_steps) tf.summary.scalar(\\"loss/conf_loss\\", conf_loss, step=global_steps) tf.summary.scalar(\\"loss/prob_loss\\", prob_loss, step=global_steps) writer.flush() for epoch in range(cfg.TRAIN.EPOCHS): for index, (image_data, target) in enumerate(trainset): train_step(image_data, target, epoch) model.save_weights(\\"./checkpoints/yolov3-{}-{}.h5\\".format(cfg.WEIGHT_NAME_TO_SAVE, epoch)) #### Reference - YOLOv3 \u6e90\u7801\u89e3\u6790 1-5, https://blog.csdn.net/sxlsxl119/article/details/103028021 - YOLOv3 \u7b97\u6cd5\u7684\u4e00\u70b9\u7406\u89e3, https://yunyang1994.gitee.io/2018/12/28/YOLOv3/ - Joseph Redmon, Ali Farhadi, <a href=\\"https://arxiv.org/abs/1804.02767\\"> <i>YOLOv3: An Incremental Improvement </i></a> - **[TY]** Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He and Piotr Dollar, <a href=\\"https://arxiv.org/pdf/1708.02002.pdf?fbclid=IwAR38T65chV0UNPhBDbAExH021_afC0L6o9PEztpBPBAzqY3dBR8vOGy2qwg\\"><i>Focal Loss for Dense Object Detection</i></a> - Adrian Rosebrock, <a href=\\"https://www.pyimagesearch.com/2020/03/23/using-tensorflow-and-gradienttape-to-train-a-keras-model/\\"><i>Using TensorFlow and GradientTape to train a Keras model</i></a> ","title":"YOLOv3 Deep Dive","date":"2022-04-22T00:00:00.000Z","id":"blog065","tag":"deep-learning","intro":"I made this study note on 5th of March, 2021: <a href=\'https://github.com/machingclee/deep-learning-study/blob/main/2021-02-28-yolov3-algorithm-drilling/YOLOv3StudyNotes.pdf\'><i>2021-02-28-yolov3-algorithm-drilling</i></a> in pdf format. For a refresh of memory I reformat the content into an md file for this blog for easy reading and reference."},{"content":" #### Results After the implementation and the results I was amazed by the horrible performance of WGAN-GP compared to that of WGAN. Although mnist is a toy dataset, but among all the GANs I have implemented none of them can converge that quickly even without tuning parameters for many times. ##### By WGAN Using 4000 Batches <center> <img src=\\"/assets/tech/043-01.png\\" width=\\"170\\"/> <img src=\\"/assets/tech/043-02.png\\" width=\\"170\\"/> <img src=\\"/assets/tech/043-03.png\\" width=\\"170\\"/> <img src=\\"/assets/tech/043-04.png\\" width=\\"170\\"/> <img src=\\"/assets/tech/043-05.png\\" width=\\"170\\"/> <img src=\\"/assets/tech/043-06.png\\" width=\\"170\\"/> <img src=\\"/assets/tech/043-07.png\\" width=\\"170\\"/> <img src=\\"/assets/tech/043-09.png\\" width=\\"170\\"/> </center> ##### By WGAN-GP Using less than 1000 Batches <center> <img src=\\"/assets/tech/046-01.png\\" width=\\"170\\"/> <img src=\\"/assets/tech/046-02.png\\" width=\\"170\\"/> <img src=\\"/assets/tech/046-03.png\\" width=\\"170\\"/> </center> #### Wasserstein Metric ##### Definitions, Basic Examples and some Mathematics Given the real distribution $P_r$ of real data, we try to approximate it by a distribution $P_\\\\theta$, such that $g_\\\\theta(Z)\\\\sim P_\\\\theta $ for some network $g_\\\\theta$ with $Z$ being an unknown random variable (usually this is constructed from Gaussian distribution $\\\\mathcal N(0,1)$). From **[MA]** the **_Wasserstein metric_** behaves the best from experiment and theoretical observations: $$ W(P_r, P_\\\\theta) := \\\\inf_{\\\\gamma\\\\in \\\\Pi (P_r, P_\\\\theta) } \\\\mathbb E(\\\\|x-y\\\\|), $$ where for any probability measures $\\\\mu, \\\\nu $ on $X$, $\\\\Pi (\\\\mu, \\\\nu)$ denotes the set of all **_transportation plans_** form $\\\\mu$ to $\\\\nu$ such that $\\\\pi^1\\\\# \\\\gamma = P_r$ and $\\\\pi^2\\\\#\\\\gamma = P_\\\\theta$. Here $\\\\pi^i:X\\\\times X\\\\to X$\'s are canonical projections. By an abuse of notation, for any probability measure $\\\\rho$ that is absolutely continuous w.r.t. Lebesuge measure $dx$ on $\\\\mathbb R^n$ we refer $\\\\rho$ also the density function $\\\\rho\\\\in L^1(\\\\mathbb R^n)$ and write $\\\\rho = \\\\rho\\\\,dx$ interchangeably, therefore $P_r=P_r\\\\,dx$ and $P_\\\\theta = P_\\\\theta\\\\,dx$ in our case, and thus the notation $W(P_r, P_\\\\theta)$ makes sense. The following example is taken from **[MA]**, let\'s fill in the detail: <example> **Example.** Let $Z\\\\sim U[0,1]$ be a random variable sampled from uniform distribution on $[0,1]$, the sets $X_0 := \\\\{0\\\\} \\\\times [0,1]$ and $X_\\\\theta := \\\\{\\\\theta\\\\} \\\\times[0,1]$ form a disjoint subsets in $\\\\mathbb R^2$. Consider $X = X_0\\\\sqcup X_\\\\theta$, denote $P_0$ the distribution of $(0, Z)$ in $X$ and $P_\\\\theta$ that of $(\\\\theta, Z)$ in $X$, then $P_0$ and $P_\\\\theta$ obviously have no nonempty common support. <center> <img src=\\"/assets/tech/045.png\\"/> </center> <p><p/> We consider the following metrics among these two distributions: - $W(P_0,P_\\\\theta) = |\\\\theta|$ **Reason.** $W(P_0,P_\\\\theta)\\\\ge |\\\\theta|$ is obvious, for equality define the map $t:(0,x)\\\\mapsto (\\\\theta, x)$ on $X_0$ and $(\\\\theta,x)\\\\mapsto (\\\\theta,x)$ on $X_\\\\theta$, then $t:X\\\\to X$ satisfies $t\\\\# P_0 = P_\\\\theta$. From standard result in optimal transport, the map $(\\\\mathrm{id}, t):X\\\\to X\\\\times X$ induces a transportation plan $\\\\gamma := t\\\\# P_0\\\\in \\\\Pi(P_0, P_\\\\theta)$ which is a measure on $X\\\\times X$. Denote $d(x,y)=\\\\|x-y\\\\|$, we have $$ \\\\begin{aligned} \\\\int_{X\\\\times X} d(x,y)\\\\, d\\\\gamma(x,y) &= \\\\int_{X\\\\times X}\\\\,d(x,y) \\\\,d \\\\big((\\\\mathrm{id}, t)\\\\# P_0\\\\big)(x,y)\\\\\\\\ &= \\\\int_X d\\\\big((\\\\mathrm{id},t) (x)\\\\big)) \\\\,d P_0(x)\\\\\\\\ &= \\\\int_X d(x, t(x))\\\\,dP_0(x)\\\\\\\\ &= \\\\int_X |\\\\theta| \\\\,dP_0(x) \\\\\\\\ &= |\\\\theta|, \\\\end{aligned} $$ therefore $W(P_0,P_\\\\theta)=|\\\\theta|$. - $JS(P_0,P_\\\\theta) = \\\\frac{1}{2}(KL(P_0\\\\|P_m) + KL(P_\\\\theta\\\\|P_m)) = \\\\log 2$, where $P_m = \\\\frac{P_0+P_\\\\theta}{2}$ **Reason.** $KL(P_0\\\\|P_m)=\\\\int_{X} \\\\log (\\\\frac{P_0}{P_m}) P_0\\\\,dx$, since $X=X_0\\\\sqcup X_\\\\theta$ on which $P_0$ and $P_\\\\theta$ have no common support, therefore $$ KL(P_0\\\\|P_m) = \\\\int_{X_0} \\\\log \\\\frac{P_0}{\\\\frac{P_0 + 0}{2}}P_0\\\\,dx = \\\\log 2\\\\int_{X_0} P_0(x)\\\\,dx = \\\\log 2. $$ Similarly, $KL(P_\\\\theta\\\\|P_m) = \\\\log 2$, and the result follows. - $KL(P_0\\\\|P_\\\\theta) = KL(P_\\\\theta\\\\|P_0) = \\\\infty$ **Reason.** Demoninator 0 on a set of positive measure. </example> ##### The Usual form of Wasserstein Metric for Coding A remarkable result named **_Kantorovich-Rubinstein duality_** from optimal transport states that $$ W(P_r,P_\\\\theta) = \\\\sup_{\\\\|f\\\\|_L \\\\leq 1} (\\\\mathbb E_{x\\\\sim P_r}[f(x)] - \\\\mathbb E_{x\\\\sim P_\\\\theta}[f(x)]) $$ Since $g_\\\\theta(Z) \\\\sim P_\\\\theta$, we can further write $$ W(P_r,P_\\\\theta) = \\\\sup_{\\\\|f\\\\|_L \\\\leq 1} (\\\\mathbb E_{x\\\\sim P_r}[f(x)] - \\\\mathbb E_{z\\\\sim p(z)}[f(g_\\\\theta(z))]), $$ where we assume that $Z\\\\sim p$, for some density $p\\\\in L^1(\\\\mathbb R^{\\\\texttt{z_dim}})$ with $\\\\texttt{z_dim}$ being the latent dimension which we need to search by experiment (rigorously the **_equality holds_** by a standard real analysis trick: prove it for first characteristic functions, second simple functions, finally use monotone convergence etc). ###### WGAN Version In coding we transform the last equality into an approximated form in $(*)$ $$ \\\\begin{align*} W(P_r,P_\\\\theta) &\\\\approx \\\\sup_{w} \\\\bigg(\\\\,\\\\,\\\\,\\\\mathbb E_{x\\\\sim P_r}[ f_w(\\\\!\\\\!\\\\!\\\\!\\\\underbrace{x}_{\\\\text{real sample}}\\\\!\\\\!\\\\!\\\\!)] - \\\\mathbb E_{z\\\\sim p(z)}[f_w(\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\underbrace{g_\\\\theta(z)}_{\\\\text{generated sample}}\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!) ]\\\\quad \\\\bigg) \\\\tag*{$(*)$}\\\\\\\\ &=- \\\\inf_{w} \\\\bigg(\\\\mathbb E_{z\\\\sim p(z)}[f_w(g_\\\\theta(z))] - \\\\mathbb E_{x\\\\sim P_r}[f_w(x)]\\\\bigg). \\\\tag*{$(**)$} \\\\end{align*} $$ We will be replacing the set of 1-Lipschitz functions $f$ by a parametrized family $\\\\{f_w\\\\}$ and perform the following in train loop: <center> <a href=\\"/assets/tech/044.png\\"> <img src=\\"/assets/tech/044.png\\" width=\\"600\\"/> </a> </center> <p></p> - **Line 6 in Algorithm.** - We minimize the term $$ \\\\mathcal L_c:=\\\\mathbb E_{z\\\\sim p(z)}[f_w(g_\\\\theta(z))] - \\\\mathbb E_{x\\\\sim P_r}[f_w(x)] $$ in the RHS of $(**)$ w.r.t. $w = \\\\texttt{critic.trainable_variables}$ to get a result that is hopefully close to $W(P_r,P_\\\\theta)$. <br/> - We update critic by $\\\\partial \\\\mathcal L_c/\\\\partial w$. - **Line 11 in Algorithm.** - We minimize $$ \\\\mathcal L_g := -\\\\mathbb E_{z\\\\sim p(z)}[f_w(g_\\\\theta(z))] $$ in the RHS of $(*)$ w.r.t. $\\\\theta=\\\\texttt{gen.trainable_variables}$ to get smaller $W(P_r,P_\\\\theta)$. - We update generator by $\\\\partial \\\\mathcal L_g/\\\\partial \\\\theta$. - Usually the candidates of $\\\\{f_w\\\\}$ are modified from our discriminator. - (cont\'d) As we will not use signmoid output any more, usually we call $f_w$ a **_critic_** which replaces the role played by the usual $\\\\log D$ trick. Then we can modify our discriminator to output any tensor of shape $(\\\\texttt{batch_size}, 1)$. ###### WGAN-GP Version As discussed in **[IG]** the weight clipping causes our critics to learn very simple functions, therefore we remove the constraint on the norm of $\\\\nabla_x f_w(x)$ for $x\\\\sim P_r$ and force the norms to be bounded by introducing a loss term in the following: $$ \\\\mathcal L = \\\\underbrace{ \\\\mathbb E_{z\\\\sim p(z)}[f_w(g_\\\\theta(z))] - \\\\mathbb E_{x\\\\sim P_r}[f_w(x)]}_{\\\\text{original critic loss}} + \\\\lambda\\\\times \\\\underbrace{\\\\mathbb E_{\\\\hat x \\\\sim P_{\\\\hat x}} \\\\big[(\\\\|\\\\nabla f_w(\\\\hat x)\\\\|_2 - 1)^2\\\\big]}_{\\\\text{gradient penality}}. $$ Where $P_{\\\\hat x}$ denotes the uniform distribution of points along straight lines between any pair of points from $P_r$ and $P_\\\\theta$, - The first term is the old critic term that we need to minimize in order to approximate the Wasserstein distance. - The second term will be mininized w.r.t. $w$ to control the growth of the gradients of $\\\\{f_w\\\\}$. <center> <img src=\\"/assets/tech/047.png\\" width=\\"640\\"/> </center> <p></p> - **Line 7 in Algorithm.** We minimize $$ \\\\mathcal L_c := \\\\mathbb E_{x\\\\sim P_\\\\theta}[D_w(x)] - \\\\mathbb E_{x\\\\sim P_r}[D_w(x)] + \\\\lambda \\\\cdot \\\\mathbb E_{\\\\hat x \\\\sim P_{\\\\hat x}} \\\\big[(\\\\|\\\\nabla_x D_w(\\\\hat x)\\\\|_2 - 1)^2\\\\big] $$ w.r.t. $w = \\\\texttt{critic.trainable_variables}$, where $D_w$ denotes critic, we will take $\\\\lambda = 10$. - (cont\'d) We update the critic by $\\\\partial \\\\mathcal L_c/\\\\partial w$. - **Line 12 in Algorithm.** Update of generator remains the same, we still minimize $$ \\\\mathcal L_g:=-\\\\mathbb E_{z\\\\sim p(z)}[D_w(g_\\\\theta(z))] $$ w.r.t. $\\\\theta = \\\\texttt{gen.trainable_variables}$. - (cont\'d) We update the generator by $\\\\partial \\\\mathcal L_g/\\\\partial \\\\theta$. - No BatchNormalization in critic, instead we use LayerNormalization. - Remove weight-clipping. - We use smaller learning rate. - Anything else remain the same. #### Implementation of WGAN ##### Constants With exactly the same setup as in <a href=\\"/blog/article/GAN-and-DCGAN-in-Tensorflow\\"><i>DCGAN</i></a>, we edit the following constants: python img_rows = 28 img_cols = 28 channels = 1 weight_clip = 0.01 batch_size = 64 critic_iteration = 5 img_shape = (img_rows, img_cols, channels) learning_rate = 1e-5 z_dim = 128 ##### Critic We have the same generator, but different discriminator, which we call critic as it no longer output a number in $[0,1]$: python gen = build_generator() critic = build_critic() def build_critic(): model = Sequential() model.add( Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\'same\') ) model.add(LeakyReLU(alpha=0.01)) model.add( Conv2D(64, kernel_size=3, strides=2, input_shape=img_shape, padding=\'same\')) model.add(BatchNormalization()) model.add(LeakyReLU(alpha=0.01)) model.add( Conv2D(128, kernel_size=3, strides=2, input_shape=img_shape, padding=\'same\')) model.add(BatchNormalization()) model.add(LeakyReLU(alpha=0.01)) model.add(Flatten()) model.add(Dense(1)) return model ##### Train Loop with Wasserstein Metric in Place of the Adversarial Loss python def train(iterations, batch_size, sample_interval): (x_train, _), (_, _) = mnist.load_data() x_train = x_train/127.5 - 1.0 # np.shape(x_train) = (60000, 28, 28), for conv2D we need the channel dimension at the last axis x_train = np.expand_dims(x_train, axis=3) gen_opt = RMSprop(lr=learning_rate) critic_opt = RMSprop(lr=learning_rate) for i in range(iterations): print(f\\"iteration: {i+1}\\", end = \\"\\\\r\\") for j in range(critic_iteration): z = tf.random.normal((batch_size, z_dim), 0, 1) update_gen = ((j+1) % (critic_iteration)) == 0 with tf.GradientTape() as critic_tape, tf.GradientTape() as gen_tape: idxs = np.random.randint(0, x_train.shape[0], batch_size) imgs = x_train[idxs] gen_imgs = gen(z, training=True) critic_fake = critic(gen_imgs) critic_real = critic(imgs) loss_critic = -(tf.math.reduce_mean(critic_real) - tf.math.reduce_mean(critic_fake)) if update_gen: loss_gen = - tf.math.reduce_mean(critic_fake) grad_of_critic = critic_tape.gradient(loss_critic, critic.trainable_variables) critic_opt.apply_gradients(zip(grad_of_critic, critic.trainable_variables)) weights = critic.get_weights() weights = [tf.clip_by_value(w, -weight_clip, weight_clip) for w in weights] critic.set_weights(weights) if update_gen: grad_of_gen = gen_tape.gradient(loss_gen, gen.trainable_variables) gen_opt.apply_gradients(zip(grad_of_gen, gen.trainable_variables)) if (i+1) % sample_interval == 0: iteration_checkpoints.append(i+1) sample_images(gen) Finally: python iterations = 20000 sample_interval = 100 train(iterations, batch_size, sample_interval) #### Implementation of WGAN-GP There are just few modifications from WGAN, first for the constants: ##### New Constants python img_rows = 28 img_cols = 28 channels = 1 batch_size = 64 critic_iteration = 5 img_shape = (img_rows, img_cols, channels) learning_rate = 1e-4 z_dim = 128 We just decrease learning_rate and remove critic_iteration. ##### New Critic We replace all BatchNormalization by LayerNormalization: python def build_critic(): model = Sequential() model.add( Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\'same\') ) model.add(LeakyReLU(alpha=0.01)) model.add( Conv2D(64, kernel_size=3, strides=2, input_shape=img_shape, padding=\'same\')) model.add(LayerNormalization()) model.add(LeakyReLU(alpha=0.01)) model.add( Conv2D(128, kernel_size=3, strides=2, input_shape=img_shape, padding=\'same\')) model.add(LayerNormalization()) model.add(LeakyReLU(alpha=0.01)) model.add(Flatten()) model.add(Dense(1)) return model ##### Gradient Penality Next we define function to compute gradient penality: python def gradient_penality(critic, real_sample, fake_sample): epsilon = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0) interpolated = epsilon * real_sample + (1 - epsilon) * fake_sample with tf.GradientTape() as gp_tape: gp_tape.watch(interpolated) critic_inter = critic(interpolated, training=True) grads = gp_tape.gradient(critic_inter, [interpolated])[0] norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3])) penality = tf.reduce_mean((norm - 1.0) ** 2) return penality ##### New Train Loop We remove weight-clipping and modify the weight update of critic by adding gradient penality: python def train(iterations, batch_size, sample_interval): (x_train, _), (_, _) = mnist.load_data() x_train = x_train/127.5 - 1.0 # np.shape(x_train) = (60000, 28, 28), for conv2D we need the channel dimension at the last axis x_train = np.expand_dims(x_train, axis=3) gen_opt = RMSprop(lr=learning_rate) critic_opt = RMSprop(lr=learning_rate) for i in range(iterations): print(f\\"iteration: {i+1}\\", end = \\"\\\\r\\") for j in range(critic_iteration): z = tf.random.normal((batch_size, z_dim), 0, 1) update_gen = ((j+1) % (critic_iteration)) == 0 with tf.GradientTape() as critic_tape, tf.GradientTape() as gen_tape: idxs = np.random.randint(0, x_train.shape[0], batch_size) imgs = x_train[idxs] gen_imgs = gen(z, training=True) critic_fake = critic(gen_imgs, training=True) critic_real = critic(imgs, training=True) gp = gradient_penality(critic, imgs, gen_imgs) loss_critic = tf.math.reduce_mean(critic_fake) \\\\ - tf.math.reduce_mean(critic_real) + 10 * gp if update_gen: loss_gen = - tf.math.reduce_mean(critic_fake) grad_of_critic = critic_tape.gradient(loss_critic, critic.trainable_variables) critic_opt.apply_gradients(zip(grad_of_critic, critic.trainable_variables)) if update_gen: grad_of_gen = gen_tape.gradient(loss_gen, gen.trainable_variables) gen_opt.apply_gradients(zip(grad_of_gen, gen.trainable_variables)) if (i+1) % sample_interval == 0: iteration_checkpoints.append(i+1) # print(\\"%d [D loss: %f] [G loss: %f]\\" % (i + 1, loss_critic, loss_gen)) sample_images(gen) #### Reference - **[MA]** Martin Arjovsky, Soumith Chintala, and Leon Bottou, <a href=\\"https://arxiv.org/pdf/1701.07875.pdf\\"><i>Wasserstein GAN</i></a> - **[IG]** Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville, <a href=\\"https://arxiv.org/pdf/1704.00028.pdf\\"><i>Improved Training of Wasserstein GANs</i></a> ","title":"WGAN and WGAN-GP","date":"2022-04-22T00:00:00.000Z","id":"blog066","tag":"deep-learning","intro":"We modify from DCGAN from <a href=\\"/blog/article/GAN-and-DCGAN-in-Tensorflow\\"><i>this post</i></a> to experiment on WGAN by weight-clipping, then we try to experiment WGAN-GP using gradient penality."},{"content":" Traditonal $\\\\LaTeX$ software compiles our .tex file into .div, .log, .out, aux, etc, files along with our target file .pdf, if we were to generate several separated reports it will be an headache seeing all these files in the same directory. With pandoc + markdown, it results in only one single pdf file. Most importantly, everyone can change the md file and preview the change in the previewer without converting it into a pdf. To enable this feature, install vscode-pandoc extension in vscode, install pandoc as in the instruction, then we can try to compile the following! text --- documentclass: article fontsize: 11pt header-includes: | \\\\usepackage{amsmath} \\\\usepackage{amsfonts} \\\\usepackage{amssymb} \\\\usepackage{color} \\\\usepackage{graphicx} \\\\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry} \\\\usepackage{hyperref} \\\\usepackage{array} \\\\usepackage{enumitem} \\\\usepackage{graphbox} \\\\usepackage{float} \\\\usepackage{multicol} \\\\thispagestyle{empty} \\\\usepackage[htt]{hyphenat} \\\\usepackage{fancyvrb} \\\\RecustomVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\\\\footnotesize} --- # I am James! $\\\\int_a^b f(x)\\\\,dx$. Finally by pressing F1 -> Pandoc Render -> pdf, we are done! A single .pdf file will be generated in the same directory. In case we want a beamer presentation: text --- documentclass: beamer fontsize: 12pt header-includes: | \\\\usetheme{Boadilla} \\\\usepackage{amsmath} \\\\usepackage{amsfonts} \\\\usepackage{listings} \\\\usepackage{amssymb} \\\\usepackage{color} \\\\usepackage{graphicx} \\\\usepackage{hyperref} \\\\usepackage{array} \\\\usepackage{enumitem} \\\\usepackage{graphbox} \\\\usepackage{float} \\\\usepackage{multicol} \\\\usepackage[htt]{hyphenat} \\\\usepackage{pifont} \\\\usepackage{fancyvrb} \\\\RecustomVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\\\\footnotesize} --- with beamer tutorial: https://www.overleaf.com/learn/latex/Beamer ","title":"Pandoc","date":"2022-04-29T00:00:00.000Z","id":"blog067","tag":"latex","intro":"Write latex document using markdown in vscode.","toc":false},{"content":" #### Why do we want First Principal Component? Suppose that the data $x_1,x_2,\\\\dots,x_n$ with $d$ features are represented as a row vector, denote the matrix $$ \\\\hat B := \\\\begin{bmatrix} -\\\\!\\\\!\\\\!-x_1-\\\\!\\\\!\\\\!-\\\\\\\\ -\\\\!\\\\!\\\\!-x_2-\\\\!\\\\!\\\\!-\\\\\\\\ \\\\vdots\\\\\\\\ -\\\\!\\\\!\\\\!-x_n -\\\\!\\\\!\\\\!- \\\\end{bmatrix} - \\\\begin{bmatrix} 1\\\\\\\\ 1 \\\\\\\\ \\\\vdots \\\\\\\\ 1\\\\end{bmatrix} \\\\underbrace{\\\\frac{1}{n}\\\\sum_{i=1}^n \\\\begin{bmatrix} -\\\\!\\\\!\\\\!-x_i-\\\\!\\\\!\\\\!- \\\\end{bmatrix}}_{=:\\\\mu} = \\\\begin{bmatrix} -\\\\!\\\\!\\\\!-(x_1 - \\\\mu)-\\\\!\\\\!\\\\!-\\\\\\\\ -\\\\!\\\\!\\\\!-(x_2 - \\\\mu)-\\\\!\\\\!\\\\!-\\\\\\\\ \\\\vdots\\\\\\\\ -\\\\!\\\\!\\\\!-(x_n - \\\\mu)-\\\\!\\\\!\\\\!- \\\\end{bmatrix}. $$ For every unit vector $\\\\hat n\\\\in \\\\mathbb R^d$, it is easy to see that $\\\\hat B \\\\hat n$ represents all the magnitude of $x_i-\\\\mu$ projected onto $\\\\mathrm{span} (\\\\hat n)$. Next, $$ \\\\|\\\\hat B \\\\hat n\\\\|_2 = \\\\left\\\\| \\\\begin{bmatrix} \\\\langle x_1 - \\\\mu, \\\\hat n \\\\rangle \\\\\\\\ \\\\langle x_2 - \\\\mu, \\\\hat n \\\\rangle\\\\\\\\ \\\\vdots\\\\\\\\ \\\\langle x_n - \\\\mu, \\\\hat n \\\\rangle \\\\end{bmatrix} \\\\right \\\\|_2 =\\\\sqrt{\\\\sum_{i=1}^n \\\\langle x_i - \\\\mu, \\\\hat n \\\\rangle^2} $$ is the standard deviation of the set of values $\\\\{\\\\langle x_i-\\\\mu, \\\\hat n \\\\rangle\\\\}_{i=1}^n$ with mean $0$. Our target is now to find a direction $\\\\hat n $ so that the standard deviation is ***as huge as possible***, therefore along that direction our data $\\\\{x_1-\\\\mu,\\\\dots,x_n-\\\\mu\\\\}$ will be much more comparable and hopefully we can find clustering values for which the data can be grouped together. Finding such a direction is the same as finding $v\\\\in \\\\mathbb R^d$ with $\\\\|v\\\\|_2=1$ such that $$ \\\\sigma = \\\\max_{x\\\\in \\\\mathbb R\\\\setminus\\\\{0\\\\}} \\\\frac{\\\\|\\\\hat Bx\\\\|_2}{\\\\|x\\\\|_2} =\\\\| \\\\hat B v\\\\|_2. $$ We call $v$ the **_first principal component_**. > **Conclusion.** **_First Principal Component_** is a unit vector in $v\\\\in \\\\mathbb R^d$ that maximizes the standard deviation of $\\\\{\\\\langle x_i-\\\\mu, v\\\\rangle\\\\}_{i=1}^n$, where $\\\\mu = \\\\frac{1}{n}\\\\sum_{i=1}^n x_i$. #### In Terms of Singular Value Decomposition $v$ is in fact the right singular vector corresponding to the largest singular value. The **_$k$-th principal component_** is accordingly the $k$-th right singular vector corresponding to the $k$-th largest singular value. Note that by constructon $v_{k+1} \\\\in (\\\\mathrm{span} \\\\{v_1,v_2,\\\\dots,v_k\\\\})^\\\\perp$. ","title":"Principal Component Analysis","date":"2022-04-30T00:00:00.000Z","id":"blog069","tag":"math","intro":"Before I forget, record my basic understanding on what is Principal Component Analysis and how it works."},{"content":" **Convention.** $\\\\mathcal O$ denotes a zero matrix of appropriate size. > **Theorem (Singular Value Decomposition).** > > 1. Every matrix $A\\\\in \\\\mathbb C^{m\\\\times n}$ has a SVD: > > $$ > A=U\\\\Sigma V^* > $$ > > where $U\\\\in \\\\mathbb C^{m\\\\times m}, V\\\\in \\\\mathbb C^{n\\\\times n}$ are unitary and $\\\\Sigma\\\\in \\\\mathbb R^{m\\\\times n}$ is \\"diagonal\\". > > Furthermore, The singular values $\\\\sigma_j$\'s > > $$ > \\\\sigma_1\\\\ge\\\\sigma_2\\\\ge\\\\dots\\\\ge \\\\sigma_{\\\\min\\\\{m,n\\\\}}\\\\ge 0 > $$ > > are **_uniquely determined_**. > > 2. If $A$ is **_square_** and $\\\\sigma_j$\'s are **_distinct_**, the left and right singular vectors $\\\\{u_j\\\\}$ and $\\\\{v_j\\\\}$ are **_unique up to a multiplicative constant with modulus 1_**. <center></center> **Remark.** The technique in the proof below remains valid when all $\\\\mathbb C$ is replaced by $\\\\mathbb R$. <proof> **Proof of Part (1).** The case that $m=1$ or $n=1$ is simple, let\'s assume $m,n\\\\ge 2$. Let $\\\\sigma_1=\\\\|A\\\\|_2$, then due to compactness of $\\\\{x\\\\in \\\\mathbb C^n:\\\\|x\\\\|_2=1\\\\}$ in $\\\\mathbb C^n$ and the continuity of the map $x\\\\mapsto \\\\|Ax\\\\|_2$, there must be $v_1\\\\in \\\\mathbb C^n$ with $\\\\|v_1\\\\|_2=1$ s.t. $\\\\|Av_1\\\\|_2=\\\\sigma_1$, so there is $u_1\\\\in \\\\mathbb C^m$, $\\\\|u_1\\\\|_2=1$, $Av_1=\\\\sigma_1 u_1$. Hence $\\\\|A\\\\|_2$ is our first singular value. Extend $u_1$ to an o.n. basis $\\\\{u_1,\\\\dots,u_m\\\\}$ of $\\\\mathbb C^m$ and $v_1$ to an o.n. basis $\\\\{v_1,\\\\dots,v_n\\\\}$ of $\\\\mathbb C^n$. Let $U_1$ be the matrix with columns $u_i$ and $V_1$ be that with columns $v_i$, then (see remark next page for the explanation of $B$) $$ \\\\begin{align} U_1^* AV_1 &=[A]_{(v_1,\\\\dots,v_n)}^{(u_1,\\\\dots,u_m)} \\\\nonumber \\\\\\\\ &= \\\\begin{array}{c c} & \\\\begin{array} {@{} c c c c @{}} Av_1 \\\\hspace{1.5cm}& Av_2 \\\\hspace{1.6cm} & \\\\cdots &\\\\hspace{0.6cm} Av_n \\\\hspace{0.6cm} \\\\end{array} \\\\\\\\ \\\\begin{array}{c} u_1 \\\\\\\\[0.6cm] u_2 \\\\\\\\ \\\\vdots \\\\\\\\ u_m \\\\end{array}\\\\hspace{-1em} & \\\\left( \\\\begin{array}{@{} c | c c c @{}} \\\\sigma_1 & & \\\\hspace{1.5cm} w^*\\\\text{(to be proved $\\\\mathcal O$)}& \\\\\\\\ \\\\hline & & & \\\\\\\\ & & & & \\\\\\\\ \\\\mathcal O & & \\\\hspace{1.5cm} B:=[A]_{(v_2,\\\\dots,v_n)}^{\\\\normalsize(u_2,\\\\dots,u_m)} & \\\\\\\\ \\\\hspace{1.5cm} & & && \\\\end{array} \\\\right) \\\\\\\\ \\\\mbox{} % Blank line to match column names so as to align the = vertically \\\\end{array} =:S.\\\\tag*{(1)} \\\\\\\\ \\\\\\\\[-12pt] \\\\end{align} $$ Now $$ \\\\left\\\\|S \\\\begin{bmatrix}\\\\sigma_1\\\\\\\\ w\\\\end{bmatrix}\\\\right\\\\|_2 \\\\ge \\\\sigma_1^2 + w^*w=\\\\sqrt{\\\\sigma_1^2 + w^* w} \\\\left\\\\|\\\\begin{bmatrix}\\\\sigma_1\\\\\\\\ w\\\\end{bmatrix}\\\\right\\\\|_2, $$ this implies $w=\\\\mathcal O$. Note that we have $x\\\\perp v_1\\\\implies Ax \\\\perp Av_1$, and the only assumption to derive this result is $\\\\|Av_1\\\\|_2=\\\\|A\\\\|_2$, with $\\\\|v_1\\\\|_2=1$. We extract this as a technical corollary. > **Corollary.** Let $A\\\\in \\\\mathbb C^{m\\\\times n}$, $v\\\\in \\\\mathbb C^n$ with $\\\\|v\\\\|_2=1$. Then if $\\\\|Av\\\\|_2 = \\\\|A\\\\|_2$, > > $$ > w\\\\perp v \\\\implies Aw\\\\perp Av. > $$ > > The same is true when $\\\\mathbb C$ is replaced by $\\\\mathbb R$. <proof> **Proof.** Repeat what we have done so far, i.e., replace $v$ by $v_1$ and $\\\\frac{Av_1}{\\\\|Av_1\\\\|_2}$ by $u_1$ in the argument preceding the corollary. Then once $w\\\\perp v$, one has $Aw\\\\perp Av_1=Av$. </proof> To finish the proof let\'s induct on $k \\\\ge 4$, where $m+n=k$. Suppose any $m\\\\times n$ matrix with $m+n = 4,5,\\\\dots, k-1$ has SVD with uniquely determined singular values in descending order. Then for $m+n=k$, by induction hypothesis and according to equation (1), $B= U_2\\\\Sigma V_2^*$ with unique $\\\\Sigma$, and the existence of SVD follows from the formula: $$ U_1^* A V_1= \\\\left[ \\\\begin{array}{c|c} \\\\sigma_1 & \\\\mathcal O\\\\\\\\ \\\\hline \\\\mathcal O& U_2\\\\Sigma V_2^* \\\\end{array} \\\\right] = \\\\left[ \\\\begin{array}{c|c} 1 & \\\\mathcal O\\\\\\\\ \\\\hline \\\\mathcal O& U_2 \\\\end{array} \\\\right] \\\\left[ \\\\begin{array}{c|c} \\\\sigma_1 & \\\\mathcal O\\\\\\\\ \\\\hline \\\\mathcal O& \\\\Sigma \\\\end{array} \\\\right] \\\\left[ \\\\begin{array}{c|c} 1 & \\\\mathcal O\\\\\\\\ \\\\hline \\\\mathcal O& V_2^* \\\\end{array} \\\\right]. $$ Although $\\\\Sigma$ is unique, it is dependent on $B$, while $B$ is dependent on the choice of basis. Fortunately under any changes of $(u_2,\\\\dots,u_m)$ and $(v_2,\\\\dots,v_n)$ to other o.n. bases, $U_2$ and $V_2$ will be replaced by other unitary matrices and $\\\\Sigma$ remains unchanged, hence singular values of $A$ are unique. The proof is almost completed by induction, except for the base case $m+n=4$, which is obvious by (1). </proof> <proof> **Proof of Part (2).** Let\'s assume $A\\\\in \\\\mathbb C^{n\\\\times n}$ is square. It is clear that $\\\\sigma_1=\\\\|A\\\\|_2$ since $\\\\|A\\\\|_2$ is the largest possible singular value of $A$. We first prove that if the right singular vector of $\\\\sigma_1$ is not unique\\", then $\\\\sigma_1$ is not simple, i.e., $\\\\sigma_1$ is repeated in $\\\\Sigma$. Let $Av_1=\\\\sigma_1u_1$, $\\\\|v_1\\\\|_2=\\\\|u_1\\\\|_2=1$. Suppose there are other vectors $w,w\'\\\\in \\\\mathbb C^n$, with $\\\\|w\\\\|_2=\\\\|w\'\\\\|_2=1$ s.t. $Aw=\\\\sigma_1 w\'$. For the sake of contradiction, let\'s assume $w\\\\not\\\\in \\\\mathbb C v_1$, then the unit vector $v_2 := \\\\frac{w-\\\\langle v_1,w\\\\rangle v_1}{\\\\|w-\\\\langle v_1,w\\\\rangle v_1\\\\|_2}$ is orthogonal to $v_1$. Now $\\\\|Av_2\\\\|_2\\\\leq \\\\|A\\\\|_2 = \\\\sigma_1$, the inequality cannot be strict, otherwise since $w = c v_1 + sv_2$ with $|c|^2+|s|^2=1$, we have $$ \\\\sigma_1^2=\\\\|Aw\\\\|_2^2 = \\\\|c \\\\sigma_1 u_1 + s Av_2\\\\|_2^2= |c|^2|\\\\sigma_1|^2 + |s|^2 \\\\|Av_2\\\\|_2^2< \\\\sigma_1^2, $$ absurd. We conclude $Av_2=\\\\sigma_1 u_2$, for some unit vector $u_2\\\\in (\\\\mathbb C u_1)^\\\\perp$. Now by the corollary one observes that $$ A|_{(\\\\mathrm{span}_\\\\mathbb C(v_1,v_2))^\\\\perp}:(\\\\mathrm{span}_\\\\mathbb C(v_1,v_2))^\\\\perp\\\\to (\\\\mathrm{span}_\\\\mathbb C(u_1,u_2))^\\\\perp, $$ and thus we can get a complete list of singular values with $\\\\sigma_1$ appears twice, a contradiction. Hence if $\\\\sigma_j$\'s are distinct, $w\\\\in \\\\mathbb C v_1$, i.e., $w$ and $v_1$ differ by a multiplicative constant with modulus 1. It follows that $u_1$ is unique up to a complex sign. Finally since $A|_{(\\\\mathbb C v_1)^\\\\perp} : (\\\\mathbb C v_1)^\\\\perp\\\\to (\\\\mathbb C u_1)^\\\\perp$, by choosing the bases of these two spaces, the uniqueness follows from induction on dimension of the square matrix. </proof> ","title":"Singular Value Decomposition","date":"2022-04-30T00:00:00.000Z","id":"blog070","tag":"math","toc":false,"intro":"We discuss the rigorous proof of Singular Value Decomposition."},{"content":" #### Results <center> <a href=\\"/assets/tech/051.png\\"> <img src=\\"/assets/tech/051.png\\" width=\\"650\\"/> </a> </center> <center> <a href=\\"/assets/tech/052.png\\"> <img src=\\"/assets/tech/052.png\\" width=\\"650\\"/> </a> </center> #### Imports python import numpy as np import cv2 import tensorflow as tf import matplotlib.pyplot as plt import os import numpy as np import random from numpy.random import randint from tensorflow.keras.optimizers import Adam, RMSprop from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError, CategoricalCrossentropy from tensorflow.keras.initializers import HeNormal from tensorflow.keras.preprocessing.image import img_to_array, load_img from tensorflow.keras.layers import ( Input, Conv2D, Conv2DTranspose, LeakyReLU, ReLU, Flatten, Dense, Activation, Concatenate, BatchNormalization, ZeroPadding2D ) from glob import glob from tensorflow.keras import activations from tensorflow.keras.models import Model from tqdm.notebook import tqdm from skimage.color import rgb2lab, lab2rgb from tensorflow.keras import initializers from tensorflow_addons.layers import InstanceNormalization, SpectralNormalization %matplotlib inline #### Cuda python os.environ[\'TF_XLA_FLAGS\'] = \'--tf_xla_enable_xla_devices\' config = tf.compat.v1.ConfigProto( gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8) ) config.gpu_options.allow_growth = True session = tf.compat.v1.Session(config=config) tf.compat.v1.keras.backend.set_session(session) #### Constants python # ----- model config ----- dataset_dir = \\"./dataset_with_deepcrack\\" labels = [\\"crack\\"] weight_init = initializers.GlorotNormal() weight_regularizer = None image_shape = (224, 224, 3) noise_dim = (224,224,3) SIZE=image_shape[0] ch = 64 # default filter depth for the conv blocks n_dis = 6 # number of covolution in the block of discriminator c_dim = len(labels) + 1 # number of domains for image translation, crack, water_seepage, normal n_res = 6 # number of res_block in bottleneck of image-to-image module # ----- training config ----- buffer_size = 1000 batch_size=1 lr = 1e-4 n_disc_iteration = 1 # critic iteration before update of generator #### Load DCT Dictionary and Mask python U = tf.convert_to_tensor(np.load(\\"./DCT_U.npy\\")) U_T = tf.convert_to_tensor(np.load(\\"./DCT_U_T.npy\\")) energy_mask = tf.convert_to_tensor(np.load(\\"./DCT_mask.npy\\")) DCT_U_ = tf.cast(U, tf.float32)[tf.newaxis, ...,tf.newaxis] DCT_U = tf.concat([DCT_U_] * 2352, axis=0) DCT_U_T_ = tf.cast(U_T, dtype=tf.float32)[tf.newaxis,...,tf.newaxis] DCT_U_T = tf.concat([DCT_U_T_] * 2352, axis=0) DCT_freq_domain_mask_ = tf.cast(energy_mask, dtype=tf.float32)[tf.newaxis,..., tf.newaxis] DCT_freq_domain_mask = tf.concat([DCT_freq_domain_mask_] * 2352, axis=0) #### Model Definitions ##### conv python def conv(x_init, filters, kernel_size=4, strides=2, pad=0, pad_type=\'zero\', use_bias=True, sn=False): x = ZeroPadding2D(padding=(pad, pad))(x_init) if sn: w = tf.compat.v1.get_variable(\\"kernel\\", shape=[kernel_size, kernel_size, x.get_shape()[-1], channels], initializer=weight_init, regularizer=weight_regularizer) x = Conv2D(input=x, filter=spectral_norm(w), strides=[1, strides, strides, 1], padding=\'VALID\') if use_bias: bias = tf.get_variable(\\"bias\\", [filters], initializer=tf.constant_initializer(0.0)) x = tf.nn.bias_add(x, bias) else: x = Conv2D(filters, kernel_size=(kernel_size,kernel_size), kernel_initializer=weight_init, kernel_regularizer=weight_regularizer, strides=(strides,strides), use_bias=use_bias)(x) return x ##### deconv python def deconv(x, filters, kernel_size=4, strides=2, use_bias=True): x = Conv2DTranspose(filters=filters, kernel_size=kernel_size, kernel_initializer=weight_init, kernel_regularizer=weight_regularizer, strides=strides, padding=\'same\', use_bias=use_bias)(x) return x ##### discriminator python def discriminator(): channel = ch input = Input(shape=(image_shape)) x = conv(input, channel, kernel_size=4, strides=2, pad=1, use_bias=True) x = LeakyReLU(0.01)(x) for i in range(1, n_dis): x = conv(x, channel * 2, kernel_size=4, strides=2, pad=1, use_bias=True) x = LeakyReLU(0.01)(x) channel = channel * 2 c_kernel = int(image_shape[0] / np.power(2, n_dis)) logit = conv(x, 1, kernel_size=3, strides=1, pad=1, use_bias=False) c = conv(x, c_dim, kernel_size=c_kernel, strides=1, use_bias=False) c = tf.reshape(c, shape=[-1, c_dim]) c = Dense(c_dim, activation=\\"softmax\\")(c) model = Model(input, [logit, c]) return model python # Check shape: disc = discriminator() x = tf.random.normal((1,)+image_shape, 0, 1) print(disc(x)[0].shape) print(disc(x)[1].shape) ##### res_block python def res_block(x_in, filters, use_bias=True, scope=\'resblock\'): input = x_in x = conv(input, filters, kernel_size=3, strides=1, pad=1, use_bias=use_bias) x = InstanceNormalization()(x) x = ReLU()(x) x = conv(x, filters, kernel_size=3, strides=1, pad=1, use_bias=use_bias) x = InstanceNormalization()(x) return x + input ##### down_sample python def down_sample(x, scale_factor_h, scale_factor_w): _, h, w, _ = x.get_shape().as_list() new_size = [h // scale_factor_h, w // scale_factor_w] return tf.image.resize(x, new_size, method=\'nearest\') ##### param_free_norm python def param_free_norm(x, epsilon=1e-5): x_mean, x_var = tf.nn.moments(x, axes=[1, 2]) x_std = tf.sqrt(x_var + epsilon) return (x - x_mean) / x_std ##### spade python def spade(segmap, x_init, filters=None, use_bias=True, sn=False) : x = param_free_norm(x_init) _, x_h, x_w, n_filters = x_init.get_shape().as_list() _, segmap_h, segmap_w, _ = segmap.get_shape().as_list() factor_h = segmap_h // x_h # 256 // 4 = 64 factor_w = segmap_w // x_w segmap_down = down_sample(segmap, factor_h, factor_w) segmap_down = conv(segmap_down, filters=128, kernel_size=5, strides=1, pad=2, use_bias=use_bias, sn=sn) segmap_down = ReLU()(segmap_down) segmap_gamma = conv(segmap_down, filters=filters, kernel_size=5, strides=1, pad=2, use_bias=use_bias, sn=sn) segmap_beta = conv(segmap_down, filters=filters, kernel_size=5, strides=1, pad=2, use_bias=use_bias, sn=sn) x = x * (1 + segmap_gamma) + segmap_beta return x ##### spade_resblock python def spade_resblock(segmap, x_init, filters, use_bias=True, sn=False): channel_in = x_init.get_shape().as_list()[-1] channel_middle = min(channel_in, filters) x = spade(segmap, x_init, channel_in, use_bias=use_bias, sn=False) x = LeakyReLU(0.2)(x) x = conv(x, filters=channel_middle, kernel_size=3, strides=1, pad=1, use_bias=use_bias, sn=sn) x = spade(segmap, x, filters=channel_middle, use_bias=use_bias, sn=False) x = LeakyReLU(0.2)(x) x = conv(x, filters=filters, kernel_size=3, strides=1, pad=1, use_bias=use_bias, sn=sn) if channel_in != filters: x_init = spade(segmap, x_init, filters=channel_in, use_bias=use_bias, sn=False) x_init = conv(x_init, filters=filters, kernel_size=1, strides=1, use_bias=False, sn=sn) return x + x_init ##### adative_noise_multiplier python def adative_noise_multiplier(z): x = Conv2D(32, (5,5), 2, padding=\\"same\\")(z) x = ReLU()(x) x = Conv2D(64, (3,3), 2, padding=\\"same\\")(x) x = ReLU()(x) x = Flatten()(x) x = Dense(128, activation=\\"relu\\")(x) x = Dense(1, activation=\\"sigmoid\\")(x) return x ##### generator python def generator(): channel = ch x_in = Input(shape=image_shape) c_in = Input(shape=(image_shape[0], image_shape[1], c_dim)) z = Input(shape=noise_dim) lambda_z = adative_noise_multiplier(z) x = tf.concat([x_in, lambda_z * z], axis=-1) # in the past, c_in is of shape (c_dim,), now c_in has also the spacial information, therefore no need to tf.tile(c_in, [1, x_in.shape[1], x_in.shape[2], 1]) x = conv(x, channel, kernel_size=7, strides=1, pad=3, use_bias=False) x = InstanceNormalization()(x) x = ReLU()(x) # Down-Sampling for i in range(3): x = conv(x, channel*2, kernel_size=4, strides=2, pad=1, use_bias=False) x = InstanceNormalization()(x) x = ReLU()(x) channel = channel * 2 # Bottleneck for i in range(n_res): x = res_block(x, channel, use_bias=False) # Up-Sampling for i in range(3): # need to feed the spatial control map here x = deconv(x, channel//2, kernel_size=4, strides=2, use_bias=False) # spade normalization instead x = spade_resblock(c_in, x, channel//2) x = ReLU()(x) channel = channel // 2 pre_mask = conv(x, filters=1, kernel_size=7, strides=1, pad=3, use_bias=False) mask = Activation(\\"sigmoid\\")(pre_mask) defect_overlay = conv(x, filters=3, kernel_size=7, strides=1, pad=3, use_bias=False) defect_overlay = tf.reshape(defect_overlay, (2352, 8, 8, 1)) defect_overlay = tf.einsum(\\"oik...,okj...->oij...\\", tf.einsum(\\"oikb,okjb->oijb\\", DCT_U_T, defect_overlay), DCT_U) defect_overlay = tf.einsum(\\"oij...,oij...->oij...\\", DCT_freq_domain_mask, defect_overlay) defect_overlay = tf.einsum(\\"oik...,okj...->oij...\\", tf.einsum(\\"oik...,okj...->oij...\\", DCT_U, defect_overlay), DCT_U_T) defect_overlay = tf.reshape(defect_overlay, (224,224,3)) defect_overlay = Activation(\\"tanh\\")(defect_overlay) model = Model([x_in, c_in, z], [defect_overlay, mask]) return model #### Dataset Pipeline ##### Helper Functions ###### one_hot python def one_hot(index, length): zeros = np.zeros((length, )) zeros[index] = 1 return zeros ###### get_segment_and_spatialCatMap_and_defectOneHot_and_normalOnehot python import glob def get_segment_and_spatialCatMap_and_defectOneHot_and_normalOnehot(segment_filepath, index): # only EagerTensor can be decoded as follows: index = int(bytes.decode(index)) segment_filepath = bytes.decode(segment_filepath) segment = img_to_array(load_img(segment_filepath, target_size=(SIZE,SIZE), color_mode=\\"grayscale\\")) segment = np.where(segment>0.5, 1, 0) spatial_cat_map = np.zeros((SIZE, SIZE, c_dim)) # create spatial_categorical map from segmentation and index of the defect class for x in range(SIZE): for y in range(SIZE): if segment[x,y,0] > 0: spatial_cat_map[x,y,index] = 1 defect_onehot = one_hot(index, c_dim) normal_onehot = one_hot(c_dim-1, c_dim) # onehot with last entry as 1, else 0 return tf.convert_to_tensor(segment, dtype=tf.float32), \\\\ tf.convert_to_tensor(spatial_cat_map, dtype=tf.float32), \\\\ tf.convert_to_tensor(defect_onehot, dtype=tf.float32),\\\\ tf.convert_to_tensor(normal_onehot, dtype=tf.float32) ###### path_to_img python import random def path_to_img(zipped_filepaths): normal_filepath = zipped_filepaths[0] defect_filepath = zipped_filepaths[1] segment_filepath = zipped_filepaths[2] defect_index = zipped_filepaths[3] normal = tf.io.read_file(normal_filepath) normal = tf.image.decode_png(normal, channels=3) normal = tf.image.resize(normal, (SIZE, SIZE)) normal = (normal/127.5) - 1 defect = tf.io.read_file(defect_filepath) defect = tf.image.decode_png(defect, channels=3) defect = tf.image.resize(defect, (SIZE, SIZE)) defect = (defect/127.5) - 1 segment, spatial_cat_map, defect_onehot, normal_onehot = tf.numpy_function( func=get_segment_and_spatialCatMap_and_defectOneHot_and_normalOnehot, inp=[segment_filepath, defect_index], Tout = [tf.float32, tf.float32, tf.float32, tf.float32] ) return (normal, defect, segment, defect_index, spatial_cat_map, defect_onehot, normal_onehot) ##### Final Dataset Generator: get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen python def get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen(random_segmentations=False): normals = [] defects = [] defect_segmentations = [] defect_indexes = [] for label in labels: index = labels.index(label) normals += glob.glob(f\\"{dataset_dir}/{label}/normal/*.jpg\\") defects += glob.glob(f\\"{dataset_dir}/{label}/defect/*.jpg\\") defect_segmentations += glob.glob(f\\"{dataset_dir}/{label}/defect_segmentation/*.png\\") if random_segmentations: random.shuffle(defect_segmentations) defect_indexes += [str(index)] * len(normals) # normals, defects, defect_segementations are all of the same length, just use one of them normal_defect_defectSeg = list(zip(normals, defects, defect_segmentations, defect_indexes)) dataset = tf.data.Dataset.from_tensor_slices(normal_defect_defectSeg)\\\\ .map(path_to_img)\\\\ .batch(batch_size)\\\\ .shuffle(buffer_size)\\\\ .cache() return iter(dataset) python def view_dataset_sample(): normals, defects, segments, indexes, spatial_cat_maps, _, _ = next(get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen()) # spatial_cat_map_[:,:,index] = segments for i in range(0, 1): normal = normals[i].numpy().astype(\\"float32\\") defect = defects[i].numpy().astype(\\"float32\\") segment = segments[i].numpy().astype(\\"float32\\") spatial_cat_map = spatial_cat_maps[i] index = int(indexes[i]) label = labels[index] print(\\"label\\", label) plt.figure(figsize=(6, 15)) plt.subplot(1, 3, 1) plt.imshow(((normal+1)*127.5).astype(\\"uint8\\")) plt.subplot(1, 3, 2) plt.imshow(((defect+1)*127.5).astype(\\"uint8\\")) plt.subplot(1, 3, 3) plt.imshow(spatial_cat_map[:,:,index]) view_dataset_sample() #### Train Loop ##### Init Loss Util Functions python mae = MeanAbsoluteError() cce = CategoricalCrossentropy() python def gradient_penality(critic, real_sample, fake_sample): epsilon = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0) interpolated = epsilon * real_sample + (1 - epsilon) * fake_sample with tf.GradientTape() as gp_tape: gp_tape.watch(interpolated) critic_inter = critic(interpolated, training=True) grads = gp_tape.gradient(critic_inter, [interpolated])[0] norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3])) penality = tf.reduce_mean((norm - 1.0) ** 2) return penality ##### Init Models python gen = generator() disc = discriminator() ##### View Intermediate Performance python def view_generator_sample(image_name=None, show_in_notebook=True): datagen = get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen() dategen_randomSeg = get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen(random_segmentations=True) normals, defects, segments, indexes, spatial_cat_maps, _, _ = next(datagen) normals_, defects_, segments_, indexes_, spatial_cat_maps_, _, _ = next(dategen_randomSeg) # spatial_cat_map_[:,:,index] = segments if image_name is not None: print(\\"\\") print(\\"---------------------------------------------\\") print(\\"---------\\") print(image_name) plt.figure(figsize=(17, 10)) for i in range(0, 1): spatial_cat_map_randomSeg = spatial_cat_maps_[i] spatial_cat_map = spatial_cat_maps[i] normal = normals[i].numpy().astype(\\"float32\\") defect = defects[i].numpy().astype(\\"float32\\") for row_count, (from_, spa_cat, description) in enumerate([ (defect, spatial_cat_map, \\"from_defect\\"), (normal, spatial_cat_map, \\"from_normal\\"), (normal, spatial_cat_map_randomSeg, \\"from_random_segment\\") ]): segment = segments[i].numpy().astype(\\"float32\\") z = np.random.normal(0, 1, (1, )+noise_dim) top, m = gen.predict([from_[np.newaxis,...], spa_cat[np.newaxis,...], z]) top_layer = np.squeeze(top) index = int(indexes[i]) label = labels[index] plt.subplot(3, 5, 5*row_count + 1) plt.imshow(((from_+1)*127.5).astype(\\"uint8\\")) plt.subplot(3, 5, 5*row_count + 2) plt.imshow(spa_cat[:,:,index]) plt.subplot(3, 5, 5*row_count + 3) plt.imshow(((top_layer+1)*127.5).astype(\\"uint8\\")) plt.subplot(3, 5, 5*row_count + 4) print(\\"max mask value inside mask:\\", np.amax(m)) plt.imshow(np.squeeze(m)) plt.subplot(3, 5, 5*row_count + 5) gened_defects = from_ * (1-m) + top_layer * m plt.imshow(((gened_defects[0]+1)*127.5).astype(\\"uint8\\")) if image_name is not None: plt.savefig(f\\"{image_name}\\", dpi=80, bbox_inches=\\"tight\\") if show_in_notebook: plt.show() view_generator_sample() ##### Train Loop Definition python epoches=8 sample_per_batch=15 show_in_notebook=True # whether to show image in the notebook g_opt = Adam(lr=lr, beta_1=0, beta_2=0.9) d_opt = Adam(lr=lr, beta_1=0, beta_2=0.9) for epoch in range(epoches): batch = 0 dataset_gen = get_normal_defect_defectSegment_defectIndex_spatialCat_defectOneHot_normalOnehot_gen() if epoch >= 0 and epoch <= 2: lambda_mask = 5.0 elif epoch >= 3 and epoch <= 5: lambda_mask = 0.5 else: lambda_mask = 0 for normals, defects, _, _, spatial_cat_maps, defect_onehots, normal_onehots in dataset_gen: batch += 1 print(\\"epoch\\", epoch+1, \\"batch\\", batch, \\"lambda_mask\\", lambda_mask, end=\\"\\\\r\\") for iteration in range(n_disc_iteration): update_gen = (iteration+1)%n_disc_iteration == 0 for from_, to_, from_onehots, target_onehots in [(normals, defects, normal_onehots, defect_onehots), (defects, normals, defect_onehots, normal_onehots)]: z_1 = tf.random.normal((1,)+noise_dim, 0, 1) z_2 = tf.random.normal((1,)+noise_dim, 0, 1) with tf.GradientTape(persistent=True) as tape: # naming convetion is from normal --\x3e defect. Roles of normal and defect can be interchanged defect_overlays, n2d_masks = gen([from_, spatial_cat_maps, z_1], training=True) gened_defects = from_ * (1-n2d_masks) + defect_overlays * n2d_masks restore_overlays, d2n_masks = gen([gened_defects, spatial_cat_maps, z_2], training=True) restoration = gened_defects * (1-d2n_masks) + restore_overlays * d2n_masks d_logit_on_gened, d_cls_on_gened = disc(gened_defects, training=True) d_logit_on_real, d_cls_on_real = disc(from_, training=True) cls_loss_on_gened = cce(target_onehots, d_cls_on_gened) cls_loss_on_real = cce(from_onehots, d_cls_on_real) gp = gradient_penality(disc, from_, gened_defects) d_wgan_gp_loss = tf.math.reduce_mean(d_logit_on_gened) - tf.reduce_mean(d_logit_on_real) + 10 * gp g_cycle_loss = mae(from_, restoration) g_mask_cycle_loss = mae(n2d_masks, d2n_masks) g_mask_vanishing_loss = -tf.math.log(tf.math.reduce_mean(mae(n2d_masks, 0.0) + mae(d2n_masks, 0.0))) g_mask_spatial_constraint_loss = tf.math.reduce_mean(mae(n2d_masks, 0.0) + mae(d2n_masks, 0.0)) g_wgan_gp_loss = - tf.math.reduce_mean(d_logit_on_gened) d_loss = tf.tensordot( [1, 5.0 ], [d_wgan_gp_loss, cls_loss_on_real], axes = 1 ) if update_gen: g_loss = tf.tensordot( [10.0, 5.0, lambda_mask, 5.0, 10.0, 1.0 ], [g_cycle_loss, g_mask_cycle_loss, g_mask_vanishing_loss, g_mask_spatial_constraint_loss, cls_loss_on_gened, g_wgan_gp_loss], axes = 1 ) grad_of_d = tape.gradient(d_loss, disc.trainable_variables) d_opt.apply_gradients(zip(grad_of_d, disc.trainable_variables)) if update_gen: grad_of_g = tape.gradient(g_loss, gen.trainable_variables) g_opt.apply_gradients(zip(grad_of_g, gen.trainable_variables)) if (batch % sample_per_batch) == 0: view_generator_sample( image_name=\\"epoch-{}-batch-{}-224-DCT-ver\\".format( str(epoch+1).zfill(3), str(batch).zfill(5) ), show_in_notebook=show_in_notebook ) ","title":"DefectGAN","date":"2022-05-09T00:00:00.000Z","id":"blog072","tag":"deep-learning, tensorflow","toc":true,"intro":"Some implementation on DefectGAN."},{"content":" #### tensorflow-gpu, cudatoolkit and cuDNN First scroll down the to bottom of <a href=\\"https://www.tensorflow.org/install/source_windows?hl=zh-tw\\">this page</a> to get a list of compatible version of tensorflow-gpu, cuDNN and CUDA. For me I have been using tensorflow-gpu 2.4.0, cuDNN 8.x and cuda 11.x. We can: - Install tensorflow-gpu 2.4.0 by pip install tensorflow-gpu==2.4.0. - Install cudatoolkit 11.0 by conda install cudatoolkit==11.0. - Download cuDNN 8.4.0 from <a href=\\"https://developer.nvidia.com/rdp/cudnn-download#a-collapse805-110\\">here</a>. Copy the files from - downloaded\\\\cuDNN8.4.0\\\\{bin, include, lib} into - C:\\\\Users\\\\<name>\\\\anaconda3\\\\envs\\\\<env name>\\\\Library\\\\{bin, include, lib} respectively. #### Possible error: zlibwapi.dll When running script with tensorflow, the program may raise the following error: text Could not locate zlibwapi.dll To solve it we following the instruction from sectoin **_3.1.3. Installing zlib_** from <a href=\\"https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#install-zlib-windows\\">here</a>. #### Indication of Running Tensorflow with GPU Successfully The following messages should be logged in our console when we are running a script with tensorflow imported. text I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6 coreClock: 1.725GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0 I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6 coreClock: 1.725GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0 I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267] 0 I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0: N I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21821 MB memory) -> physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6) I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0 I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0 I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once. ","title":"Install tensorflow-gpu","date":"2022-05-15T00:00:00.000Z","id":"blog073","tag":"deep-learning","intro":"Record standard step to set up tensorflow-gpu, cuda and cudnn."},{"content":" #### Iterables ##### Types of Iterable Objects There are several common objects that are iterable in shell scripts: - String separated by \\\\n. For example, consider the string iterable: python iterable=$\'file1\\\\nfile2\' # or iterable=\\" file1 file2 \\" for char in $iterable; do echo $char done outputs none file1 file2 - <a href=\\"http://mywiki.wooledge.org/glob\\">glob (click to see detail)</a> : python for image in some/folder/*; do echo \\"$image\\" done Due to character-encoding problem, it is never a good idea to loop the array generated by files=$(ls some/dir), as always we should use glob. - Array object like files=(\\"file1\\" \\"file2\\"). Common source of array can be built using glob, for example, arr=($some/folder/*.jpg). There is special syntax to loop array elements: python for file in ${files[@]}; do echo $file done ##### Example of Iterables Some real example I created in my work: python EXPER_IN=some/folder/INPUT EXPER_OUT=some/folder/OUTPUT # stock of images STOCK=some/folder/stock RESULT=some/folder/result CONFIG_PATH=some/folder/config.json images_arr=($STOCK/*.jpg) # can choose either one of the below: # for image in $images_arr; do # for image in $STOCK/*; do for image in $STOCK/*; do image_name=$(basename -- $image) image_name_with_jpg=$(echo $image_name | sed \\"s/.jpg//g\\") cp $STOCK/$image_name $EXPER_IN python dsds_main.py debug-defect --config $CONFIG_PATH --tag dev target_dir=$RESULT/$image_name_with_jpg mkdir $target_dir mv $EXPER_OUT/* $target_dir rm $EXPER_IN/$image_name done **Remarks.** - We use basename -- \\"a/b/c/d.jpg to get d.jpg. - There is no mv -r command, we just have mv. #### If Statements and Booleans ##### No Booleans in Bash From <a href=\\"https://devmanual.gentoo.org/tools-reference/bash/index.html#string-comparison-in-bash\\">this post</a>: - There are no Booleans in Bash - Always compare against strings or numbers Therefore asking how to construct _boolean_ in bash is the same thing as asking how to write _comparison_ in bash. We usally wrap our comparison inside [[ comparison ]]. ##### List of All Comparisons Copied from <a href=\\"https://devmanual.gentoo.org/tools-reference/bash/index.html\\">here</a>. <center></center> > The [[ ]] form is generally safer than [ ] and should be used in all new code. | <div style=\\"width:500px\\">Operator</div> | Purpose | | --------------------------------------- | -------------------------------------------------------------------------------------------------- | ------------------- | | <img width=170/> | <img style=\\"width:calc(100%)\\"/> | | == | String equality | | != | String inequality | | < | String lexiographic | comparison (before) | | > | String lexiographic comparison (after) | | =~ | String regular expression match (bash 3 only, not currently allowed in ebuilds) | | -z \\"string\\" | String has zero length | | -n \\"string\\" | String has non-zero length | | -eq | Integer equality | | -ne | Integer inequality | | -lt | Integer less than | | -le | Integer less than or equal to | | -gt | Integer greater than | | -ge | Integer greater than or equal to | | -a file | Exists (use -e instead) | | -b file | Exists and is a block special file | | -c file | Exists and is a character special file | | -d file | Exists and is a directory | | -e file | Exists | | -f file | Exists and is a regular file | | -g file | Exists and is set-group-id | | -h file | Exists and is a symbolic link | | -k file | Exists and its sticky bit is set | | -p file | Exists and is a named pipe (FIFO) | | -r file | Exists and is readable | | -s file | Exists and has a size greater than zero | | -t fd | Descriptor fd is open and refers to a terminal | | -u file | Exists and its set-user-id bit is set | | -w file | Exists and is writable | | -x file | Exists and is executable | | -O file | Exists and is owned by the effective user id | | -G file | Exists and is owned by the effective group id | | -L file | Exists and is a symbolic link | | -S file | Exists and is a socket | | -N file | Exists and has been modified since it was last read | | file1 -nt file2 | file1 is newer (according to modification date) than file2, or if file1 exists and file2 does not. | | file1 -ot file2 | file1 is older than file2, or if file2 exists and file1 does not. | | file1 -ef file2 | file1 and file2 refer to the same device and inode numbers. | | first \\\\|\\\\| second | first or second (short circuit) | | first && second | first and second (short circuit) | | ! condition | not condition | ##### Example of If Statement python for image in echo $STOCK/*; do if [[ \\"$image\\" =~ \\\\.jpg || \\"$image\\" =~ \\\\.png ]]; then echo $image fi done #### Point to Values in an Array Take images_arr=($STOCK/*.jpg) as in the example of iterables above, we can get specific element by: - ${images_arr[k]} the $(k+1)$-th element - ${images_arr[-1]} the last element - ${images_arr[@]:0:k} the first $k$ elements #### Read First few/Last few Lines or Skip Lines in std-out Suppose I am going read a file by cat main.py, then: - Read the first 3 lines: python cat main.py | head -3 - Read the last 3 lines: python cat main.py | tail -3 - Skip 3 lines from the top: python cat main.py | tail -n +3 #### Try-Catch in Bash We mix iterable and try-catch in this example. The principles are - command_1 && command_2 $\\\\implies$ if command_1 succeeds, run command_2. - command_1 || command_2 $\\\\implies$ if command_1 fails, run command_2. python trip01=some/other/folder/w0m8fnyb trip02=some/other/folder/i8y6v_yn trip03=some/other/folder/cght2jy4 IMAGES=\\" $trip02/P202203252_732_4395.jpg $trip02/P202203252_743_4456.jpg $trip03/P202203253_299_1789.jpg \\" TARGET_DIR=\\"some/folder/stock\\" LOG=$(date +%Y-%m-%d--%Hh%Mm%Ss).log # output 2022-05-20--10h24m52s.log: for img in $IMAGES; do { { # try cp $img $TARGET_DIR } && { echo \\"copied from $img to $TARGET_DIR\\" } } || { # catch, save the log etc echo \\"fails to copy\\" > $LOG } done **Remarks.** The || will not surpress the exception handling of bash itself, it will keep reporting the root cause. The command after || helps us do extra work when problem occurs. #### Reference - <a href=\\"https://www.gnu.org/software/bash/manual/html_node/\\">Bash Reference Manual</a> ","title":"Shell Script Fundamentals","date":"2022-05-20T00:00:00.000Z","id":"blog075","tag":"coding","intro":"Record the basic knowledge we need to know in shell script."},{"content":" #### Conv2DSamePadding: The good day in Tensorflow with padding=\\"same\\" The output shape of python tf.keras.layers.Conv2D(channels, kernel_size, s, padding=\\"same\\") in tensorflow is more predictable because it always takes an input of shape (N, H, W, C) into an output of shape (N, H//s, W//s, channels). Unfortunately in pytorch padding=\\"same\\" will fail if $\\\\texttt{s} > 1$, but we usually need such an option to downsize the spatial dimenion by a constant multiple (usually that multiple is stride=2). Therefore we create an equivalent version in pytorch: python import torch import torch.nn as nn import torch.nn.functional as F import math class Conv2dSamePadding(nn.Conv2d): \\"\\"\\" This conv layer is used if we want to make the shape of convolution more predictable. \\"\\"\\" def __init__(self, *args, **kwargs): super(Conv2dSamePadding, self).__init__(*args, **kwargs) def get_padding_for_same(self, kernel_size, stride, padding, input: torch.Tensor): if isinstance(padding, int): input = F.pad(input, (padding, padding, padding, padding)) if isinstance(kernel_size, int): kernel_size = (kernel_size, kernel_size) if isinstance(stride, int): stride = (stride, stride) _, _, H, W = input.shape s_H = stride[0] s_W = stride[1] k_H = kernel_size[0] k_W = kernel_size[1] h2 = math.floor(H / s_H) w2 = math.floor(W / s_W) pad_W = (w2 - 1) * s_W + (k_W - 1) + 1 - W pad_H = (h2 - 1) * s_H + (k_H - 1) + 1 - H padding = (pad_W // 2, pad_W - pad_W // 2, pad_H // 2, pad_H - pad_H // 2) return padding def forward(self, input): padding = self.get_padding_for_same(self.kernel_size, self.stride, self.padding, input) return self._conv_forward(F.pad(input, padding), self.weight, self.bias) Checking: python result = Conv2dSamePadding(3, 6, 100, 3)(torch.randn(10, 3, 300, 300)) print(result.shape) result = Conv2dSamePadding(3, 6, 100, 3)(torch.randn(10, 3, 299, 299)) print(result.shape) result = Conv2dSamePadding(3, 6, 100, 3)(torch.randn(10, 3, 298, 298)) print(result.shape) result = Conv2dSamePadding(3, 6, 100, 3)(torch.randn(10, 3, 297, 297)) print(result.shape) Result: none torch.Size([10, 6, 100, 100]) torch.Size([10, 6, 99, 99]) torch.Size([10, 6, 99, 99]) torch.Size([10, 6, 99, 99]) #### Spectral Normalization to Every Convolution Layer in a Module python def _add_sn(m): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)): return spectral_norm(m) else: return m def add_sn_(model: nn.Module): model.apply(_add_sn) #### Weights Initialization to Every Convolution Layer in a Module python def initialize_weights(model): for m in model.modules(): if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)): nn.init.xavier_normal_(m.weight.data) #### Device Which Auto-Detect Cuda python import torch device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") #### Save and Load Model python torch.save(gen, \\"gen.pt\\") torch.load(\\"gen.pt\\") #### Dataloader In pytorch: - We use Dataset to construct the logic to generate input and ground truth in training. For this, we subclass from Dataset and implement: - __getitem__(self, index) given an index i, give me a data dataset[i]. - __len__ give me total length of the dataset. - We then use DataLoader class to wrap Dataset object to construct a dataset pipeline. Now we can test the outout of our dataset by dataset[0]. ##### Example of Dataset Object python import os import numpy as np import torch from torch.utils.data import Dataset, DataLoader from torchvision.io import read_image, image from torchvision import transforms as T from device import device from config import ModelAndTrainingConfig as config class DefectDataset(Dataset): def __init__(self): super(DefectDataset, self).__init__() self.defect_filepath_arr = None self.load_fect_filepath() def load_fect_filepath(self): if self.defect_filepath_arr is None: defect_data = [] dataset_dir = config.dataset_dir for defect in config.defect_labels: normal_dir = f\\"{dataset_dir}/{defect}/normal\\" defect_dir = f\\"{dataset_dir}/{defect}/defect\\" mask_dir = f\\"{dataset_dir}/{defect}/defect_segmentation\\" defect_index = config.labels.index(defect) for basename in os.listdir(normal_dir): filename = basename.replace(\\".jpg\\", \\"\\") normal_path = f\\"{normal_dir}/{filename}.jpg\\" defect_path = f\\"{defect_dir}/{filename}.jpg\\" defect_mask_path = f\\"{mask_dir}/{filename}.png\\" defect_data.append([defect_index, normal_path, defect_path, defect_mask_path]) self.defect_filepath_arr = defect_data def load_cls_index_and_imgs_from_index(self, index): cls_index, normal_path, defect_path, defect_seg_path = self.defect_filepath_arr[index] resize = T.Resize(config.image_shape[1:3]) np_normal = resize(read_image(normal_path)) / 127.5 - 1 np_defect = resize(read_image(defect_path)) / 127.5 - 1 np_defect_mask = resize(read_image(defect_seg_path, mode=image.ImageReadMode.GRAY)) / 255 np_defect_mask = torch.where(np_defect_mask > 0.5, 1, 0) return cls_index, np_normal, np_defect, np_defect_mask def load_spatial_charactergorical_map_from_index(self, index): cls_index, _, _, defect_mask = self.load_cls_index_and_imgs_from_index(index) spartial_dim = tuple(config.image_shape[1:3]) spatial_cat_map = np.zeros((len(config.labels),) + spartial_dim) spatial_cat_map[cls_index] = defect_mask[0] return spatial_cat_map def get_num_of_batches(self): return (len(self.defect_filepath_arr) // config.batch_size) + (0 if config.dataset_drop_last else 1) def __getitem__(self, index): defect_cls_index, np_normal, np_defect, np_defect_seg = \\\\ self.load_cls_index_and_imgs_from_index(index) spatial_cat_map = self.load_spatial_charactergorical_map_from_index(index) return ( torch.as_tensor(defect_cls_index).type(torch.LongTensor).to(device), torch.as_tensor(np_normal, dtype=torch.float, device=device), torch.as_tensor(np_defect, dtype=torch.float, device=device), torch.as_tensor(np_defect_seg, dtype=torch.float, device=device), torch.as_tensor(spatial_cat_map, dtype=torch.float, device=device) ) def __len__(self): return len(self.defect_filepath_arr) ##### Example of DataLoader Object python defect_dataset = DefectDataset() defectDataloader = DataLoader(dataset=defect_dataset, batch_size=config.batch_size, shuffle=True, # prefetch_factor=config.batch_size, drop_last=config.dataset_drop_last, num_workers=config.dataset_num_workers) ##### On num_workers and prefetch_factor Note that from https://github.com/fastai/fastbook/issues/85 > We always need to set **_num_workers=0_** when creating a DataLoader > because Pytorch multiprocessing does not work on Windows. <center></center> Moreover, prefetch_factor can be positive only when num_workers is positive. Therefore in windows, both prefetch_factor and num_workers must be 0. ","title":"Pytorch Fundamentals","date":"2022-05-23T00:00:00.000Z","id":"blog076","tag":"deep-learning, pytorch","intro":"Record useful tools and commands in pytorch which I learn from translating DefectGAN model in tensorflow into pytorch."},{"content":" #### What we can Archive: <center> <img src=\\"/assets/tech/054.gif\\"/> </center> #### Update Log Without new line in Console In the image above, only the numerical values are changing, no new line will be further created. For this, we need to know <a href=\\"https://gist.github.com/fnky/458719343aabd01cfb17a3a4f7296797\\"><b><i>ANSI Escape Sequences</i></b></a>. We usually use the following: |code|purpose| |---|---| |<img width=100/>|<img style=\\"width:calc(100%)\\"/>| |ESC[#A | moves cursor ups| | ESC[#B| moves cursor down | | ESC[#C| moves cursor right | | ESC[#D| moves cursor left | | ESC[#E| moves cursor to beginning of next line, # lines down | | ESC[#F| moves cursor to beginning of previous line, # lines up | | ESC[0K | clears line| <p></p> <center></center> In python the **escape code** is prefixed by Hexadecimal: \\\\x1B (in nodejs we use \\\\u001b). #### Procesure to Create Continuous Log Suppose that we want to print 8 lines of log continuosly, then: - **Step 1.** Printed 8 lines by print(\\"\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\"). - **Step 2.** Go up by 8 lines, for this, we need: python print(\\"\\\\x1B[8A\\") - **Step 3.** Now we can print our own 8 lines of log. We need to append ESC[0K at the end of each string to print to make sure no extra character survives. Why? We can see what happends when print(\\"1 2 3 4\\", end=\\"\\\\r\\") and print(\\"5 6\\", end=\\"\\\\r\\"), since the first line is longer, the result becomes 5 6 3 4 instead of 5 6. - **Step 4.** Upon completion of logging, we move cursor up by 8 rows again (step 2). #### Example of Continuous Logger We summarize this workflow in print method below: python from pydash.objects import get, set_ class ConsoleLog(): def __init__(self, lines_up_on_end=0): self.CLR = \\"\\\\x1B[0K\\" self.lines_up_on_batch_end = lines_up_on_end self.record = {} def UP(self, lines): return \\"\\\\x1B[\\" + str(lines + 1) + \\"A\\" def DOWN(self, lines): return \\"\\\\x1B[\\" + str(lines) + \\"B\\" def on_print_end(self): print(self.UP(self.lines_of_log)) print(self.UP(self.lines_up_on_batch_end)) def print(self, key_values): lines_of_log = len(key_values) self.lines_of_log = lines_of_log # for the first time, # print self.lines_of_log number of lines to occupy the space print(\\"\\".join([\\"\\\\n\\"] * (self.lines_of_log))) print(self.UP(self.lines_of_log)) for key, value in key_values: if key == \\"\\" and value == \\"\\": print() else: if key != \\"\\" and value != \\"\\": prev_value = get(self.record, key, 0.) curr_value = value diff = curr_value - prev_value sign = \\"+\\" if diff >= 0 else \\"\\" print(\\"{0: <35} {1: <30}\\".format(key, value) + sign + \\"{:.5f}\\".format(diff) + self.CLR) set_(self.record, key, value) self.on_print_end() def clear_log_on_epoch_end(self): # usually before calling this line, print() has been run, therefore we are at the top of the log. for _ in range(self.lines_of_log): # clear lines print(self.CLR) # ready for next epoch print(self.UP(self.lines_of_log)) Now we can call it at the end of the for loop: python console_log = ConsoleLog(lines_up_on_end=1) for ... in tqdm(data_gen, total=n_batches, desc=\\"Epoch {}\\".format(epoch), bar_format=config.bar_format): console_log.print( [ (\\"\\", \\"\\"), (\\"d_loss\\", d_loss.item()), (\\"- d_wgan_gp_loss\\", d_wgan_gp_loss.item()), (\\"- d_cls_loss_on_real\\", cls_loss_on_real.item()), (\\"\\", \\"\\"), (\\"g_loss\\", g_loss.item()), (\\"- g_spatial_cat_similarity_loss\\", g_spatial_cat_similarity_loss.item()), (\\"- g_cycle_loss\\", g_cycle_loss.item()), (\\"- g_mask_cycle_loss\\", g_mask_cycle_loss.item()), (\\"- g_mask_vanishing_loss\\", g_mask_vanishing_loss.item()), (\\"- g_mask_spatial_constraint_loss\\", g_mask_spatial_constraint_loss.item()), (\\"- g_cls_loss_on_gened\\", g_cls_loss_on_gened.item()), (\\"- g_wgan_gp_loss\\", g_wgan_gp_loss.item()) ]) # depends on whether we have next for loop: # console_log.clear_log_on_epoch_end() Here lines_up_on_end is provided because tqjm also needs to upate on its line, so we will need lines_up_on_end to be 1, normally it is 0. ","title":"Logging Without Printing New Lines","date":"2022-05-23T00:00:00.000Z","id":"blog077","tag":"python","intro":"Create a console log that freeze the position but keep updating the numerics in training."},{"content":" #### Syntax In training we can create a generator by data_gen = iter(dataloader). By using tqdm we can create a progress bar to visuallize the progress of the current Epoch: python for ... in tqdm(data_gen, total=n_batches, desc=\\"Epoch {}\\".format(epoch), bar_format=config.bar_format): ... - total Since generator cannot provide the total legnth, we must provide total as kwarg. - Note that total depends on drop_last in DataLoader (in pytorch). - desc is easy to understand. #### bar_format The bar_format we use can be formatted by: text \\"{desc}: {percentage:.1f}%|{bar:15}| {n}/{total_fmt} [{elapsed}, {rate_fmt}{postfix}]\\" This will show the information as shown in the picture: <center> <a href=\\"/assets/tech/053.png\\"> <img width=\\"600\\" src=\\"/assets/tech/053.png\\"/> </a> </center> <p></p> ","title":"tqdm: The Progress bar for Iterations","date":"2022-05-27T00:00:00.000Z","id":"blog079","tag":"python","intro":"Record the use of a progress bar when using for loop, and record how to customize the output."},{"content":" #### Masks Suppose that - target_masks is of shape (N, width, height, 10, 2), - where N denotes the number of batches, - width, height denote the size of a feature, - and for every given n, j, i, k, target_masks[n, j, i, k] is a one-hot vector. Then we can create a mask by python positive_sample = target_masks[..., 1] == 1 This will create a mask of shape (N, width, height, 10) in which all values are boolean, but how do we know all the indexes n, j, i, k that the mask indicates as True? Sometimes the mask itself is enough to filter out other related np.array\'s (by applying the mask like other_nparray[positive_sample]), but sometimes the spartial index j, i also provides us the information we need. For that we will use the following function: #### get_indexes_from_mask(mask) This function is as simple as: python def get_indexes_from_mask(mask): return np.array(list(zip(*np.where(mask)))) For example, suppose that each position j, i was assigned a box in boxes with shape (N, height, width, 4) and a score in scores with shape (N, height, width, 1), then we can create a mask by python score_mask = scores[..., 0] > 0.8 Now apart from getting the desired scores by - selected_scores=scores[score_mask] (this will be **_flattened_** and of shape (n\', 1)), we can equivalently: - python selected_indexes = get_indexes_from_mask(score_mask) selected_scores = np.array([scores[tuple(index)] for index in selected_indexes]) Note that - numpy array does not accept int-np-array as an index (unlike bool-np-array), - We need to convert int-np-array into int-tuple. The conversion is simply by tuple(np-array). #### tf.where, tf.gather (to be updated) Equivalently we can use tf.where and tf.gather. ","title":"Masks in Numpy and Retrive Corresponding Indexes","date":"2022-05-28T00:00:00.000Z","id":"blog080","tag":"python","intro":"Mask in numpy, tensorflow and pytorch is very useful to filter out desired values, but sometimes the index that the mask marks as True is also import, we record a function to retrive those information."},{"content":" #### CV2 - Read image and correct channels python img = cv2.imread(img_path) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) - Draw rectangle python cv2.rectangle(img, (x1, y1), (x2, y2), config.bbox_color, config.bbox_stroke) - Write image into disk: python cv2.imwrite(img_path, img) - Fill a polygon by coordinates: python # [[x1,y1], [x2,y2], ... ] points = np.array( [[[10,10],[100,10],[100,100],[10,100]]], dtype=np.int32 ) img = np.zeros([240,320],dtype=np.uint8) cv2.fillPoly(img, pts=points, color=(0, 255, 0)) #### Pillow - Read image python from PIL import Image img = Image.open(img_path) - Resize image python img = img.resize((new_width, new_height), Image.BILINEAR) - Rotate image python img = img.rotate(angle, Image.BILINEAR) # in degree - Get image height and width: python h, w = img.height, img.width - Crop image: python box = (start_w, start_h, start_w + length, start_h + length) region = img.crop(box) - Draw bounding box: python from PIL import ImageDraw draw = ImageDraw.Draw(img) # img: PIL.Image.Image for box in boxes: xmin, ymin, xmax, ymax = box draw.rectangle(((xmin, ymin), (xmax, ymax)), outline=(255,255,255,150), width=1) #### shapely.geometry.Polygon To check intersection of two polygonal regions, we can: python from shapely.geometry import Polygon poly1 = np.array([[x1, y1, x2, y2, x3, y3, x4, y4]]) poly2 = np.array([[x1_, y1_, x2_, y2_, x3_, y3_, x4_, y4_]]) p1 = Polygon(poly1).convex_hull p2 = Polygon(poly2).convex_hull inter_area = p1.intersection(p2).area iou = inter_area / (p1.area + p2.area - inter_area) ","title":"CV2, Pillow and shapely for Polygons","date":"2022-05-28T00:00:00.000Z","id":"blog081","tag":"python","intro":"Record usual api for drawing apis in cv2, pillow and shapely.geometry."},{"content":" #### Results <center> <a href=\\"/assets/tech/054.jpg\\" target=\\"_blank\\"> <img width=\\"600\\" src=\\"/assets/tech/054.jpg\\"/> </a> </center> #### dataset.py ##### Import python from shapely.geometry import Polygon from abc import ABC, abstractmethod from torch.utils.data import Dataset from PIL import Image from torchvision.transforms import transforms import torch import os import numpy as np import math import cv2 ##### CustomDataset python class CustomDataset(Dataset): def __init__(self, img_path, gt_path, scale=0.25, length=512): super(CustomDataset, self).__init__() self.img_files = [] for img_file in sorted(os.listdir(img_path)): if img_file.endswith(\\".jpg\\") or img_file.endswith(\\".png\\"): self.img_files.append(os.path.join(img_path, img_file)) self.gt_files = [] for gt_file in sorted(os.listdir(gt_path)): if gt_file.endswith(\\".txt\\"): self.gt_files.append(os.path.join(gt_path, gt_file)) self.scale = scale self.length = length def __getitem__(self, index): with open(self.gt_files[index], \'r\', encoding=\\"utf-8\\") as f: lines = f.readlines() vertices, labels = extract_vertices(lines) img = Image.open(self.img_files[index]) img, vertices = adjust_height(img, vertices) img, vertices = rotate_img(img, vertices) img, vertices = crop_img(img, vertices, labels, self.length) transform = transforms.Compose([transforms.ColorJitter(0.5, 0.5, 0.5, 0.25), transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))]) score_map, geo_map, ignored_map = get_score_geo(img, vertices, labels, self.scale, self.length) return transform(img), score_map, geo_map, ignored_map def __len__(self): return len(self.img_files) ##### extract_vertices python def extract_vertices(lines): \'\'\'extract vertices info from txt lines Input: lines : list of string info Output: vertices: vertices of text regions <numpy.ndarray, (n,8)> labels : 1->valid, 0->ignore, <numpy.ndarray, (n,)> \'\'\' labels = [] vertices = [] for line in lines: vertices.append(list(map(int, line.rstrip(\'\\\\n\').lstrip(\'\\\\ufeff\').split(\',\')[:8]))) label = 0 if \'###\' in line else 1 labels.append(label) return np.array(vertices), np.array(labels) ##### adjust_height python def adjust_height(img, vertices, ratio=0.2): \'\'\'adjust height of image to aug data Input: img : PIL Image vertices : vertices of text regions <numpy.ndarray, (n,8)> ratio : height changes in [0.8, 1.2] Output: img : adjusted PIL Image new_vertices: adjusted vertices \'\'\' ratio_h = 1 + ratio * (np.random.rand() * 2 - 1) old_h = img.height new_h = int(np.around(old_h * ratio_h)) img = img.resize((img.width, new_h), Image.BILINEAR) # PIL api (caution, widthxheight) new_vertices = vertices.copy() if vertices.size > 0: new_vertices[:, [1, 3, 5, 7]] = vertices[:, [1, 3, 5, 7]] * (new_h / old_h) return img, new_vertices ##### rotate_img python def rotate_img(img, vertices, angle_range=10): \'\'\'rotate image [-10, 10] degree to aug data Input: img : PIL Image vertices : vertices of text regions <numpy.ndarray, (n,8)> angle_range : rotate range Output: img : rotated PIL Image new_vertices: rotated vertices \'\'\' center_x = (img.width - 1) / 2 center_y = (img.height - 1) / 2 angle = angle_range * (np.random.rand() * 2 - 1) # from -10 to 10 img = img.rotate(angle, Image.BILINEAR) # PIL api new_vertices = np.zeros(vertices.shape) for i, vertice in enumerate(vertices): new_vertices[i, :] = rotate_vertices(vertice, -angle / 180 * math.pi, np.array([[center_x], [center_y]])) return img, new_vertices ##### rotate_vertices python def rotate_vertices(vertices, theta, anchor=None): \'\'\'rotate vertices around anchor Input: vertices: vertices of text region <numpy.ndarray, (8,)> theta : angle in radian measure anchor : fixed position during rotation Output: rotated vertices <numpy.ndarray, (8,)> \'\'\' v = vertices.reshape((4, 2)).T if anchor is None: anchor = v[:, :1] rotate_mat = get_rotate_mat(theta) res = np.dot(rotate_mat, v - anchor) return (res + anchor).T.reshape(-1) ##### get_rotate_mat python def get_rotate_mat(theta): \'\'\'positive theta value means rotate clockwise\'\'\' return np.array([[math.cos(theta), -math.sin(theta)], [math.sin(theta), math.cos(theta)]]) ##### crop_img python def crop_img(img, vertices, labels, length): \'\'\'crop img patches to obtain batch and augment Input: img : PIL Image vertices : vertices of text regions <numpy.ndarray, (n,8)> labels : 1->valid, 0->ignore, <numpy.ndarray, (n,)> length : length of cropped image region Output: region : cropped image region new_vertices: new vertices in cropped region \'\'\' h, w = img.height, img.width # confirm the shortest side of image >= length if h >= w and w < length: img = img.resize((length, int(h * length / w)), Image.BILINEAR) elif h < w and h < length: img = img.resize((int(w * length / h), length), Image.BILINEAR) ratio_w = img.width / w ratio_h = img.height / h assert(ratio_w >= 1 and ratio_h >= 1) new_vertices = np.zeros(vertices.shape) if vertices.size > 0: new_vertices[:, [0, 2, 4, 6]] = vertices[:, [0, 2, 4, 6]] * ratio_w new_vertices[:, [1, 3, 5, 7]] = vertices[:, [1, 3, 5, 7]] * ratio_h # find random position remain_h = img.height - length remain_w = img.width - length flag = True cnt = 0 while flag and cnt < 1000: cnt += 1 start_w = int(np.random.rand() * remain_w) start_h = int(np.random.rand() * remain_h) flag = is_cross_text([start_w, start_h], length, new_vertices[labels == 1, :]) box = (start_w, start_h, start_w + length, start_h + length) region = img.crop(box) if new_vertices.size == 0: return region, new_vertices new_vertices[:, [0, 2, 4, 6]] -= start_w new_vertices[:, [1, 3, 5, 7]] -= start_h return region, new_vertices ##### is_cross_text python def is_cross_text(start_loc, length, vertices): \'\'\'check if the crop image crosses text regions Input: start_loc: left-top position length : length of crop image vertices : vertices of text regions <numpy.ndarray, (n,8)> Output: True if crop image crosses text region \'\'\' if vertices.size == 0: return False start_w, start_h = start_loc a = np.array([start_w, start_h, start_w + length, start_h, start_w + length, start_h + length, start_w, start_h + length]).reshape((4, 2)) p1 = Polygon(a).convex_hull epsilon = 1e-6 for vertice in vertices: p2 = Polygon(vertice.reshape((4, 2))).convex_hull inter = p1.intersection(p2).area if 0.01 <= inter / (p2.area + epsilon) <= 0.99: return True return False ##### get_score_geo python def get_score_geo(img, vertices, labels, scale, length): \'\'\'generate score gt and geometry gt Input: img : PIL Image vertices: vertices of text regions <numpy.ndarray, (n,8)> labels : 1->valid, 0->ignore, <numpy.ndarray, (n,)> scale : feature map / image length : image length Output: score gt, geo gt, ignored \'\'\' score_map = np.zeros((int(img.height * scale), int(img.width * scale), 1), np.float32) geo_map = np.zeros((int(img.height * scale), int(img.width * scale), 5), np.float32) ignored_map = np.zeros((int(img.height * scale), int(img.width * scale), 1), np.float32) index = np.arange(0, length, int(1 / scale)) index_x, index_y = np.meshgrid(index, index) ignored_polys = [] polys = [] for i, vertice in enumerate(vertices): if labels[i] == 0: ignored_polys.append(np.around(scale * vertice.reshape((4, 2))).astype(np.int32)) continue poly = np.around(scale * shrink_poly(vertice).reshape((4, 2))).astype(np.int32) # scaled & shrinked polys.append(poly) temp_mask = np.zeros(score_map.shape[:-1], np.float32) cv2.fillPoly(temp_mask, [poly], 1) theta = find_min_rect_angle(vertice) rotate_mat = get_rotate_mat(theta) rotated_vertices = rotate_vertices(vertice, theta) x_min, x_max, y_min, y_max = get_boundary(rotated_vertices) rotated_x, rotated_y = rotate_all_pixels(rotate_mat, vertice[0], vertice[1], length) # given p in Polygon(vertice), top_distance = p - Pr_{top_L}(p) = r(p)_y - r(Pr_{top_L}(p))_y = r(p)_y - ymin # where r is the rotation anchored at the top-left corner # and p in Polygon(vertice) only if r(p)_y - ymin >= 0 # d1 = distance from top to point (j, i) # the gt is top, bottom, left, right (\u4e0a, \u4e0b, \u5de6, \u53f3) d1 = rotated_y - y_min d1[d1 < 0] = 0 d2 = y_max - rotated_y d2[d2 < 0] = 0 d3 = rotated_x - x_min d3[d3 < 0] = 0 d4 = x_max - rotated_x d4[d4 < 0] = 0 geo_map[:, :, 0] += d1[index_y, index_x] * temp_mask geo_map[:, :, 1] += d2[index_y, index_x] * temp_mask geo_map[:, :, 2] += d3[index_y, index_x] * temp_mask geo_map[:, :, 3] += d4[index_y, index_x] * temp_mask geo_map[:, :, 4] += theta * temp_mask cv2.fillPoly(ignored_map, ignored_polys, 1) cv2.fillPoly(score_map, polys, 1) return torch.Tensor(score_map).permute(2, 0, 1), torch.Tensor(geo_map).permute(2, 0, 1), torch.Tensor(ignored_map).permute(2, 0, 1) ##### shrink_poly python-1 def shrink_poly(vertices, coef=0.3): \'\'\'shrink the text region Input: vertices: vertices of text region <numpy.ndarray, (8,)> coef : shrink ratio in paper Output: v : vertices of shrinked text region <numpy.ndarray, (8,)> \'\'\' x1, y1, x2, y2, x3, y3, x4, y4 = vertices r1 = min(cal_distance(x1, y1, x2, y2), cal_distance(x1, y1, x4, y4)) r2 = min(cal_distance(x2, y2, x1, y1), cal_distance(x2, y2, x3, y3)) r3 = min(cal_distance(x3, y3, x2, y2), cal_distance(x3, y3, x4, y4)) r4 = min(cal_distance(x4, y4, x1, y1), cal_distance(x4, y4, x3, y3)) r = [r1, r2, r3, r4] # obtain offset to perform move_points() automatically if cal_distance(x1, y1, x2, y2) + cal_distance(x3, y3, x4, y4) > \\\\ cal_distance(x2, y2, x3, y3) + cal_distance(x1, y1, x4, y4): offset = 0 # two longer edges are (x1y1-x2y2) & (x3y3-x4y4) else: offset = 1 # two longer edges are (x2y2-x3y3) & (x4y4-x1y1) v = vertices.copy() v = move_points(v, 0 + offset, 1 + offset, r, coef) - The movement is always parellel to the edges. - In each move_points, two adjacent vectice will be pushed towards each other. - Each vertex will be adjusted twice in two directions in order to move towards center. python-25 v = move_points(v, 2 + offset, 3 + offset, r, coef) v = move_points(v, 1 + offset, 2 + offset, r, coef) v = move_points(v, 3 + offset, 4 + offset, r, coef) return v ##### find_min_rect_angle python def find_min_rect_angle(vertices): \'\'\'find the best angle to rotate poly and obtain min rectangle Input: vertices: vertices of text region <numpy.ndarray, (8,)> Output: the best angle <radian measure> \'\'\' angle_interval = 1 angle_list = list(range(-90, 90, angle_interval)) area_list = [] for theta in angle_list: rotated = rotate_vertices(vertices, theta / 180 * math.pi) x1, y1, x2, y2, x3, y3, x4, y4 = rotated temp_area = (max(x1, x2, x3, x4) - min(x1, x2, x3, x4)) * \\\\ (max(y1, y2, y3, y4) - min(y1, y2, y3, y4)) area_list.append(temp_area) sorted_area_index = sorted(list(range(len(area_list))), key=lambda k: area_list[k]) min_error = float(\'inf\') best_index = -1 rank_num = 10 # find the best angle with correct orientation for index in sorted_area_index[:rank_num]: rotated = rotate_vertices(vertices, angle_list[index] / 180 * math.pi) temp_error = cal_error(rotated) if temp_error < min_error: min_error = temp_error best_index = index return angle_list[best_index] / 180 * math.pi ##### cal_distance python def cal_distance(x1, y1, x2, y2): \'\'\'calculate the Euclidean distance\'\'\' return math.sqrt((x1 - x2)**2 + (y1 - y2)**2) ##### get_boundary python def get_boundary(vertices): \'\'\'get the tight boundary around given vertices Input: vertices: vertices of text region <numpy.ndarray, (8,)> Output: the boundary \'\'\' x1, y1, x2, y2, x3, y3, x4, y4 = vertices x_min = min(x1, x2, x3, x4) x_max = max(x1, x2, x3, x4) y_min = min(y1, y2, y3, y4) y_max = max(y1, y2, y3, y4) return x_min, x_max, y_min, y_max ##### rotate_all_pixels python def rotate_all_pixels(rotate_mat, anchor_x, anchor_y, length): \'\'\'get rotated locations of all pixels for next stages Input: rotate_mat: rotatation matrix anchor_x : fixed x position anchor_y : fixed y position length : length of image Output: rotated_x : rotated x positions <numpy.ndarray, (length,length)> rotated_y : rotated y positions <numpy.ndarray, (length,length)> \'\'\' x = np.arange(length) y = np.arange(length) x, y = np.meshgrid(x, y) x_lin = x.reshape((1, x.size)) y_lin = y.reshape((1, x.size)) coord_mat = np.concatenate((x_lin, y_lin), 0) rotated_coord = np.dot(rotate_mat, coord_mat - np.array([[anchor_x], [anchor_y]])) + \\\\ np.array([[anchor_x], [anchor_y]]) rotated_x = rotated_coord[0, :].reshape(x.shape) rotated_y = rotated_coord[1, :].reshape(y.shape) return rotated_x, rotated_y ##### move_points python def move_points(vertices, index1, index2, r, coef): \'\'\'move the two points to shrink edge Input: vertices: vertices of text region <numpy.ndarray, (8,)> index1 : offset of point1 index2 : offset of point2 r : [r1, r2, r3, r4] in paper coef : shrink ratio in paper Output: vertices: vertices where one edge has been shinked \'\'\' index1 = index1 % 4 index2 = index2 % 4 x1_index = index1 * 2 + 0 y1_index = index1 * 2 + 1 x2_index = index2 * 2 + 0 y2_index = index2 * 2 + 1 r1 = r[index1] r2 = r[index2] length_x = vertices[x1_index] - vertices[x2_index] length_y = vertices[y1_index] - vertices[y2_index] length = cal_distance(vertices[x1_index], vertices[y1_index], vertices[x2_index], vertices[y2_index]) if length > 1: ratio = (r1 * coef) / length vertices[x1_index] += ratio * (-length_x) vertices[y1_index] += ratio * (-length_y) ratio = (r2 * coef) / length vertices[x2_index] += ratio * length_x vertices[y2_index] += ratio * length_y return vertices ##### cal_error python def cal_error(vertices): \'\'\'default orientation is x1y1 : left-top, x2y2 : right-top, x3y3 : right-bot, x4y4 : left-bot calculate the difference between the vertices orientation and default orientation Input: vertices: vertices of text region <numpy.ndarray, (8,)> Output: err : difference measure \'\'\' x_min, x_max, y_min, y_max = get_boundary(vertices) x1, y1, x2, y2, x3, y3, x4, y4 = vertices err = cal_distance(x1, y1, x_min, y_min) + cal_distance(x2, y2, x_max, y_min) + \\\\ cal_distance(x3, y3, x_max, y_max) + cal_distance(x4, y4, x_min, y_max) return err #### losses.py ##### Import python import torch import torch.nn as nn ##### get_dice_loss python def get_dice_loss(gt_score, pred_score): inter = torch.sum(gt_score * pred_score) union = torch.sum(gt_score) + torch.sum(pred_score) + 1e-5 return 1. - (2 * inter / union) ##### get_geo_loss python def get_geo_loss(gt_geo, pred_geo): d1_gt, d2_gt, d3_gt, d4_gt, angle_gt = torch.split(gt_geo, 1, 1) d1_pred, d2_pred, d3_pred, d4_pred, angle_pred = torch.split(pred_geo, 1, 1) area_gt = (d1_gt + d2_gt) * (d3_gt + d4_gt) area_pred = (d1_pred + d2_pred) * (d3_pred + d4_pred) w_union = torch.min(d3_gt, d3_pred) + torch.min(d4_gt, d4_pred) h_union = torch.min(d1_gt, d1_pred) + torch.min(d2_gt, d2_pred) area_intersect = w_union * h_union area_union = area_gt + area_pred - area_intersect iou_loss_map = -torch.log((area_intersect + 1.0) / (area_union + 1.0)) angle_loss_map = 1 - torch.cos(angle_pred - angle_gt) return iou_loss_map, angle_loss_map ##### Loss python class Loss(nn.Module): def __init__(self, weight_angle=10): super(Loss, self).__init__() self.weight_angle = weight_angle def forward(self, gt_score, pred_score, gt_geo, pred_geo, ignored_map): if torch.sum(gt_score) < 1: return torch.sum(pred_score + pred_geo) * 0 classify_loss = get_dice_loss(gt_score, pred_score * (1 - ignored_map)) iou_loss_map, angle_loss_map = get_geo_loss(gt_geo, pred_geo) angle_loss = torch.sum(angle_loss_map * gt_score) / torch.sum(gt_score) iou_loss = torch.sum(iou_loss_map * gt_score) / torch.sum(gt_score) geo_loss = self.weight_angle * angle_loss + iou_loss return geo_loss + classify_loss #### train.py ##### import python import torch from torch.utils import data from torch import nn from torch.optim import lr_scheduler from dataset import CustomDataset from detect import performance_check from models import EAST from losses import Loss from tqdm import tqdm from device import device from utils import ConsoleLog import os import time ##### train python console_log = ConsoleLog(lines_up_on_end=1) def train(train_img_path, train_gt_path, pths_path, batch_size, lr, num_workers, epoch_iter, interval): file_num = len(os.listdir(train_img_path)) trainset = CustomDataset(train_img_path, train_gt_path) train_loader = data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True) criterion = Loss() model = EAST() data_parallel = False if torch.cuda.device_count() > 1: model = nn.DataParallel(model) data_parallel = True model.to(device) optimizer = torch.optim.Adam(model.parameters(), lr=lr) scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[epoch_iter // 2], gamma=0.1) for epoch in range(epoch_iter): model.train() epoch_loss = 0 epoch_time = time.time() for batch, (img, gt_score, gt_geo, ignored_map) in enumerate(tqdm( train_loader, total=len(trainset) // batch_size, bar_format=\\"{desc}: {percentage:.1f}%|{bar:15}| {n}/{total_fmt} [{elapsed}, {rate_fmt}{postfix}]\\" )): start_time = time.time() img = img.to(device) gt_score = gt_score.to(device) gt_geo = gt_geo.to(device) ignored_map = ignored_map.to(device) pred_score, pred_geo = model(img) loss = criterion(gt_score, pred_score, gt_geo, pred_geo, ignored_map) epoch_loss += loss.item() optimizer.zero_grad() loss.backward() optimizer.step() scheduler.step() if (batch + 1) % save_interval == 0: performance_check(model, save_image_path=\\"results/epoch_{}_batch_{}.jpg\\".format(epoch, batch + 1)) console_log.print( \'Epoch is [{}/{}], mini-batch is [{}/{}], time consumption is {:.8f}, batch_loss is {:.8f}\'.format( epoch + 1, epoch_iter, batch + 1, int(file_num / batch_size), time.time() - start_time, loss.item()), is_key_value=False ) if (epoch + 1) % interval == 0: state_dict = model.module.state_dict() if data_parallel else model.state_dict() torch.save(state_dict, os.path.join(pths_path, \'model_epoch_{}.pth\'.format(epoch + 1))) if __name__ == \'__main__\': train_img_path = os.path.abspath(\'dataset/images\') train_gt_path = os.path.abspath(\'dataset/annotations\') pths_path = \'./pths\' batch_size = 24 lr = 1e-3 num_workers = 4 epoch_iter = 600 save_interval = 5 train(train_img_path, train_gt_path, pths_path, batch_size, lr, num_workers, epoch_iter, save_interval) #### detect.py ##### Import python from torchvision import transforms from PIL import Image, ImageDraw from models import EAST from dataset import get_rotate_mat from utils import nms_locality from device import device import config import torch import os import numpy as np import random ##### resize_img python def resize_img(img): \'\'\'resize image to be divisible by 32 \'\'\' w, h = img.size resize_w = w resize_h = h resize_h = resize_h if resize_h % 32 == 0 else int(resize_h / 32) * 32 resize_w = resize_w if resize_w % 32 == 0 else int(resize_w / 32) * 32 img = img.resize((resize_w, resize_h), Image.BILINEAR) ratio_h = resize_h / h ratio_w = resize_w / w return img, ratio_h, ratio_w ##### load_pil python def load_pil(img): \'\'\'convert PIL Image to torch.Tensor \'\'\' t = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))]) return t(img).unsqueeze(0) ##### is_valid_poly python def is_valid_poly(res, score_shape, scale): \'\'\'check if the poly in image scope Input: res : restored poly in original image score_shape: score map shape scale : feature map -> image Output: True if valid \'\'\' cnt = 0 for i in range(res.shape[1]): if res[0, i] < 0 or res[0, i] >= score_shape[1] * scale or \\\\ res[1, i] < 0 or res[1, i] >= score_shape[0] * scale: cnt += 1 return True if cnt <= 1 else False ##### restore_polys python def restore_polys(valid_pos, valid_geo, score_shape, scale=4): \'\'\'restore polys from feature maps in given positions Input: valid_pos : potential text positions <numpy.ndarray, (n,2)> valid_geo : geometry in valid_pos <numpy.ndarray, (5,n)> score_shape: shape of score map scale : image / feature map Output: restored polys <numpy.ndarray, (n,8)>, index \'\'\' polys = [] index = [] valid_pos *= scale d = valid_geo[:4, :] # 4 x N angle = valid_geo[4, :] # N, for i in range(valid_pos.shape[0]): x = valid_pos[i, 0] y = valid_pos[i, 1] y_min = y - d[0, i] y_max = y + d[1, i] x_min = x - d[2, i] x_max = x + d[3, i] rotate_mat = get_rotate_mat(-angle[i]) temp_x = np.array([[x_min, x_max, x_max, x_min]]) - x temp_y = np.array([[y_min, y_min, y_max, y_max]]) - y coordidates = np.concatenate((temp_x, temp_y), axis=0) res = np.dot(rotate_mat, coordidates) res[0, :] += x res[1, :] += y if is_valid_poly(res, score_shape, scale): index.append(i) polys.append([res[0, 0], res[1, 0], res[0, 1], res[1, 1], res[0, 2], res[1, 2], res[0, 3], res[1, 3]]) return np.array(polys), index ##### get_boxes python def get_boxes(score, geo, score_thresh=config.detection_score_threshold, nms_thresh=0.2): \'\'\'get boxes from feature map Input: score : score map from model <numpy.ndarray, (1,row,col)> geo : geo map from model <numpy.ndarray, (5,row,col)> score_thresh: threshold to segment score map nms_thresh : threshold in nms Output: boxes : final polys <numpy.ndarray, (n,9)> \'\'\' score = score[0, :, :] xy_text = np.argwhere(score > score_thresh) # n x 2, format is [r, c] if xy_text.size == 0: return None xy_text = xy_text[np.argsort(xy_text[:, 0])] valid_pos = xy_text[:, ::-1].copy() # n x 2, [x, y] # Due to ::-1, pos is now following (x, y) = (i, j) notational convention valid_geo = geo[:, xy_text[:, 0], xy_text[:, 1]] # 5 x n # So is valid_geo polys_restored, index = restore_polys(valid_pos, valid_geo, score.shape) if polys_restored.size == 0: return None boxes = np.zeros((polys_restored.shape[0], 9), dtype=np.float32) boxes[:, :8] = polys_restored boxes[:, 8] = score[xy_text[index, 0], xy_text[index, 1]] boxes = nms_locality(boxes.astype(\'float32\'), nms_thresh) return boxes ##### adjust_ratio python def adjust_ratio(boxes, ratio_w, ratio_h): \'\'\'refine boxes Input: boxes : detected polys <numpy.ndarray, (n,9)> ratio_w: ratio of width ratio_h: ratio of height Output: refined boxes \'\'\' if boxes is None or boxes.size == 0: return None boxes[:, [0, 2, 4, 6]] /= ratio_w boxes[:, [1, 3, 5, 7]] /= ratio_h return np.around(boxes) ##### detect python def detect(img, model, device): \'\'\'detect text regions of img using model Input: img : PIL Image model : detection model device: gpu if gpu is available Output: detected polys \'\'\' img, ratio_h, ratio_w = resize_img(img) with torch.no_grad(): score, geo = model(load_pil(img).to(device)) boxes = get_boxes(score.squeeze(0).cpu().numpy(), geo.squeeze(0).cpu().numpy()) return adjust_ratio(boxes, ratio_w, ratio_h) ##### plot_boxes python def plot_boxes(img, boxes): \'\'\'plot boxes on image \'\'\' if boxes is None: return img draw = ImageDraw.Draw(img) for box in boxes: draw.polygon([box[0], box[1], box[2], box[3], box[4], box[5], box[6], box[7]], outline=(0, 255, 0)) return img ##### detect_dataset python def detect_dataset(model, device, test_img_path, submit_path): \'\'\'detection on whole dataset, save .txt results in submit_path Input: model : detection model device : gpu if gpu is available test_img_path: dataset path submit_path : submit result for evaluation \'\'\' img_files = os.listdir(test_img_path) img_files = sorted([os.path.join(test_img_path, img_file) for img_file in img_files]) for i, img_file in enumerate(img_files): print(\'evaluating {} image\'.format(i), end=\'\\\\r\') boxes = detect(Image.open(img_file), model, device) seq = [] if boxes is not None: seq.extend([\',\'.join([str(int(b)) for b in box[:-1]]) + \'\\\\n\' for box in boxes]) with open(os.path.join(submit_path, \'res_\' + os.path.basename(img_file).replace(\'.jpg\', \'.txt\')), \'w\') as f: f.writelines(seq) ##### performance_check python def performance_check(model, save_image_path): model.eval() images = os.listdir(\\"dataset/images\\") random.shuffle(images) img = Image.open(\\"dataset/images/{}\\".format(images[0])) boxes = detect(img, model, device) plot_img = plot_boxes(img, boxes) plot_img.save(save_image_path) plot_img.save(\\"results/latest_output.jpg\\") model.train() #### device python import torch device = torch.device(\\"cuda:0\\" if torch.cuda.is_available() else \\"cpu\\") #### utils ##### Import python from pydash.objects import get, set_ from shapely.geometry import Polygon import numpy as np ##### iou python def iou(g, p): g = Polygon(g[:8].reshape((4, 2))) p = Polygon(p[:8].reshape((4, 2))) if not g.is_valid or not p.is_valid: return 0 inter = Polygon(g).intersection(Polygon(p)).area union = g.area + p.area - inter if union == 0: return 0 else: return inter / union ##### weighted_merge python def weighted_merge(g, p): g[:8] = (g[8] * g[:8] + p[8] * p[:8]) / (g[8] + p[8]) g[8] = (g[8] + p[8]) return g ##### standard_nms python def standard_nms(S, thres): order = np.argsort(S[:, 8])[::-1] keep = [] while order.size > 0: i = order[0] keep.append(i) ious = np.array([iou(S[i], S[t]) for t in order[1:]]) inds = np.where(ious <= thres)[0] # since order[0] is taken out order = order[inds + 1] return S[keep] ##### nms_locality python def nms_locality(polys, thres=0.3): \'\'\' locality aware nms of EAST :param polys: a N*9 numpy array. first 8 coordinates, then prob :return: boxes after nms \'\'\' S = [] p = None for g in polys: if p is not None and iou(g, p) > thres: p = weighted_merge(g, p) else: if p is not None: S.append(p) p = g if p is not None: S.append(p) if len(S) == 0: return np.array([]) return standard_nms(np.array(S), thres) ","title":"EAST: A Text Detection Algorithm","date":"2022-05-29T00:00:00.000Z","id":"blog082","tag":"pytorch, deep-learning","intro":"Break down the code for my own study. e.g., the way it does image cropping, resizing, etc augmentations while keeping the vertice consistent are very valuable reference, the way it does nms is also succint! ***Source Code:*** <a href=\'https://github.com/SakuraRiven/EAST\'>Link</a>"},{"content":" #### Repo - Feature of Single-Scale: https://github.com/machingclee/Minimal-Code-for-Faster-RCNN-in-pytorch - Feature of Multi-Scale with Feature Pyramid Network https://github.com/machingclee/Minimal-Code-for-Faster-RCNN-with-FPN-in-pytorch #### Results Here the faded white boxes are the ROIs, and blue boxes are refined ROIs that are estimated from the feature of ROIAlign module. <div> <center> <a href=\\"/assets/tech/057.jpg\\"> <img src=\\"/assets/tech/057.jpg\\" width=\\"45%\\" style=\\"margin-right:10pt\\"/> </a> <a href=\\"/assets/tech/058.jpg\\"> <img src=\\"/assets/tech/058.jpg\\" width=\\"45%\\"/> </a> </center> </div> #### Model Structure <a href=\\"/assets/tech/056.png\\" target=\\"_blank\\"> <img src=\\"/assets/tech/056.png\\" width=\\"100%\\"/> </a> #### How to Read the Source Code It is not easy to explain everything in a blog post. Rather one can delve into the source code and see how it works! Reader can treat src/faster_rcnn.py as an entry point, the class FasterRCNN is our target result. The FasterRCNN.forward method behaves differently when it is in: - **Training Mode.** It returns - rpn_cls_loss - rpn_reg_loss - roi_cls_loss - roi_reg_loss - **Evaluation Mode.** It returns - scores - roi_refined_box - cls_idxes - rois (no use, debugging purpose) ","title":"Faster RCNN in PyTorch","date":"2022-06-27T00:00:00.000Z","id":"blog084","tag":"pytorch, deep-learning","intro":"Minimal functioning implementation of faster rcnn (with and without fpn)."},{"content":" We define the transform: python albumentation_transform = A.Compose([ A.ShiftScaleRotate(shift_limit=0, rotate_limit=10, p=0.7), A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.9), A.HorizontalFlip(p=0.5), A.GaussNoise(p=0.5), A.RandomBrightnessContrast(p=0.5), A.RandomGamma(p=0.5), A.OneOf([ A.Blur(blur_limit=3, p=0.5), A.ColorJitter(p=0.5) ], p=0.8), A.LongestMaxSize(max_size=config.input_height, interpolation=1, p=1), A.PadIfNeeded( min_height=config.input_height, min_width=config.input_height, border_mode=0, value=(0, 0, 0), position=\\"top_left\\" ), ], p=1, bbox_params=A.BboxParams(format=\\"pascal_voc\\", min_area=0.1) ) In this way the combination of A.LongestMaxSize and A.PadIfNeeded have - resized the image and - padded the image into a square so that our numpy array is of suitable shape and ready to be fed into our network. Now we transform the image and coordinate of bounding box at the same time: python def data_augmentation(img, bboxes): if isinstance(img, Image.Image): img = np.array(img) transformed = albumentation_transform(image=img, bboxes=bboxes) img = transformed[\\"image\\"] bboxes = transformed[\\"bboxes\\"] return img, bboxes Note that bboxes is of type: python bboxes: List[List[float, float, float, float, int | string]] Since our format is pascal_voc, the 4 float\'s there are (xmin, ymin, xmax, ymax). The final coordinate bboxes[:, 4] is usually the class labels of the bounding boxes, it will not be transformed by albumentation and will be kept unchanged there. ","title":"Data Augmentation for Object Detection","date":"2022-06-30T00:00:00.000Z","id":"blog085","tag":"pytorch, deep-learning","toc":false,"intro":"Record my data augmentation used in object detection."},{"content":" #### How to find the Slice of Layers Usually we can inspect a model by print(the_model), from that it is easy to find the correct slice indexes of the corresponding layers. #### Backbones ##### VGG-16 python class Vgg16FeatureExtractor(nn.Module): def __init__(self): super(FeatureExtractor, self).__init__() self.vgg = models.vgg16(pretrained=True).to(device) self.features = self.vgg.features self.out_channels = None self.conv_blk1 = self.features[0:4] self.conv_blk2 = self.features[4:9] self.conv_blk3 = self.features[9:16] self.conv_blk4 = self.features[16:23] self.conv_blk5 = self.features[23:29] self.freeze_vgg_bottom_layers() def unfreeze_layers(self, from_layer, to_layer): for layer in list(self.features)[from_layer: to_layer]: if isinstance(layer, nn.Conv2d): for param in layer.parameters(): param.requires_grad = True def freeze_vgg_bottom_layers(self): for layer in (list(self.conv_blk1) + list(self.conv_blk2) + list(self.conv_blk3)): if isinstance(layer, nn.Conv2d): for param in layer.parameters(): param.requires_grad = False def vgg_weight_init_upper_layers(self): for layer in list(self.feature_extraction.children())[9:]: if isinstance(layer, nn.Conv2d): torch.nn.init.normal_(layer.weight, std=0.01) torch.nn.init.constant_(layer.bias, 0) def unfreeze_vgg(self): for param in self.vgg.parameters(): param.requires_grad = True def forward(self, x): x = self.conv_blk1(x) x = self.conv_blk2(x) x = self.conv_blk3(x) x = self.conv_blk4(x) x = self.conv_blk5(x) return x ##### Resnet-34 python class Resnet34FeatureExtractor(nn.Module): def __init__(self): # type: (Backbone) -> None super(FeatureExtractor, self).__init__() self.resnet34 = models.resnet34(pretrained=True).to(device) # self.layer9 = self.resnet34. self.conv1 = self.resnet34.conv1 self.bn1 = self.resnet34.bn1 self.relu = self.resnet34.relu self.maxpool = self.resnet34.maxpool self.layer1 = self.resnet34.layer1 self.layer2 = self.resnet34.layer2 self.layer3 = self.resnet34.layer3 self.freeze_resnet34_bottom_layers() def freeze_resnet34_bottom_layers(self): for layer in ([self.conv1] + list(self.layer1) + list(self.layer2)): if isinstance(layer, nn.Conv2d): for param in layer.parameters(): param.requires_grad = False def forward(self, x): x= self.conv1(x) x= self.bn1(x) x= self.relu(x) x= self.maxpool(x) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) return x ##### Resnet-50-FPN python class ResnetFPNFeactureExtractor(nn.Module): def __init__(self): super(ResnetFPNFeactureExtractor, self).__init__() self.resnet50 = models.resnet50(pretrained=True) self.conv2 = nn.Sequential( self.resnet50.conv1, self.resnet50.bn1, self.resnet50.relu, self.resnet50.maxpool, self.resnet50.layer1 ) self.conv3 = self.resnet50.layer2 self.conv4 = self.resnet50.layer3 self.conv5 = self.resnet50.layer4 self.lateral_conv5 = nn.Conv2d(2048, config.fpn_feat_channels, 1, 1) self.lateral_conv4 = nn.Conv2d(1024, config.fpn_feat_channels, 1, 1) self.lateral_conv3 = nn.Conv2d(512, config.fpn_feat_channels, 1, 1) self.lateral_conv2 = nn.Conv2d(256, config.fpn_feat_channels, 1, 1) self.upscale = lambda input: F.interpolate(input, scale_factor=2) self.freeze_params() def freeze_params(self): modules = [ self.conv2, self.conv3, # self.conv4, # self.conv5 ] for module in modules: for layer in module: if isinstance(layer, nn.Conv2d): for param in layer.parameters(): param.requires_grad = False def forward(self, x): c2 = self.conv2(x) c3 = self.conv3(c2) c4 = self.conv4(c3) c5 = self.conv5(c4) p5 = self.lateral_conv5(c5) p4 = self.lateral_conv4(c4) + self.upscale(p5) p3 = self.lateral_conv3(c3) + self.upscale(p4) p2 = self.lateral_conv2(c2) + self.upscale(p3) return [p2, p3, p4, p5] ","title":"Feature Extractors","date":"2022-07-14T00:00:00.000Z","id":"blog086","tag":"pytorch, deep-learning","intro":"Different backbones have different APIs for extracting features from an image, we record some of them."},{"content":" Given a training function with the following interface: python class TrainingErrorMessage(TypedDict): curr_epoch: int message: Literal[\\"nan_loss\\"] def train( faster_rcnn: FasterRCNNResnet50FPN, lr, start_epoch, epochs, save_weight_interval=5 ) -> None | TrainingErrorMessage: ... for epoch in range(epochs): for batch_id, data in enumerate(tqdm(data_loader)): ... total_loss = ... if torch.isnan(total_loss): return TrainingErrorMessage(message=\\"nan_loss\\", curr_epoch=epoch) ... Then we wrap the function train by python def train_with_nan( faster_rcnn, lr=1e-5, start_epoch=1, epoches=60, save_weight_interval=5 ): continue_training = True restart_ep = start_epoch restart_for_eps = epoches curr_model = faster_rcnn while continue_training: result = train( curr_model, lr, restart_ep, restart_for_eps, save_weight_interval ) if result is not None: message = result[\\"message\\"] if message == \\"nan_loss\\": curr_epoch = result[\\"curr_epoch\\"] if curr_epoch > (start_epoch + epoches): print(\\"stop training\\") continue_training = False else: continue_training = True model_latest_epoch = (curr_epoch-1) - ((curr_epoch-1) % save_weight_interval) restart_ep = model_latest_epoch + 1 restart_for_eps = epoches - (model_latest_epoch - start_epoch) model_path = f\\"pths/model_epoch_{model_latest_epoch}.pth\\" curr_model = FasterRCNNResnet50FPN().to(device) curr_model.load_state_dict(torch.load(model_path)) curr_model.train() print(f\\"Get nan loss, restart training at epoch {restart_ep} for additional {restart_for_eps} epochs\\" + \\"\\\\x1B[0K\\") print(f\\"Loading weight from {model_path}\\" + \\"\\\\x1B[0K\\") else: continue_training = False else: continue_training = False ","title":"Retrain Model when Nan Occurs","date":"2022-07-22T00:00:00.000Z","id":"blog087","tag":"pytorch, deep-learning","toc":false,"intro":"Sometimes a loss becomes nan in rare occasion (e.g., it occurs once per 5~7 epochs), in this case we record a script to restart the training using the latest weight."},{"content":" #### Grahpical Structure of Transformer <Center> <img src=\\"/assets/tech/088-transformer/transformer_structure.png\\" width=\\"550\\"> </Center> #### Constants in the Config python d_model = 512 # Embedding Size d_ff = 2048 # FeedForward dimension d_k = d_v = 64 # dimension of K(=Q), V n_layers = 6 # number of Encoder of Decoder Layer n_heads = 8 # number of heads in Multi-Head Attention #### Positional Encoding <Center> <img src=\\"/assets/tech/088-transformer/positional.png\\" width=\\"550\\"/> </Center> <p/> <center></center> A **positional encoding** $$ \\\\mathrm{PE}: \\\\mathbb {N} \\\\to [0,1]^{d_\\\\text{model}} $$ is a mapping that takes a positional index to a vector of word embedding dimension. In terms of $\\\\texttt{np}$-array, $$ \\\\begin{aligned} \\\\mathrm{PE}{(\\\\texttt{pos},2i)} & = \\\\sin(\\\\texttt{pos} / 10000^{2i/d_{\\\\text{model}}}) \\\\\\\\ \\\\mathrm{PE}{(\\\\texttt{pos},2i+1)} & = \\\\cos(\\\\texttt{pos} / 10000^{2i/d_{\\\\text{model}}}) \\\\end{aligned} $$ where $\\\\texttt{pos} \\\\in \\\\mathbb N$ and $i = 0, 1, 2,\\\\dots, \\\\frac{1}{2} d_\\\\text{model}-1$. python class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout=0.1, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp( torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model) ) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) # unfreeze to prepare for batches of word embeddings pe = pe.unsqueeze(0).transpose(0, 1) self.register_buffer(\'pe\', pe) def forward(self, x): \'\'\' x: [seq_len, batch_size, d_model] \'\'\' x = x + self.pe[:x.size(0), :] return self.dropout(x) #### PoswiseFeedForwardNet <Center> <img src=\\"/assets/tech/088-transformer/poswise.png\\" width=\\"550\\"/> </Center> python class PoswiseFeedForwardNet(nn.Module): def __init__(self): super(PoswiseFeedForwardNet, self).__init__() self.fc = nn.Sequential( nn.Linear(d_model, d_ff, bias=False), nn.ReLU(), nn.Linear(d_ff, d_model, bias=False) ) def forward(self, inputs): \'\'\' inputs: [batch_size, seq_len, d_model] \'\'\' residual = inputs output = self.fc(inputs) # [batch_size, seq_len, d_model] return nn.LayerNorm(d_model).to(device)(output + residual) #### Attentions ##### Scaled Dot Product Attention <Center> <img src=\\"/assets/tech/088-transformer/scaled-dot-product-attention.png\\"/ width=\\"400\\"> </Center> python class ScaledDotProductAttention(nn.Module): def __init__(self): super(ScaledDotProductAttention, self).__init__() def forward(self, Q, K, V, attn_mask): \'\'\' Q: [batch_size, n_heads, len_q, d_k] K: [batch_size, n_heads, len_k, d_k] V: [batch_size, n_heads, len_v(=len_k), d_v] attn_mask: [batch_size, n_heads, seq_len, seq_len] \'\'\' # scores : [batch_size, n_heads, len_q, len_k] scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # Fills elements of self tensor with value where mask is True. scores.masked_fill_(attn_mask, -1e9) attn = nn.Softmax(dim=-1)(scores) # [batch_size, n_heads, len_q, d_v]: context = torch.matmul(attn, V) return context, attn attn_mask is used to mask out value of undesired position such as - those from padding or - those from \\"future word\\" in self-attention module of decoder. ##### Repeated Use of ScaledDotProductAttention: Multi-head Attention python class MultiHeadAttention(nn.Module): def __init__(self): super(MultiHeadAttention, self).__init__() self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False) self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False) self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False) self.fc = nn.Linear(n_heads * d_v, d_model, bias=False) def forward(self, input_Q, input_K, input_V, attn_mask): \'\'\' input_Q: [batch_size, len_q, d_model] input_K: [batch_size, len_k, d_model] input_V: [batch_size, len_v(=len_k), d_model] attn_mask: [batch_size, seq_len, seq_len] \'\'\' residual, batch_size = input_Q, input_Q.size(0) # (B, S, D) -proj-> (B, S, D_new) -split-> (B, S, H, W) -trans-> (B, H, S, W) # Q: [batch_size, n_heads, len_q, d_k] Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2) # K: [batch_size, n_heads, len_k, d_k] K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1, 2) # V: [batch_size, n_heads, len_v(=len_k), d_v] V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1, 2) # attn_mask : [batch_size, n_heads, seq_len, seq_len] attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k] context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask) # context: [batch_size, len_q, n_heads * d_v] context = context.transpose(1, 2).reshape( batch_size, -1, n_heads * d_v) output = self.fc(context) # [batch_size, len_q, d_model] return nn.LayerNorm(d_model).to(device)(output + residual), attn #### Mask Creation <Center> <img src=\\"/assets/tech/088-transformer/module_that_use_mask.png\\"/ width=\\"550\\"> </Center> <p/> <center></center> ##### get_attn_pad_mask python-1 def get_attn_pad_mask(seq_q, seq_k, pad=0): \'\'\' seq_q: [batch_size, seq_len] seq_k: [batch_size, seq_len] seq_len could be src_len or it could be tgt_len seq_len in seq_q and seq_len in seq_k maybe not equal \'\'\' batch_size, len_q = seq_q.size() batch_size, len_k = seq_k.size() # eq(pad) is PAD token # [batch_size, 1, len_k], True is masked pad_attn_mask = seq_k.eq(pad).unsqueeze(1) # [batch_size, len_q, len_k] return pad_attn_mask.expand(batch_size, len_q, len_k) To apply get_attn_pad_mask, input that we want to mask will be fed into the second argument. ##### get_attn_subsequence_mask python def get_attn_subsequence_mask(seq): \'\'\' seq: [batch_size, tgt_len] \'\'\' attn_shape = [seq.size(0), seq.size(1), seq.size(1)] # Upper triangular matrix subsequence_mask = np.triu(np.ones(attn_shape), k=1) subsequence_mask = torch.from_numpy(subsequence_mask).byte() return subsequence_mask # [batch_size, tgt_len, tgt_len] ##### How will Attention Mask be Used In get_attn_pad_mask, seq_q means a sequence of indexes, i.e, $\\\\texttt{seq_q}\\\\in \\\\mathbb N^{\\\\texttt{max_seq_len}}$, which is to be embedded into $(\\\\mathbb R^{d_\\\\text{model}})^{\\\\texttt{max_seq_len}}$ and form queries $Q\\\\in \\\\mathbb R^{q\\\\times d_\\\\text{model}}$, the same is true for seq_k and $K\\\\in \\\\mathbb R^{k\\\\times d_\\\\text{model}}$, they will be multiplied together to get $$ Q K^T\\\\in\\\\mathbb R^{q\\\\times k} $$ for scaled dot-product attention. <Center> <img src=\\"/assets/tech/088-transformer/scaled-dot-product-attention.png\\"/ width=\\"400\\"> </Center> <p/> <center></center> ##### On Padding Logic Note that our padding logic just depends on seq_k and pad (see line 12), seq_q is only used to expand the number of rows to match the dimension of $QK^T$ (see line 14) in order to apply the mask. Our mask will be applied right before applying softmax, i.e., we eventually get scores in rows. Denote $d_k$ the dimension in attention module, where $$ d_k= \\\\begin{cases} d_\\\\text{model} & \\\\text{for }\\\\texttt{ScaledDotProductAttention,}\\\\\\\\ d_\\\\text{model}\\\\,/\\\\!\\\\!/\\\\,d_\\\\text{heads} & \\\\text{for }\\\\texttt{MultiHeadAttention}. \\\\end{cases} $$ We get the scores (the attention) by soft-maxing along the last dimension: $$ \\\\texttt{attn} = \\\\texttt{softmax}\\\\bigg[\\\\frac{\\\\texttt{masked_fill_}(QK^T, \\\\,\\\\texttt{attn_pad_mask})\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!\\\\!}{\\\\sqrt{d_k}}\\\\bigg] \\\\in \\\\mathbb R^{q\\\\times k}. $$ The matrix $\\\\texttt{attn}$ can be decomposed as $A\\\\times 0$ for some $A\\\\in \\\\mathbb R^{q\\\\times k_1}$ and $0\\\\in \\\\mathbb R^{q\\\\times k_2}$, where $k_1+k_2=d_k$ with $k_2$ being the number of paddings counted from the end of the sentence. Our attention will be the linear span of values (the rows) in $V\\\\in \\\\mathbb R^{k\\\\times d_k}$: $$ \\\\begin{aligned} \\\\texttt{context} & = \\\\texttt{attn} \\\\times V\\\\\\\\ & = (V^T [\\\\texttt{attn}]^T)^T\\\\\\\\ & = \\\\left( \\\\begin{bmatrix} |&\\\\cdots&| \\\\\\\\ v_1&\\\\ddots & v_k \\\\\\\\ | &\\\\cdots & | \\\\\\\\ \\\\end{bmatrix} \\\\cdot \\\\begin{bmatrix} A^T \\\\\\\\ 0 \\\\end{bmatrix} \\\\right)^T \\\\\\\\ & = \\\\left( \\\\begin{bmatrix} |&\\\\cdots&| \\\\\\\\ v_1&\\\\ddots & v_{k_1}\\\\\\\\ | &\\\\cdots & | \\\\\\\\ \\\\end{bmatrix} \\\\cdot A^T \\\\right)^T \\\\in \\\\mathbb R^{q\\\\times d_k} \\\\end{aligned} $$ The last few rows of $V$ ($v_i$ for $k_1\\\\leq i<d_k$) has no contribution to the context. Note that in each flow of computation we are focusing on one sentence with multiple indexes. The last few $v_i$\'s are value from the padding position that we are not concerned about. #### Encoder Layer and Encoder <Center> <img src=\\"/assets/tech/088-transformer/Encoder.png\\" width=\\"550\\"> </Center> <p/> <center></center> In transformer an Encoder is a iteration of several EncoderLayer: ##### EncoderLayer python class EncoderLayer(nn.Module): def __init__(self): super(EncoderLayer, self).__init__() self.enc_self_attn = MultiHeadAttention() self.pos_ffn = PoswiseFeedForwardNet() def forward(self, enc_inputs, enc_self_attn_mask): \'\'\' enc_inputs: [batch_size, src_len, d_model] enc_self_attn_mask: [batch_size, src_len, src_len] \'\'\' # enc_outputs: [batch_size, src_len, d_model], # attn: [batch_size, n_heads, src_len, src_len] enc_outputs, attn = self.enc_self_attn( # enc_inputs to same Q,K,V enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask ) # enc_outputs: [batch_size, src_len, d_model] enc_outputs = self.pos_ffn(enc_outputs) return enc_outputs, attn Here the enc_self_attn_mask is usually just a mask that masks out the value contributed from padding (prevent the value from padding from joining the calculation). ##### Encoder python class Encoder(nn.Module): def __init__(self): super(Encoder, self).__init__() self.src_emb = nn.Embedding(src_vocab_size, d_model) self.pos_emb = PositionalEncoding(d_model) self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)]) def forward(self, enc_inputs): \'\'\' enc_inputs: [batch_size, src_len] \'\'\' # [batch_size, src_len, d_model]: enc_outputs = self.src_emb(enc_inputs) # [batch_size, src_len, d_model]: enc_outputs = self.pos_emb(enc_outputs.transpose(0, 1)).transpose(0, 1) # [batch_size, src_len, src_len]: enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) enc_self_attns = [] for layer in self.layers: # enc_outputs: [batch_size, src_len, d_model], # enc_self_attn: [batch_size, n_heads, src_len, src_len] enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask) enc_self_attns.append(enc_self_attn) # enc_self_attns is only for graph-plotting purpose: return enc_outputs, enc_self_attns #### Decoder Layer and Decoder <Center> <img src=\\"/assets/tech/088-transformer/Decoder.png\\" width=\\"550\\"> </Center> <p/> <center></center> ##### DecoderLayer python class DecoderLayer(nn.Module): def __init__(self): super(DecoderLayer, self).__init__() self.dec_self_attn = MultiHeadAttention() self.dec_enc_attn = MultiHeadAttention() self.pos_ffn = PoswiseFeedForwardNet() def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask): \'\'\' dec_inputs: [batch_size, tgt_len, d_model] enc_outputs: [batch_size, src_len, d_model] dec_self_attn_mask: [batch_size, tgt_len, tgt_len] dec_enc_attn_mask: [batch_size, tgt_len, src_len] \'\'\' # dec_outputs: [batch_size, tgt_len, d_model], # dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len] dec_outputs, dec_self_attn = self.dec_self_attn( dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask ) # dec_outputs: [batch_size, tgt_len, d_model], # dec_enc_attn: [batch_size, h_heads, tgt_len, src_len] dec_outputs, dec_enc_attn = self.dec_enc_attn( dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask ) # [batch_size, tgt_len, d_model] dec_outputs = self.pos_ffn(dec_outputs) return dec_outputs, dec_self_attn, dec_enc_attn ##### Decoder python-1 class Decoder(nn.Module): def __init__(self): super(Decoder, self).__init__() self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model) self.pos_emb = PositionalEncoding(d_model) self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)]) def forward(self, dec_inputs, enc_inputs, enc_outputs): \'\'\' dec_inputs: [batch_size, tgt_len] enc_intpus: [batch_size, src_len] enc_outputs: [batch_size, src_len, d_model] \'\'\' # [batch_size, tgt_len, d_model]: dec_outputs = self.tgt_emb(dec_inputs) # [batch_size, tgt_len, d_model]: dec_outputs = self.pos_emb(dec_outputs.transpose(0, 1))\\\\ .transpose(0, 1)\\\\ .to(device) # [batch_size, tgt_len, tgt_len]: dec_self_attn_pad_mask = get_attn_pad_mask( dec_inputs, dec_inputs ).to(device) # [batch_size, tgt_len, tgt_len]: dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(device) # [batch_size, tgt_len, tgt_len]: dec_self_attn_mask = torch.gt( (dec_self_attn_pad_mask + dec_self_attn_subsequence_mask), 0 ).to(device) # [batc_size, tgt_len, src_len]: dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) dec_self_attns, dec_enc_attns = [], [] for layer in self.layers: # dec_outputs: [batch_size, tgt_len, d_model], # dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], # dec_enc_attn: [batch_size, h_heads, tgt_len, src_len] dec_outputs, dec_self_attn, dec_enc_attn = layer( dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask ) dec_self_attns.append(dec_self_attn) dec_enc_attns.append(dec_enc_attn) return dec_outputs, dec_self_attns, dec_enc_attns ##### Why Feed enc_inputs into Decoder? - From the structural graph of transformer it may seem weird to also feed enc_inputs into the Decoder. - In fact, enc_inputs is only used in the creation of padding mask (see line 37 of the Decoder code block) which help ignore the last few rows of the context value matrix in the enc_outputs (think of it as another form of embedding from the original sequence of word indexes). #### The Transformer python class Transformer(nn.Module): def __init__(self): super(Transformer, self).__init__() self.encoder = Encoder().to(device) self.decoder = Decoder().to(device) self.projection = nn.Linear( d_model, tgt_vocab_size, bias=False ).to(device) def forward(self, enc_inputs, dec_inputs): \'\'\' enc_inputs: [batch_size, src_len] dec_inputs: [batch_size, tgt_len] \'\'\' # tensor to store decoder outputs # outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device) # enc_outputs: [batch_size, src_len, d_model] # enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len] enc_outputs, enc_self_attns = self.encoder(enc_inputs) # dec_outpus: [batch_size, tgt_len, d_model] # dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len] # dec_enc_attn: [n_layers, batch_size, tgt_len, src_len] dec_outputs, dec_self_attns, dec_enc_attns = self.decoder( dec_inputs, enc_inputs, enc_outputs ) # dec_logits: [batch_size, tgt_len, tgt_vocab_size] dec_logits = self.projection(dec_outputs) return ( dec_logits.view(-1, dec_logits.size(-1)), enc_self_attns, dec_self_attns, dec_enc_attns ) - Note that dec_logits will be of shape [batch_size, tgt_max_len, tgt_vocab_size]. - dec_logits.view(-1, dec_logits.size(-1)) will be of shape none [batch_size*tgt_max_len, tgt_vocab_size] #### Training: Naive Dataset ##### Dataset and data_loader Let\'s get a rough feeling from a naive dataset: python sentences = [ # enc_input dec_input dec_output [\'ich mochte ein bier <P>\', \'<sos> i want a beer .\', \'i want a beer . <eos>\'], [\'ich mochte ein cola <P>\', \'<sos> i want a coke .\', \'i want a coke . <eos>\'] ] It is not necessary to write <P> in the dataset, we can pad our sequence of word indexes by 0\'s in our dataset pipeline. Next we define our data_loader: python src_word_index = {\'<P>\': 0, \'ich\': 1, \'mochte\': 2, \'ein\': 3, \'bier\': 4, \'cola\': 5} src_vocab_size = len(src_word_index) tgt_word_index = {\'<P>\': 0, \'i\': 1, \'want\': 2, \'a\': 3, \'beer\': 4, \'coke\': 5, \'<sos>\': 6, \'<eos>\': 7, \'.\': 8} src_index_word = {i: w for i, w in enumerate(src_word_index)} tgt_index_word = {i: w for i, w in enumerate(tgt_word_index)} tgt_vocab_size = len(tgt_word_index) src_len = 5 # enc_input max sequence length tgt_len = 6 # dec_input(=dec_output) max sequence length def make_data(sentences): enc_inputs, dec_inputs, dec_outputs = [], [], [] for i in range(len(sentences)): enc_input_, dec_input_, dec_output_ = sentences[i] enc_input = [src_word_index[n] for n in enc_input_.split()] # ^^^^ [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]] dec_input = [tgt_word_index[n] for n in dec_input_.split()] # ^^^^ [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]] dec_output = [tgt_word_index[n] for n in dec_output_.split()] # ^^^^ [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]] enc_inputs.append(enc_input) dec_inputs.append(dec_input) dec_outputs.append(dec_output) return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs) class MyDataSet(Data.Dataset): def __init__(self): super(MyDataSet, self).__init__() enc_inputs, dec_inputs, dec_outputs = make_data(sentences) self.enc_inputs = enc_inputs self.dec_inputs = dec_inputs self.dec_outputs = dec_outputs def __len__(self): return self.enc_inputs.shape[0] def __getitem__(self, idx): return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx] data_loader = Data.DataLoader(MyDataSet(), batch_size=2, shuffle=True) ##### Train to see if the Model can Overfit python def train(): transformer = Transformer().to(device) criterion = nn.CrossEntropyLoss(ignore_index=0) optimizer = optim.SGD(transformer.parameters(), lr=1e-3, momentum=0.99) epochs = 30 for epoch in range(epochs): for enc_inputs, dec_inputs, dec_outputs in data_loader: \'\'\' enc_inputs: [batch_size, src_len] dec_inputs: [batch_size, tgt_len] dec_outputs: [batch_size, tgt_len] \'\'\' enc_inputs = enc_inputs.to(device) dec_inputs = dec_inputs.to(device) dec_outputs = dec_outputs.to(device) outputs, enc_self_attns, dec_self_attns, dec_enc_attns = transformer( enc_inputs, dec_inputs ) loss = criterion(outputs, dec_outputs.view(-1)) print(\'Epoch:\', \'%04d\' % (epoch + 1), \'loss =\', \'{:.6f}\'.format(loss)) optimizer.zero_grad() loss.backward() optimizer.step() state_dict = transformer.state_dict() torch.save(state_dict, os.path.join(\\"pths\\", f\\"model_epoch_{epochs}.pth\\")) Upon executing the training script, our model can indeed overfit the naive dataset: none Epoch: 0001 loss = 2.579853 Epoch: 0002 loss = 2.416333 Epoch: 0003 loss = 2.135146 Epoch: 0004 loss = 1.839263 Epoch: 0005 loss = 1.538042 Epoch: 0006 loss = 1.303803 Epoch: 0007 loss = 1.134294 Epoch: 0008 loss = 0.898169 Epoch: 0009 loss = 0.751822 Epoch: 0010 loss = 0.609819 Epoch: 0011 loss = 0.477917 Epoch: 0012 loss = 0.374170 Epoch: 0013 loss = 0.280897 Epoch: 0014 loss = 0.225249 Epoch: 0015 loss = 0.175177 Epoch: 0016 loss = 0.143766 Epoch: 0017 loss = 0.139490 Epoch: 0018 loss = 0.114785 Epoch: 0019 loss = 0.090137 Epoch: 0020 loss = 0.087496 Epoch: 0021 loss = 0.076527 Epoch: 0022 loss = 0.069732 Epoch: 0023 loss = 0.061058 Epoch: 0024 loss = 0.055305 Epoch: 0025 loss = 0.042023 Epoch: 0026 loss = 0.040661 Epoch: 0027 loss = 0.039919 Epoch: 0028 loss = 0.023459 Epoch: 0029 loss = 0.022219 Epoch: 0030 loss = 0.027067 ##### Define Translator We predict the target result word by word as in the teacher forcing approach in our training process. python class Translator(): def __init__(self, transformer: Transformer): self.transformer = transformer def translate(self, enc_input, start_index): dec_input = torch.zeros(1, 0).type_as(enc_input) terminated = False next_tgt_word_index = start_index while not terminated: dec_input = torch.cat( [ dec_input.detach(), torch.tensor([[next_tgt_word_index]],dtype=enc_input.dtype).to(device) ], -1 ) dec_output_logits, _, _, _= self.transformer(enc_input, dec_input) next_tgt_word_index = torch.argmax(dec_output_logits[-1]) if next_tgt_word_index == tgt_word_index[\\".\\"]: terminated = True print(\\"next_word\\", tgt_index_word[next_tgt_word_index.item()]) # remove batch, remove <sos> return dec_input.squeeze(0)[1:] ##### Translation Script Now we test our function by using the script: python transformer = Transformer().to(device) model_path = \\"pths/model_epoch_30.pth\\" if model_path is not None: transformer.load_state_dict(torch.load(model_path)) translator = Translator(transformer) enc_inputs, _, _ = next(iter(data_loader)) enc_inputs = enc_inputs.to(device) # e.g. enc_inputs = tensor([ # [1, 2, 3, 4, 0], [1, 2, 3, 5, 0] # ], device=\'cuda:0\') for i in range(len(enc_inputs)): enc_input = enc_inputs[i] sentence = \\" \\".join([src_index_word[i.item()] for i in enc_input]) print(\\"source sentence:\\", sentence) predict = translator.translate( enc_input.unsqueeze(0), # expand as batch start_index=tgt_word_index[\\"<sos>\\"] ) print(enc_input, \'->\', [tgt_index_word[n.item()] for n in predict.squeeze()]) yields none source sentence: ich mochte ein bier <P> next_word i next_word want next_word a next_word beer next_word . tensor([1, 2, 3, 4, 0], device=\'cuda:0\') -> [\'i\', \'want\', \'a\', \'beer\'] source sentence: ich mochte ein cola <P> next_word i next_word want next_word a next_word coke next_word . tensor([1, 2, 3, 5, 0], device=\'cuda:0\') -> [\'i\', \'want\', \'a\', \'coke\'] ","title":"Transformer 1: The Model Definition and Naive Training Dataset for Machine Translation","date":"2022-08-11T00:00:00.000Z","id":"blog088","tag":"pytorch, deep-learning","intro":"Record the basic building block and structure of a Transformer. The main reference of this article is this <i><a href=\'https://wmathor.com/index.php/archives/1438/\'> blog post</a></i>."},{"content":" #### Training: Real World Dataset ##### Repository The code block below are partially taken from my repo: - https://github.com/machingclee/2022-07-04-transformer-from-scratch ##### New Practice from torchtext 0.10.1 onwards Old NLP material in pytorch usually import the following: python # or from torchtext.legacy.data from torchtext.data import Field, Field, BucketIterator, TabularDataset but these are completely removed from torchtext 0.10.1 onwards. It seems that the official pytorch suggests users creating their dataset via _traditional_ classes: python from torch.utils.data import Dataset, DataLoader ##### spacy package - We will rely on a package called spacy for tokenization. We will need this in our Corpus class, which is called implicitly in get_tokenizer imported from torchtext.data.utils. - We install the spacy and the desired lanauges by following the guide in https://spacy.io/usage. ##### Download Source of EN-JP Dataset Interested reader can download it from: https://nlp.stanford.edu/projects/jesc/ ##### Prepare Dataset Pipeline Suppose that our dataset is a txt file in which every line is an English sentence paired with its Japanese translation, separated by \\\\t: none or the relative risk of drugs\\t\u307b\u307c\u7121\u95a2\u4fc2\u3067\u3059 gail, are you drunk?\\t\u30b2\u30a4\u30eb \u9154\u3063\u3066\u308b? be careful.\\t\u6ce8\u610f\u3057\u308d ... Let\'s first define our Corpus class that iterates the dataset to provide these sentences. Note that get_src_sentence_iter and get_tgt_sentence_iter **_below_** are dependent on the structure of the dataset. python import pickle import torch from collections import Counter from random import shuffle from torchtext.data.utils import get_tokenizer from torch.utils.data import Dataset, DataLoader from torchtext.vocab import vocab, Vocab from src.device import device from src import config from typing import Optional class Corpus: def __init__( self, src_lang=\\"en_core_web_sm\\", tgt_lang=\\"ja_core_news_sm\\", delimiter=\\"\\\\t\\", src_vocab: Optional[Vocab] = None, tgt_vocab: Optional[Vocab] = None ): # the lang keys are used in defining \\"field object\\" # which is exactly the csv\'s header, the column name, the json key, etc. self.delimiter=delimiter self.src_tokenizer = get_tokenizer(\\"spacy\\", language=src_lang) self.tgt_tokenizer = get_tokenizer(\\"spacy\\", language=tgt_lang) src_counter = Counter() tgt_counter = Counter() if src_vocab is not None and tgt_vocab is not None: self.src_vocab = src_vocab self.tgt_vocab = tgt_vocab else: for src_line in self.get_src_sentence_iter(): src_counter.update(self.src_tokenizer(src_line)) for tgt_line in self.get_tgt_sentence_iter(): tgt_counter.update(self.tgt_tokenizer(tgt_line)) # for label, line in self.src_vocab = vocab( src_counter, min_freq=2, specials=(\'<ukn>\', \'<pad>\') ) self.tgt_vocab = vocab( tgt_counter, min_freq=2, specials=(\'<ukn>\', \'<sos>\', \'<eos>\', \'<pad>\') ) def get_src_sentence_iter(self): with open(config.data_path, encoding=\\"utf-8\\") as f: for line in f: src_line, _ = line.split(self.delimiter) yield src_line def get_tgt_sentence_iter(self): with open(config.data_path, encoding=\\"utf-8\\") as f: for line in f: _, tgt_line = line.split(self.delimiter) yield tgt_line def save_vocabs(self): vocabs = { \\"src\\": self.src_vocab, \\"tgt\\": self.tgt_vocab } for lang, vocab in vocabs.items(): with open(f\\"{lang}.pickle\\", \'wb+\') as handle: pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL) Since creating vocabs involve accessing to all sentences in a dataset, it can take quite long and it is worth saving the vocabs, which include - index_to_word (obtained by Vocab.get_itos()) and - word_to_index (obtained by Vocab.get_stoi()) somewhere else so that we can reuse later without iterating the whole dataset again, that\'s why we have the save_vocabs method. After saving the vocabs into pickle files, we can retrieve it by the class: python class Vocabs: src_vocab = None tgt_vocab = None def __init__(self, src_vocab_pickle_path, tgt_vocab_pickle_path): self.src_vocab_pickle_path = src_vocab_pickle_path self.tgt_vocab_pickle_path = tgt_vocab_pickle_path def get_src_vocab(self) -> Vocab: if Vocabs.src_vocab is None: with open(self.src_vocab_pickle_path, \'rb\') as handle: Vocabs.src_vocab = pickle.load(handle) return Vocabs.src_vocab def get_tgt_vocab(self): if Vocabs.tgt_vocab is None: with open(self.tgt_vocab_pickle_path, \'rb\') as handle: Vocabs.tgt_vocab = pickle.load(handle) return Vocabs.tgt_vocab Now our NLP dataset dedicated to Transformer becomes: python class TransformerDataset(Dataset): def __init__(self, corpus: Corpus): self.corpus = corpus self.src_sentences = list(self.corpus.get_src_sentence_iter()) self.tgt_sentences = list(self.corpus.get_tgt_sentence_iter()) def __getitem__(self, index): src_text = self.src_sentences[index] tgt_text = self.tgt_sentences[index] src_stoi = self.corpus.src_vocab.get_stoi() tgt_stoi = self.corpus.tgt_vocab.get_stoi() src_tokens= self.corpus.src_tokenizer(src_text) tgt_tokens = self.corpus.tgt_tokenizer(tgt_text) src_pad_len = config.src_max_len - len(src_tokens) tgt_pad_len = config.tgt_max_len - len(tgt_tokens) if src_pad_len > 0: src_idxes = [src_stoi.get(token, src_stoi[\\"<ukn>\\"]) for token in src_tokens] + [src_stoi[\\"<pad>\\"]] * src_pad_len else: src_idxes = [src_stoi.get(token, src_stoi[\\"<ukn>\\"]) for token in src_tokens[:config.src_max_len]] if tgt_pad_len > 0: tgt_idxes = [tgt_stoi[\'<sos>\']] + \\\\ [tgt_stoi.get(token, src_stoi[\\"<ukn>\\"]) for token in tgt_tokens] + \\\\ [tgt_stoi[\'<eos>\']] + \\\\ [tgt_stoi[\\"<pad>\\"]] * tgt_pad_len else: tgt_idxes = [tgt_stoi[\'<sos>\']] + \\\\ [tgt_stoi.get(token, src_stoi[\\"<ukn>\\"]) for token in tgt_tokens[:config.tgt_max_len]] + \\\\ [tgt_stoi[\'<eos>\']] return torch.as_tensor(src_idxes, device=device), torch.as_tensor(tgt_idxes, device=device) def __len__(self): return len(self.src_sentences) ##### Training Script According to our dataset output we change our training script as follows, we always feed our model by \\"indexed\\" version of our **_naive dataset_**. python console_log = ConsoleLog(lines_up_on_end=1) def train( epochs=10, use_saved_vocab=False, learning_rate=1e-3 ): if use_saved_vocab: vocabs = Vocabs(src_vocab_pickle_path=\\"src.pickle\\", tgt_vocab_pickle_path=\\"tgt.pickle\\") corpus = Corpus(src_vocab=vocabs.get_src_vocab(), tgt_vocab=vocabs.get_tgt_vocab()) else: corpus = Corpus() corpus.save_vocabs() src_vocab_size = len(corpus.src_vocab.get_stoi()) tgt_vocab_size = len(corpus.tgt_vocab.get_stoi()) transformer = Transformer( src_vocab_size=src_vocab_size, tgt_vocab_size=tgt_vocab_size ).to(device) criterion = nn.CrossEntropyLoss(ignore_index=0) optimizer = optim.Adamax(transformer.parameters(), lr=learning_rate) dataset = TransformerDataset(corpus) data_loader = DataLoader(dataset=dataset, batch_size=config.batch_size, shuffle=True ) # when arrived this step, pickle file must have been saved vocabs = Vocabs(src_vocab_pickle_path=\\"src.pickle\\", tgt_vocab_pickle_path=\\"tgt.pickle\\") src_vocabs = vocabs.get_src_vocab() tgt_vocabs = vocabs.get_tgt_vocab() for epoch in range(epochs): for batch_id, (src_idxes, tgt_idxes) in enumerate(tqdm(data_loader)): batch_id += 1 enc_inputs = src_idxes.to(device) dec_inputs = tgt_idxes[:, :-1].to(device) dec_outputs = tgt_idxes[:, 1:].to(device) outputs, _, _, _ = transformer( enc_inputs, dec_inputs ) loss = criterion(outputs, dec_outputs.flatten()) with torch.no_grad(): console_log.print([ (\\"loss\\", loss.item()) ]) if batch_id % config.visualize_result_per_epochs == 0: visualize(transformer, enc_inputs[0], src_vocabs, tgt_vocabs ) optimizer.zero_grad() loss.backward() optimizer.step() state_dict = transformer.state_dict() torch.save(state_dict, os.path.join(\\"pths\\", f\\"model_epoch_{epoch}.pth\\")) ##### New Translator Next we build our translator based on the vocab objects: python class Translator(): def __init__(self, transformer: Transformer): self.transformer = transformer def translate_input_index(self, enc_input, src_start_index, tgt_word_index, tgt_index_word): dec_input = torch.zeros(1, 0).type_as(enc_input) terminated = False next_tgt_word_index = src_start_index word_count = 0 while not terminated: dec_input = torch.cat( [ dec_input.detach(), torch.tensor([[next_tgt_word_index]],dtype=enc_input.dtype).to(device) ], -1 ) word_count += 1 dec_output_logits, _, _, _= self.transformer(enc_input, dec_input) next_tgt_word_index = torch.argmax(dec_output_logits[-1]) if next_tgt_word_index == tgt_word_index[\\"<eos>\\"] or word_count == config.tgt_max_len + 1: terminated = True # remove batch, remove <sos> return dec_input.squeeze(0)[1:] ","title":"Transformer 2: A More in Depth Training with Real World Dataset Using Modern NLP Dataset Pipeline in Pytorch","date":"2022-08-17T00:00:00.000Z","id":"blog089","tag":"pytorch, deep-learning","intro":"Continuation of the previous blog post on transformer, discuss more modern pipeline for training a transformer (or any NLP task in general) using the latest torchtext."},{"content":" #### Points to note, will be organized later - To create a project component: text ng new angular-crash-course - To create a new component: text ng generate component components/button - To create a new service that is to be used by a component: text ng generate service services/task - For loop in angular\'s xxx.component.html javascript <div *ngFor=\\"let task of tasks\\"> {{ task.text }} </div> - Pass object as a props: javascript <app-task *ngFor=\\"let task of tasks\\" [task]=\\"task\\"> </app-task> - javascript import { Observable, of } from \\"rxjs\\"; - **json server.** After yarn add json-server we add the following in package.json as our script: text \\"server\\": \\"json-server --watch db.json --port 5000\\" - import { HttpClient, HttpHeaders } from \\"@angular/common/http\\" then javascript import { HttpClientModule } from \\"@angular/common/http\\"; and add that into app.module.ts\'s imports field. Next we can add HttpClient in the TaskService, then we can call the get method to get Observable. - To call a function in a view model, we use <div (click)=\\"onClick()\\">, where onClick is a method in xxx.component.ts, the view model. - Suppose we have javascript import { Input, Output, EventEmitter } from \\"@angular/core\\"; then: - @Input() is used to define **_object_** as a props from parent to its child. - @Output() is used to define **_function_** that pass data from child to parent, which is usually an EventEmitter<T>. - Suppose that we have defined javascript @Output() onDeleteTask: EventEmitter<Task> = new EventEimitter() inside <app-task-item></app-task-item>, then we can define js deleteTask(task) { this.onDeleteTask.emit(task)} inside that item component. - In task-item (child) level, we can define (click)=deleteTask(task), which will emit the event that ship with data task. - In tasks (parent) level, we can define <app-tasks (onDeleteTask)=doSth(task)></app-tasks> to subscribe for the Event. - We add conditional class reminder based on shouldRemind: boolean: js <div class=\\"task\\" [ngClass]=\\"{ reminder: shouldRemind }\\">...</div> Once shouldRemind === true, the div element will be of class task reminder, otherwise it is merely be of class task. - Apart from the usual EventEmitter: (click), we have (dblclick) for double-click. - To enable two-way data binding (bind data between both view-model and UI), we: js // src/app/app.module.ts import { BrowserModule } from \\"@angular/platform-browser\\"; ... @NgModule({ ... imports: [ BrowserModule, FormsModule ], ... }) Then we can use ngModel directive as follows: jsx <input [(ngModel)]=\\"text\\"/> <div> {{ text }} </div> where text is an attribute defined in xxx.component.ts. - To conditionally show an element, we use *ngIf=\\"classAttribute\\". ","title":"Angular Fundamental","date":"2022-08-18T00:00:00.000Z","id":"blog090","tag":"angular","toc":false,"intro":"Record the basic syntax in learning angular."},{"content":" #### Imports for Eninops python import torch import torch.nn.functional as F import matplotlib.pyplot as plt from torch import nn from torch import Tensor from PIL import Image from torchvision.transforms import Compose, Resize, ToTensor from einops import rearrange, reduce, repeat from einops.layers.torch import Rearrange, Reduce #### Patch Embedding ##### Objective Given an image of shape (3, 224, 224) we perform the following steps in order to produce an output whose shape is the same as what we feed into a transformer (the input should be of shape (batches, seq_len, embed_size)) <Center> <a href=\\"/assets/tech/091-VIT/004.png\\" target=\\"_blank\\"> <img src=\\"/assets/tech/091-VIT/004.png\\" width=\\"100%\\"> </a> </Center> <p/> - We first divide an image into patches by squares of size 16x16 (i.e., patch_size=16); - Each of these patches (there are $14\\\\times 14=196$ of them) will be mapped into a 1-d feature vector of size $16\\\\times 16\\\\times 3=768$. The two steps above are achieved by a single convolution layer (with kernel size and stride size being equal to patch_size). Eventually after the patch embedding our feature vector will be of shape: none [batches, 14, 14, 768] ##### cls Token <Center> <img src=\\"/assets/tech/091-VIT/001.png\\" width=\\"560\\"> </Center> <p/> <center></center> The picture above is taken from the original paper of ViT, we can notice there are 10 \\"tokens\\" insteand of 9. The additional one called **_cls token_** is added at the beginning as a learning parameter which has the same embedding size as our patched embedding feature vectors. At the first iteration of transformer encoder block, the cls token already learned the information of all patches due to the self-attention mechanism. We will be doing 12 iterations, and the resulting cls token will be fed into classification head for final prediction. Another prediction approch can be done **_without_** cls token, instead, we keep doing iteration of transformer encoder blocks and take an average along the _sequential dimension_ (see Reduce(\'b n e -> b e\', reduction=\'mean\') in ClassificationHead below). This process can be thought of as a global average pooling. ##### PatchEmbedding ###### Step 1: Divide Images into Patches and Map them into 1-d features python class PatchEmbedding(nn.Module): def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768): self.patch_size = patch_size super().__init__() self.projection = nn.Sequential( # the conv layer can be replaced by # Rearrange(\'b c (h s1) (w s2) -> b (h w) (s1 s2 c)\', s1=patch_size, s2=patch_size), nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size), Rearrange(\'b e (h) (w) -> b (h w) e\'), ) def forward(self, x: Tensor) -> Tensor: x = self.projection(x) return x ###### Step 2: Add cls Token python class PatchEmbedding(nn.Module): def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768): self.patch_size = patch_size super().__init__() self.projection = nn.Sequential( # using a conv layer instead of a linear one -> performance gains nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size), Rearrange(\'b e (h) (w) -> b (h w) e\'), ) self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size)) def forward(self, x: Tensor) -> Tensor: b, _, _, _ = x.shape x = self.projection(x) cls_tokens = repeat(self.cls_token, \'() n e -> b n e\', b=b) # prepend the cls token to the input x = torch.cat([cls_tokens, x], dim=1) return x ###### Step 3: Add PositionEmbedding python class PatchEmbedding(nn.Module): def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224): self.patch_size = patch_size super().__init__() self.projection = nn.Sequential( # using a conv layer instead of a linear one -> performance gains nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size), Rearrange(\'b e (h) (w) -> b (h w) e\'), ) self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size)) self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size)) def forward(self, x: Tensor) -> Tensor: b, _, _, _ = x.shape x = self.projection(x) cls_tokens = repeat(self.cls_token, \'() n e -> b n e\', b=b) # prepend the cls token to the input x = torch.cat([cls_tokens, x], dim=1) # add position embedding x += self.positions return x #### Transformer Encoder <Center> <img src=\\"/assets/tech/091-VIT/002.png\\" width=\\"200\\"/> </Center> ##### MultiheadAttention Unlike transformer in machine translation, we only need the encoder part without any masking before softmax. ###### Version 1 python class MultiHeadAttention(nn.Module): def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0): super().__init__() self.emb_size = emb_size self.num_heads = num_heads self.keys = nn.Linear(emb_size, emb_size) self.queries = nn.Linear(emb_size, emb_size) self.values = nn.Linear(emb_size, emb_size) self.att_drop = nn.Dropout(dropout) self.projection = nn.Linear(emb_size, emb_size) self.scaling = (self.emb_size // num_heads) ** -0.5 def forward(self, x : Tensor, mask: Tensor = None) -> Tensor: # split keys, queries and values in num_heads queries = rearrange(self.queries(x), \\"b n (h d) -> b h n d\\", h=self.num_heads) keys = rearrange(self.keys(x), \\"b n (h d) -> b h n d\\", h=self.num_heads) values = rearrange(self.values(x), \\"b n (h d) -> b h n d\\", h=self.num_heads) # sum up over the last axis energy = torch.einsum(\'bhqd, bhkd -> bhqk\', queries, keys) # batch, num_heads, query_len, key_len if mask is not None: fill_value = torch.finfo(torch.float32).min energy.mask_fill(~mask, fill_value) att = F.softmax(energy, dim=-1) * self.scaling att = self.att_drop(att) # sum up over the third axis out = torch.einsum(\'bhal, bhlv -> bhav \', att, values) out = rearrange(out, \\"b h n d -> b n (h d)\\") out = self.projection(out) return out ###### Version 2 python class MultiHeadAttention(nn.Module): def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0): super().__init__() self.emb_size = emb_size self.num_heads = num_heads # fuse the queries, keys and values in one matrix self.qkv = nn.Linear(emb_size, emb_size * 3) self.att_drop = nn.Dropout(dropout) self.projection = nn.Linear(emb_size, emb_size) def forward(self, x : Tensor, mask: Tensor = None) -> Tensor: # split keys, queries and values in num_heads qkv = rearrange(self.qkv(x), \\"b n (h d qkv) -> (qkv) b h n d\\", h=self.num_heads, qkv=3) queries, keys, values = qkv[0], qkv[1], qkv[2] # sum up over the last axis energy = torch.einsum(\'bhqd, bhkd -> bhqk\', queries, keys) # batch, num_heads, query_len, key_len if mask is not None: fill_value = torch.finfo(torch.float32).min energy.mask_fill(~mask, fill_value) scaling = self.emb_size ** (1/2) att = F.softmax(energy, dim=-1) / scaling att = self.att_drop(att) # sum up over the third axis out = torch.einsum(\'bhal, bhlv -> bhav \', att, values) out = rearrange(out, \\"b h n d -> b n (h d)\\") out = self.projection(out) return out ##### MLP python class FeedForwardBlock(nn.Sequential): def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.): super().__init__( nn.Linear(emb_size, expansion * emb_size), nn.GELU(), nn.Dropout(drop_p), nn.Linear(expansion * emb_size, emb_size), ) ##### ResidualAdd python class (nn.Module): def __init__(self, fn): super().__init__() self.fn = fn def forward(self, x, **kwargs): res = x x = self.fn(x, **kwargs) x += res return x ##### TransformerEncoderBlock <Center> <img src=\\"/assets/tech/091-VIT/003.png\\" width=\\"200\\"/> </Center> python class TransformerEncoderBlock(nn.Sequential): def __init__(self, emb_size: int = 768, drop_p: float = 0., forward_expansion: int = 4, forward_drop_p: float = 0., ** kwargs): super().__init__( ResidualAdd(nn.Sequential( nn.LayerNorm(emb_size), MultiHeadAttention(emb_size, **kwargs), nn.Dropout(drop_p) )), ResidualAdd(nn.Sequential( nn.LayerNorm(emb_size), FeedForwardBlock( emb_size, expansion=forward_expansion, drop_p=forward_drop_p), nn.Dropout(drop_p) )) ) ##### TransformerEncoder python class TransformerEncoder(nn.Sequential): def __init__(self, depth: int = 12, **kwargs): super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)]) ##### ClassificationHead python class ClassificationHead(nn.Sequential): def __init__(self, emb_size: int = 768, n_classes: int = 1000): super().__init__( # It is also possible to just take the first entry in the last dimension Reduce(\'b n e -> b e\', reduction=\'mean\'), nn.LayerNorm(emb_size), nn.Linear(emb_size, n_classes) ) #### Vi-sual T-ransformer python class ViT(nn.Sequential): def __init__(self, in_channels: int = 3, patch_size: int = 16, emb_size: int = 768, img_size: int = 224, depth: int = 12, n_classes: int = 1000, **kwargs): super().__init__( PatchEmbedding(in_channels, patch_size, emb_size, img_size), TransformerEncoder(depth, emb_size=emb_size, **kwargs), ClassificationHead(emb_size, n_classes) ) ","title":"Vision Transformer with einops Implementation","date":"2022-09-06T00:00:00.000Z","id":"blog091","tag":"deep-learning, pytorch","intro":"Record the study of the basic mechanism how a transformer perform classification task on images."},{"content":" The following python coords_h = torch.tensor([0, 1, 2]) coords_w = torch.tensor([0, 1, 2]) xys = torch.stack(torch.meshgrid(coords_h, coords_w)).flatten(1) print(xys) gives none tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]]) Now we rearrange python xys = rearrange(xys, \\"i coord -> coord i\\") print(xys) to get none tensor([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]]) ","title":"Mesh Grid Trick","date":"2022-09-07T00:00:00.000Z","id":"blog092","tag":"deep-learning, pytorch","toc":false,"intro":"Record a trick to create mesh grid coordinate."},{"content":" #### Repository - https://github.com/machingclee/2022-09-16-swin-transformer-einops-implementation/tree/main/src #### Basic Imports python import torch import torch.nn.functional as F import matplotlib.pyplot as plt from torch import nn from torch import Tensor from PIL import Image from torchvision.transforms import Compose, Resize, ToTensor from einops import rearrange, reduce, repeat from einops.layers.torch import Rearrange, Reduce from typing import Tuple, Optional #### PatchEmbedding python class PatchEmbedding(nn.Module): def __init__(self, in_channels=3, patch_size=4, embed_dim=768): self.patch_size = patch_size super().__init__() self.projection = nn.Sequential( nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size), Rearrange(\'b e (h) (w) -> b (h w) e\'), ) def forward(self, x: Tensor) -> Tensor: x = self.projection(x) return x #### MLP python class MLP(nn.Sequential): def __init__(self, dim, mlp_ratio=4.0, dropout=0.): super(MLP, self).__init__( nn.Linear(dim, int(dim * mlp_ratio)), nn.GELU(), nn.Dropout(dropout), nn.Linear(int(dim * mlp_ratio), dim), nn.Dropout(dropout) ) #### windows_partition python def windows_partition(x, window_size): x = rearrange( x, \\"b (win_size1 h2) (win_size2 w2) c -> (b h2 w2) (win_size1 win_size2) c\\", win_size1=window_size, win_size2=window_size ) return #### windows_reverse python def windows_reverse(windows, window_size, h, w): h2 = h // window_size w2 = w // window_size # logically we should reverse to the shape \\"b (win_size1 h2) (win_size2 w2) c\\", # but technically we are going to add the result with skip connection, # therefore we reshape directly to \\"b (win_size1 h2 win_size2 w2) c\\" x = rearrange( windows, \\"(b h2 w2) (win_size1 win_size2) c -> b (win_size1 h2 win_size2 w2) c\\", win_size1=window_size, win_size2=window_size, h2=h2, w2=w2 ) return x #### WindowAttention python class WindowAttention(nn.Module): def __init__(self, dim, num_heads, window_size): super(WindowAttention, self).__init__() self.dim = dim self.window_size = window_size self.head_dim = dim // num_heads self.num_heads = num_heads self.scale = self.head_dim ** -0.5 self.softmax = nn.Softmax(-1) self.qkv = nn.Linear(dim, dim * 3) self.proj = nn.Linear(dim, dim) \\"\\"\\" <--- Create Relative Position Index \\"\\"\\" coords_h = torch.arange(window_size) coords_w = torch.arange(window_size) coords_flatten = torch.stack(torch.meshgrid([coords_h, coords_w])).flatten(1) relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :] relative_coords = relative_coords.permute(1, 2, 0).contiguous() relative_coords[:, :, 0] += self.window_size - 1 relative_coords[:, :, 1] += self.window_size - 1 relative_coords[:, :, 0] *= 2 * self.window_size - 1 # record the index from which we take value from a feature vector relative_position_index = relative_coords.sum(-1) # we don\'t need to learn the indexing self.register_buffer(\\"relative_position_index\\", relative_position_index) \\"\\"\\" Create Relative Position Index ---\x3e\\"\\"\\" self.relative_position_bias_table = nn.Parameter( torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads) ) nn.init.trunc_normal_(self.relative_position_bias_table) def forward(self, x, mask=None): # x: [b, num_img_tokens, embed_dim] # mask: [n, ws*ws, ws*ws] x = self.qkv(x) qkv = rearrange(x, \\"b n (h d qkv) -> (qkv) b h n d\\", h=self.num_heads, qkv=3) q, k, v = qkv[0], qkv[1], qkv[2] q = q * self.scale attn = torch.einsum(\\"bhqd, bhkd -> bhqk\\", q, k) # attn = Q * K^T relative_position_bias = self.relative_position_bias_table.index_select( 0, self.relative_position_index.reshape((-1,)) ).reshape((self.window_size**2, self.window_size**2, -1)) # shift number of heads back to the first dimension # unsqueeze in order to broadcast for batches relative_position_bias = relative_position_bias.permute((2, 0, 1)).unsqueeze(0) attn = attn + relative_position_bias if mask is not None: discard_mask = 1 - mask discard_mask = discard_mask * -1e10 attn = attn + discard_mask attn = self.softmax(attn) out = torch.einsum(\\"bhai, bhid -> bhad\\", attn, v) # attn * V out = rearrange(out, \\"b h n d -> b n (h d)\\") out = self.proj(out) return out #### generate_mask python def generate_mask(window_size=4, shift_size=2, input_resolution=(8, 8)): H, W = input_resolution img_mask = torch.zeros((1, H, W, 1)) # we keep the last dimension becuase we want to apply windows_partition h_slices = [slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None)] w_slices = [slice(0, -window_size), slice(-window_size, -shift_size), slice(-shift_size, None)] count = 0 for h in h_slices: for w in w_slices: img_mask[:, h, w, :] = count count += 1 windows_mask = windows_partition(img_mask, window_size) # windows_mask: [(b h2 w2), (win_size1 win_size2), 1] windows_mask = windows_mask.reshape((-1, window_size * window_size)) # [n, 1, ws*ws] - [n, ws*ws, 1] attn_mask = windows_mask.unsqueeze(1) - windows_mask.unsqueeze(2) attn_mask = torch.where(attn_mask == 0, 1., 0.) return attn_mask #### SwinBlock python class SwinBlock(nn.Module): def __init__(self, dim, input_resolution, num_heads, window_size, shift_size=0): super(SwinBlock, self).__init__() self.dim = dim self.resolution = input_resolution self.window_size = window_size self.shift_size = shift_size self.attn_norm = nn.LayerNorm(dim) self.attn = WindowAttention(dim, num_heads, window_size) self.mlp_norm = nn.LayerNorm(dim) self.mlp = MLP(dim) if self.shift_size > 0: attn_mask = generate_mask(window_size=self.window_size, shift_size=self.shift_size, input_resolution=self.resolution) else: attn_mask = None self.register_buffer(\'attn_mask\', attn_mask) def forward(self, x): # x: [b, n, d] H, W = self.resolution B, N, C = x.shape h = x x = self.attn_norm(x) x = rearrange(x, \\"b (h w) c -> b h w c\\", h=H, w=W) if self.shift_size > 0: shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)) else: shifted_x = x x_windows = windows_partition(shifted_x, self.window_size) if self.attn_mask is not None: self.attn_mask = repeat(self.attn_mask[None, ...], \\"() num_patches h w -> b num_patches h w\\", b=B) # exactly the same rearrange with that in windows_reverse self.attn_mask = rearrange(self.attn_mask, \\"b num_patches h w -> (b num_patches) () h w\\") attn_windows = self.attn(x_windows, mask=self.attn_mask) attn_windows = windows_reverse(attn_windows, window_size=self.window_size, h=H, w=W) # reverse cyclic shift if self.shift_size > 0: x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)) else: x = shifted_x x = h + attn_windows h = x x = self.mlp_norm(x) x = self.mlp(x) x = h + x return #### SwinStage python class SwinStage(nn.Module): def __init__(self, dim: int, input_resolution: Tuple[int, int], depth: int, num_heads: int, window_size: int, patch_merging: Optional[PatchMerging] = None): super(SwinStage, self).__init__() self.blocks = nn.ModuleList() for i in range(depth): self.blocks.append( SwinBlock(dim=dim, input_resolution=input_resolution, num_heads=num_heads, window_size=window_size, shift_size=0 if i % 2 == 0 else window_size // 2) ) if patch_merging is None: self.patch_merging = nn.Identity() else: self.patch_merging = patch_merging(input_resolution, dim) def forward(self, x): for block in self.blocks: x = block(x) x = self.patch_merging(x) return #### SwinTransformer python class SwinTransformer(nn.Module): def __init__(self, image_size=224, patch_size=4, embed_dim=96, window_size=7, num_heads=[3, 6, 12, 24], depths=[2, 2, 6, 2], num_classes=1000, output_hidden_states=False ): super(SwinTransformer, self).__init__() self.num_class = num_classes self.depths = depths self.num_heads = num_heads self.embed_dim = embed_dim self.num_stages = len(depths) self.num_features = int(self.embed_dim * (2 ** (self.num_stages - 1))) self.patch_resolution = [image_size // patch_size, image_size // patch_size] self.patch_embedding = PatchEmbedding(patch_size=patch_size, embed_dim=embed_dim) self.stages = nn.ModuleList() self.output_hidden_states = output_hidden_states for idx, (depth, n_heads) in enumerate(zip(self.depths, self.num_heads)): h, w = self.patch_resolution stage = SwinStage(dim=int(self.embed_dim * (2 ** idx)), input_resolution=(h // (2**idx), w // (2**idx)), depth=depth, num_heads=n_heads, window_size=window_size, patch_merging=PatchMerging if (idx < self.num_stages-1) else None) self.stages.append(stage) self.window_size = window_size self.norm = nn.LayerNorm(self.num_features) self.avgpool = nn.AdaptiveAvgPool1d(1) # last diemnsion will be shrinked to 1 self.fc = nn.Linear(self.num_features, num_classes) def forward(self, x): x = self.patch_embedding(x) for stage in self.stages: x = stage(x) x = self.norm(x) if self.output_hidden_states: x = rearrange( x, \\"b (win_size_h win_size_w) embed_dim -> b embed_dim win_size_h win_size_w\\", win_size_h=self.window_size, win_size_w =self.window_size ) return x x = rearrange(x, \\"b num_windows embed_dim -> b embed_dim num_windows\\") x = self.avgpool(x) x = rearrange(x, \\"b embed_dim c -> b (embed_dim c)\\") # c = 1 due to avgpool x = self.fc(x) return x #### Test Script python t = torch.randn([4, 3, 224, 224]) patch_embedding = PatchEmbedding(patch_size=4, embed_dim=96) swin_block = SwinBlock(dim=96, input_resolution=[56, 56], num_heads=4, window_size=7) shifted_swin_block = SwinBlock(dim=96, input_resolution=[56, 56], num_heads=4, window_size=7, shift_size=7 // 2) patch_merging = PatchMerging(input_resolution=[56, 56], dim=96) out = patch_embedding(t) # result: [4, 56*56, 96], here (224/4) * (224/4) = 56*56 out = swin_block(out) # result: [4, 56*56, 96] out = shifted_swin_block(out) # result: [4, 56*56, 96] out = patch_merging(out) # result: [4, 784, 192], here 56*56 / 4 = 784 # 784 = 28*28 is considered as new number of windows ","title":"Swin Transformer with einops Implementation","date":"2022-09-19T00:00:00.000Z","id":"blog093","tag":"deep-learning, pytorch","intro":"In practice we can directly use torchvision.models.swin_t to use pretrained model. The Reimplementation of the model is for sheer purpose of my self-learning."},{"content":" #### Repository - https://github.com/machingclee/2022-09-20-Swin-Transformer-backbone-FasterRCNN #### Target Features The red arrows indicates the features we want: <Center> <a href=\\"/assets/tech/094-fasterrcnn-swin/001.png\\" target=\\"_blank\\"> <img src=\\"/assets/tech/094-fasterrcnn-swin/001.png\\" width=\\"700\\"/> </a> </Center> #### Produce Features Pyramid from the Desired Features An appendix of models.swin_t has been provided at the end of this blog post. Here config.fpn_feat_channels = 192 which is to match the smallest number of features in the pyramid (so that we can do addition). Here swin_t can be replaced by swin_s and swin_b for different experiments. Their output shape are still the same (with different number of blocks of transformer encoder). python class SwinFeatureExtractor(nn.Module): def __init__(self): super(SwinFeatureExtractor, self).__init__() self.model = models.swin_t(weights=\\"DEFAULT\\").to(device) self.layer1 = self.model.features[0:2] self.layer2 = self.model.features[2:4] self.layer3 = self.model.features[4:6] self.layer4 = self.model.features[6:8] self.lateral_conv5 = nn.Conv2d(768, config.fpn_feat_channels, 1, 1) self.lateral_conv4 = nn.Conv2d(384, config.fpn_feat_channels, 1, 1) self.lateral_conv3 = nn.Conv2d(192, config.fpn_feat_channels, 1, 1) self.lateral_conv2 = nn.Conv2d(96, config.fpn_feat_channels, 1, 1) self.upscale = lambda input: F.interpolate(input, scale_factor=2) self.freeze_params() def freeze_params(self): for param in self.model.parameters(): param.requires_grad = False def forward(self, x): x_4: Tensor = self.layer1(x) x_8 = self.layer2(x_4) x_16 = self.layer3(x_8) x_32 = self.layer4(x_16) x_4 = x_4.permute([0, 3, 1, 2]) x_8 = x_8.permute([0, 3, 1, 2]) x_16 = x_16.permute([0, 3, 1, 2]) x_32 = x_32.permute([0, 3, 1, 2]) p5 = self.lateral_conv5(x_32) p4 = self.lateral_conv4(x_16) + self.upscale(p5) p3 = self.lateral_conv3(x_8) + self.upscale(p4) p2 = self.lateral_conv2(x_4) + self.upscale(p3) return [p2, p3, p4, p5] Here x_k denotes the feature of spatial dimension $\\\\frac{H}{k} \\\\times \\\\frac{W}{k}$, where _, _, H, W = x.shape. The major changes are just the img_shapes and fpn_feat_channels. The rest are the same. #### Sample Result <Center> <img src=\\"/assets/tech/094-fasterrcnn-swin/002.jpg\\" width=\\"600\\"/> </Center> #### Appendix: Printed Info of the Structure of SwinTransformer text SwinTransformer( (features): Sequential( (0): Sequential( (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4)) (1): Permute() (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True) ) (1): Sequential( (0): SwinTransformerBlock( (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=96, out_features=288, bias=True) (proj): Linear(in_features=96, out_features=96, bias=True) ) (stochastic_depth): StochasticDepth(p=0.0, mode=row) (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=96, out_features=384, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=384, out_features=96, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) (1): SwinTransformerBlock( (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=96, out_features=288, bias=True) (proj): Linear(in_features=96, out_features=96, bias=True) ) (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row) (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=96, out_features=384, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=384, out_features=96, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) ) (2): PatchMerging( (reduction): Linear(in_features=384, out_features=192, bias=False) (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True) ) (3): Sequential( (0): SwinTransformerBlock( (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=192, out_features=576, bias=True) (proj): Linear(in_features=192, out_features=192, bias=True) ) (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row) (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=192, out_features=768, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=768, out_features=192, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) (1): SwinTransformerBlock( (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=192, out_features=576, bias=True) (proj): Linear(in_features=192, out_features=192, bias=True) ) (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row) (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=192, out_features=768, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=768, out_features=192, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) ) (4): PatchMerging( (reduction): Linear(in_features=768, out_features=384, bias=False) (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) ) (5): Sequential( (0): SwinTransformerBlock( (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=384, out_features=1152, bias=True) (proj): Linear(in_features=384, out_features=384, bias=True) ) (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row) (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=384, out_features=1536, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=1536, out_features=384, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) (1): SwinTransformerBlock( (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=384, out_features=1152, bias=True) (proj): Linear(in_features=384, out_features=384, bias=True) ) (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row) (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=384, out_features=1536, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=1536, out_features=384, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) (2): SwinTransformerBlock( (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=384, out_features=1152, bias=True) (proj): Linear(in_features=384, out_features=384, bias=True) ) (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row) (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=384, out_features=1536, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=1536, out_features=384, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) (3): SwinTransformerBlock( (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=384, out_features=1152, bias=True) (proj): Linear(in_features=384, out_features=384, bias=True) ) (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row) (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=384, out_features=1536, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=1536, out_features=384, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) (4): SwinTransformerBlock( (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=384, out_features=1152, bias=True) (proj): Linear(in_features=384, out_features=384, bias=True) ) (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row) (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=384, out_features=1536, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=1536, out_features=384, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) (5): SwinTransformerBlock( (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=384, out_features=1152, bias=True) (proj): Linear(in_features=384, out_features=384, bias=True) ) (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row) (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=384, out_features=1536, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=1536, out_features=384, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) ) (6): PatchMerging( (reduction): Linear(in_features=1536, out_features=768, bias=False) (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True) ) (7): Sequential( (0): SwinTransformerBlock( (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=768, out_features=2304, bias=True) (proj): Linear(in_features=768, out_features=768, bias=True) ) (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row) (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=768, out_features=3072, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=3072, out_features=768, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) (1): SwinTransformerBlock( (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (attn): ShiftedWindowAttention( (qkv): Linear(in_features=768, out_features=2304, bias=True) (proj): Linear(in_features=768, out_features=768, bias=True) ) (stochastic_depth): StochasticDepth(p=0.2, mode=row) (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (mlp): MLP( (0): Linear(in_features=768, out_features=3072, bias=True) (1): GELU(approximate=none) (2): Dropout(p=0.0, inplace=False) (3): Linear(in_features=3072, out_features=768, bias=True) (4): Dropout(p=0.0, inplace=False) ) ) ) ) (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True) (avgpool): AdaptiveAvgPool2d(output_size=1) (head): Linear(in_features=768, out_features=1000, bias=True) ) ","title":"Swin-Transformer Backbone in Faster RCNN","date":"2022-09-20T00:00:00.000Z","id":"blog094","tag":"deep-learning, pytorch","intro":"Describe how to fed the features from Swin Transformer into Faster RCNN."},{"content":" python # model is the nn.Module object that we are going to train: torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clipping_thres) ","title":"Gradient Clipping","date":"2022-09-22T00:00:00.000Z","id":"blog095","tag":"deep-learning, pytorch","toc":false,"intro":"Record a script to clip gradient to avoid graident explosion."},{"content":" #### Paper https://arxiv.org/abs/1709.01507 #### Code Implementation python import torch from torch import nn from torch.nn import init class SEAttention(nn.Module): def __init__(self, channel=512,reduction=16): super().__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.fc = nn.Sequential( nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid() ) def init_weights(self): for m in self.modules(): if isinstance(m, nn.Conv2d): init.kaiming_normal_(m.weight, mode=\'fan_out\') if m.bias is not None: init.constant_(m.bias, 0) elif isinstance(m, nn.BatchNorm2d): init.constant_(m.weight, 1) init.constant_(m.bias, 0) elif isinstance(m, nn.Linear): init.normal_(m.weight, std=0.001) if m.bias is not None: init.constant_(m.bias, 0) def forward(self, x): b, c, _, _ = x.size() y = self.avg_pool(x).view(b, c) y = self.fc(y).view(b, c, 1, 1) return x * y.expand_as(x) ","title":"Squeeze and Excitation Network","date":"2022-09-29T00:00:00.000Z","id":"blog097","tag":"deep-learning, pytorch","intro":"Record an attention-based component."},{"content":" #### Source Code - https://github.com/machingclee/2022-09-20-Swin-Transformer-backbone-Cascade-RCNN/blob/main/src/cascade_faster_rcnn_swin_transformer_fpn.py #### Paper - https://arxiv.org/pdf/1712.00726.pdf #### From Faster RCNN <Center> <img src=\\"/assets/tech/098-cascade-rcnn/002.png\\"/> </Center> <p/> <center></center> Let\'s have a rough review on faster rcnn. In the figure above I is an input image, it is fed into a convolutional backbone and we get a feature tensor. - That feature can be tought of as the local representation of a patched image (the smaller the spatial dimension of the feature, the larger the patched window). - For exmaple, if a feature is of size (B, 256, H/16, W/16), that means each \\"patched window\\" is of size $16\\\\times 16$ and the feature is captured by 256 channels. - In Faster RCNN each feature of patched windows is transformed into the prediction of number of anchors by nn.Conv2d(256, n_anchors*4, 1, 1) and transformed into the coresponding \\"objectness\\" (whether it is a background) by nn.Conv2d(256, n_anchors*2, 1, 1). - These anchors prediction are called **_region proposal_**, they are scored by the \\"objectness\\" scores (which we call logits) in another branch that we have mention aboved. They are then filtered by that scores and also their intersection of union (iou) which measures the overlap of two anchors (if they are too close, the one with lower objectness score will be removed). - The filtered anchors are then used in a pooling layer called ROI-Align. Therefore we get a feature vector of size (B, n_filtered_anchors, 7, 7). These features are used to further predict the **_refinements_** of region proposals that eventually become our final bounding box prediction. #### Improvement from Faster RCNN: Cascade RCNN <Center> <img src=\\"/assets/tech/098-cascade-rcnn/001.png\\"/> </Center> <p/> <center></center> Cascade RCNN goes a little bit further, we get our bounding boxes from the last paragraph in B1. let\'s call these stage-1-prediction. In the next stage, we treat those boxes as our new region proposals and repeat the process to predict a more refined version of the final bounding boxes. The key is: - In this second stage, our proposal is allowed to have higher **_iou threshold_**. - In the thrid stage, we keep increasing our iou threshold. Eventually we will get more and more precise bounding box with smaller and smaller chance of overlapping each another. In terms of code, they look: python roi_cls_losses = 0 roi_reg_losses = 0 pred_fg_bg_logits = flattened_pred_fg_bg_logits levels = flattened_levels for i in range(len(config.cascade_proposal_ious)): if (levels.shape[0] != rois.shape[0]): print(\\"something wrong\\") roi_cls_loss, roi_reg_loss, cls_logits, rois, pred_fg_bg_logits, levels = \\\\ self.stage_prediction_in_training( out_feat, target_boxes, target_cls_indexes, rois, pred_fg_bg_logits, levels, n_levels, config.cascade_proposal_ious[i] ) roi_cls_losses += roi_cls_loss roi_reg_losses += roi_reg_loss Since my work is done on feature pyramid, the implementation will be a lot more complicated, but the idea behind it is simple, the reader can try to implement it on faster rcnn with single-feature-backbone. ","title":"Cascade RCNN","date":"2022-09-30T00:00:00.000Z","id":"blog098","tag":"deep-learning","intro":"Code implementation in pytorch on Cascade RCNN, the code base is mainly a modification of my previous project."},{"content":" json { \\"tasks\\": [ { \\"type\\": \\"cppbuild\\", \\"label\\": \\"C/C++: cl.exe build active file\\", \\"command\\": \\"cl.exe\\", \\"args\\": [ \\"/Zi\\", \\"/EHsc\\", \\"/nologo\\", \\"/std:c++20\\", \\"/Fe:\\", \\"${fileDirname}\\\\\\\\${fileBasenameNoExtension}.exe\\", \\"${file}\\" ], \\"options\\": { \\"cwd\\": \\"${fileDirname}\\" }, \\"problemMatcher\\": [\\"$msCompile\\"], \\"group\\": { \\"kind\\": \\"build\\", \\"isDefault\\": true }, \\"detail\\": \\"Task generated by Debugger.\\" } ], \\"version\\": \\"2.0.0\\" } We may also need to set the C++ compilePath, to find the path of cl.exe, we use the following none gcm cl.exe | select Source in developer powershell for visual studio. ","title":"CPP VSCode Configuration for 2020 Standard","date":"2022-10-08T00:00:00.000Z","id":"blog099","tag":"coding, C++","toc":false,"intro":"Vscode task.json for using 2020 standard."},{"content":" #### Trouble in Standard Dataset Object We come into the following scenario very often in which we need to return the (normalized) image tensor as well as the corresponding annotations. python class WFLWDatasets(data.Dataset): ... def __getitem__(self, index): data = self.get_data() curr_data = data[index] rel_img_path, annotations = curr_data self.img = np.array(Image.open(os.path.join(self.img_dir, os.path.normpath(rel_img_path)))) target = np.array(annotations) # list of 15-dim arrays # img, target become tensor after self.preproc img, target = self.preproc(self.img, target) return img, target ... However, an error will occur when loading the dataset via DataLoader by python from torch.utils.data as DataLoader batch_iterator = iter(data.DataLoader(dataset, batch_size, # >= 2 shuffle=True)) **Reason.** When we return a batch of images, the number of annotations (e.g., 2 bboxes for the first image and 15 bboxes for the second image) will differ, we cannot concat [2, 4]-dimensional tensor with [15, 4]-dimensional tensor. But by default, Dataloader has an attribute collate_fn which is initialized to the following function. python def default_collate(batch): elem = batch[0] elem_type = type(elem) if isinstance(elem, torch.Tensor): out = None if torch.utils.data.get_worker_info() is not None: # If we\'re in a background process, concatenate directly into a # shared memory tensor to avoid an extra copy numel = sum(x.numel() for x in batch) storage = elem.storage()._new_shared(numel, device=elem.device) out = elem.new(storage).resize_(len(batch), *list(elem.size())) return torch.stack(batch, 0, out=out) elif elem_type.__module__ == \'numpy\' and elem_type.__name__ != \'str_\' \\\\ and elem_type.__name__ != \'string_\': if elem_type.__name__ == \'ndarray\' or elem_type.__name__ == \'memmap\': # array of string classes and object if np_str_obj_array_pattern.search(elem.dtype.str) is not None: raise TypeError(default_collate_err_msg_format.format(elem.dtype)) return default_collate([torch.as_tensor(b) for b in batch]) elif elem.shape == (): # scalars return torch.as_tensor(batch) elif isinstance(elem, float): return torch.tensor(batch, dtype=torch.float64) elif isinstance(elem, int): return torch.tensor(batch) elif isinstance(elem, string_classes): return batch elif isinstance(elem, collections.abc.Mapping): try: return elem_type({key: default_collate([d[key] for d in batch]) for key in elem}) except TypeError: # The mapping type may not support __init__(iterable). return {key: default_collate([d[key] for d in batch]) for key in elem} elif isinstance(elem, tuple) and hasattr(elem, \'_fields\'): # namedtuple return elem_type(*(default_collate(samples) for samples in zip(*batch))) elif isinstance(elem, collections.abc.Sequence): # check to make sure that the elements in batch have consistent size it = iter(batch) elem_size = len(next(it)) if not all(len(elem) == elem_size for elem in it): raise RuntimeError(\'each element in list of batch should be of equal size\') transposed = list(zip(*batch)) # It may be accessed twice, so we use a list. if isinstance(elem, tuple): return [default_collate(samples) for samples in transposed] # Backwards compatibility. else: try: return elem_type([default_collate(samples) for samples in transposed]) except TypeError: # The sequence type may not support __init__(iterable) (e.g., range). return [default_collate(samples) for samples in transposed] raise TypeError(default_collate_err_msg_format.format(elem_type)) For short, it unsqueezes each positional entry at 0 position and concat them accordingly to form a batch (e.g., two [3, 256, 256] image tensors will be concated into a [2, 3, 256, 256] tensor). An error occurs when this concatenation process fails. #### Solution by collate_fn Is it necessary for the target (annotation) objects to be concated any way? No, instead, we can return a list (array) of target objects, and reshape our targets (no matter it is [2, 4]-dimensional or [15, 4]-dimensional) into a tensor of consistent shape, and finally concat these consistent target tensors and compute loss against the batch of predictions from the model. Example: - line 68 of [RetinaFace\'s multibox_loss](https://github.com/machingclee/2022-10-05-Retinaface-study/blob/main/layers/modules/multibox_loss.py). The trick is done by defining our own collate_fn: python def collate_fn(batch): imgs = [] batch_annotations = [] for i in range(len(batch)): data = batch[i] img, annotation = data imgs.append(img.unsqueeze(0)) batch_annotations.append(annotation) return torch.cat(imgs, dim=0).to(device), batch_annotations Note that when batch_size=2, then each batch is a list: python [dataset.__getitem__(n_1), dataset.__getitem__(n_2)] for some n_1, n_2 with $\\\\texttt{n_1}\\\\neq \\\\texttt{n_2}$. Therefore - For the first positional entry of our returned output, we concat our image tensor in the usual way (.unsqueeze(0) and torch.cat), but - For the second positional entry, we concat them into a usual list and we return that list instead of a tensor. We plug our collate_fn to DataLoader object by python batch_iterator = iter(data.DataLoader(dataset, batch_size, # >= 2 shuffle=True, collate_fn=collate_fn)) and the iteration result next(batch_iterator) is of type Tensor, List[Tensor]. ","title":"collate_fn in pytorch","date":"2022-11-03T00:00:00.000Z","id":"blog0102","tag":"deep-learning, pytorch","intro":"Discuss how to customize the patching of the results from __getitem__ method of Dataset object in pytorch."},{"content":" #### Common Helper Functions ##### resize_and_padding resize_and_padding makes sure all input are of hte same shape: python def resize_img(img): \\"\\"\\" img: Pillow image \\"\\"\\" h, w = img.height, img.width if h >= w: ratio = config.input_height / h new_h, new_w = int(h * ratio), int(w * ratio) else: ratio = config.input_width / w new_h, new_w = int(h * ratio), int(w * ratio) if new_w > config.input_width: ratio = config.input_width / new_w new_h, new_w = int(new_h * ratio), int(new_w * ratio) img = img.resize((new_w, new_h), Image.BILINEAR) return img, (w, h) python def pad_img(img): h = img.height w = img.width img = np.array(img) img = np.pad(img, pad_width=((0, config.input_height - h), (0, config.input_width - w), (0, 0)), mode=\\"reflect\\") img = Image.fromarray(img) assert img.height == config.input_height assert img.width == config.input_width return img python def resize_and_padding(img, return_window=False): img, (ori_w, ori_h) = resize_img(img) w = img.width h = img.height padding_window = (w, h) img = pad_img(img) if not return_window: return img else: return img, padding_window, (ori_w, ori_h) The padding_window and (ori_w, ori_h) are used to reverse the transform to get the original image. ##### torch_img_transform The built-in transform by pytorch are used to normalized the input: python from torchvision import transforms torch_img_transform = transforms.Compose([ # numpy array will have channels permuted to the second index: (b, c, h, w) transforms.ToTensor(), # normalize from [0, 1] to [-1, 1] transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) ]) ##### torch_imgnet_transform Normalization specific to imagenet data: python from torchvision import transforms torch_imgnet_transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) ##### Inverse of torch_imgnet_transform python def torch_imgnet_denormalization_to_pil(img: torch.Tensor) -> Image.Image: mean = torch.as_tensor([0.485, 0.456, 0.406])[None, :, None, None].to(device) std = torch.as_tensor([0.229, 0.224, 0.225])[None, :, None, None].to(device) img = (img * std + mean) * 255 img = Image.fromarray(img.squeeze(0).permute(1, 2, 0).cpu().numpy().astype(\\"uint8\\")) return img ##### draw_dots and draw_boxes The following are helpful for facial landmarks visualization: python def draw_box(pil_img: Image.Image, bboxes, confs=None, color=(255, 255, 255, 150)): draw = ImageDraw.Draw(pil_img) for i, bbox in enumerate(bboxes): xmin, ymin, xmax, ymax = bbox draw.rectangle(((xmin, ymin), (xmax, ymax)), outline=color, width=2) if confs is not None: conf = confs[i] draw.text( (xmin, max(ymin - 10, 4)), \\"{:.2f}\\".format(conf.item()), color ) python def draw_dots(pil_img: Image.Image, pred_boxes, pred_landmarks: Tuple[float], r=2, constrain_pts=False): draw = ImageDraw.Draw(pil_img) for bbox, landmark in zip(pred_boxes, pred_landmarks): xmin, ymin, xmax, ymax = bbox for x, y in np.array_split(landmark, 5): if not constrain_pts: draw.ellipse((x - r, y - r, x + r, y + r), fill=(255, 0, 0)) else: if xmin <= x and x <= xmax and ymin <= y and y <= ymax: draw.ellipse((x - r, y - r, x + r, y + r), fill=(255, 0, 0)) ##### xyxy_to_cxcywh and cxcywh_to_xyxy python def xyxy_to_cxcywh(bboxes): if len(bboxes) == 0: return bboxes cxcy = (bboxes[:, 0:2] + bboxes[:, 2:4]) / 2 wh = (bboxes[:, 2:4] - bboxes[:, 0:2]) if isinstance(bboxes, torch.Tensor): def cat_func(arr_to_concat): return torch.cat(arr_to_concat, dim=-1) else: def cat_func(arr_to_concat): return np.concatenate(arr_to_concat, axis=-1) out = cat_func([cxcy, wh]) return out python def cxcywh_to_xyxy(bboxes): if len(bboxes) == 0: return bboxes xmin_ymin = bboxes[:, 0:2] - bboxes[:, 2:4] / 2 xmax_ymax = bboxes[:, 0:2] + bboxes[:, 2:4] / 2 if isinstance(bboxes, torch.Tensor): def cat_func(arr_to_concat): return torch.cat(arr_to_concat, dim=-1) else: def cat_func(arr_to_concat): return np.concatenate(arr_to_concat, axis=-1) out = cat_func([xmin_ymin, xmax_ymax]) return out #### Augmentation by Albumentations ##### Resize and Padding python resize_and_padding_transforms_list = [ A.LongestMaxSize(max_size=config.longest_side_length, interpolation=1, p=1), A.PadIfNeeded( min_height=config.input_height, min_width=config.input_height, border_mode=0, value=(0, 0, 0), position=\\"top_left\\" ) ] ##### Miscellaneous Transforms python import albumentations as A albumentation_transform = A.Compose([ A.ShiftScaleRotate(shift_limit=0, scale_limit=(0.5, 2), p=1), A.Perspective(p=0.4), A.Rotate(limit=10, p=0.8), A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.9), A.OneOf([ A.Blur(blur_limit=3, p=0.5), A.ColorJitter(p=0.5) ], p=1.0), *resize_and_padding_transforms_list ], additional_targets={\\"image1\\": \\"image\\"} ) Here is the tricky part, note that python additional_targets={\\"image1\\": \\"image\\"} is used to tell albumentation_transform to accept kwarg image1 which is an image (but not mask). Moreover: - albumentation_transform itself understands not to do certain operations on mask. For example, it makes no sense to do RGB-shift to a mask. - The kwargs image and mask are built-in and ready to be used. - Note that albumentation_process **_only accpets numpy arrays_**. python def albumentation_process(bg_img, img, mask): # bg_img, img, mask are numpy arrays transformsed = albumentation_transform(image=bg_img, image1=img, mask=mask) bg_img = transformsed[\\"image\\"] img = transformsed[\\"image1\\"] mask = transformsed[\\"mask\\"] return bg_img, img, mask #### Results A planar, axes-aligned synthetic image becomes: <center> <img src=\\"/assets/tech/055.png\\"/ width=\\"500\\"> </center> ","title":"Albumentations and Common Helper Functions with PyTorch","date":"2022-11-05T00:00:00.000Z","id":"blog0103","tag":"pytorch, deep-learning","intro":"Record the most recently used combination of data augmentations."},{"content":" #### Repository - https://github.com/machingclee/2022-10-05-Retinaface-study #### Data Processing (Landmarks Specific) ##### Nice Augmentation Implementation for Record keypoints argument in albumentations package causes incorrect augmented annotations for landmarks, for bbox augmentation we should stick with albumentations. In case we need to augment landmarks we can try the following: ###### Cropping python # image is a [H, W, 3]-dimensional numpy array def _crop(image, target_boxes, labels, landm, img_dim): height, width, _ = image.shape pad_image_flag = True for _ in range(250): \\"\\"\\" if random.uniform(0, 1) <= 0.2: scale = 1.0 else: scale = random.uniform(0.3, 1.0) \\"\\"\\" PRE_SCALES = [0.3, 0.45, 0.6, 0.8, 1.0] scale = random.choice(PRE_SCALES) short_side = min(width, height) w = int(scale * short_side) h = w if width == w: l = 0 else: l = random.randrange(width - w) if height == h: t = 0 else: t = random.randrange(height - h) roi_xyxy = np.array((l, t, l + w, t + h)) value = matrix_iof(target_boxes, roi_xyxy[np.newaxis]) flag = (value >= 1) if not flag.any(): continue target_centers = (target_boxes[:, :2] + target_boxes[:, 2:]) / 2 mask_a = np.logical_and(roi_xyxy[:2] < target_centers, target_centers < roi_xyxy[2:]).all(axis=1) boxes_t = target_boxes[mask_a].copy() labels_t = labels[mask_a].copy() landms_t = landm[mask_a].copy() landms_t = landms_t.reshape([-1, 5, 2]) if boxes_t.shape[0] == 0: continue image_t = image[roi_xyxy[1]:roi_xyxy[3], roi_xyxy[0]:roi_xyxy[2]] boxes_t[:, :2] = np.maximum(boxes_t[:, :2], roi_xyxy[:2]) boxes_t[:, :2] -= roi_xyxy[:2] boxes_t[:, 2:] = np.minimum(boxes_t[:, 2:], roi_xyxy[2:]) boxes_t[:, 2:] -= roi_xyxy[:2] # landm landms_t[:, :, :2] = landms_t[:, :, :2] - roi_xyxy[:2] landms_t[:, :, :2] = np.maximum(landms_t[:, :, :2], np.array([0, 0])) landms_t[:, :, :2] = np.minimum(landms_t[:, :, :2], roi_xyxy[2:] - roi_xyxy[:2]) landms_t = landms_t.reshape([-1, 10]) # make sure that the cropped image contains at least one face > 16 pixel at training image scale b_w_t = (boxes_t[:, 2] - boxes_t[:, 0] + 1) / w * img_dim b_h_t = (boxes_t[:, 3] - boxes_t[:, 1] + 1) / h * img_dim mask_b = np.minimum(b_w_t, b_h_t) > 0.0 boxes_t = boxes_t[mask_b] labels_t = labels_t[mask_b] landms_t = landms_t[mask_b] if boxes_t.shape[0] == 0: continue pad_image_flag = False return image_t, boxes_t, labels_t, landms_t, pad_image_flag return image, target_boxes, labels, landm, pad_image_flag For successful cropping, we always get a square. Note that most of the time pad_image_flag == False, the next padding method pads the image into a square in case we are too unfortunate to augment the data correctly in 250 trials (in that case, pad_image_flag == True): python def _pad_to_square(image, rgb_mean, pad_image_flag): if not pad_image_flag: return image height, width, _ = image.shape long_side = max(width, height) image_t = np.empty((long_side, long_side, 3), dtype=image.dtype) image_t[:, :] = rgb_mean image_t[0:0 + height, 0:0 + width] = image return image_t ###### Resize and Mean Subtraction python def _resize_subtract_mean(image, insize, rgb_mean): interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4] interp_method = interp_methods[random.randrange(5)] image = cv2.resize(image, (insize, insize), interpolation=interp_method) image = image.astype(np.float32) image -= rgb_mean return image ###### Mirroring Mirror the image with 0.5 probability: python def _mirror(image, boxes, landms): _, width, _ = image.shape if random.randrange(2): image = image[:, ::-1] boxes = boxes.copy() boxes[:, 0::2] = width - boxes[:, 2::-2] # landm landms = landms.copy() landms = landms.reshape([-1, 5, 2]) landms[:, :, 0] = width - landms[:, :, 0] tmp = landms[:, 1, :].copy() landms[:, 1, :] = landms[:, 0, :] landms[:, 0, :] = tmp tmp1 = landms[:, 4, :].copy() landms[:, 4, :] = landms[:, 3, :] landms[:, 3, :] = tmp1 landms = landms.reshape([-1, 10]) return image, boxes, landms ###### Complete Pipeline Based on the Above python def augment(self, image, targets): assert targets.shape[0] > 0, \\"this image does not have gt\\" target_boxes = targets[:, :4].copy() labels = targets[:, -1].copy() landm = targets[:, 4:-1].copy() # this random crop can also change the landmarks, which is problematic in # albumentation (keyponits argument cause some incorrect augmented # annotation) image_t, boxes_t, labels_t, landm_t, pad_image_flag = _crop(image, target_boxes, labels, landm, self.img_dim) image_t = _distort(image_t) image_t = _pad_to_square(image_t, self.rgb_means, pad_image_flag) image_t, boxes_t, landm_t = _mirror(image_t, boxes_t, landm_t) height, width, _ = image_t.shape # change channel dimension ahead of height, width as well: # also pad to square: image_t = _resize_subtract_mean(image_t, self.img_dim, self.rgb_means) # for pytorch image_t = image_t.transpose(2, 0, 1) # normalize bboxes and landmarks: boxes_t[:, 0::2] /= width boxes_t[:, 1::2] /= height landm_t[:, 0::2] /= width landm_t[:, 1::2] /= height labels_t = np.expand_dims(labels_t, 1) targets_t = np.hstack((boxes_t, landm_t, labels_t)) return image_t, targets_t #### RetinaFace.forward(inputs) ##### Forward python-1 def forward(self, inputs): # inputs is a batch of images out = self.body(inputs) # out = [(1, t_1), (2, t_2), (3, t_3)] where self.body is the resnset50 model, with $$ \\\\texttt{t_i.shape} = (512\\\\times 2^{i}, H/2^{i+3}, W/2^{i+3}) $$ for $i=0, 1, 2$, i.e., features of strides 8, 16, 32 respectively. These features are then mapped to have the same channel size (by what we call lateral embedding and upsampling) and **concatenated** by addition in a usual fpn network. python-4 fpn = self.fpn(out) ##### FPN Here self.fpn is an instance of: python class FPN(nn.Module): def __init__(self,in_channels_list,out_channels): super(FPN,self).__init__() leaky = 0 if (out_channels <= 64): leaky = 0.1 self.output1 = conv_bn1X1(in_channels_list[0], out_channels, stride = 1, leaky = leaky) self.output2 = conv_bn1X1(in_channels_list[1], out_channels, stride = 1, leaky = leaky) self.output3 = conv_bn1X1(in_channels_list[2], out_channels, stride = 1, leaky = leaky) self.merge1 = conv_bn(out_channels, out_channels, leaky = leaky) self.merge2 = conv_bn(out_channels, out_channels, leaky = leaky) def forward(self, input): # names = list(input.keys()) input = list(input.values()) output1 = self.output1(input[0]) output2 = self.output2(input[1]) output3 = self.output3(input[2]) up3 = F.interpolate(output3, size=[output2.size(2), output2.size(3)], mode=\\"nearest\\") output2 = output2 + up3 output2 = self.merge2(output2) up2 = F.interpolate(output2, size=[output1.size(2), output1.size(3)], mode=\\"nearest\\") output1 = output1 + up2 output1 = self.merge1(output1) out = [output1, output2, output3] return out ##### SSH (Single Stage Headless detector) We next encode the features further by single stage headless detector: python-5 feature1 = self.ssh1(fpn[0]) feature2 = self.ssh2(fpn[1]) feature3 = self.ssh3(fpn[2]) features = [feature1, feature2, feature3] Here self.ssh1, self.ssh2 and self.ssh3 are instances of (we set in_channel and out_channel to 256): python class SSH(nn.Module): def __init__(self, in_channel, out_channel): super(SSH, self).__init__() assert out_channel % 4 == 0 leaky = 0 if (out_channel <= 64): leaky = 0.1 self.conv3X3 = conv_bn_no_relu(in_channel, out_channel//2, stride=1) self.conv5X5_1 = conv_bn(in_channel, out_channel//4, stride=1, leaky = leaky) self.conv5X5_2 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1) self.conv7X7_2 = conv_bn(out_channel//4, out_channel//4, stride=1, leaky = leaky) self.conv7x7_3 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1) def forward(self, input): conv3X3 = self.conv3X3(input) conv5X5_1 = self.conv5X5_1(input) conv5X5 = self.conv5X5_2(conv5X5_1) conv7X7_2 = self.conv7X7_2(conv5X5_1) conv7X7 = self.conv7x7_3(conv7X7_2) out = torch.cat([conv3X3, conv5X5, conv7X7], dim=1) out = F.relu(out) return out ##### End of forward: Predictions by ClassHead, BboxHead, LandmarkHead We finally estimate bbox_regressions, classifications and ldm_regressions by identical detection heads on different feature scale: python-9 bbox_regressions = torch.cat( [self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1 ) classifications = torch.cat( [self.ClassHead[i](feature) for i, feature in enumerate(features)], dim=1 ) ldm_regressions = torch.cat( [self.LandmarkHead[i](feature) for i, feature in enumerate(features)], dim=1 ) if self.phase == \'train\': output = (bbox_regressions, classifications, ldm_regressions) else: output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions) return output where python self.ClassHead = self._make_class_head(fpn_num=3, inchannels=cfg[\'out_channel\']) self.BboxHead = self._make_bbox_head(fpn_num=3, inchannels=cfg[\'out_channel\']) self.LandmarkHead = self._make_landmark_head(fpn_num=3, inchannels=cfg[\'out_channel\']) def _make_class_head(self, fpn_num=3, inchannels=64, anchor_num=2): classhead = nn.ModuleList() for i in range(fpn_num): classhead.append(ClassHead(inchannels, anchor_num)) return classhead def _make_bbox_head(self, fpn_num=3, inchannels=64, anchor_num=2): bboxhead = nn.ModuleList() for i in range(fpn_num): bboxhead.append(BboxHead(inchannels, anchor_num)) return bboxhead def _make_landmark_head(self, fpn_num=3, inchannels=64, anchor_num=2): landmarkhead = nn.ModuleList() for i in range(fpn_num): landmarkhead.append(LandmarkHead(inchannels, anchor_num)) class ClassHead(nn.Module): def __init__(self, inchannels=512, num_anchors=3): super(ClassHead, self).__init__() self.num_anchors = num_anchors self.conv1x1 = nn.Conv2d( inchannels, self.num_anchors * 2, kernel_size=(1, 1), stride=1, padding=0 ) def forward(self, x): out = self.conv1x1(x) out = out.permute(0, 2, 3, 1).contiguous() return out.view(out.shape[0], -1, 2) class BboxHead(nn.Module): def __init__(self, inchannels=512, num_anchors=3): super(BboxHead, self).__init__() self.conv1x1 = nn.Conv2d( inchannels, num_anchors * 4, kernel_size=(1, 1), stride=1, padding=0 ) def forward(self, x): out = self.conv1x1(x) out = out.permute(0, 2, 3, 1).contiguous() return out.view(out.shape[0], -1, 4) class LandmarkHead(nn.Module): def __init__(self, inchannels=512, num_anchors=3): super(LandmarkHead, self).__init__() self.conv1x1 = nn.Conv2d( inchannels, num_anchors * 10, kernel_size=(1, 1), stride=1, padding=0 ) def forward(self, x): out = self.conv1x1(x) out = out.permute(0, 2, 3, 1).contiguous() return out.view(out.shape[0], -1, 10) ##### Summary for Detection Modules - RetinaFace network is nothing but a RPN network in faster RCNN. - The only difference is: we use SSH module to decode the features of different scale from mobilenet/resnet instead of passing those features directly to detection head as in faster RCNN. #### Loss Calculation ##### Loss Entry Point python criterion = MultiBoxLoss(num_classes, 0.35, True, 0, True, 7, 0.35, False) priors = PriorBox(cfg, image_size=(img_dim, img_dim)).forward().cuda() loss_l, loss_c, loss_landm = criterion(out, priors, targets) We explain PriorBox in the following: ##### PriorBox In short, PriorBox provides us tiled grids onto a plane with given image_size and stride (in the cfg). python class PriorBoxConfig(TypedDict): min_sizes: int steps: int clip: bool class PriorBox(object): def __init__(self, cfg: PriorBoxConfig, image_size=None, phase=\'train\'): super(PriorBox, self).__init__() self.min_sizes = cfg[\'min_sizes\'] # min sizes = list of sizes of each sq anchor box w.r.t. feature scales, # e.g., [[16, 32], [64, 128], [256, 512]] self.steps = cfg[\'steps\'] # steps = strides self.clip = cfg[\'clip\'] self.image_size = image_size self.feature_maps = [ [ceil(self.image_size[0]/step), ceil(self.image_size[1]/step)] for step in self.steps ] self.name = \\"s\\" def forward(self): anchors = [] for k, f in enumerate(self.feature_maps): min_sizes = self.min_sizes[k] for i, j in product(range(f[0]), range(f[1])): for min_size in min_sizes: # e.g., min_sizes = [16, 32] s_kx = min_size / self.image_size[1] s_ky = min_size / self.image_size[0] cx = (j + 0.5) * self.steps[k] / self.image_size[1] cy = (i + 0.5) * self.steps[k] / self.image_size[0] anchors += [cx, cy, s_kx, s_ky] # back to torch land output = torch.Tensor(anchors).view(-1, 4) if self.clip: output.clamp_(max=1, min=0) return output ##### MultiBoxLoss ###### MultiBoxLoss.\\\\_\\\\_init\\\\_\\\\_ The loss function is abstracted in an nn.Module: python class MultiBoxLoss(nn.Module): \\"\\"\\"SSD Weighted Loss Function Compute Targets: 1) Produce Confidence Target Indices by matching ground truth boxes with (default) \'priorboxes\' that have jaccard index > threshold parameter (default threshold: 0.5). 2) Produce localization target by \'encoding\' variance into offsets of ground truth boxes and their matched \'priorboxes\'. 3) Hard negative mining to filter the excessive number of negative examples that comes with using a large number of default bounding boxes. (default negative:positive ratio 3:1) Objective Loss: L(x,c,l,g) = (Lconf(x, c) + \u03b1Lloc(x,l,g)) / N Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss weighted by \u03b1 which is set to 1 by cross val. Args: c: class confidences, l: predicted boxes, g: ground truth boxes N: number of matched default boxes See: https://arxiv.org/pdf/1512.02325.pdf for more details. \\"\\"\\" def __init__( self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target ): super(MultiBoxLoss, self).__init__() self.num_classes = num_classes self.threshold = overlap_thresh self.background_label = bkg_label self.encode_target = encode_target self.use_prior_for_matching = prior_for_matching self.do_neg_mining = neg_mining self.negpos_ratio = neg_pos self.neg_overlap = neg_overlap self.variance = [0.1, 0.2] ###### MultiBoxLoss.forward python-1 def forward(self, predictions, priors, targets): \\"\\"\\"Multibox Loss Args: predictions (tuple): A tuple containing loc preds, conf preds, and prior boxes from SSD net. conf shape: torch.size(batch_size,num_priors,num_classes) loc shape: torch.size(batch_size,num_priors,4) priors shape: torch.size(num_priors,4) ground_truth (tensor): Ground truth boxes and labels for a batch, shape: [batch_size,num_objs,5] (last idx is the label). \\"\\"\\" loc_data, conf_data, landm_data = predictions priors = priors num = loc_data.size(0) num_priors = (priors.size(0)) # match priors (default boxes) and ground truth boxes loc_t = torch.Tensor(num, num_priors, 4) landm_t = torch.Tensor(num, num_priors, 10) conf_t = torch.LongTensor(num, num_priors) for idx in range(num): truths = targets[idx][:, :4] labels = targets[idx][:, -1] landms = targets[idx][:, 4:14] defaults = priors.data match(self.threshold, truths, defaults, self.variance, labels, landms, loc_t, conf_t, landm_t, idx) ###### The Match Function loc_t, landm_t and conf_t are passed into match function as a reference and will be mutated to get normalized data from targets relative to the anchor with which the target bounding box fit the best. python def match(threshold, truths, priors, variances, labels, landms, loc_t, conf_t, landm_t, idx): \\"\\"\\"Match each prior box with the ground truth box of the highest jaccard overlap, encode the bounding boxes, then return the matched indices corresponding to both confidence and location preds. Args: threshold: (float) The overlap threshold used when mathing boxes. truths: (tensor) Ground truth boxes, Shape: [num_obj, 4]. priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4]. variances: (tensor) Variances corresponding to each prior coord, Shape: [num_priors, 4]. labels: (tensor) All the class labels for the image, Shape: [num_obj]. landms: (tensor) Ground truth landms, Shape [num_obj, 10]. loc_t: (tensor) Tensor to be filled w/ endcoded location targets. conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds. landm_t: (tensor) Tensor to be filled w/ endcoded landm targets. idx: (int) current batch index Return: The matched indices corresponding to 1)location 2)confidence 3)landm preds. \\"\\"\\" # jaccard index overlaps = jaccard( truths, point_form(priors) ) # (Bipartite Matching) # [1,num_objects] best prior for each ground truth best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True) # ignore hard gt valid_gt_idx = best_prior_overlap[:, 0] >= 0.2 best_prior_idx_filter = best_prior_idx[valid_gt_idx, :] if best_prior_idx_filter.shape[0] <= 0: loc_t[idx] = 0 conf_t[idx] = 0 return # [1,num_priors] best ground truth for each prior best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True) best_truth_idx.squeeze_(0) best_truth_overlap.squeeze_(0) best_prior_idx.squeeze_(1) best_prior_idx_filter.squeeze_(1) best_prior_overlap.squeeze_(1) best_truth_overlap.index_fill_(0, best_prior_idx_filter, 2) # ensure best prior # TODO refactor: index best_prior_idx with long tensor # ensure every gt matches with its prior of max overlap for j in range(best_prior_idx.size(0)): # \u5224\u522b\u6b64anchor\u662f\u9884\u6d4b\u54ea\u4e00\u4e2aboxes best_truth_idx[best_prior_idx[j]] = j matches = truths[best_truth_idx] # Shape: [num_priors,4] \u6b64\u5904\u4e3a\u6bcf\u4e00\u4e2aanchor\u5bf9\u5e94\u7684bbox\u53d6\u51fa\u6765 conf = labels[best_truth_idx] # Shape: [num_priors] \u6b64\u5904\u4e3a\u6bcf\u4e00\u4e2aanchor\u5bf9\u5e94\u7684label\u53d6\u51fa\u6765 conf[best_truth_overlap < threshold] = 0 # label as background overlap<0.35\u7684\u5168\u90e8\u4f5c\u4e3a\u8d1f\u6837\u672c loc = encode(matches, priors, variances) matches_landm = landms[best_truth_idx] landm = encode_landm(matches_landm, priors, variances) loc_t[idx] = loc # [num_priors,4] encoded offsets to learn conf_t[idx] = conf # [num_priors] top class label for each prior landm_t[idx] = landm The assignment matches = truths[best_truth_idx] is actually distributing the ground truths to appropriate prior (anochor box) index, as is the assignment matches_landm = landms[best_truth_idx]. ###### Bounding Boxes Encoding and Landmarks Encoding Both encoding functions map the ground truth data to a relative shifting data to the corresponding best anchor **_with all values being normalizaed_**: python def encode(matched, priors, variances): \\"\\"\\"Encode the variances from the priorbox layers into the ground truth boxes we have matched (based on jaccard overlap) with the prior boxes. Args: matched: (tensor) Coords of ground truth for each prior in point-form Shape: [num_priors, 4]. priors: (tensor) Prior boxes in center-offset form, e.g., (cx, cy, w, h) Shape: [num_priors,4]. variances: (list[float]) Variances of priorboxes Return: encoded boxes (tensor), Shape: [num_priors, 4] \\"\\"\\" # dist b/t match center and prior\'s center g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2] # encode variance g_cxcy /= (variances[0] * priors[:, 2:]) # match wh / prior wh g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:] g_wh = torch.log(g_wh) / variances[1] # return target for smooth_l1_loss return torch.cat([g_cxcy, g_wh], 1) # [num_priors,4] def encode_landm(matched, priors, variances): \\"\\"\\"Encode the variances from the priorbox layers into the ground truth boxes we have matched (based on jaccard overlap) with the prior boxes. Args: matched: (tensor) Coords of ground truth for each prior in point-form Shape: [num_priors, 10]. priors: (tensor) Prior boxes in center-offset form Shape: [num_priors,4]. variances: (list[float]) Variances of priorboxes Return: encoded landm (tensor), Shape: [num_priors, 10] \\"\\"\\" # dist b/t match center and prior\'s center matched = torch.reshape(matched, (matched.size(0), config.n_landmarks, 2)) priors_cx = priors[:, 0].unsqueeze(1).expand(matched.size(0), config.n_landmarks).unsqueeze(2) priors_cy = priors[:, 1].unsqueeze(1).expand(matched.size(0), config.n_landmarks).unsqueeze(2) priors_w = priors[:, 2].unsqueeze(1).expand(matched.size(0), config.n_landmarks).unsqueeze(2) priors_h = priors[:, 3].unsqueeze(1).expand(matched.size(0), config.n_landmarks).unsqueeze(2) priors = torch.cat([priors_cx, priors_cy, priors_w, priors_h], dim=2) g_cxcy = matched[:, :, :2] - priors[:, :, :2] # encode variance g_cxcy /= (variances[0] * priors[:, :, 2:]) # g_cxcy /= priors[:, :, 2:] g_cxcy = g_cxcy.reshape(g_cxcy.size(0), -1) # return target for smooth_l1_loss return g_cxcy The variances are hyper-parameters that try to let the model learn more quickly. ###### Calculate the Loss We continue from the **MultiBoxLoss.forward** section: python-29 zeros = torch.tensor(0).cuda() # landm Loss (Smooth L1) # Shape: [batch,num_priors,10] pos1 = conf_t > zeros num_pos_landm = pos1.long().sum(1, keepdim=True) N1 = max(num_pos_landm.data.sum().float(), 1) pos_idx1 = pos1.unsqueeze(pos1.dim()).expand_as(landm_data) landm_p = landm_data[pos_idx1].view(-1, 10) landm_t = landm_t[pos_idx1].view(-1, 10) loss_landm = F.smooth_l1_loss(landm_p, landm_t, reduction=\'sum\') pos = conf_t != zeros conf_t[pos] = 1 # Localization Loss (Smooth L1) # Shape: [batch,num_priors,4] pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data) loc_p = loc_data[pos_idx].view(-1, 4) loc_t = loc_t[pos_idx].view(-1, 4) loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction=\'sum\') # Compute max conf across batch for hard negative mining batch_conf = conf_data.view(-1, self.num_classes) loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1)) # Hard Negative Mining loss_c[pos.view(-1, 1)] = 0 # filter out pos boxes for now loss_c = loss_c.view(num, -1) _, loss_idx = loss_c.sort(1, descending=True) _, idx_rank = loss_idx.sort(1) num_pos = pos.long().sum(1, keepdim=True) num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1) neg = idx_rank < num_neg.expand_as(idx_rank) # Confidence Loss Including Positive and Negative Examples pos_idx = pos.unsqueeze(2).expand_as(conf_data) neg_idx = neg.unsqueeze(2).expand_as(conf_data) conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1,self.num_classes) targets_weighted = conf_t[(pos+neg).gt(0)] loss_c = F.cross_entropy(conf_p, targets_weighted, reduction=\'sum\') # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + \u03b1Lloc(x,l,g)) / N N = max(num_pos.data.sum().float(), 1) loss_l /= N loss_c /= N loss_landm /= N1 return loss_l, loss_c, loss_landm ","title":"RetinaFace","date":"2022-11-06T00:00:00.000Z","id":"blog0104","tag":"deep-learning, pytorch","intro":"Record the study of headless detector for face and landmarks."},{"content":" #### Repository - [Repo Link](https://github.com/machingclee/2022-11-11-landmarks-trial-frontend) - [Deployed Frontend](https://onnx-trial.vercel.app) #### Script to Convert Pytorch Model into ONNX model Let\'s take a model called RetinaFace as an example. Given that we have a target model that has been trained, we convert it into onnx model by pytorch built-in command: python import torch retina_face = RetinaFace(cfg=cfg) retina_face.eval() regression_parser = SimplifiedRegressionParser() regression_parser.eval() dummy_inputs_retina_face = torch.randn( 1, 3, config.input_img_size, config.input_img_size ) dummy_inputs_regression_parser = ( torch.randn(1, config.n_priors, 4), torch.randn(1, config.n_priors), torch.randn(1, config.n_priors, 196) ) torch.onnx.export( retina_face, dummy_inputs_retina_face, \'FaceDetector.onnx\', export_params=True, verbose=False, input_names=[\\"inputImage\\"], output_names=[\\"bbox_regressions\\", \\"scores\\", \\"ldm_regressions\\"], opset_version=11 ) torch.onnx.export( regression_parser, args=dummy_inputs_regression_parser, f=\'RegressionParser.onnx\', export_params=True, verbose=False, input_names=[\\"bbox_regressions\\", \\"scores\\", \\"landm_regressions\\"], output_names=[\\"boxes\\", \\"scores\\", \\"landms\\"], opset_version=11 ) #### Script to Deploy the ONNX model in Frontend ##### Loading Detection Head Model and Prediction Parser Model Let\'s create a file utils/modelUtils. In this inference we break our model into two parts - faceSession - faceRegressionSession and load them via: javascript // utils/modelUtils.ts import * as ort from \\"onnxruntime-web\\"; import { useEffect, useState, Dispatch, SetStateAction } from \\"react\\"; import { getImageTensorFromPath } from \\"./imageUtils\\"; import { time } from \\"./timeUtils\\"; export let faceSession: ort.InferenceSession; export let faceRegressionSession: ort.InferenceSession; export const loadModels = async () => { faceSession = await ort.InferenceSession.create(faceUrl, { executionProviders: [\\"webgl\\"], graphOptimizationLevel: \\"all\\", }); faceRegressionSession = await ort.InferenceSession.create(regressionUrl, { executionProviders: [\\"wasm\\"], graphOptimizationLevel: \\"all\\", }); }; ##### Why the Hack we Need two Models? As can be seen in the parameter executionProviders above, we can choose to use webgl and wasm backend for our model. webgl employes GPU in the inference, but it comes with limitations: - webgl does not support all operators in the given opset_version - Many, many usual operators have no support in webgl - A complete list of supported operators can be found in [this link](https://github.com/microsoft/onnxruntime/blob/main/js/web/docs/operators.md) The way to get around this problem is: - Unsupported operators should be moved into another model - wasm supports all operators in a given opset_version - Therefore we load that additional model with wasm backend in order to execute those unsupported operators ##### Script to Execute Prediction There are many image utils functions, you can unfold the following when you are curious: <details> <summary> Click to inspect utils/imageUtils.ts </summary> javascript // utils/imageUtils.ts import * as Jimp from \'jimp\'; import { Tensor } from \'onnxruntime-web\'; import { getConfig } from \'./configUtils\'; const config = getConfig(); export async function getImageTensorFromPath( path: string, dims: number[] = [ 1, 3, config.modelRequiredSizes?.width || 640, config.modelRequiredSizes?.height || 640 ] ): Promise<Tensor> { var image = await loadImagefromPath(path, dims[2], dims[3]); var imageTensor = imageDataToTensor(image, dims); return imageTensor; } async function loadImagefromPath( path: string, width: number = config.modelRequiredSizes?.width || 640, height: number = config.modelRequiredSizes?.height || 640 ): Promise<Jimp> { // Use Jimp to load the image and resize it. var imageData = await Jimp.default.read(path).then((imageBuffer: Jimp) => { return imageBuffer }); return imageData; } function imageDataToTensor(image: Jimp, dims: number[]): Tensor { // 1. Get buffer data from image and create R, G, and B arrays. var imageBufferData = image.bitmap.data; const [redArray, greenArray, blueArray] = new Array(new Array<number>(), new Array<number>(), new Array<number>()); // 2. Loop through the image buffer and extract the R, G, and B channels for (let i = 0; i < imageBufferData.length; i += 4) { redArray.push(imageBufferData[i]); greenArray.push(imageBufferData[i + 1]); blueArray.push(imageBufferData[i + 2]); // skip data[i + 3] to filter out the alpha channel } // 3. Concatenate RGB to transpose [224, 224, 3] -> [3, 224, 224] to a number array const transposedData = redArray.concat(greenArray).concat(blueArray); // 4. convert to float32 let i, l = transposedData.length; // length, we need this for the loop // create the Float32Array size 3 * 224 * 224 for these dimensions output const float32Data = new Float32Array(dims[1] * dims[2] * dims[3]); for (i = 0; i < l; i++) { float32Data[i] = transposedData[i] / 255.0; // convert to float } // 5. create the tensor object from onnxruntime-web. const inputTensor = new Tensor(\\"float32\\", float32Data, dims); return inputTensor; } --- </details> javascript // utils/modelUtils.ts import * as ort from \'onnxruntime-web\'; import { useEffect, useState, Dispatch, SetStateAction } from \'react\'; import { getImageTensorFromPath } from \'./imageUtils\'; import { time } from \'./timeUtils\'; export const getPredictionFromImagePath = async (imgUrl: string): Promise<{ hasPositiveResult: boolean, result: { inferenceTime: number, regressionTime: number, box: number[], landm: number[], scores: number[] } }> => { if (faceSession && faceRegressionSession) { const inferenceStartTime = time.time() const imgTensor = await getImageTensorFromPath(imgUrl || \\"\\") const feeds: Record<string, ort.Tensor> = {}; feeds[faceSession.inputNames[0]] = imgTensor; const outputData = await faceSession.run(feeds); const output0 = outputData[faceSession.outputNames[0]]; const output1 = outputData[faceSession.outputNames[1]]; const output2 = outputData[faceSession.outputNames[2]]; const inferenceEndtime = time.time() const inferenceTime = inferenceEndtime - inferenceStartTime; const feeds_: Record<string, ort.Tensor> = {}; feeds_[faceRegressionSession.inputNames[0]] = output0; feeds_[faceRegressionSession.inputNames[1]] = output1; feeds_[faceRegressionSession.inputNames[2]] = output2; const regressionStartTime = time.time(); const output = await faceRegressionSession.run(feeds_); const regressionEndtime = time.time(); const regressionTime = regressionEndtime - regressionStartTime const { boxes, landms, scores } = output; const hasPositiveResult = boxes.data.length > 0 && boxes && landms && scores; if (hasPositiveResult) { return ({ hasPositiveResult: true, result: { inferenceTime, regressionTime, box: (boxes.data as any) as number[], landm: (landms.data as any) as number[], scores: (scores.data as any) as number[] } }) } } return ({ hasPositiveResult: false, result: { inferenceTime: 0, regressionTime: 0, box: [], landm: [], scores: [], } }) } ##### Start the Prediction Loop and Draw Result in Canvas javascript const startPrediction_ = async () => { if (continuePredictionRef.current) { const webcamRef = webcamVideoRef.current; const video = webcamRef?.video; const canvas = canvasRef.current; if (video && canvas) { if (canvasWidthHeight.width == 0 && canvasWidthHeight.height == 0) { // both are zero when being initialized like after changing the camera device setCanvasWidthHeight({ width: video?.videoWidth, height: video?.videoHeight, }); } const imgUrl = webcamRef.getScreenshot( config?.modelRequiredSizes || { width: 640, height: 640 } ); const ctx = canvas.getContext(\\"2d\\"); if (ctx) { if (imgUrl) { const prediction = await getPredictionFromImagePath(imgUrl); const { hasPositiveResult, result } = prediction; if (hasPositiveResult) { const { box, landm, scores, inferenceTime, regressionTime } = result; setInfTimes({ inferenceTime, regressionTime }); // shape check: if (box.length == 4 && landm.length == 196 && scores.length == 1) { setScore(scores[0]); const [xmin, ymin, xmax, ymax] = box; ctx.clearRect(0, 0, video?.videoWidth, video?.videoHeight); drawRectangle(ctx, xmin, ymin, xmax, ymax); for (let i = 0; i < 98; i++) { const x = landm[2 * i]; const y = landm[2 * i + 1]; drawDot(ctx, x, y); } } } } } } await startPrediction_(); } onPredictionStop(); }; ","title":"Onnx Model Deployment to Frontend From Pytorch Model","date":"2022-11-14T00:00:00.000Z","id":"blog0105","tag":"deep-learning, pytorch, react","intro":"Record how to change pytorch model into onnx model and deploy it to frontend."},{"content":" #### Structure Let\'s create a folder at the same directory level as pages. <Center> <img src=\\"/assets/tech/106-config-frontend/2022-11-15_034821.png\\"/> </Center> <p/> - default.ts is config in common - dev.ts: development specific config - prod.ts: production specific config We define TConfig to shape the interface of our configuration data: javascript // TConfig.ts export type TConfig = { modelRequiredSizes: { width: number, height: number, }, }; and all of our config files will strictly follow this interface: javascript // default.ts import { TConfig } from \\"./TConfig\\" const config = { modelRequiredSizes: { width: 320, height: 320 } } as Partial<TConfig> export default config javascript // dev.ts import { TConfig } from \\"./TConfig\\" const config = { } as Partial<TConfig> export default config javascript // prod.ts import { TConfig } from \\"./TConfig\\" const config = { } as Partial<TConfig> export default config #### Get Config We combine all the config files under different environments: javascript import _ from \\"lodash\\"; import defaultConfig from \\"../config/default\\"; import devConfig from \\"../config/dev\\"; import prodConfig from \\"../config/prod\\"; import { TConfig } from \\"../config/TConfig\\"; export const getConfig = (): Partial<TConfig> => { const { NODE_ENV } = process.env; const targetCofig: Partial<TConfig> = NODE_ENV == \\"production\\" ? prodConfig : devConfig; const combinedConfig = { ...defaultConfig, ...targetCofig }; return combinedConfig; }; ","title":"Config Files Organization in Frontend Project (Next.js Specific)","date":"2022-11-15T00:00:00.000Z","id":"blog0106","tag":"react, nextjs","intro":"Record how to change pytorch model into onnx model and deploy it to frontend."},{"content":" #### Dependency text yarn add @reduxjs/toolkit react-redux @types/react-redux #### Folder Structures <Center> <img src=\\"/assets/tech/107-reduxtoolkit/2022-11-15_040250.png\\"/> </Center> <p/> ##### hooks.ts js import { TypedUseSelectorHook, useDispatch, useSelector } from \'react-redux\'; import type { RootState, AppDispatch } from \'./store\'; // Use throughout your app instead of plain useDispatch and useSelector export const useAppDispatch = () => useDispatch<AppDispatch>(); export const useAppSelector: TypedUseSelectorHook<RootState> = useSelector; ##### store.ts js import { configureStore, ThunkAction, Action } from \\"@reduxjs/toolkit\\"; import dictSlice from \\"../slices/dictSlice\\"; export const store = configureStore({ reducer: { dict: dictSlice.reducer, }, devTools: true, }); export type AppDispatch = typeof store.dispatch; export type RootState = ReturnType<typeof store.getState>; export type AppThunk<ReturnType = void> = ThunkAction< ReturnType, RootState, unknown, Action<string> >; ##### dictSlice.ts The following is a simple start-up template: js import { createSlice, PayloadAction } from \\"@reduxjs/toolkit\\"; export type DictState = { selectedNoteId: string, selectedPageId: string, selectedVocabId: string, imageText: string, searchText: string, }; const initialState: DictState = { selectedNoteId: \\"\\", selectedPageId: \\"\\", selectedVocabId: \\"\\", imageText: \\"\\", searchText: \\"\\", }; const dictSlice = createSlice({ name: \\"dict\\", initialState, reducers: { setSearchText: (state, action: PayloadAction<string>) => { state.searchText = action.payload; }, }, }); export default dictSlice; ","title":"Redux Toolkit Quick Setup","date":"2022-11-15T00:00:00.000Z","id":"blog0107","tag":"react","intro":"Record the setup for redux toolkit."},{"content":" #### New Syntax js import { makeStyles } from \'tss-react/mui\'; const useStyles = makeStyles()((theme) => ({ button: { color: \\"black\\", textTransform: \\"none\\", borderColor: \\"rgba(0,0,0,0.5)\\", padding: \\"3px 10px\\", borderRadius: 4, \\"&:hover\\": { color: \\"rgba(0,0,0,0.55)\\", borderColor: \\"rgba(0,0,0,0.3)\\" } } })); const SomeComponent = () => { ... const { classes, cx } = useStyles(); ... } #### New Config for Nextjs to Parse CSS on Server Side ##### src/createEmotionCache.js js import createCache from \\"@emotion/cache\\"; const isBrowser = typeof document !== \\"undefined\\"; // On the client side, Create a meta tag at the top of the <head> and set it as insertionPoint. // This assures that MUI styles are loaded first. // It allows developers to easily override MUI styles with other styling solutions, like CSS modules. export default function createEmotionCache() { let insertionPoint; if (isBrowser) { const emotionInsertionPoint = document.querySelector( \'meta[name=\\"emotion-insertion-point\\"]\' ); insertionPoint = emotionInsertionPoint ?? undefined; } return createCache({ key: \\"mui-style\\", insertionPoint }); } ##### pages/\\\\_document.js js import * as React from \\"react\\"; import Document, { Html, Head, Main, NextScript } from \\"next/document\\"; import createEmotionServer from \\"@emotion/server/create-instance\\"; import createEmotionCache from \\"../src/createEmotionCache\\"; export default class MyDocument extends Document { render() { return ( <Html lang=\\"en\\"> <Head> {/* PWA primary color */} <link rel=\\"shortcut icon\\" href=\\"/favicon.ico\\" /> <meta name=\\"emotion-insertion-point\\" content=\\"\\" /> {this.props.emotionStyleTags} </Head> <body> <Main /> <NextScript /> </body> </Html> ); } } // getInitialProps belongs to _document (instead of _app), // it\'s compatible with static-site generation (SSG). MyDocument.getInitialProps = async (ctx) => { // Resolution order // // On the server: // 1. app.getInitialProps // 2. page.getInitialProps // 3. document.getInitialProps // 4. app.render // 5. page.render // 6. document.render // // On the server with error: // 1. document.getInitialProps // 2. app.render // 3. page.render // 4. document.render // // On the client // 1. app.getInitialProps // 2. page.getInitialProps // 3. app.render // 4. page.render const originalRenderPage = ctx.renderPage; // You can consider sharing the same Emotion cache between all the SSR requests to speed up performance. // However, be aware that it can have global side effects. const cache = createEmotionCache(); const { extractCriticalToChunks } = createEmotionServer(cache); ctx.renderPage = () => originalRenderPage({ enhanceApp: (App) => function EnhanceApp(props) { return <App emotionCache={cache} {...props} />; }, }); const initialProps = await Document.getInitialProps(ctx); // This is important. It prevents Emotion to render invalid HTML. // See https://github.com/mui/material-ui/issues/26561#issuecomment-855286153 const emotionStyles = extractCriticalToChunks(initialProps.html); const emotionStyleTags = emotionStyles.styles.map((style) => ( <style data-emotion={${style.key} ${style.ids.join(\\" \\")}} key={style.key} // eslint-disable-next-line react/no-danger dangerouslySetInnerHTML={{ __html: style.css }} /> )); return { ...initialProps, emotionStyleTags, }; }; ##### pages/\\\\_app.tsx js ... import createEmotionCache from \'../src/createEmotionCache\'; import { EmotionCache } from \\"@emotion/cache\\" import { CacheProvider } from \'@emotion/react\'; const clientSideEmotionCache = createEmotionCache(); function MyApp({ Component, pageProps, emotionCache = clientSideEmotionCache }: AppProps & { emotionCache: EmotionCache }) { const AnyComponent = Component as any; useEffect(() => { const jssStyles = document.querySelector(\'#jss-server-side\'); if (jssStyles) { jssStyles.parentElement?.removeChild(jssStyles); } }, []); return ( <CacheProvider value={emotionCache}> ... </CacheProvider> ); } export default MyApp ","title":"makeStyles for react-mui v5.0 and tss-react/mui","date":"2022-11-15T00:00:00.000Z","id":"blog0108","tag":"react, nextjs","intro":"The latest react mui version (v5.0) has breaking changes that make the config files in [THIS POST](/blog/article/Additional-Configuration-for-makeStyles-in-Next-js) fail. We discuss the corresponding necessary changes."},{"content":" 1. Right click the name of current solution 2. Click on Properties 3. Click on General 4. On the top, change Configuration to All Configuration; 5. change Platform to All Platforms 6. In General Properties > Output Directory, type none $(SolutionDir)bin\\\\$(Platform)\\\\$(Configuration)\\\\ 7. In General Properties > Intermediate Directory, type none $(SolutionDir)bin\\\\intermediates\\\\$(Platform)\\\\$(Configuration)\\\\ ","title":"Visual Studio Solution Configuration","date":"2022-11-16T00:00:00.000Z","id":"blog0109","tag":"C++","toc":false,"intro":"Record solution setup for cleaner folder structure"},{"content":" #### Our String Class cpp-1 #include <iostream> using std::cout; using std::cin; using std::endl; using std::ostream; class String { private: \\tsize_t m_size; \\tchar* m_buffer; public: \\tString(const char* string) { \\t\\tm_size = strlen(string); \\t\\tm_buffer = new char[m_size + 1]; \\t\\tmemcpy(m_buffer, string, m_size); \\t\\tm_buffer[m_size] = 0; \\t} \\t~String() { \\t\\tdelete[] m_buffer; \\t} \\tString(const String& other) \\t\\t: m_size(other.m_size) \\t{ \\t\\tm_buffer = new char[m_size + 1]; \\t\\tmemcpy(m_buffer, other.m_buffer, m_size); \\t\\tm_buffer[m_size] = 0; \\t} \\tchar& operator[](int index) { \\t\\treturn m_buffer[index]; \\t} \\tfriend ostream& operator<<(ostream& stream, const String& string); }; We also define our own << for printing our String object. cpp ostream& operator<<(ostream& stream, const String& string) { \\tstream << string.m_buffer; \\treturn stream; } #### Why Copy Constructor is Always of this Signature? The Implicit Conversion The function started at line 23, String(const String& other), is called the **_copy constructor_** of String. In fact, when we do the assignment: cpp String name(\\"James\\"); String name2 = name; The second line undergoes the following processes: 1. Compiler will determine whether name can be fed into one of the overloadings of our constructors. 2. If yes, it will do an **_implicit conversion_** and fed that parameter into that constructor, which is the copy constructor in our case. Btw, anytime we see = we are always copying something unless we are doing auto& a = b, i.e., creating an alias. Note that even a function returns a reference T& some_function() doesn\'t mean auto a = some_function() will store it as a reference, it keeps copying everything and create a in the copy constructor. #### Problem Without our own Copy Constructor - By default the compiler will copy all the member variables, including the pointer m_buffer without copying the heap-allocated array. That\'s what we call **_shallow copy_**. - Exception will be caught when both name and name2 are our of the scope they live since ~String() will be called twice but both name.m_buffer and name2.m_buffer point to the same heap-allocated array. #### Test our String Class in main() cpp int main() { \\t{ \\t\\tString name(\\"James\\"); \\t\\tcout << name << endl; \\t\\tString name2 = name; \\t\\tname2[3] = \'s\'; \\t\\tcout << name2 << endl; \\t} \\tcin.get(); } ","title":"Copy Constructor","date":"2022-11-23T00:00:00.000Z","id":"blog0110","tag":"C++","intro":"We implement our string class and demonstrate how to create a copy constructor for deep copying a string."},{"content":" #### Step 1 We first prepare a header file that contains all the header that we want: cpp // pch.h #pragma once #include <iostream> #include <array> #include <vector> #include <functional> #### Step 2 At the same directory we create a pch.cpp that includes that header file cpp // pch.cpp #include \\"pch.h\\" #### Step 3 Change the properties of pch.cpp as follows: <Center> <a \\ttarget=\\"_blank\\" \\thref=\\"/assets/tech/111-precompiled-header/001.png\\" > \\t<img \\t\\tsrc=\\"/assets/tech/111-precompiled-header/001.png\\" \\t\\twidth=\\"650\\" \\t/> </a> </Center> #### Step 4 Change the properties of **_the whole project_** as follows: <Center> <a \\ttarget=\\"_blank\\" \\thref=\\"/assets/tech/111-precompiled-header/002.png\\" > \\t<img \\t\\tsrc=\\"/assets/tech/111-precompiled-header/002.png\\" \\t\\twidth=\\"650\\" \\t/> </a> </Center> ","title":"Precompiled Header","date":"2022-11-27T00:00:00.000Z","id":"blog0111","tag":"C++","toc":false,"intro":"Record standard procedure to make a precompiled header file to avoid rebuilding it every time we build the whole project."},{"content":" #### Repo - https://github.com/machingclee/cpp-study/blob/main/study/Study/Array.h - https://github.com/machingclee/cpp-study/blob/main/study/Study/Vector.h #### Fixed Size Array The following exmaple is very raw, it serves as the purpose of showing the importance why we need to overload [] twice with different signatures. ##### Definition cpp template<typename T, size_t N> class Array { private: \\tT m_data[N]; public: \\tArray() { \\t\\t//signature: memset(T*, int value, int (size of the array)) \\t\\t//memset(data(), 0, size() * sizeof(T)); \\t\\t//or \\t\\tmemset(&data()[0], 0, size() * sizeof(T)); \\t\\t//as C style arrays are always continguous in memory \\t} \\tconstexpr size_t size() const { \\t\\treturn N; \\t} \\tT& operator[](size_t index) { \\t\\tif (index >= N) { \\t\\t\\t__debugbreak(); \\t\\t} \\t\\treturn m_data[index]; \\t} \\tconst T& operator[] (size_t index) const { \\t\\tif (index >= N) { \\t\\t\\t__debugbreak(); \\t\\t} \\t\\treturn m_data[index]; \\t} \\tT* data() { \\t\\treturn m_data; \\t} }; ##### The main() for Fixed Size Array We test it out by our own main function below: cpp-1 int main(){ \\tconst Array<int, 5> arr; \\tfor (int i = 0; i < arr.size(); i++) { \\t\\tcout << arr[i] << endl; \\t} \\tArray<int, 5> arr2; \\tfor (int i = 0; i < arr2.size(); i++) { \\t\\tarr2[i] = 3; \\t} \\tfor (int i = 0; i < arr2.size(); i++) { \\t\\tcout << arr2[i] << endl; \\t} } Thera are two overloadings for the [] operator on Array class. - **line 2 to line 5.** Since we declare const Array<int, 5>, the operator [] needs to be overloaded with output const T&. - **line 9 to line 11** In order for assigment operator to work, the operator [] needs to be overloaded to return a value T& for each index. The output of the code above: none 0 0 0 0 0 3 3 3 3 3 The initilization and assignment are working correctly. #### Dynamically Sized Arrays ##### Definition ###### Vector3 To simulate real world situation we define a Vector3 class that contains a pointer that points to heap-allocated memory. cpp struct Vector3 { \\tfloat x = 0.0f, y = 0.0f, z = 0.0f; \\tint* m_MemoryBlock; \\tVector3() { \\t\\tm_MemoryBlock = new int[5]; \\t} \\tVector3(float scalar) \\t\\t: x(scalar), y(scalar), z(scalar) \\t{ \\t\\tm_MemoryBlock = new int[5]; \\t} \\tVector3(float x, float y, float z) \\t\\t: x(x), y(y), z(z) \\t{ \\t\\tm_MemoryBlock = new int[5]; \\t} \\tVector3(const Vector3& other) : \\t\\tx(other.x), y(other.y), z(other.z) \\t{ \\t\\tstd::cout << \\"Copy\\\\n\\"; \\t\\tm_MemoryBlock = new int[5]; \\t} \\t~Vector3() { \\t\\tstd::cout << \\"Destroy\\\\n\\"; \\t\\tdelete[] m_MemoryBlock; \\t} \\t//Vector3& operator=(const Vector3& other) { \\t// \\t//\\tstd::cout << \\"Copy\\\\n\\"; \\t//\\tx = other.x; \\t//\\ty = other.y; \\t//\\tz = other.z; \\t//\\treturn *this; \\t//} \\tVector3& operator=(Vector3&& other) noexcept { \\t\\tm_MemoryBlock = other.m_MemoryBlock; \\t\\tother.m_MemoryBlock = nullptr; \\t\\tstd::cout << \\"Move\\\\n\\" ; \\t\\tx = other.x; \\t\\ty = other.y; \\t\\tz = other.z; \\t\\treturn *this; \\t} }; ostream& operator<<(ostream& stream, const Vector3& vec3) { \\tstream << \\"(\\" << vec3.x << \\", \\" << vec3.y << \\", \\" << vec3.z << \\")\\"; \\treturn stream; } ###### Vector (The Dynamic Array) cpp template<typename T> class Vector { public: \\tVector() { \\t\\tReAlloc(2); \\t} \\t~Vector() { \\t\\tClearByIndividualDestructor(); \\t\\t::operator delete(m_Data, m_Capacity * sizeof(T)); \\t} \\tsize_t Size() { \\t\\treturn m_Size; \\t} \\tT& operator [] (int index) { \\t\\tif (index >= m_Size) { \\t\\t\\t__debugbreak(); \\t\\t} \\t\\treturn m_Data[index]; \\t} \\tconst T& operator [] (int index) const { \\t\\treturn m_Data[index]; \\t} \\ttemplate<typename... Args> \\tT& EmplaceBack(Args&&... args) { \\t\\tstd::cout << \\"EmplaceBack\\\\n\\"; \\t\\tif (m_Size >= m_Capacity) { \\t\\t\\tm_Capacity = (size_t)(m_Capacity * 1.5); \\t\\t\\tReAlloc(m_Capacity); \\t\\t} \\t\\t//m_Data[m_Size] = T(std::forward<Args>(args)...); \\t\\tnew(&m_Data[m_Size])T(std::forward<Args>(args)...); \\t\\tm_Size++; \\t\\treturn m_Data[m_Size]; \\t} \\tvoid PopBack() { \\t\\tif (m_Size > 0) { \\t\\t\\tm_Size--; \\t\\t\\tm_Data[m_Size].~T(); \\t\\t} \\t} \\tvoid ClearByIndividualDestructor() { \\t\\tfor (size_t i = 0; i < m_Size; i++) { \\t\\t\\tm_Data[i].~T(); \\t\\t} \\t\\tm_Size = 0; \\t} \\tvoid PushBack(const T& value) { \\t\\tLOG(\\"here\\"); \\t\\tif (m_Size >= m_Capacity) { \\t\\t\\tm_Capacity = (size_t)(m_Capacity * 1.5); \\t\\t\\tReAlloc(m_Capacity); \\t\\t} \\t\\tm_Data[m_Size] = value; \\t\\tm_Size++; \\t} \\tvoid PushBack(const T* valuePtr) { \\t\\tif (m_Size >= m_Capacity) { \\t\\t\\tm_Capacity = (size_t)(m_Capacity * 1.5); \\t\\t\\tReAlloc(m_Capacity); \\t\\t} \\t\\tm_Data[m_Size] = std::move((T) *valuePtr); \\t\\tm_Size++; \\t} \\tvoid PushBack(T&& value) { \\t\\tif (m_Size >= m_Capacity) { \\t\\t\\tm_Capacity = (size_t)(m_Capacity * 1.5); \\t\\t\\tReAlloc(m_Capacity); \\t\\t} \\t\\t// when value (being a Rvalue) goes into this function, value itself become an Lvalue inside this function \\t\\t// we need to convert value into a temporary value by std::move \\t\\tm_Data[m_Size] = std::move(value); \\t\\tm_Size++; \\t} private: \\tT* m_Data = 0; \\tsize_t m_Size = 0; \\tsize_t m_Capacity = 0; \\tvoid ReAlloc(size_t newCapacity) { \\t\\tT* newBlock = (T*)::operator new(newCapacity * sizeof(T)); \\t\\tif (newCapacity < m_Size) { \\t\\t\\tm_Size = newCapacity; \\t\\t} \\t\\tfor (size_t i = 0; i < m_Size; i++) { \\t\\t\\t// newBlock[i] = std::move(m_Data[i]); \\t\\t\\tnew(&newBlock[i]) T(std::move(m_Data[i])); \\t\\t} \\t\\tfor (size_t i = 0; i < m_Size; i++) { \\t\\t\\tm_Data[i].~T(); \\t\\t} \\t\\t// this will not call the destructor of all object inside the array \\t\\t// therefore allocated memory will not be deleted twice \\t\\t::operator delete(m_Data, m_Capacity * sizeof(T)); \\t\\tm_Data = newBlock; \\t\\tm_Capacity = newCapacity; \\t} }; ##### The main() for Dynamic Array cpp void PrintVector(Vector<T>& vector) { \\tfor (size_t i = 0; i < vector.Size(); i++) { \\t\\tstd::cout << vector[i] << std::endl; \\t} \\tstd::cout << \\"---------------------------\\" << std::endl; } int main() { Vector<Vector3> vectors; vectors.EmplaceBack(5); vectors.EmplaceBack(); vectors.PushBack(Vector3(3, 3, 1)); vectors.PushBack(new Vector3(1, 5, 5)); vectors.EmplaceBack(5, 5, 1); vectors.EmplaceBack(6); vectors.EmplaceBack(7); vectors.EmplaceBack(5, 5, 2); vectors.EmplaceBack(2, 2, 1); vectors.PopBack(); vectors.PopBack(); PrintVector(vectors); } No copy constructor as been called (in fact if it were called we get an error as we have commented the copy constructor out). The output: none EmplaceBack EmplaceBack Move Move Destroy Destroy Move Destroy EmplaceBack Move Move Move Destroy Destroy Destroy EmplaceBack Move Move Move Move Destroy Destroy Destroy Destroy EmplaceBack EmplaceBack Move Move Move Move Move Move Destroy Destroy Destroy Destroy Destroy Destroy EmplaceBack Destroy Destroy (5, 5, 5) (0, 0, 0) (3, 3, 1) (5, 5, 1) (6, 6, 6) (7, 7, 7) --------------------------- Destroy Destroy Destroy Destroy Destroy Destroy ","title":"Array and Dynamic Array Class","date":"2022-11-29T00:00:00.000Z","id":"blog0113","tag":"C++","intro":"We can avoid heap memory allocation by using C style array stored in stack memory, given that we don\'t need such a flexibility of dynamic array (the std::vector class) and the size of the target array is known beforehand."},{"content":" #### Repo - https://github.com/machingclee/cpp-study/blob/main/study/Study/Vector.h #### Iterator By comparing the old and modern form of iterator we will understand which functions are required to be implemented. ##### Original form of Iterator cpp for (Vector<int>::Iterator it = values.begin(); \\tit != values.end(); \\tit++ ) { \\t\\tstd::cout << *it << std::endl; } ##### Modern Simplified form of Iterator cpp for (int value : values) { \\tstd::cout << value << std::endl; } ##### Defining Iterator Class Therefore we need to - define an iterator object (which is to be instantiated inside Vector class) that has defined: - ++ - * (dereference) - == - != - define methods begin() and start() to output Iterator. Next, a specific classname cannot be used as a template parameter, but a generic class that is parametrized by a typename: cpp template<typename T> class Vector { \\t... } can as well be used as a template parameter as follow: cpp template<typename Vector> class VectorIterator { public: \\tusing ValueType = typename Vector::ValueType; \\tusing PointerType = ValueType*; \\tusing ReferenceType = ValueType&; public: \\tVectorIterator(PointerType ptr) : m_Ptr(ptr) { \\t} \\t// prefix ++ \\tVectorIterator& operator++() { \\t\\tm_Ptr++; \\t\\treturn *this; \\t} \\tReferenceType operator*() { \\t\\treturn *m_Ptr; \\t} \\tbool operator ==(const VectorIterator& other) const { \\t\\treturn m_Ptr == other.m_Ptr; \\t} \\tbool operator !=(const VectorIterator& other) const { \\t\\treturn !(*this == other); \\t} private: \\tPointerType m_Ptr; }; Some important takeaway: - Here using are all used to give various alias to types. - The **_scoped-alias_** created by using can be accessed by VectorIterator::PointerType. - In order to let compiler distinguish between types and static members, we add typename keyword when defining ValueType, this Vector::ValueType will be added in the next section. #### Expand our Vector Class that Returns Iterator A lot of hard work has been done in the VectorIterator class above. In our original Vector class we just point out the additional code that bring VectorIterator into play. cpp template<typename T> class Vector { public: \\tusing ValueType = T; \\tusing Iterator = VectorIterator<Vector<T>>; \\t... and cpp public: \\tIterator begin() { \\t\\treturn Iterator(m_Data); \\t} \\tIterator end() { \\t\\treturn Iterator(m_Data + m_Size); \\t} #### Test our Iterator in main() Now we can test our iterator by: cpp int main(){ \\tVector<Vector3> vectors; \\tvectors.EmplaceBack(5); \\tvectors.EmplaceBack(); \\tvectors.PushBack(Vector3(3, 3, 1)); \\tvectors.PushBack(new Vector3(1, 5, 5)); \\tvectors.EmplaceBack(5, 5, 1); \\tvectors.EmplaceBack(6); \\tvectors.EmplaceBack(7); \\tvectors.EmplaceBack(5, 5, 2); \\tvectors.EmplaceBack(2, 2, 1); \\tvectors.PopBack(); \\tvectors.PopBack(); \\t//PrintVector(vectors); \\tfor (Vector3& vector : vectors) { \\t\\tstd::cout << vector << std::endl; \\t} } and the output is the same as before: none EmplaceBack EmplaceBack Move Move Destroy Destroy Move Destroy Move Move Move Destroy Destroy Destroy Copy Move Destroy EmplaceBack Move Move Move Move Destroy Destroy Destroy Destroy EmplaceBack EmplaceBack Move Move Move Move Move Move Destroy Destroy Destroy Destroy Destroy Destroy EmplaceBack EmplaceBack Destroy Destroy (5, 5, 5) (0, 0, 0) (3, 3, 1) (1, 5, 5) (5, 5, 1) (6, 6, 6) (7, 7, 7) Destroy Destroy Destroy Destroy Destroy Destroy Destroy ","title":"Iterator","date":"2022-11-30T00:00:00.000Z","id":"blog0114","tag":"C++","intro":"We try to implement iterator that iterates elements inside our vector class in the [PREVIOUS BLOG](/blog/article/Array-and-Dynamic-Array-Class)."},{"content":" cpp template <class F, class First, class... Rest> void do_for(F f, First first, Rest... rest) { f(first); do_for(f, rest...); } template <class F> void do_for(F f) { std::cout << \\"\\\\n\\"; } template <class... Args> void print(Args... args) { do_for([](auto& arg) { std::cout << arg; }, args...); } ","title":"Variadic Version of Print in C++","date":"2022-12-09T00:00:00.000Z","id":"blog0115","tag":"C++","toc":false,"intro":"Record a useful print function which behaves like console.log in javascript and print in python."},{"content":" #### Repo The C++ project we made will be a CMake project. - https://github.com/machingclee/2022-12-12-CMake-TCP-Server-Study #### Install Boost Asio - We have a static library called MOYFNetworking, we just need to include the boost header once inside our MOYFNetworking/common.h. - We will be able to use boost library once we include this common.h file. - Inside the library MOYFNetworking, we need to define the environment variable BOOST_ROOT which points to the include directory of the boost library. text set(BOOST_ROOT \\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\C++Libraries\\\\\\\\boost_1_80_0\\") as is done in [CMakeLists.txt](https://github.com/machingclee/2022-12-12-CMake-TCP-Server-Study/blob/main/MUDOnYourFace/MOYFNetworking/CMakeLists.txt). #### Networking ##### common.h We include all common header files and create useful utility functions inside this file: cpp Networking/include/Networking/common.h #pragma once #include \\"boost/asio.hpp\\" #include <iostream> #include <iterator> #include <vector> namespace MOYF { using tcp = boost::asio::ip::tcp; } enum class MyEnum { One, Two, Three }; template<class F, class First, class... Rest> void do_for(F f, First first, Rest... rest) { f(first); do_for(f, rest...); } template<class F> void do_for(F f) { std::cout << \\"\\\\n\\"; } template<class... Args> void print(Args... args) { do_for([](auto &arg) { std::cout << arg; }, args...); } ##### TCPConnection cpp // tcp_connection.h #pragma once #include \\"MOYFNetworking/common.h\\" #include <memory> namespace MOYF { class TCPConnection : public std::enable_shared_from_this<TCPConnection> { public: using Pointer = std::shared_ptr<TCPConnection>; static Pointer Create(boost::asio::io_context &ioContext) { return Pointer(new TCPConnection(ioContext)); } tcp::socket &Socket() { return _socket; } void Start(); private: explicit TCPConnection(boost::asio::io_context &ioContext); private: tcp::socket _socket; std::string _message{\\"Hello, client!\\"}; }; } cpp // tcp_connection.cpp #include \\"MOYFNetworking/tcp_connection.h\\" #include <memory> namespace MOYF { TCPConnection::TCPConnection(boost::asio::io_context &ioContext) : _socket(ioContext) { } void TCPConnection::Start() { auto strongThis = shared_from_this(); auto dataToSend = boost::asio::buffer(_message); boost::asio::async_write( _socket, dataToSend, [strongThis](const boost::system::error_code &error, size_t bytesTransferred) { if (error) { print(\\"Failed to send message\\"); } else { print(\\"Sent \\", bytesTransferred, \\" bytes of data.\\"); } } ); boost::asio::streambuf buffer; _socket.async_receive( buffer.prepare(512), [this](const boost::system::error_code &error, size_t bytesTransferred) { if (!error) { print(\\"Client Disconnected Properly.\\"); } else { print(\\"Client Disconnected in Bad Way.\\"); } }); } } ##### TCPServer cpp // tcp_server.h #pragma once #include \\"MOYFNetworking/common.h\\" #include \\"MOYFNetworking/tcp_connection.h\\" namespace MOYF { enum class IPV { V4, V6 }; class TCPServer { public: TCPServer(IPV ip_version, int port); int Run(); private: void StartAccept(); private: IPV _ipVersion; int _port; boost::asio::io_context _ioContext{}; boost::asio::ip::tcp::acceptor _acceptor; std::vector<TCPConnection::Pointer> _connections{}; }; } cpp // tcp_server.cpp #include \\"MOYFNetworking/tcp_server.h\\" #include \\"MOYFNetworking/tcp_connection.h\\" namespace MOYF { using tcp = boost::asio::ip::tcp; TCPServer::TCPServer(IPV ip_version, int port) : _ipVersion(ip_version), _port(port), _acceptor(tcp::acceptor( _ioContext, tcp::endpoint(ip_version == IPV::V4 ? tcp::v4() : tcp::v6(), _port) )) {} int TCPServer::Run() { try { StartAccept(); _ioContext.run(); } catch (std::exception &e) { print(e.what()); return -1; } return 0; } void TCPServer::StartAccept() { // this connection will be destroyed once it is out of scope auto connection = TCPConnection::Create(_ioContext); _connections.push_back(connection); _acceptor.async_accept( connection->Socket(), [this, connection](const boost::system::error_code ec) { // capture connection by value; if (!ec) { connection->Start(); } else { print(ec.what()); } StartAccept(); }); } } #### NetClient This is just a single cpp file which creates a binary file that pings our server: cpp // main.cpp #include <iostream> #include \\"MOYFNetworking/common.h\\" #include <array> using tcp = boost::asio::ip::tcp; int main(int argc, char *argv[]) { try { boost::asio::io_context ioContext; \\t\\t\\t\\ttcp::socket socket{ioContext}; tcp::resolver resolver{ioContext}; auto endpoints = resolver.resolve(\\"127.0.0.1\\", \\"8080\\"); boost::asio::connect(socket, endpoints); while (true) { std::array<char, 128> receivedDataBuffer{}; boost::system::error_code error; size_t len = socket.read_some(boost::asio::buffer(receivedDataBuffer), error); if (error == boost::asio::error::eof) { break; } else if (error) { throw boost::system::system_error(error); }; std::cout.write(receivedDataBuffer.data(), len); } } catch (std::exception &e) { print(e.what()); } } Recall that main is the only special function that needs not to return anything though the return type is int. #### NetServer cpp // main.cpp #include \\"boost/asio.hpp\\" #include \\"MOYFNetworking/tcp_server.h\\" int port_number = 8080; int main() { MOYF::TCPServer server{MOYF::IPV::V4, port_number}; server.Run(); } ","title":"TCP Server Fundamental","date":"2022-12-12T00:00:00.000Z","id":"blog0116","tag":"C++","intro":"A simple TCP server that receive and hold a connection."},{"content":" #### Usage The md file consumed by the gray-matter package is of the form: md --- id: portfolio001 title: Desktop App to Capture Text From Images intro: A desktop app by python using google\'s vison api. thumbnail: /assets/portfolios/thumbnails/tkinter.jpg tech: Python, tkinter thumbWidth: 350 thumbTransX: -20 thumbTransY: -45 date: 2019-12-29 --- #### Repository - In Python with Tkinter: \\\\ https://github.com/machingclee/TextCaptrue_FirstTrial ... Now we can separate attributes and content as follows: js import matter from \\"gray-matter\\"; ... const { content, data } = matter(mdText); content will always be a string, data will be of type: typescript { id: string, title:string, intro: string, thumbnail: string, tech: string, thumbWidth: number, thumbTransX: number, thumbTransY: number, date: Date } The parser in gray-matter will implicitly convert our data into corresponding data type. We list some specific cases for the conversion in the next section. Don\'t forget to cast your data to your custom type for clean coding: js const data_ = data as CustomType; #### Remark on Implicit Conversion Specific keywords will trigger implicit conversion from plain text to available data type. - 111, a plain number, will be converted to a number - false, and true will be converted to booleans - 2022-01-01 will be converted to a Date object To avoid them, we can enclose desired string by \\"\'s. For example, update: \\"true\\" in the attribute list will produce data.update: string. ","title":"Gray Matter","date":"2022-12-14T00:00:00.000Z","id":"blog0117","tag":"react, javascript, coding","intro":"Record the usage of gray-matter."},{"content":" #### Resource to Learn CMake - [An Introduction to Modern CMake](https://cliutils.gitlab.io/modern-cmake/) #### Starting Template cmake cmake_minimum_required(VERSION 3.22) set(This SuperHelloWorld) project( ${This} VERSION 1.0.0 LANGUAGES C CXX ) list(APPEND CMAKE_MODULE_PATH ${CMAKE_BINARY_DIR}) list(APPEND CMAKE_PREFIX_PATH ${CMAKE_BINARY_DIR}) set(CMAKE_CXX_STANDARD 20) set(CMAKE_CXX_STANDARDCONFIGURED_FILE_INCLUDE_DIR_REQUIRED ON) set(CMAKE_CXX_EXTENSIONS OFF) set(LIBRARY_MY_LIB my_lib) set(EXE_APP app) # include(FetchContent) add_subdirectory(src) # message(\\"Using FetchContent\\") # FetchContent_Declare( # nlohmann_json # GIT_REPOSITORY https://github.com/nlohmann/json.git # GIT_TAG v3.11.2 # GIT_SHALLOW TRUE # ) # FetchContent_MakeAvailable(nlohmann_json) #### Common Commands - cmake add_library(one two.cpp three.h) - Declare a target called one that refers to the current directory, and list all sources files to be compiled. Only the compilation unit two.cpp will be compiled, we include the headers **_for IDE only_**. - cmake add_executable(${THIS}_exe main.cpp) - Same as add_library, it adds a target that refers to the current directory, and that target points to an executable. - We cannot use add_library and add_target at the same time. In other words, we should separate main.cpp and \\"source files\\" in separate folder. - For example, we can separate like - some_proj/app/main.cpp and - some_proj/src/some_lib/some_file.cpp Then write - some_proj/app/CMakeLists.txt - some_proj/src/CMakeLists.txt - some_proj/src/some_lib/CMakeLists.txt separately. Note that some_proj/src/CMakeLists.txt can be as simple as just one line cmake add_subdirectory(some_lib) as it helps point out which directory contains a CMakeLists.txt to look at. - cmake add_subdirectory(src) - It tells cmake compiler which directory to look for and execute a CMakeLists.txt. If the directory src contains no CMakeLists.txt file, cmake will give an exception. - cmake # src/one/CMakeLists.txt target_include_directories(one PUBLIC ../../include) - It tells cmake our source files in the target one have included header files in **_other diectory_** such as ../../include. - We don\'t need to include it again in other target that links to one, by simply target_link_libraries (we introduce it right below) we can directly include header files in source code that one have already included. - If the include dir contains include/four/five.h, then we can include four/five.h in our source code. Note that the string to include our header file is independent of the target name we name in cmake. - In general, target_include_directories is used when the header files are from other directory. - cmake target_link_libraries(another PUBLIC one) - This is to build dependency between different targets. Which means that the target another and its downstream linkers will need library one in the compilation process. - Ya we have included the include directory but very likely it just contains function declarations, we need the function body definitions by linking those libraries $\\\\iff$ linking the source files. - if PUBLIC is replaced by PRIVATE, it indicates that the downstream linkers of another do not need the library one, and another is the only target that needs one. - cmake target_compile_definitions(one PRIVATE SOME_CONSTANT=\\"${SOME_CONSTANT}\\") - Target library one we have a constant SOME_CONSTANT defined by using #define in the header. - cmake target_compile_features(one INTERFACE cxx_std_20) - Require specific feature for a target. - cmake set( SOME_CONSTANT \\"\\" # <value> CACHE STRING \\"Description of the cached constant\\" ) - SOME_CONSTANT is the variable name. - We set \\"\\" as a default value. - The value <value> can also be passed by -D argument: none cmake -B build -DSOME_CONSTANT=ABCDEFG - STRING defined the data type of the cached value <value>. - cmake if(SOME_CONSTANT STREQUAL \\"\\") message(SEND_ERROR \\"SOME_CONSTANT\\" must not be empty) endif() - This is a standard if statement. - cmake option(COMPILE_EXECUTABLE \\"Whether to compile to executable\\" OFF) - It defines a boolean for cmake files to use. - We can pass this variable in command line by text cmake .. -DCOMPILE_EXECUTABLE=ON #### Include Predefined CMake Functions Usually every cmake project contains a cmake/ folder that contains custom cmake script: cmake # project_root/CMakeLists.txt set(CMAKE_MODULE_PATH \\"${PROJECT_SOURCE_DIR}/cmake/\\") include(AddGitSubModule) - CMAKE_MODULE_PATH points to that cmake folder - include is used to include the cmake file cmake/AddGitSubmodule.cmake. Exmaple of a .cmake file: cmake # project_root/cmake/AddGitSubmodule.cmake function(add_git_submodule install_destination) find_package(Git REQUIRED) if (NOT EXISTS ${install_destination}/CMakeLists.txt) execute_process(COMMAND ${GIT_EXECUTABLE} submodule update --init --recursive -- ${install_destination} WORKING_DIRECTORY ${PROJECT_SOURCE_DIR} ) endif() add_subdirectory(${dir}) endfunction(add_git_submodule) #### Which Folder to Create CMakeLists.txt? - **Root directory.** - Set all necessary variables and add appropriate add_subdirectory\'s to look for CMakeLists.txt. - **Source file directory src.** - It can consists of simply the add_subdirectory commands. - **Source file subdirectories src/one, src/two, ....** - Each of the subdirectory should consists of add_library for linkage and target_include_directories to include corresponding header files. - If the project structure is simpler, the rules to src/<lib_name>/CMakeLists.txt can be moved down to src/CMakeLists.txt. - **Directory that does not need CMakeLists.txt.** - Directory that contains just header files **_needs no_** CMakeLists.txt, as they will be included in target_include_directories somewhere else once needed. #### Show all Constants in a CMake Project cmake cmake -LAH #### Special Functions to Check Variables in CMakeLists.txt cmake cmake_minimum_required(VERSION 3.22) project(CMakeTutorial LANGUAGES CXX) set(FOO \\"test\\") add_executable(CMakeTutorial main.cpp) function(print) foreach(var ${ARGN}) message(\\"[${var}]: ${${var}}\\") endforeach() endfunction() function(print_env) foreach(var ${ARGN}) message(\\"[${var}]: $ENV{${var}}\\") endforeach() endfunction() print(PROJECT_NAME FOO) - The print function will print defined constant and print_env will print the defined environment variable. #### Graph Visualization of Dependencies Between cmake Files ##### Install graphviz For windows we can download a .msi file for installation [HERE](http://www.graphviz.org/). For mac we run brew install graphviz. ##### MakeFile ###### Scrips We create a file called MakeFile: makefile # MakeFile dependency: \\tcd build && cmake .. --graphviz=graph.dot && \\\\ dot -Tpng graph.dot -o ../dep_graph.png prepare: \\trm -rf build \\tmkdir build and run cmake dependency. The prepare script is also included here, it is a default command that can be run by simply calling cmake. From my own experience on a simple hello-world project I have: <Center> <a href=\\"/assets/tech/118-cmake/dep_graph.png\\" target=\\"_blank\\"> <img width=\\"680\\" src=\\"/assets/tech/118-cmake/dep_graph.png\\"/> </a> </Center> ###### Cmake Erorr: incompatible versions of the cygwin DLL It is a known problem in windows, go to C:\\\\Program Files\\\\Git\\\\usr\\\\bin and rename msys-2.0.dll to _msys-2.0.dll to temporarily mitigate the problem. Later you may need to rename it back since it affects commands such as npm and yarn. #### Cross Platform Built Command ##### Source of Study - [An Introduction to Mdern CMake](https://cliutils.gitlab.io/modern-cmake/) ##### How to Build a CMake Project text mkdir build cd build cmake .. -G \\"Visual Studio 17 2022\\" -A x64 -DCMAKE_BUILD_TYPE=Debug VERBOSE=1 cmake --build . ##### How to Know Which Generator (specified by -G) is Available? By text cmake --help apart from the list of available arguments, it also prints a list of available generators for us. In my case, I get none Visual Studio 17 2022 = Generates Visual Studio 2022 project files. Use -A option to specify architecture. Visual Studio 16 2019 = Generates Visual Studio 2019 project files. Use -A option to specify architecture. Visual Studio 15 2017 [arch] = Generates Visual Studio 2017 project files. Optional [arch] can be \\"Win64\\" or \\"ARM\\". Visual Studio 14 2015 [arch] = Generates Visual Studio 2015 project files. Optional [arch] can be \\"Win64\\" or \\"ARM\\". Visual Studio 12 2013 [arch] = Generates Visual Studio 2013 project files. Optional [arch] can be \\"Win64\\" or \\"ARM\\". Visual Studio 11 2012 [arch] = Generates Visual Studio 2012 project files. Optional [arch] can be \\"Win64\\" or \\"ARM\\". Visual Studio 10 2010 [arch] = Deprecated. Generates Visual Studio 2010 project files. Optional [arch] can be \\"Win64\\" or \\"IA64\\". Visual Studio 9 2008 [arch] = Generates Visual Studio 2008 project files. Optional [arch] can be \\"Win64\\" or \\"IA64\\". Borland Makefiles = Generates Borland makefiles. NMake Makefiles = Generates NMake makefiles. NMake Makefiles JOM = Generates JOM makefiles. ... #### External Library (CMake Project) ##### By Direct Cloning text git submodule add https://target/repo.git external/some_name and then add_subdirectory(external/some_name) to seek for and execute the CMakeLists.txt. ##### By FetchContent By adding include(FetchContent) in our CMakeLists.txt we can import functions - FetchContent_Declare - FetchContent_MakeAvailable For example, suppose that we want to import the following CMake projects hosted in github: cmake include(FetchContent) FetchContent_Declare( nlohmann_json GIT_REPOSITORY https://github.com/nlohmann/json.git GIT_TAG v3.11.2 GIT_SHALLOW TRUE ) FetchContent_Declare( fmt GIT_REPOSITORY https://github.com/fmtlib/fmt GIT_TAG 9.1.0 GIT_SHALLOW TRUE ) FetchContent_Declare( spdlog GIT_REPOSITORY https://github.com/gabime/spdlog GIT_TAG v1.11.0 GIT_SHALLOW TRUE ) FetchContent_Declare( cxxopts GIT_REPOSITORY https://github.com/jarro2783/cxxopts GIT_TAG v3.0.0 GIT_SHALLOW TRUE ) FetchContent_Declare( Catch2 GIT_REPOSITORY https://github.com/catchorg/Catch2 GIT_TAG v2.13.9 GIT_SHALLOW TRUE ) FetchContent_MakeAvailable(nlohmann_json) FetchContent_MakeAvailable(fmt) FetchContent_MakeAvailable(spdlog) FetchContent_MakeAvailable(cxxopts) FetchContent_MakeAvailable(Catch2) where the GIT_TAG can be found in the release page of the corresponding repository. These library will be downloaded in build/_deps directory when we execute cmake build command. To let our target (executable target or library target) link to these library, we run cmake target_link_libraries( ${LIBRARY_MY_LIB} PUBLIC cxxopts::cxxopts nlohmann_json::nlohmann_json fmt::fmt spdlog::spdlog Catch2::Catch2 ) The naming convention of the target is - <project_name>:<library_name> The <project_name> and <library_name> can be traced by looking the CMakeLists.txt of the repo. Now our target LIBRARY_MY_LIB or any target that links to it can run the following preprocessor directives: cpp #include \\"nlohmann/json.hpp\\" #include \\"cxxopts.hpp\\" #include \\"fmt/format.h\\" #include \\"spdlog/spdlog.h\\" #include \\"catch2/catch.hpp\\" ##### By conan ###### Files to Create Create a virtual environment in Python, which I name it conan, then conda activate conan and pip install conan. As if requirements.txt in Python we have an analog in conan, we create a text file conanfile.txt in project root and add the following content: - project_root/conanfile.txt conan [requires] nlohmann_json/3.11.2 fmt/9.1.0 spdlog/1.11.0 catch2/2.13.9 cxxopts/3.0.0 [generators] cmake_find_package cmake_paths - project_root/CMakeLists.txt cmake message(\\"Using Conan\\") include(${CMAKE_BINARY_DIR}/conan_paths.cmake) find_package(nlohmann_json) find_package(fmt) find_package(spdlog) find_package(Catch2) find_package(cxxopts) In Makefile of project root directory we add text ifeq \'$(findstring ;,$(PATH))\' \';\' CONAN_FLAGS = -s compiler=\'Visual Studio\' -s compiler.version=17 -s cppstd=20 --build missing else CONAN_FLAGS = -s cppstd=17 --build missing endif prepare_conan: \\trm -rf build \\tmkdir build \\tcd build && conan install .. $(CONAN_FLAGS) and run make prepare_conan. ###### Remarks to conan I myself fail to work with conan in windows, maybe unix based system can make it work. In general the database of conan usually lag behind to the latest release for at least half year, it is suggested not to use it when FetchContent suffices to serve the purpose. #### CMake Examples ##### ChatClient TCP Server ###### Repo and Video The whole project implementes a chatting function between multiple clients. - [Repo Link](https://github.com/machingclee/2022-12-12-CMake-TCP-Server-Study) - [Video Link](https://www.youtube.com/watch?v=Bz38jjFB3H8) This blog post focuses on the CMakeLists.txt files. ###### Outermost CMakeLists.txt, the Project Level <Center> <img src=\\"/assets/tech/118-cmake/outermost.png\\"/> </Center> <p/> cmake cmake_minimum_required(VERSION 3.22.2) set(CMAKE_CXX_STANDARD 20) add_subdirectory(MOYFNetworking) add_subdirectory(MOYFClient) add_subdirectory(MOYFServer) ###### The Main Library: Networking <Center> <img src=\\"/assets/tech/118-cmake/networking.png\\"/> </Center> cmake cmake_minimum_required(VERSION 3.22.2) project(MOYFNetworking) set(CMAKE_CXX_STANDARD 20) set(BOOST_ROOT \\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\C++Libraries\\\\\\\\boost_1_80_0\\") find_package(Boost REQUIRED) file(GLOB_RECURSE SOURCES src/*.cpp) add_library(${PROJECT_NAME} ${SOURCES}) # this says when building ${PROJECT_NAME} library, what follows must also be included target_include_directories( ${PROJECT_NAME} PUBLIC $<INSTALL_INTERFACE:include> $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include> ${Boost_INCLUDE_DIRS} PRIVATE ) # PRIVATE means downstream linkers that link to ${PROJECT_NAME} # library will not have access to ${Boost_LIBRARIES} # in other words, ${Boost_LIBRARIES} is # only used by the library ${PROJECT_NAME}. # If PRIVATE is replaced by PUBLIC, the downstream linkers that links # ${PROJECT_NAME} will be able to use those libraries (${Boost_LIBRARIES} in this case). target_link_libraries( ${PROJECT_NAME} PRIVATE ${Boost_LIBRARIES} ) ###### NetClient <Center> <img src=\\"/assets/tech/118-cmake/client.png\\"/> </Center> <p/> cmake cmake_minimum_required(VERSION 3.22.2) project(MOYFClient) set(CMAKE_CXX_STANDARD 20) add_executable(${PROJECT_NAME} main.cpp) target_include_directories( ${PROJECT_NAME} PUBLIC MOYFNetworking ) target_link_libraries( ${PROJECT_NAME} PUBLIC MOYFNetworking ) ###### NetServer <Center> <img src=\\"/assets/tech/118-cmake/server.png\\"/> </Center> cmake cmake_minimum_required(VERSION 3.22.2) project(MOYFServer) set(CMAKE_CXX_STANDARD 20) add_executable(${PROJECT_NAME} main.cpp) target_include_directories( ${PROJECT_NAME} PUBLIC MOYFNetworking ) target_link_libraries( ${PROJECT_NAME} PUBLIC MOYFNetworking ws2_32 ) ##### Examples from Other Project ###### Sockets > **Repo.** https://github.com/rhymu8354/SocketTutorial <center></center> This is a CMakeLists.txt file inside a directory called Sockets. cmake-1 set(This Sockets) set(Sources include/Sockets/ClientSocket.hpp include/Sockets/DatagramSocket.hpp include/Sockets/ServerSocket.hpp src/Abstractions.hpp src/ClientSocket.cpp src/Connection.hpp src/Connection.cpp src/DatagramSocket.cpp src/ServerSocket.cpp ) if(MSVC) list(APPEND Sources src/AbstractionsWin32.cpp ) else() list(APPEND Sources src/AbstractionsPosix.cpp src/PipeSignal.cpp src/PipeSignal.hpp ) endif() add_library(${This} ${Sources}) set_target_properties(${This} PROPERTIES FOLDER Libraries) target_include_directories(${This} PUBLIC include) if(UNIX) target_link_libraries(${This} PUBLIC pthread) endif(UNIX) From the section [The-Main-Library:-Networking](#The-Main-Library:-Networking) we have read a use case cmake file(GLOB_RECURSE SOURCES src/*.cpp) and plugged this into add_library. However, the [Do\'s and Don\'ts](https://cliutils.gitlab.io/modern-cmake/chapters/intro/dodonot.html) states that **_Don\'t GLOB files_** because: > Make or another tool will not know if you add files without rerunning CMake. Note that CMake 3.12 adds a CONFIGURE_DEPENDS flag that makes this far better if you need to use it. In other words, listing the source files explicitly will be a better practice. ","title":"Simple Introduction to CMake Files","date":"2022-12-15T00:00:00.000Z","id":"blog0118","tag":"C++","intro":"Record some experience and the CMakeLists.txt\'s that I have used."},{"content":" #### Utils - Mimic the Split Functions for Text as in JS and Python **Header.** cpp #pragma once #include <string> #include <vector> std::vector<std::string> split(std::string text, std::string space_delimiter = \\" \\"); **Implementation.** cpp #include \\"utils/split.h\\" std::vector<std::string> split(std::string text, std::string space_delimiter) { bool hasNext = false; std::vector<std::string> words{}; do { size_t nextPos = text.find(space_delimiter); hasNext = (nextPos != std::string::npos); words.push_back(text.substr(0, nextPos)); text.erase(0, nextPos + space_delimiter.length()); } while (hasNext); return words; } #### TcpListener **Header.** cpp #pragma once #include <WS2tcpip.h> #include <iostream> #include <sstream> using namespace std; class TcpListener { public: TcpListener(const char* ipAddress, int port); int init(); // initialize the listener int run(); // run the listener protected: virtual void onClientConnected(int clientSocket) = 0; // handler for client connection virtual void onClientDisconnected(int clientSocket) = 0; // handler for client disconnection virtual void onMessageReceived(int currSock, char* buffer, int bytesReceived) = 0; // handler for received message void sendToClient(int clientSocket, const char* msg, int length); // send message to client void broadcastToClients(int sendingClient, const char* msg, int length); private: const char* _ipAddress; // ip of the web server int _port; // port for the web service int _socket; // internal socket fd fd_set _fd_set; // master fd set }; **Implementation.** cpp #include \\"TcpListener.h\\" TcpListener::TcpListener(const char* ipAddress, int port) : _ipAddress(ipAddress), _port(port){}; int TcpListener::init() { // initialize winsock WSADATA wsData; WORD ver = MAKEWORD(2, 2); int wsOk = WSAStartup(ver, &wsData); if (wsOk != 0) { cerr << \\"Can\'t Initialize winsock! Quitting\\" << endl; return wsOk; } // create a socket _socket = socket(AF_INET, SOCK_STREAM, 0); if (_socket == INVALID_SOCKET) { cerr << \\"can\'t create a socket, quitting\\" << endl; return WSAGetLastError(); } // bind the ip address and port to a socket sockaddr_in hint; hint.sin_family = AF_INET; hint.sin_port = htons(_port); // hint.sin_addr.S_un.S_addr = INADDR_ANY; // hint.sin_addr.S_un.S_addr = inet_addr(_ipAddress); inet_pton(AF_INET, _ipAddress, &hint.sin_addr); int bindResult = bind(_socket, (sockaddr*)&hint, sizeof(hint)); if (bindResult == SOCKET_ERROR) { return WSAGetLastError(); } // tell winsock the socket is for listening int listeningResult = listen(_socket, SOMAXCONN); if (listeningResult == SOCKET_ERROR) { return WSAGetLastError(); } FD_ZERO(&_fd_set); FD_SET(_socket, &_fd_set); return 0; }; int TcpListener::run() { while (true) { fd_set copy = _fd_set; int socketCount = select(0, &copy, nullptr, nullptr, nullptr); for (int i = 0; i < socketCount; i++) { SOCKET currSock = copy.fd_array[i]; if (currSock == _socket) { // accept a new connection // sockaddr_in client; // int clientSize = sizeof(client); // SOCKET clientSocket = accept(listening, (sockaddr *)&client, &clientSize); SOCKET clientSocket = accept(_socket, nullptr, nullptr); // add the new connection the list of connected clients // socket <-> fd <-> u_int FD_SET(clientSocket, &_fd_set); onClientConnected(clientSocket); } else { char buffer[4096]; ZeroMemory(buffer, 4096); // receive message int bytesReceived = recv(currSock, buffer, 4096, 0); if (bytesReceived <= 0) { // drop the client // TODO: client disconnected; onClientDisconnected(currSock); closesocket(currSock); FD_CLR(currSock, &_fd_set); } else { // send message toimage.png other clients, excluding the listening socket onMessageReceived(currSock, buffer, bytesReceived); bool isNullSentence = buffer[0] == 13 && buffer[1] == 10; if (!isNullSentence) { for (int i = 0; i < _fd_set.fd_count; i++) { SOCKET outSock = _fd_set.fd_array[i]; if (outSock != _socket && outSock != currSock) { // ostringstream ss; // ss << \\"SOCKET #\\" << sock << \\": \\" << buffer << \\"\\\\r\\\\n\\"; // string strOut = ss.str(); // send(outSock, strOut.c_str(), strOut.size() + 1, 0); } } } } } } } // remove the listening socket from the master fd set and close it // to prevent anyone else trying to connect. FD_CLR(_socket, &_fd_set); closesocket(_socket); while (_fd_set.fd_count > 0) { SOCKET sock = _fd_set.fd_array[0]; FD_CLR(sock, &_fd_set); closesocket(sock); } WSACleanup(); return 0; }; void TcpListener::sendToClient(int clientSocket, const char* msg, int length) { send(clientSocket, msg, length, 0); } void TcpListener::broadcastToClients(int sendingClientToExlcude, const char* msg, int length) { for (int i = 0; i < _fd_set.fd_count; i++) { int outSock = _fd_set.fd_array[i]; if (sendingClientToExlcude != _socket && sendingClientToExlcude != outSock) { sendToClient(outSock, msg, length); } } } #### MultiClientServer **Header.** cpp #pragma once #include \\"TcpListener.h\\" class MultiClientChat : public TcpListener { public: MultiClientChat(const char* ipAddress, int port) : TcpListener(ipAddress, port){}; protected: void onClientConnected(int clientSocket); void onClientDisconnected(int clientSocket); void onMessageReceived(int currSock, char* buffer, int bytesReceived); }; **Implementation.** cpp #include \\"MultiClientChat.h\\" void MultiClientChat::onClientConnected(int clientSocket) { // send a welcome message to the connected client std::string welcomeMsg = \\"Welcome to the chat server\\"; sendToClient(clientSocket, welcomeMsg.c_str(), welcomeMsg.size() + 1); }; void MultiClientChat::onClientDisconnected(int clientSocket) { ostringstream ss; ss << \\"Client #\\" << clientSocket << \\" has disconnected\\"; std::string msg = ss.str(); broadcastToClients(clientSocket, msg.c_str(), msg.size() + 1); }; void MultiClientChat::onMessageReceived(int currSock, char* buffer, int bytesReceived) { broadcastToClients(currSock, buffer, bytesReceived); } #### WebServer **Header.** cpp #pragma once #include \\"TcpListener.h\\" class WebServer : public TcpListener { public: WebServer(const char* ipAddress, int port) : TcpListener(ipAddress, port){}; protected: void onClientConnected(int clientSocket); void onClientDisconnected(int clientSocket); void onMessageReceived(int currSock, char* buffer, int bytesReceived); }; **Implementation.** cpp #include \\"WebServer.h\\" #include \\"utils/split.h\\" #include <fstream> #include <iostream> #include <sstream> #include <string> #include <vector> void WebServer::onClientConnected(int clientSocket){ }; void WebServer::onClientDisconnected(int clientSocket){ }; void WebServer::onMessageReceived(int currSock, char* buffer, int bytesReceived) { // GET /index.html HTTP/1.1 // parse out the doucment requested // open the document in local file system // write the document back to the client std::string clientMessage{buffer}; // std::istringstream iss(buffer); std::vector<std::string> parsed = split(clientMessage, \\" \\"); std::string method = parsed[0]; std::string targetHtml = parsed[1]; if (method == \\"GET\\") { std::string fileRootLocation = \\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\Javascript\\\\\\\\2021-02-18-machingclee.github.io\\"; int htmlPos = targetHtml.find(\\".html\\"); // remove .html in case it exists if (htmlPos != std::string::npos) { targetHtml.erase(htmlPos, htmlPos + 5); } // naive routing targetHtml.erase(0, 1); std::string filePath = fileRootLocation + \\"\\\\\\\\\\" + (targetHtml == \\"\\" ? \\"\\" : (targetHtml + \\"\\\\\\\\\\")) + \\"index.html\\"; std::ifstream file{filePath}; std::string content{\\"404 Not Found\\"}; if (file.good()) { std::ostringstream ss; ss << file.rdbuf(); content = ss.str(); } file.close(); std::ostringstream ss; ss << \\"HTTP/1.1 200 OK\\\\r\\\\n\\" << \\"Cache-Control: no-cache, private\\\\r\\\\n\\" << \\"Content-Type: text/html\\\\r\\\\n\\" << \\"Content-Length: \\" << content.size() << \\"\\\\r\\\\n\\" << \\"\\\\r\\\\n\\" << content; std::string res = ss.str(); sendToClient(currSock, res.c_str(), res.size() + 1); } }; ","title":"Socket Programming Fundamentals by winsock","date":"2023-01-10T00:00:00.000Z","id":"blog0119","tag":"C++","intro":"We create a simple TCP Server via winsock."},{"content":" #### Install libtorch in Windows From [libtorch official web site](https://pytorch.org/get-started/locally/) we select <Center> <a href=\\"/assets/tech/120/001.png\\"> <img src=\\"/assets/tech/120/001.png\\" width=\\"680\\"/> </a> </Center> <p/> <center></center> and click the link it pops up to download. #### Models from pytorch to pytorch C++ ##### A Model in Python python import torch import torch.nn.functional as F class Net(torch.nn.Module): def __init__(self): super(Net, self).__init__() self.layer1 = torch.nn.Linear(100, 256) self.layer2 = torch.nn.Linear(256, 1) def forward(self, x): x = self.layer1(x) x = F.relu(x) x = self.layer2(x) x = F.relu(x) return x if __name__ == \\"__main__\\": traced_net = torch.jit.trace(Net(), torch.randn(1, 100)) torch.jit.save(traced_net, \\"models/net.pt\\") ##### Inference in C++ ###### Minimal Working CMakeLists.txt cmake add_executable(pymode_to_cpp main.cpp) target_link_libraries(pymode_to_cpp PUBLIC ${TORCH_LIBRARIES}) if (MSVC) message(\\"copying dll files\\") file(GLOB TORCH_DLLS \\"${TORCH_INSTALL_PREFIX}/lib/*.dll\\") add_custom_command(TARGET pymode_to_cpp POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy_if_different ${TORCH_DLLS} $<TARGET_FILE_DIR:pymode_to_cpp>) endif (MSVC) ###### The main.cpp File cpp #include \\"torch/script.h\\" #include \\"torch/torch.h\\" #include <iostream> #include <string> std::string model_pt_path{\\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\C++\\\\\\\\ 2023-01-12-DGGAN-in-pytorch-cpp-and-CMake-practice\\\\\\\\models\\\\\\\\net.pt\\"}; int main() { torch::jit::script::Module net = torch::jit::load(model_pt_path); torch::Tensor x = torch::randn({1, 100}); torch::Tensor y = torch::randn({1, 100}); torch::Tensor inputs = torch::cat({x, y}); std::vector<torch::IValue> x_{inputs}; torch::Tensor yTensor = net.forward(x_).toTensor(); size_t ySize = yTensor.sizes()[0]; float* yDataPtr = (float*)yTensor.data_ptr(); try { // float result = output.toTensor().item<float>(); for (int i = 0; i < ySize; i++) { float value = yDataPtr[i]; std::cout << \\"The Float Value output: \\" << value << std::endl; } } catch (const c10::Error& e) { std::cerr << e.msg() << std::endl; } } which yields text The Float Value output: 0.606562 The Float Value output: 0.160477 Note that when yTensor is known to have batchSize 1, we may use Tensor.item<float>() to retrieve the data instead of resorting to the data_ptr() method. ","title":"Pytorch/libtorch with CPP API","date":"2023-01-11T00:00:00.000Z","id":"blog0120","tag":"C++, pytorch","intro":"We discuss how to import models trained in pytorch into cpp project."},{"content":" Sometimes soruce file cannot be found even cmake tool can build the project successfully, this is because an incorrect configuration provider is set, we can fix it by F1 and text >C++: Change Configuration Provider and choose cmake tool. Which essentially modifies .vscode/settings.json and sets text \\"C_Cpp.default.configurationProvider\\": \\"ms-vscode.cmake-tools\\" ","title":"C++ VSCode says it cannot open source file when it really can","date":"2023-01-28T00:00:00.000Z","id":"blog0121","tag":"C++","toc":false,"intro":"Fix the error of failing to find source files when it really can for cmake projects."},{"content":" #### Python\'s print cpp #include <string> #include <vector> #include <iostream> std::vector<std::string> split(std::string text, std::string space_delimiter = \\" \\"); template <class F, class First, class... Rest> void do_for(F f, First first, Rest... rest) { f(first); do_for(f, rest...); } template <class F> void do_for(F f) { std::cout << \\"\\\\n\\"; } template <class... Args> void print(Args... args) { do_for([](auto& arg) { std::cout << arg; }, args...); } #### Python\'s os.sep cpp std::string get_os_sep() { #ifdef _WIN32 return \'\\\\\\\\\'; #else return \'/\'; #endif } #### Python\'s z_fill cpp #include <sstream> #include <iomanip> std::string z_fill(int n, int numberOfLeadingZeros) { std::ostringstream s; s << std::setw(numberOfLeadingZeros) << std::setfill(\'0\') << n; return s.str(); } #### Python\'s os.listdir cpp #include <string> #include <iostream> #include <filesystem> #include <vector> std::vector<std::string> list_dir(std::string directory_path) { std::vector<std::string> file_paths; for (const auto& entry : fs::directory_iterator(directory_path)) { file_paths.push_back(entry.path()); } return file_paths; } #### Python\'s String.split() cpp #include <string> #include <vector> std::vector<std::string> split(std::string text, std::string space_delimiter) { bool hasNext = false; std::vector<std::string> words{}; do { size_t nextPos = text.find(space_delimiter); hasNext = (nextPos != std::string::npos); words.push_back(text.substr(0, nextPos)); text.erase(0, nextPos + space_delimiter.length()); } while (hasNext); return words; } #### Regular Expression and Exhaust all Possible Matchings **Situation.** There will be files called recording000001.avi, recording000002.avi among a list of files inside a directory. We need to find out all these avi files, sort the digits, and produce 000003 as a counter for the next .avi file. cpp #include <regex> #include <algorithm> std::string get_file_next_digit(std::string dir, std::string file_ext) { std::string sep = get_os_sep(); std::regex number_regex(\\"(\\\\\\\\d+)(?=\\\\\\\\.\\" + file_ext + \\")\\"); std::vector<std::string> file_paths = list_dir(dir); std::vector<int> integers; // for each file_path, we split them by sep, get the last string, // then exhaust all the matchings for (std::string& file_path : file_paths) { std::vector<std::string> results = split(file_path, sep); std::string file_name = results.back(); std::sregex_iterator current_match(file_name.begin(), file_name.end(), \\\\ number_regex); std::sregex_iterator lastMatch; while (current_match != lastMatch) { std::smatch match = *current_match; int num = std::stoi(match.str()); integers.push_back(num); current_match++; } } if (integers.size() > 0) { std::sort(integers.begin(), integers.end()); std::string next_number = z_fill(integers[integers.size() - 1] + 1, 6); return next_number; } else { return z_fill(0, 6); } } ","title":"C++ Useful Util Functions Mimiced from Python, Regular Expression","date":"2023-02-09T00:00:00.000Z","id":"blog0122","tag":"C++","intro":"Record useful utility functions that I have found during my project on desktop app in C++."},{"content":" # Conditional Variable We intentionally init a write_thread, sleep for 2s, and the init a read_thread to make sure write_thread has made notification before the read_thread wait for it. Without the boolean condition, the read_thread will wait indefinitely and we need to close the program forcefully, which is called a **lost wakeup**. This can be solved by introducing a shared boolean condition (which will be locked by our mutex guard, no data race) Then cv.wait(guard, [] { return condition; }), apart form waiting for cv.notify_one, will also check whether the notification as been published (by checking condition). cpp #include <iostream> #include <thread> #include <condition_variable> #include <string> using namespace std; using namespace std::chrono; // Global variables mutex mut; condition_variable cv; string sdata{\\"Empty\\"}; bool condition{false}; // Waiting thread void reader() { unique_lock<std::mutex> guard(mut); // Acquire lock cv.wait(guard, [] { return condition; }); // Wait for condition variable to be notified cout << \\"Data is \\" << sdata << endl; // Wake up and use the new value } // Modyifing thread void writer() { cout << \\"Writing data...\\" << endl; { lock_guard<std::mutex> lg(mut); // Acquire lock std::this_thread::sleep_for(1s); // Pretend to be busy... sdata = \\"Populated\\"; condition = true; // Modify the data } cv.notify_one(); // Notify the condition variable } int main() { cout << \\"Data is \\" << sdata << endl; thread write_thread{writer}; std::this_thread::sleep_for(2s); thread read_thread{reader}; write_thread.join(); read_thread.join(); } # Promise and Future cpp #include <future> #include <iostream> #include <thread> using namespace std; void produce(promise<int>& px) { // Producer function with promise int x{42}; this_thread::sleep_for(1s); cout << \\"Promise sets shared state to \\" << x << endl; px.set_value(x); // Set the result } void consume(future<int>& fx) { // Consumer function with future cout << \\"Future calling get()...\\" << endl; int x = fx.get(); // Get the result cout << \\"Future returns from calling get()\\" << endl; cout << \\"The answer is \\" << x << endl; } int main() { promise<int> p; future<int> f = p.get_future(); thread prom{produce, std::ref(p)}; thread fut{consume, std::ref(f)}; fut.join(); prom.join(); } ","title":"Commuication Between Two Threads","date":"2023-02-27T00:00:00.000Z","id":"blog0123","tag":"C++","intro":"Discuss how two threads communiate with each other."},{"content":" #### Result on Local Machine <Center> <img src=\\"/assets/tech/124/005.png\\" width=\\"100%\\" /> </Center> <p/> <center></center> Due to time constraint I didn\'t wait the model to train for long enough time as It is convincing to me that the model is trying to converge. #### Reference Main reference for coding part: - [Diffusion models from scratch in PyTorch](https://www.youtube.com/watch?v=a4Yfz2FxXiY&fbclid=IwAR14YaOBTAwTp_mmp_Q5MALMRuecTxDLUwcTAESifcB8IlqpmFYDy_IlnV4) Other references for theory: - [Denoising Diffusion Probabilitic Model](https://arxiv.org/pdf/2006.11239.pdf) - [Diffusion Model\uff1a\u6bd4\u201cGAN\\"\u8fd8\u8981\u725b\u903c\u7684\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff01\u516c\u5f0f\u63a8\u5bfc+\u8bba\u6587\u7cbe\u8bfb\uff0c\u8fea\u54e5\u6253\u4f60\u4ece\u96f6\u8be6\u89e3\u6269\u6563\u6a21\u578b\uff01](https://www.bilibili.com/video/BV1pD4y1179T/?spm_id_from=333.788.recommend_more_video.11&vd_source=eaeec3286e77493a42a3dce415ee67cc) - [Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models | ML Coding Series](https://www.youtube.com/watch?v=f6PtJKdey8E) #### Introduction <Center> <img src=\\"/assets/tech/124/001.png\\" width=\\"100%\\" /> </Center> <p/> <center></center> - Define $x_t$ as the image at time $t=T$ for $T\\\\in {\\\\mathbb N}_0$. When we travel about $t$, we add noise gradually until the image is unreadable. The noise is added by $$ x_{t} = \\\\sqrt{\\\\alpha_{t-1}} x_{t-1} + \\\\sqrt{1-\\\\alpha_{t-1}}z_1, $$ for $t\\\\ge1$, where $z_t\\\\sim \\\\mathcal N(0,1)$ and $(0,1)\\\\ni\\\\alpha_t\\\\searrow 0$. - In practice (coding) $\\\\alpha_t$ will go from $1-0.0001$ to $1-0.02$, $\\\\alpha_t$ needs not be very small when the noise is enough. - Note that started from $t=1$, we no longer consider $x_t$ as a concrete image, rather we consider $x_t$ as a random variable where only the mean and variance makes perfect sense. - The true image depense on the instance of values that a gaussian noise provide. - That means $x_t$ denotes a set of possibilities of images (data point). To understant $x_t$, we need to understand the density of the probability distribution $p(x_t)$. - By direct expansion we have $$ x_t = \\\\sqrt{\\\\alpha_t \\\\alpha_{t-1}} x_{t-2} +\\\\sqrt{\\\\alpha_t(1-\\\\alpha_{t-1})}z_2 + \\\\sqrt{1-\\\\alpha_t}z_1, $$ where $z_1,z_2\\\\sim \\\\mathcal N(0,1)$. - Since $\\\\mathcal N(0, \\\\sigma_1^2I) + \\\\mathcal N(0, \\\\sigma_2^2I) = \\\\mathcal N (0, (\\\\sigma_1^2+\\\\sigma_2^2)I)$, the last term becomes $$ x_t = \\\\sqrt{\\\\alpha_t\\\\alpha_{t-1}}x_{t-2}+\\\\sqrt{1-\\\\alpha_t\\\\alpha_{t-1}}z_2 $$ for some $z_2\\\\sim \\\\mathcal N(0, 1)$. - Define $\\\\overline{\\\\alpha}_t = \\\\prod\\\\limits_{0\\\\leq i < t}\\\\alpha_{t-i} = \\\\prod\\\\limits_{i=1}^{t}\\\\alpha_{i}$ for $k\\\\ge 1$, then $$x_t = \\\\sqrt{\\\\overline{\\\\alpha}_t}x_0 +\\\\sqrt{1-\\\\overline{\\\\alpha}_t}\\\\cdot z_t, \\\\tag{$*$}$$ for some $z_t\\\\sim \\\\mathcal N (0, 1)$. - Note that $\\\\alpha_{t+1}\\\\overline{\\\\alpha}_t = \\\\overline{\\\\alpha}_{t+1}$. - The forward process of adding noise is denoted $x_t = q(x_{t-1})$ <Center> <img src=\\"/assets/tech/124/002.png\\" width=\\"100%\\" /> </Center> <p/> <center></center> We wish to calculate the reverse (denoise) process <Center> <img src=\\"/assets/tech/124/003.png\\" width=\\"100%\\" /> </Center> <p/> <center></center> - Recall the Bayse Forumla $\\\\displaystyle P(A|B) = P(B|A)\\\\times \\\\frac{P(A)}{P(B)}$. - Given a set of images $x_t$, we wish to understand the distribution of $x_{t-1}$, i.e., we wish to calculate $p(x_{t-1}|x_t).$ - $p$ usually is used to denote known distribution. - To emphasize we don\'t truly understand the distribution, we replace $p$ by $q$ to denote unknown distribution (the distribution that we are going to find or estimate, or to learn), the problem becomes estimating the distribution $q(x_{t-1}|x_t)$. - As we know we add random noise from $t-1$ to $t$, it makes no sense to estimate the exact value of a random variable $x_{t-1}$ from $x_t$. - Therefore what we want to estiamte is the average of $x_{t-1}$ from an instance of $x_t$. - By Bayse formula, $\\\\displaystyle q(x_{t-1}|x_t,x_0) = q(x_t|x_{t-1}, x_0) \\\\frac{q(x_{t-1}|x_0)}{q(x_t|x_0)}$. - To enable ourself to do computation, we also assume $q(x_{t-1}|x_t,x_0)$ follows Gaussian distribution. - We now try to estimate the mean of $x_{t-1}$, name it $\\\\tilde\\\\mu_{t-1}$. - We have already studied the distribution of $x_t$ in $(*)$. - Suppose that $x_t$ is given and we know that it comes from the previous distribution by adding gaussian noise with some weight (in the same way as before), then $$ \\\\begin{aligned} &{\\\\color{white}=} q(x_{t-1}|x_{t}, x_0) \\\\\\\\ &\\\\propto \\\\exp \\\\left\\\\{-\\\\frac{1}{2}\\\\left(\\\\frac{(x_t-\\\\sqrt{\\\\alpha}x_{t-1})^2}{\\\\beta_t} + \\\\frac{(x_{t-1}-\\\\sqrt{\\\\overline{\\\\alpha}_{t-1}}x_0)^2}{1-\\\\overline{\\\\alpha}_{t-1}}- \\\\frac{(x_t-\\\\sqrt{\\\\overline{\\\\alpha}_t} x_0)^2}{1-\\\\overline{\\\\alpha}_t}\\\\right)\\\\right\\\\}\\\\\\\\ &=\\\\exp \\\\Bigg\\\\{ -\\\\frac{1}{2}\\\\Bigg(\\\\bigg(\\\\frac{\\\\alpha_t}{\\\\beta_t} + \\\\frac{1}{1-\\\\overline{\\\\alpha}_{t-1}}\\\\bigg)x_{t-1}^2 - \\\\bigg(\\\\frac{2\\\\sqrt{\\\\alpha_t}}{\\\\beta_t}x_t + \\\\frac{2\\\\sqrt{\\\\overline{\\\\alpha}_{t-1}}}{1-\\\\overline{\\\\alpha}_{t-1}} x_0\\\\bigg)x_{t-1}\\\\\\\\ &\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad\\\\qquad+C(x_t,x_0)\\\\Bigg) \\\\Bigg\\\\}\\\\\\\\ &=\\\\exp\\\\bigg(-\\\\frac{(x_{t-1}-\\\\mu)^2}{2\\\\sigma^2}\\\\bigg) \\\\end{aligned} $$ where $\\\\beta_t=1-\\\\alpha_t$. - By comparing coefficients we have $$ \\\\mu =\\\\mu(x_t,x_0) = \\\\frac{\\\\sqrt{\\\\alpha_t}(1-\\\\overline{\\\\alpha}_{t-1})}{1-\\\\overline{\\\\alpha}_t}x_t + \\\\frac{\\\\sqrt{\\\\overline{\\\\alpha}_{t-1}}\\\\beta_t}{1-\\\\overline{\\\\alpha}_t}x_0, $$ by $(*)$ we have $$ \\\\begin{cases} \\\\displaystyle\\\\tilde{\\\\mu}_{t-1} = \\\\frac{1}{\\\\sqrt{\\\\alpha_t}} \\\\bigg(x_t - \\\\frac{\\\\beta_t}{\\\\sqrt{1-\\\\overline{\\\\alpha}_t}}z_t\\\\bigg), \\\\\\\\ \\\\sigma_{t-1}^2= \\\\displaystyle\\\\left(\\\\frac{\\\\alpha_t}{\\\\beta_t}+ \\\\frac{1}{1-\\\\overline{\\\\alpha}_{t-1}}\\\\right)^{-1} = \\\\frac{1-\\\\overline{\\\\alpha}_{t-1}}{1-\\\\overline{\\\\alpha}_t}\\\\beta_t. \\\\end{cases} $$ - $z_t$ will be what we are trying to learn. - When we predict image in reverse timesteps, we iteratively predict image by $$ \\\\boxed{ x_{t-1} = \\\\frac{1}{\\\\sqrt{\\\\alpha_t}} \\\\bigg(x_t - \\\\frac{\\\\beta_t}{\\\\sqrt{1-\\\\overline{\\\\alpha}_t}}\\\\mathrm{model}(x_t)\\\\bigg) + \\\\sqrt{\\\\frac{1-\\\\overline{\\\\alpha}_{t-1}}{1-\\\\overline{\\\\alpha}_t}\\\\beta_t} \\\\cdot \\\\epsilon } $$ for some $\\\\epsilon$ sampled from normal distribution. In code it is implemented as follows: python @torch.no_grad() def sample_timestep(model, x, t): \\"\\"\\" Calls the model to predict the noise in the image and returns the denoised image. Applies noise to this image, if we are not in the last step yet. \\"\\"\\" betas_t = get_index_from_list(betas, t, x.shape) sqrt_one_minus_alphas_cumprod_t = get_index_from_list( sqrt_one_minus_alphas_cumprod, t, x.shape ) sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape) # Call model (current image - noise prediction) model_mean = sqrt_recip_alphas_t * ( x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t ) posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape) if t == 0: return model_mean else: noise = torch.randn_like(x) return model_mean + torch.sqrt(posterior_variance_t) * noise - From $x_1$ to $x_2$ we add a noize $z_1$. We estimate (learn) $\\\\tilde z_1$ from $x_2$ to $x_1$, then $z_1$ will be our ground truth in the model. We elaborate this in the next section. In code it is implemented as follows: python def get_loss(model, x_0, times): # times is of shape (128, ) x_noisy, noise = forward_diffusion_sample(x_0, times, device) # 128 time times, therefore 128 images, x_noisy is of shape [128, 3, 64, 64] noise_pred = model(x_noisy, times) return F.l1_loss(noise, noise_pred) #### Training Algorithm - <Center> <img src=\\"/assets/tech/124/004.png\\" width=\\"100%\\" /> </Center> <p/> <center></center> - In algorithm on the LHS: 2. means we sample an image from our collection of image dataset ($q(x_0)$ means the distribution of the images that $x_0$ lives in, like category of dogs, cats, etc) 3. means the timestamp is uniformly random 4. means the noise $\\\\epsilon$ we add from $t-1$ to $t$. 5. $\\\\epsilon_0$ is the estimate of $\\\\epsilon$ from $t-1$ to $t$ (as we want to do the reverse). This $\\\\epsilon_0$ is estimated from - $x_t$ (see $(*)$) and - timestamp $t$ our loss function becomes $L = \\\\|\\\\epsilon - \\\\epsilon_0(x_t, t)\\\\|_2^2$. #### Coding ##### Constants python def linear_beta_schedule(timesteps, start=0.0001, end=0.02): return torch.linspace(start, end, timesteps) def get_index_from_list(vals, t, x_shape): \\"\\"\\" Returns a specific index t of a passed list of values vals while considering the batch dimension. \\"\\"\\" batch_size = t.shape[0] out = vals.gather(-1, t.cpu()) # same as .reshape( (batch_size,) + ((1,) * (len(x_shape) - 1)) ) result = out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device) return result # Define beta schedule T = 300 IMG_SIZE = 64 TIMESTEPS_BATCH_SIZE = 128 betas = linear_beta_schedule(timesteps=T) # Pre-calculate different terms for closed form alphas = 1. - betas alphas_cumprod = torch.cumprod(alphas, axis=0) alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0) sqrt_recip_alphas = torch.sqrt(1.0 / alphas) sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod) sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod) posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) ##### SinusoidalPositionEmbeddings This is exactly the same as the one we use in transformer, which basically takes a time $t\\\\in\\\\mathbb N$ to a vector of size (32,). To recall, positional encoding takes the following form: for each fixed $\\\\texttt{pos}\\\\in \\\\mathbb N$, $$ \\\\begin{aligned} \\\\mathrm{PE}{(\\\\texttt{pos},2i)} & = \\\\sin(\\\\texttt{pos} / 10000^{i/d_{0.5\\\\times \\\\text{model}}}) \\\\\\\\ \\\\mathrm{PE}{(\\\\texttt{pos},2i+1)} & = \\\\cos(\\\\texttt{pos} / 10000^{i/d_{0.5\\\\times\\\\text{model}}}) \\\\end{aligned} $$ where $i=0,1,2,\\\\dots, \\\\frac{1}{2}d_\\\\text{model} - 1$. python-1 class SinusoidalPositionEmbeddings(nn.Module): def __init__(self, dim): super().__init__() # dim = 32 self.dim = dim def forward(self, times): half_dim = self.dim // 2 embeddings = math.log(10000) / (half_dim - 1) embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings) # a_i = 1/10000^(i/half_dim) # embeddings above = [a_1, a_2, a_3, ..., a_16] embeddings = times[:, None] * embeddings[None, :] # embeddings above <=> # t |-> ( sin t*a_1, cos t*a_1, sin t*a_2, cos t*a_2, sin t*a_3, cos t*a_3, ... ) # for each t, therefore the final dimension will be (128, 32) embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1) # TODO: Double check the ordering here return embeddings The variable embeddings in line 14 above is exactly $$ \\\\left[ \\\\texttt{timestep} \\\\times \\\\left[ \\\\frac{1}{10000^{i/d_{\\\\text{half_dim}}}}: 0\\\\leq i<32\\\\right]: \\\\texttt{timestep} \\\\in \\\\texttt{times} \\\\right] $$ with timestep in place of pos above. ##### UNet that Predicts Noise python class SimpleUnet(nn.Module): \\"\\"\\" A simplified variant of the Unet architecture. \\"\\"\\" def __init__(self): super().__init__() image_channels = 3 down_channels = (64, 128, 256, 512, 1024) up_channels = (1024, 512, 256, 128, 64) out_dim = 1 time_emb_dim = 32 # Time embedding self.time_mlp = nn.Sequential( SinusoidalPositionEmbeddings(time_emb_dim), nn.Linear(time_emb_dim, time_emb_dim), nn.ReLU() ).to(device) # Initial projection # stride = 1, padding = 1, no change in spatial dimension self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1).to(device) # Downsample self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i + 1], time_emb_dim).to(device) for i in range(len(down_channels) - 1)]) # Upsample self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i + 1], time_emb_dim, up=True).to(device) for i in range(len(up_channels) - 1)]) self.output = nn.Conv2d(up_channels[-1], 3, out_dim).to(device) def forward(self, x, times): # Embedd time t = self.time_mlp(times) # Initial conv x = self.conv0(x) # Unet residual_inputs = [] for down in self.downs: x = down(x, t) residual_inputs.append(x) for up in self.ups: # for the bottom block the x adds an identical copy of x (just poped out) for unity of coding. residual_x = residual_inputs.pop() # Add residual x as additional channels x = torch.cat((x, residual_x), dim=1) x = up(x, t) return self.output(x) ##### Sampling / Prediction python @torch.no_grad() def sample_timestep(model, x, t): \\"\\"\\" Calls the model to predict the noise in the image and returns the denoised image. Applies noise to this image, if we are not in the last step yet. \\"\\"\\" betas_t = get_index_from_list(betas, t, x.shape) sqrt_one_minus_alphas_cumprod_t = get_index_from_list( sqrt_one_minus_alphas_cumprod, t, x.shape ) sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape) # Call model (current image - noise prediction) model_mean = sqrt_recip_alphas_t * ( x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t ) posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape) if t == 0: return model_mean else: noise = torch.randn_like(x) return model_mean + torch.sqrt(posterior_variance_t) * noise @torch.no_grad() def sample_plot_image(model, img_path): # Sample noise img_size = IMG_SIZE img = torch.randn((1, 3, img_size, img_size), device=device) plt.figure(figsize=(15, 15)) plt.axis(\'off\') num_images = 10 stepsize = int(T / num_images) for i in range(0, T)[::-1]: # just create a tensor t of shape (1,), the result is [1], [2], ..., etc times = torch.full((1,), i, device=device, dtype=torch.long) img = sample_timestep(model, img, times) if i % stepsize == 0: plt.subplot(1, num_images, i // stepsize + 1) show_tensor_image(img.detach().cpu()) plt.savefig(img_path) ","title":"Diffusion Model Study","date":"2023-03-07T00:00:00.000Z","id":"blog0124","tag":"python, deep-learning","intro":"Beginning study of diffusion model."},{"content":" #### Windows ##### OpenCV - In the [official guide](https://docs.opencv.org/4.x/d3/d52/tutorial_windows_install.html) we can download the pre-built library, for example, we choose 4.6.0 and download the exe file <Center> <img src=\\"/assets/tech/125/001.png\\" width=\\"600\\"/> </Center> <p></p> <center></center> By running this .exe file we can decompress and extract files into a single folder that follows this structure: <Center> <img src=\\"/assets/tech/125/002.png\\" width=\\"600\\"/> </Center> <p></p> <center></center> As a routine I will save all libraries like opencv, libtorch and boost, etc, into a folder called C++Library. These libraries will be shared among different projects. - Next in the root level CMakelists.txt we include cmake set(CMAKE_CXX_STANDARD 14) set(CPACK_PROJECT_NAME ${PROJECT_NAME}) set(CPACK_PROJECT_VERSION ${PROJECT_VERSION}) set(OpenCV_DIR \\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\C++Libraries\\\\\\\\opencv\\\\\\\\build\\\\\\\\x64\\\\\\\\vc16\\") find_package(OpenCV REQUIRED) message(STATUS \\"OpenCV_INCLUDE_DIRS = ${OpenCV_INCLUDE_DIRS}\\") message(STATUS \\"OpenCV_LIBS = ${OpenCV_LIBS}\\") Change the path for OpenCV_DIR when needed. - Then we can use cmake target_link_libraries(some_target PUBLIC ... ${OpenCV_LIBS} ...) to link the library when the project needs it. ##### libtorch - We can download the libtorch pre-built library from official pytorch website. The structure is like this: <Center> <img src=\\"/assets/tech/125/003.png\\" width=\\"600\\"/> </Center> <p></p> <center></center> - In the root level CMakeLists.txt we include cmake set(CMAKE_PREFIX_PATH \\"C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Repos\\\\\\\\C++Libraries\\\\\\\\libtorch\\") find_package(Torch REQUIRED) set(CMAKE_CXX_FLAGS \\"${CMAKE_CXX_FLAGS} ${TORCH_CXX_FLAGS}\\") if (MSVC) message(\\"copying dll files\\") file(GLOB TORCH_DLLS \\"${TORCH_INSTALL_PREFIX}/lib/*.dll\\") add_custom_command(TARGET EyeCatching POST_BUILD COMMAND ${CMAKE_COMMAND} -E copy_if_different ${TORCH_DLLS} $<TARGET_FILE_DIR:EyeCatching>) endif (MSVC) - Then we can link the library by cmake target_link_libraries(some_target PUBLIC ... ${TORCH_LIBRARIES} ...) #### Mac ##### OpenCV\\\\_ We install opencv directly by homebrew: text brew install opencv Then the pre-built library can be found in /opt/homebrew/Cellar/opencv/. It remains to link it in our cmake project. The cmake instruction remains the same as windows, in my case my path to the libary is /opt/homebrew/Cellar/opencv/4.7.0_1. ##### libtorch\\\\_ We can still download pre-built libtorch library in official pytorch website. The cmake instruction remains the same as windows. ","title":"Install Opencv and Libtorch for CMake Project","date":"2023-03-20T00:00:00.000Z","id":"blog0125","tag":"C++, deep-learning","intro":"Record the flow of including opencv and libtorch into CMake project."},{"content":" In the CMakeLists.txt of library folder (or root folder), we add cmake target_compile_features(lib_name PUBLIC cxx_std_20) to override, or to determine the language standard in compilation. ","title":"Control C++ Standard Accurately","date":"2023-03-24T00:00:00.000Z","id":"blog0126","tag":"C++","intro":"Sometimes the constant CMAKE_CXX_STANDARD does not guarantee the C++ standard we use in compilation. We add a line to guarantee which target is compiled in which C++ standard.","toc":false},{"content":" Just bundle a set of headers into pch.h, refactor the project approprimately and add the following in the CMakeLists.txt. cmake target_precompile_headers(target_name PUBLIC src/pch.h) ","title":"Precompiled Header in CMake Project","date":"2023-03-27T00:00:00.000Z","id":"blog0127","tag":"C++","intro":"Previously we have mentioned how to use precompiled header in visual studio project, this time we record how to do it in cmake project","toc":false},{"content":" #### API Documentation - https://pytorch.org/cppdocs/api/library_root.html #### Tensor Slicing - [More detailed translation from pytorch to libtorch](https://pytorch.org/cppdocs/notes/tensor_indexing.html) | Python | C++ (assuming using namespace torch::indexing) | | ------------------------------------------------------ | ---------------------------------------------------------------------------------- | | tensor[None] = 1 | tensor.index_put_({None}, 1) | | tensor[:, 2] = 0 | tensor.index_put_({Slice(), 2}, 0) | | tensor[Ellipsis, ...] = 1 | tensor.index_put_({Ellipsis, \\"...\\"}, 1) | | tensor[1, 2] = 1 | tensor.index_put_({1, 2}, 1) | | tensor[True, False] = 1 | tensor.index_put_({true, false}, 1) | | tensor[1::2] = 1 | tensor.index_put_({Slice(1, None, 2)}, 1) | | tensor[torch.tensor([1, 2])] = 1 | tensor.index_put_({torch::tensor({1, 2})}, 1) | | tensor[..., 0, True, 1::2, torch.tensor([1, 2])] = 1 | tensor.index_put_({\\"...\\", 0, true, Slice(1, None, 2), torch::tensor({1, 2})}, 1) | #### How to deal with .npy file There are several packages to load .npy file in C++. However, since have pytorch, additional package for .npy file is **not necessary**. ##### Save the Numpy Array as Tensor in Pytorch We can load the array from .npy file and translate it into tensor easily by python anchors = torch.tensor(np.load(anchors_npy_path), dtype=torch.float32) then we save it by python torch.save({\\"anchors\\": anchors}, \\"anchors.pt\\") ##### Load the Tensor in Libtorch cpp std::vector<char> get_the_bytes(std::string filename) { std::ifstream input(filename, std::ios::binary); std::vector<char> bytes( (std::istreambuf_iterator<char>(input)), (std::istreambuf_iterator<char>())); input.close(); return bytes; } torch::Tensor load_anchors(std::string pt_path) { torch::Tensor anchors; std::vector<char> f = get_the_bytes(pt_path); c10::Dict<IValue, IValue> weights = torch::pickle_load(f).toGenericDict(); for (auto const& w : weights) { std::string name = w.key().toStringRef(); at::Tensor param = w.value().toTensor().toType(torch::kFloat32); if (name == \\"anchors\\") { std::cout << \\"anchors was found, loading anchors\\" << \\"\\\\n\\" << param; anchors = param; break; } } return anchors; } #### Create Simple Layers in Libtorch In the sequel we assume in our namespace: cpp using namespace torch; ##### nn::Conv2d cpp nn::Conv2d(nn::Conv2dOptions(in_channels, out_channels, kernel_size) .stride(stride) .padding(padding) .groups(in_channels) .bias(true)), ##### nn::MaxPool2d cpp nn::MaxPool2d(nn::MaxPool2dOptions({ kernel_size1, kernel_size2 }.stride({ 2, 2 }))); ##### nn::functional::pad cpp h = nn::functional::pad(x, nn::functional::PadFuncOptions({ 0, 2, 0, 2 }).value(0)); #### Create Multi-Dimensional Array in Libtorch ##### Create torch::Tensor from raw 2d-Array cpp float matrix[2][4] = { { 1, 1, 1, 1 }, { -1, 0, 2, 3 } }; torch::Tensor result = torch::from_blob(matrix, { 2, 4 }).toType(torch::kFloat32); std::cout << \\" result[0][1]\\" << result[0][1] << std::endl; float data = 0; torch::Tensor y = torch::from_blob(&data, { 1 }).toType(torch::kFloat32); std::cout << \\"y: \\" << y << std::endl; - float cannot be replaced by int as otherwise there will be numerical error in data conversion. - The size of the tensor can be accessed by auto sizes = y.sizes(). The size of each dim can be accessed by index operator sizes[i]. ##### Create torch::Tensor from cv::Mat cpp torch::Tensor result = torch::from_blob(mat.data, { 1, height, width, 3 }, torch::kByte) .permute({ 0, 3, 1, 2 }) .toType(torch::kFloat32); result.div_(255.0); ##### Create cv::Mat from torch::Tensor cpp cv::Mat from_tensor_to_mat = cv::Mat( h, w, CV_32FC3, img3.permute({ 0, 2, 3, 1 }).squeeze(0).data_ptr() ); - If A_tensor is 3-dimensional in channel, we need CV_32FC3, similarly if a tensor 1-dimensional in channel we need CV_32FC1. #### Validate a Libtorch Model is Compatible (in weight) to a Pytorch Model. ##### Custom nnModule::load_parameters and nnModule::print_parameters Essentially this will replace all torch::nn::module. ###### Header cpp class nnModule : public nn::Module { public: std::vector<char> get_the_bytes(std::string filename); void load_parameters(std::string pt_pth); void print_parameters(std::string file_path, bool with_weight = false); }; ###### Source cpp std::vector<char> nnModule::get_the_bytes(std::string filename) { std::ifstream input(filename, std::ios::binary); std::vector<char> bytes( (std::istreambuf_iterator<char>(input)), (std::istreambuf_iterator<char>())); input.close(); return bytes; } void nnModule::load_parameters(std::string pt_pth) { std::vector<char> f = this->get_the_bytes(pt_pth); c10::Dict<IValue, IValue> weights = torch::pickle_load(f).toGenericDict(); const torch::OrderedDict<std::string, at::Tensor>& model_params = this->named_parameters(); std::vector<std::string> param_names; for (auto const& w : model_params) { param_names.push_back(w.key()); } torch::NoGradGuard no_grad; for (auto const& w : weights) { std::string name = w.key().toStringRef(); at::Tensor param = w.value().toTensor(); if (std::find(param_names.begin(), param_names.end(), name) != param_names.end()) { auto target_model_param = model_params.find(name); for (int i = 0; i < target_model_param->sizes().size(); i++) { assert(target_model_param->sizes()[i] == param.sizes()[i]); } target_model_param->copy_(param); } else { std::cout << name << \\" does not exist among model parameters.\\" << std::endl; }; } } void nnModule::print_parameters(std::string file_path, bool with_weight) { std::ostringstream oss; for (const auto& pair : named_parameters()) { oss << \\"[\\" << pair.key() << \\"] \\"; int shape_arr_size = pair.value().sizes().size(); std::string size_tuple_str = \\"torch.Size([\\"; for (int i = 0; i < shape_arr_size; i++) { std::string curr_dim_len = std::to_string(pair.value().sizes()[i]); size_tuple_str += curr_dim_len; if (i != (shape_arr_size - 1)) { size_tuple_str += \\", \\"; } } size_tuple_str += \\"])\\"; oss << size_tuple_str << \\"\\\\n\\"; if (with_weight) { oss << pair.value() << \\"\\\\n\\" << \\"---------------\\" << \\"\\\\n\\"; } } std::ofstream file; file.open(file_path); try { file << oss.str(); } catch (std::exception err) { std::cout << err.what() << std::endl; } file.close(); } ###### Dual Functions in Python for Comparison python def save_model(model_, des_weight_path): w = {k: v for k, v in model_.state_dict().items()} torch.save(w, des_weight_path) def print_weight_list(model_, des_txt_filepath, with_weight=False): with open(des_txt_filepath, \\"w+\\") as f_handle: txt = \\"\\" for name, param in model_.named_parameters(): txt += \\"[{}] {}\\\\n\\".format(name, param.shape) if with_weight: txt += str(param.numpy()) txt += \\"\\\\n\\" + \\"---------------\\" + \\"\\\\n\\" f_handle.write(txt) ##### Example For example, in this [repository](https://github.com/machingclee/2023-01-25-ImGui-barebone-windows-blazeface-integrated/tree/main/mediapipe_libtorch/src/mediapipe_libtorch?fbclid=IwAR1komT77la5Eah9i9zRIE0LrvkLv2XEU4XNi06ogIy0KN1qnzh8wh3T4pk) my modules are all inherited publicly from nnModule defined above, therefore we can - **On Libtorch Side.** cpp face_detector->print_parameters(des_str) - **On Pytorch Side.** python print_weight_list(face_detector, des_str) with with_weight = false by default (you can set it true if you want to further debug the libtorch model, but the file will become very large and unreadable). These two functions are designed to output the same result: text [backbone1.0.weight] torch.Size([24, 3, 5, 5]) [backbone1.0.bias] torch.Size([24]) [backbone1.2.convs.0.weight] torch.Size([24, 1, 3, 3]) [backbone1.2.convs.0.bias] torch.Size([24]) [backbone1.2.convs.1.weight] torch.Size([24, 24, 1, 1]) [backbone1.2.convs.1.bias] torch.Size([24]) [backbone1.3.convs.0.weight] torch.Size([24, 1, 3, 3]) ... When two files are exactly the same, we are confident that the pytorch weight can be applied to libtorch model as well. #### Create a Custom Module and Register Sub-modules in Practice ##### Register Tensors form Pytorch to Libtorch ###### Parameters The following are equivalent: python class Net(torch.nn.Module): def __init__(self, N, M): super(Net, self).__init__() self.W = torch.nn.Parameter(torch.randn(N, M)) self.b = torch.nn.Parameter(torch.randn(M)) def forward(self, input): return torch.addmm(self.b, input, self.W) cpp class Net : torch::nn::Module { torch::Tensor W, b; public: Net(int64_t N, int64_t M) { W = register_parameter(\\"W\\", torch::randn({N, M})); b = register_parameter(\\"b\\", torch::randn(M)); } torch::Tensor forward(torch::Tensor input) { return torch::addmm(b, input, W); } }; ###### Modules The following are equivalent: python class Net(torch.nn.Module): def __init__(self, N, M): super(Net, self).__init__() # Registered as a submodule behind the scenes self.linear = torch.nn.Linear(N, M) self.another_bias = torch.nn.Parameter(torch.rand(M)) def forward(self, input): return self.linear(input) + self.another_bias cpp class Net : torch::nn::Module { torch::nn::Linear linear; torch::Tensor another_bias; public: Net(int64_t N, int64_t M) : { linear = register_module(\\"linear\\", torch::nn::Linear(N, M)); another_bias = register_parameter(\\"b\\", torch::randn(M)); } torch::Tensor forward(torch::Tensor input) { return linear->forward(input) + another_bias; } }; ##### Real Example in Practice ###### Rules - In libtorch all modules are created by producing a shared_ptr pointing to an nn::Module. - For example, auto net1 = nn::Conv2d(...) and auto net2 = nn::Relu(...) are both pointers. - nn::Sequential() only except smart pointers like nn::Sequential(net1, net2). For custom module, we can create such a pointer-factory by TORCH_MODULE macro. The principles are - For module just for internal use, we simply leave it as an nn::Module object. - For module that is going to be exposed to user, we write NetImpl and use TORCH_MODULE(Net) to create a special pointer class Net. ###### BlazeBlock from mediapipe\'s BlazeFace cpp // header file class BlazeBlockImpl : public nnModule { protected: int in_channels; int out_channels; int kernel_size; int stride; std::string act; nn::Conv2d skip_proj = nullptr; int channel_pad; int padding; nn::Sequential convs = nullptr; nn::MaxPool2d max_pool = nullptr; std::variant<nn::ReLU, nn::PReLU> act_layer; bool use_skip_proj = false; public: BlazeBlockImpl( int in_channels, int out_channels, int kernel_size = 3, int stride = 1, std::string act = \\"relu\\", bool use_skip_proj = false); torch::Tensor forward(torch::Tensor x); }; TORCH_MODULE(BlazeBlock); cpp // source file BlazeBlockImpl::BlazeBlockImpl( int in_channels, int out_channels, int kernel_size, int stride, std::string act, bool use_skip_proj) : in_channels(in_channels), out_channels(out_channels), kernel_size(kernel_size), stride(stride), act(act), use_skip_proj(use_skip_proj) { channel_pad = out_channels - in_channels; if (stride == 2) { max_pool = nn::MaxPool2d(nn::MaxPool2dOptions({ stride, stride })); padding = 0; } else { padding = (int)((kernel_size - 1) / 2); } nn::Sequential convs_ = nn::Sequential(); convs_->push_back(nn::Conv2d(nn::Conv2dOptions(in_channels, in_channels, kernel_size) .stride(stride) .padding(padding) .groups(in_channels) .bias(true))); convs_->push_back(nn::Conv2d(nn::Conv2dOptions(in_channels, out_channels, 1) .stride(1) .padding(0) .bias(true))); convs = register_module(\\"convs\\", convs_); if (use_skip_proj) { skip_proj = register_module( \\"skip_proj\\", nn::Conv2d(nn::Conv2dOptions(in_channels, out_channels, 1) .stride(1) .padding(0) .bias(true))); } else { skip_proj = nullptr; } if (act == \\"relu\\") { act_layer = nn::ReLU(nn::ReLUOptions(true)); } else if (\\"prelu\\") { act_layer = register_module( \\"act\\", nn::PReLU(nn::PReLUOptions().num_parameters(out_channels)) ); } else { throw std::exception(\\"activation layer not implemented.\\"); } } torch::Tensor BlazeBlockImpl::forward(torch::Tensor x) { torch::Tensor h; if (stride == 2) { if (kernel_size == 3) { h = nn::functional::pad(x, nn::functional::PadFuncOptions({ 0, 2, 0, 2 }).value(0)); } else { h = nn::functional::pad(x, nn::functional::PadFuncOptions({ 1, 2, 1, 2 }).value(0)); } x = this->max_pool(x); } else { h = x; } if (skip_proj) { x = skip_proj->forward(x); } else if (channel_pad > 0) { x = nn::functional::pad( x, n n::functional::PadFuncOptions({ 0, 0, 0, 0, 0, channel_pad }).value(0) ); } torch::Tensor y = convs->forward(h) + x; // y = reinterpret_cast<IHasForward*>(&act_layer)->forward(y); // I want to avoid the following: if (auto act_layer_ptr = std::get_if<nn::ReLU>(&act_layer)) { y = (*act_layer_ptr)->forward(y); } else if (auto act_layer_ptr = std::get_if<nn::PReLU>(&act_layer)) { y = (*act_layer_ptr)->forward(y); } return y; }; #### OpenCV ##### Load an Image cpp cv::Mat img = cv::imread(img_path, cv::IMREAD_COLOR); ##### Write a cv::Mat into an Image cpp cv::imwrite(image_path, img1); ##### Resize an Image cpp cv::Mat img1; // src, des int, int cv::resize(img, img1, cv::Size(w1, h1)); ##### Pad an Image cpp cv::Mat img1_; cv::copyMakeBorder( img1, // src img1_, // des padh1, // top padh2, // bottom padw1, // left padw2, // right cv::BORDER_CONSTANT, cv::Scalar(0) ); ##### COLOR_BGR2RGB cpp cv::cvtColor(frame, frame, cv::COLOR_BGR2RGB); ##### Drawing cpp cv::circle(frame, cv::Point2i({ x, y }), size, color, size); cv::line( frame, cv::Point2i({ x0, y0 }), cv::Point2i({ x1, y1 }), cv::Scalar({ 0, 0, 0 }), size ); ##### Read from Camera cpp cv::VideoCapture capture(1); cv::Mat frame; if (!capture.isOpened()) { throw std::exception(\\"Unable to open camera.\\"); } while (true) { capture.read(frame); if (frame.empty()) { throw std::exception(\\"Blank frame grabbed.\\"); } assert(frame.channels() == 3); ... } ","title":"Libtorch Study Notes With OpenCV","date":"2023-04-02T00:00:00.000Z","id":"blog0128","tag":"C++, pytorch, libtorch","intro":"In the course of translating pytorch model into libtorch model there are traps and tricks that are worthing being recorded. Also record the simple use of opencv as it substitutes the role of numpy in python."},{"content":" Sometime it is helpful to create a type that accept both class A and class B. A real example from deep learning is: cpp // header file class BlazeBlockImpl : public nnModule { protected: ... std::variant<nn::ReLU, nn::PReLU> act_layer; ... } and we assign this act_layer in constructor cpp // source file if (act == \\"relu\\") { act_layer = nn::ReLU(nn::ReLUOptions(true)); } else if (\\"prelu\\") { act_layer = register_module( \\"act\\", nn::PReLU(nn::PReLUOptions().num_parameters(out_channels)) ); } depending on a variable act passed into this constructor. This std::variant serves as the same purpose of union type in typescript (note!! union is a special keyword in C++ and does not work in the same way as typescript). Special type-checking will be needed to apply act_layer to a tensor, for this we separate act_layer into two cases: cpp torch::Tensor y = ...; if (auto act_layer_ptr = std::get_if<nn::ReLU>(&act_layer)) { y = (*act_layer_ptr)->forward(y); } else if (auto act_layer_ptr = std::get_if<nn::PReLU>(&act_layer)) { y = (*act_layer_ptr)->forward(y); } That\'s how std::variant work. ","title":"std::variant","date":"2023-04-05T00:00:00.000Z","id":"blog0129","tag":"C++","intro":"In typescript we have type A = B | C, we also have an analogue in C++.","toc":false},{"content":" #### Library we Use - [WinReg v6.1.0](https://github.com/GiovanniDicanio/WinReg) #### Code Implementation In fact we just need the header file in WinReg/WinReg.hpp, we can create a cmake project and simply include this file. Now to register the application to a url scheme: cpp // header #pragma once #include \\"WinReg.h\\" #include <string> namespace RegisterProtocol { inline std::wstring to_wide_string(const std::string& input); inline std::string to_byte_string(const std::wstring& input); void register_protocol(std::string app_path); } // namespace RegisterProtocol cpp // source namespace RegisterProtocol { inline std::wstring to_wide_string(const std::string& input) { std::wstring_convert<std::codecvt_utf8<wchar_t>> converter; return converter.from_bytes(input); } inline std::string to_byte_string(const std::wstring& input) { // std::wstring_convert<std::codecvt_utf8_utf16<wchar_t>> converter; std::wstring_convert<std::codecvt_utf8<wchar_t>> converter; return converter.to_bytes(input); } void register_protocol(std::string exe_path) { std::wstring w_app_path = to_wide_string(exe_path); RegKey key; key.Create(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\", KEY_CREATE_SUB_KEY); key.Open(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\", KEY_SET_VALUE); key.SetStringValue(L\\"URL Protocol\\", L\\"\\"); key.Create(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\\\\\\\shell\\\\\\\\open\\\\\\\\command\\", KEY_CREATE_SUB_KEY); key.Open(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\\\\\\\shell\\\\\\\\open\\\\\\\\command\\", KEY_SET_VALUE); key.SetStringValue(L\\"\\", w_app_path); } } // namespace RegisterProtocol #### What is it Doing - cpp key.Create(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\\\\\\\shell\\\\\\\\open\\\\\\\\command\\") means that at the folder HKEY_CURRENT_USER, we create a nested folder structure as instructed. - cpp key.Open(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\", KEY_SET_VALUE); simply changes the current working \\"directory\\" to - HKEY_CURRENT_USER\\\\Software\\\\Classes\\\\eyecatcher and grant the desired access right: KEY_SET_VALUE. - cpp key.SetStringValue(std::wstring key, std::wstring value) is the same as setting key-value pair at the current working registry folder. We use wstring instead of string for non-ascii characters. For example, what cpp key.Open(HKEY_CURRENT_USER, L\\"Software\\\\\\\\Classes\\\\\\\\eyecatcher\\", KEY_SET_VALUE); key.SetStringValue(L\\"URL Protocol\\", L\\"\\") does is to create a new key-value pair at the directory: - HKEY_CURRENT_USER\\\\Software\\\\Classes\\\\eyecatcher In the GUI of registry editor we have: <Center> <img src=\\"/assets/tech/130/registry.png\\" width=\\"600\\"/> </Center> <p></p> <center></center> Here exe_path is the path of the executable that we want to launch. - Now an anchor element html <a href=\\"eyecatcher://\\">eye-catcher protocol</a> can trigger the execution of our target .exe file. ","title":"Register URL Scheme for an Application","date":"2023-04-06T00:00:00.000Z","id":"blog0130","tag":"C++","intro":"We study how to register a custom url protocol to launch our desktop application.","toc":true},{"content":" #### Query and Update ##### Return ObjectId in Plain Text by \\\\$project js { $project:{ _id: { $toString: \\"$_id\\" } } } ##### Update Deeply Nested Object Conditionally Sometime if a conditional update is hard to write in a single update object, then we can separate it into two parts: js { \\tdb.ns_generic_form_templates.updateMany( \\t\\t{ _id: ObjectId(\\"6333d861ec5588b3b43b5183\\") }, \\t\\t{ \\t\\t\\t$set: { \\"sections.$[].questions.$[question]._mandatory\\": true }, \\t\\t}, \\t\\t{ \\t\\t\\tarrayFilters: [{ \\"question.mandatory\\": \\"Y\\" }] \\t\\t} \\t) \\tdb.ns_generic_form_templates.updateMany( \\t\\t{ _id: ObjectId(\\"6333d861ec5588b3b43b5183\\") }, \\t\\t{ \\t\\t\\t$set: { \\"sections.$[].questions.$[question]._mandatory\\": false }, \\t\\t}, \\t\\t{ \\t\\t\\tarrayFilters: [{ \\t\\t\\t\\t$or: [ \\t\\t\\t\\t\\t{ \\"question.mandatory\\": { $eq: \\"N\\" } }, \\t\\t\\t\\t\\t{ \\"question.mandatory\\": { $exists: false } } \\t\\t\\t\\t] \\t\\t\\t}] \\t\\t} \\t) } will do. ##### Query Users with Inner Join and Project Desired Fields using \\\\$map js [ { $lookup: { from: \\"%s\\", localField: \\"code\\", foreignField: \\"internal_code\\", as: \\"contacts\\" } }, { $project: { _id: { $toString: \\"$_id\\" }, code: 1, name: 1, contacts: { $map: { input: \\"$contacts\\", as: \\"this\\", in: { contact_full_name1: { $ifNull: [\\"$$this.contact_full_name1\\", \\"\\"] }, contact_email1: { $ifNull: [\\"$$this.contact_email1\\", \\"\\"] }, contact_full_name2: { $ifNull: [\\"$$this.contact_full_name2\\", \\"\\"] }, contact_email2: { $ifNull: [\\"$$this.contact_email2\\", \\"\\"] } } } } } } ] ##### Photos with Missing Signboards Signboards that are drawn by users will be of type Pending and can be queried by js db.signboards.find({ isManualCreate: true, status: \\"Pending\\" }) If we want to query for images that have pending annotation, then we do a lookup: js db.signboards.aggregate( [ { $match: { signboardId: { $regex: /2022\\\\-1st_TRIP\\\\-06/ }, isManualCreate: true, status: \\"Pending\\" } }, { $lookup: { from: \\"signboardimages\\", localField: \\"signboardId\\", foreignField: \\"signboardId\\", as: \\"signboardimage\\" } }, { $project: { signboardimage: { $arrayElemAt: [\\"$signboardimage\\", 0] }, signboardId: 1, _id: 0 } }, { $project: { originalFilename: \\"$signboardimage.originalFilename\\", signboardId: 1, _id: 0 } }, ] ) Result: none [ { signboardId: \'2022-1st_TRIP-02_00377\', originalFilename: \'2022-1st_TRIP-02/P202203252_635_3807.jpg\' }, { signboardId: \'2022-1st_TRIP-04_00347\', originalFilename: \'2022-1st_TRIP-04/P202203254_463_2778.jpg\' }, { signboardId: \'2022-1st_TRIP-04_00348\', originalFilename: \'2022-1st_TRIP-04/P202203254_463_2777.jpg\' }, ... ] ##### Photos with False Positive Signboard (that is Deleted) The annotation of false positive results will be deleted in the frontend, therefore js db.signboards.aggregate( [ { $match: { signboardId: { $regex: /2022\\\\-1st_TRIP\\\\-06/ }, \\"revisionStatus.signboardIdentificationRevised.status\\": \\"deleted\\" } }, { $lookup: { from: \\"signboardimages\\", localField: \\"signboardId\\", foreignField: \\"signboardId\\", as: \\"signboardimage\\" } }, { $project: { signboardimage: { $arrayElemAt: [\\"$signboardimage\\", 0] }, signboardId: 1, \\"revisionStatus.signboardIdentificationRevised.status\\": 1, _id: 0 } }, { $project: { originalFilename: \\"$signboardimage.originalFilename\\", signboardId: 1, \\"revisionStatus.signboardIdentificationRevised.status\\": 1, _id: 0 } } ] ) Result: none [ { revisionStatus: { signboardIdentificationRevised: { status: \'deleted\' } }, signboardId: \'2021-01_TRIP-20_00485\', originalFilename: \'2021-01_TRIP-20/P2021101920_594_3562.jpg\' } ] ##### Photos with Missing Defect If a defect is missing, we will draw polygon to annotate the defective signboard, therefore missing signboards are signboard image with defectAnnotations.isManualCreate == true: Suppose I want to get all photos with missing defect(s) in trip06: js db.signboardimages.aggregate( [ { $unwind: \\"$defectAnnotations\\" }, { $match: { signboardId: { $regex: /2022\\\\-1st_TRIP\\\\-06/ }, \\"defectAnnotations.isManualCreate\\": true } }, { $project: { _id: 0, signboardId: 1, originalFilename: 1 } } ] ) Result: none [ { originalFilename: \'2022-1st_TRIP-06/P202204066_2088_12538.jpg\', signboardId: \'2022-1st_TRIP-06_00115\' } ] ##### Photos with False Positive Defect To get photo name with false positive defects, we try to get signboardimages with deleted defect annotation, and then use the originalFilename to get the photo filenames. js db.signboardimages.aggregate( [ { $match: { signboardId: { $regex: /2022\\\\-1st_TRIP\\\\-06/ }, defectAnnotations: { $gt: {$size: 1} } } }, { $project: { signboardId: 1, originalFilename: 1, defectAnnotations: 1, _id: 0 } }, { $unwind: \\"$defectAnnotations\\" }, { $match: { \\"defectAnnotations.status\\": \\"deleted\\" } } ] ) yields none [ { originalFilename: \'2022-1st_TRIP-01/P202203251_331_1983.jpg\', defectAnnotations: { isManualCreate: true, status: \'deleted\', _id: ObjectId(\\"625eb88bdaadc41b9147a385\\"), defectType: \'damagedDisplay\', location: { coordinates: [ [ [ 0.5452018181472019, 0.11694282238757442 ], [ 0.5383045307230735, 0.5721815800254274 ], [ 0.7962630803854851, 0.5376322100261263 ], [ 0.7852274205068791, 0.06410260944746654 ], [ 0.5589963929954593, 0.11491050650526251 ] ] ], _id: ObjectId(\\"625eb88bdaadc41b9147a386\\"), type: \'Polygon\' }, defectId: \'2022-1st_TRIP-01_00001_DAMAGED_DISPLAY_14\', sequenceNum: 14, groupedSignboardId: ObjectId(\\"625eb88bdaadc41b9147a384\\") }, signboardId: \'2022-1st_TRIP-01_00001\' }, ... ] #### Output Complete List to a Json File Unforturnately the embedded mongoshell of MongoCompass is only for testing purpose, the output cannot be piped into an existing file. For that purpose, we install monogosh from https://www.mongodb.com/try/download/shell?jmp=docs <center></center> After the installation, you can run bash mongosh --help to test whether mongosh has been installed. Let\'s take the query for missing signboard as an example, in our bash shell we can run (replace $ by \\\\$ and \\" by \\\\\\") sh mongosh \\"{{connection string to DSDS db}}\\" \\\\ --eval \\"config.set(\'displayBatchSize\', 300); db.signboards.aggregate( [ { \\\\$match: { signboardId: { \\\\$regex: /2022\\\\-1st_TRIP\\\\-06/ }, isManualCreate: true, status: \\\\\\"Pending\\\\\\" } }, { \\\\$lookup: { from: \\\\\\"signboardimages\\\\\\", localField: \\\\\\"signboardId\\\\\\", foreignField: \\\\\\"signboardId\\\\\\", as: \\\\\\"signboardimage\\\\\\" } }, { \\\\$project: { signboardimage: { \\\\$arrayElemAt: [\\\\\\"\\\\$signboardimage\\\\\\", 0] }, signboardId: 1, _id: 0 } }, { \\\\$project: { originalFilename: \\\\\\"\\\\$signboardimage.originalFilename\\\\\\", signboardId: 1, _id: 0 } }, ] )\\" | sed \'s/originalFilename/\\"originalFilename\\"/g\' \\\\ | sed \'s/signboardId/\\"signboardId\\"/g\' \\\\ | sed \'s/_id/\\"_id\\\\\\"/g\' \\\\ | sed \\"s/\'/\\\\\\"/g\\" \\\\ > ~/missing-signboards-06.json && code ~/missing-signboards-06.json ","title":"List of Mongo Aggregation Pipelines that I have Used.","date":"2023-06-10T00:00:00.000Z","id":"raspect001","tag":"mongo","intro":"Record the mongo query using aggregation pipeline to get the results as in the title in my usual work. These include $lookup, $project, $arrayElemAt, {$gt: {$size: 1}}, etc interesting operations.","toc":true},{"content":"#### Thunk Actions and Extra Reducer for the Return ##### Write Data-Fetching in Slices As a usual practice we create ThunkAction in the corresponding slice file, for example: typescript // projectSlice.ts export const fetchProjects = createAsyncThunk( \\"posts/fetchProjects\\", async () => { // apiClient: an axio instance with baseUrl configured const response = await apiClient.get<ProjectResponse[]>(GET_PROJECTS); return response.data; } ); ##### Limitation of Using ExtraReducer to Listen fetchProjects.fulfilled When reading documentation ([link](https://redux-toolkit.js.org/api/createAsyncThunk)) we are instructed to create listener as follows: typescript // sample code from official tutorial extraReducers: (builder) => { // Add reducers for additional action types here, and handle loading state as needed builder.addCase(fetchProjects.fulfilled, (state, action) => { // Add the result to the state state.entities.push(action.payload); }); }; However, this approach does not allow dispatching additional action, not even speak of influencing the state in other slices. For example, we want to open a <Loading/> dialog during data-fetching, but the open state lives in another applicationSlice. We cannot simply import store object and store.dispatch the open-dialog action because store needs to be created by our current slice, namely, circular import will occur. #### Work Around: Write a Middleware for fetchProjects.fulfilled We instead listen to fetchProjects.fulfilled by creating a middleware as follows, this is very similar to redux-saga (but mutch easier): typescript // projectSlice.ts export const projectMiddleware = createListenerMiddleware(); projectMiddleware.startListening({ actionCreator: fetchProjects.pending, effect: (action, listenerApi) => { listenerApi.dispatch( appSlice.actions.updateNotification({ open: true, content: \\"Loading...\\" }) ); }, }); projectMiddleware.startListening({ actionCreator: fetchProjects.fulfilled, effect: (action, listenerApi) => { listenerApi.dispatch(appSlice.actions.updateNotification({ open: false })); }, }); Now we are free to dispatch any action that adjusts the state of other slices. > **Important.** Note that we can startListening() multiple times using the same middleware. There is no need to create multiple middlewares for multiple actions. ##### Add Middlewares to Store typescript // store.ts; // real use case, ignore the other slices const store = configureStore({ \\treducer: rootReducer, \\tmiddleware: (getDefaultMiddleware) => \\t\\t//@ts-ignore \\t\\tgetDefaultMiddleware({ \\t\\t\\tserializableCheck: false \\t\\t}).concat( \\t\\t\\tprojectMiddleware.middleware, \\t\\t\\tsomeOtherMiddleware.middleware \\t\\t) }); ##### Share a Middleware for Multiple Actions by matcher and isAnyOf In the section <a href=\\"#Work-Around:-Write-a-Middleware-for-fetchProjects.pending\\">Work Around: Write a Middleware for fetchProjects.pending</a> we: - wrote a single middleware for a fetchProjects.pending action, But the same effect should be shared amount actions like updateProjects.pending, deleteProject.pending, and even CRUD for all other entities. We can collect all those thunk actions and create middlewares specifically for all data-fetching logic: typescript //store.ts const projectMiddleware = createListenerMiddleware(); const pendingActions = [ fetchProjects.pending, fetchPages.pending, fetchStudents.pending, fetchCompanies.pending, ]; const fulfilledActions = [ fetchProjects.fulfilled, fetchPages.fulfilled, fetchStudents.fulfilled, fetchCompanies.fulfilled, ]; projectMiddleware.startListening({ matcher: isAnyOf(...pendingActions), effect: (action, listenerApi) => { listenerApi.dispatch( appSlice.actions.updateNotification({ open: true, content: \\"Loading...\\" }) ); }, }); projectMiddleware.startListening({ matcher: isAnyOf(...fulfilledActions), effect: (action, listenerApi) => { listenerApi.dispatch( appSlice.actions.updateNotification({ open: true, content: \\"Loaded\\" }) ); }, }); #### Further Simplification for Writing Middleware to Handler Multiple Actions Sometimes we have fine-grained notification pop-up messages for different thunk actions. It is tedious to write someMiddle.startListening({... for each of the actions. For not to repeat writing the same code block, we write a helper function: typescript import { AnyAction, ListenerEffect, ListenerMiddlewareInstance, ThunkDispatch, isAnyOf } from \\"@reduxjs/toolkit\\"; import snackbarUtils from \\"./snackbarUtils\\"; type Effect = ListenerEffect<any, unknown, ThunkDispatch<unknown, unknown, AnyAction>, unknown>; /** * actionMessageList consists of objects either of the form { action, content } or of the form { rejections } / { rejections, content }. When content is absent, the error message is supposed to be returned by thunkAPI.rejectWithValue * in createAsyncThunk function. */ const messageDispatch = ({ contentType, content }: { contentType: string, content: string }) => { if (contentType === \\"sucesss\\") { snackbarUtils.success(content) } else if (contentType === \\"info\\") { snackbarUtils.info(content) } else if (contentType === \\"warning\\") { snackbarUtils.warning(content) } else if (contentType === \\"error\\") { snackbarUtils.error(content); } } export default ( middleware: ListenerMiddlewareInstance< unknown, ThunkDispatch<unknown, unknown, AnyAction>, unknown >, actionMessageList: { action?: any, rejections?: any[], content?: string effect?: Effect contentType?: \\"sucesss\\" | \\"info\\" | \\"error\\" | \\"warning\\" }[] ) => { for (const actionMessage of actionMessageList) { const { action, rejections, content, effect, contentType = \\"sucesss\\" } = actionMessage; if (action) { let effect_: Effect; if (effect) { effect_ = effect; } else if (content) { effect_ = async (action, { dispatch }) => { messageDispatch({ contentType, content }) // dispatch(appSlice.actions.updateNotification( // { open: true, content: content || \\"No Message\\" } // )) }; } else { effect_ = async (action, thunkAPI) => { }; } middleware.startListening({ actionCreator: action, effect: effect_ }); } else if (rejections) { if (effect) { // @ts-ignore middleware.startListening({ matcher: isAnyOf(...rejections), effect }); } else { middleware.startListening({ // @ts-ignore matcher: isAnyOf(...rejections), effect: async (action, { dispatch }) => { if (content) { messageDispatch({ contentType, content }) // dispatch(appSlice.actions.updateNotification( // { open: true, content: content || \\"No Message\\" } // )) } else { const msg = action?.payload || \\"\\"; let errMsg = \\"Failed\\"; if (msg) { errMsg += (Reason: ${msg}); } snackbarUtils.error(errMsg) // dispatch(appSlice.actions.updateNotification( // { open: true, content: errMsg } // )) } } }) } } } } We are now happy writing multiple middlewares: typescript export const companyMiddleware = createListenerMiddleware(); registerEffects( companyMiddleware, [ { action: companyThunkAction.updateCompany.pending, content: \\"Updating Company ...\\" }, { action: companyThunkAction.updateCompany.fulfilled, content: \\"Updated.\\" }, { action: companyThunkAction.fetchCompanies.pending, content: \\"Getting companies ...\\" }, { action: companyThunkAction.fetchCompanies.fulfilled, content: \\"Loaded.\\" }, { action: companyThunkAction.uploadGenericFile.pending, content: \\"Uploading...\\" }, { action: companyThunkAction.uploadGenericFile.fulfilled, content: \\"Updated\\" }, { action: companyThunkAction.createCompany.pending, content: \\"Creating Company ...\\" }, { rejections: [companyThunkAction.createCompany.rejected] } ] ) - For actions: - If we provide content, then the listener will pop-up a notification with content as the message. - If we provide effect, then it will not pop-up notification and use custom effect instead. - For rejections: - If we just have rejections, the message is supposed to be the error message passed from thunkAPI.rejectWithValue in createAsyncThunk function. - If we pair rejections with content, then it will show our content as pop-up notification. #### Middleware that Handles all Rejected Actions (Optional) We usually learn how to react to all api error in axios by using interceptor: js apiClient.interceptors.response.use( function (response) { const param = { url: response.config.url, data: response.data, } if (${process.env.REACT_APP_ENV} === \'LOCAL\') { if (response?.data?.success === false) { } else { } } }, function (error) { if (error?.response?.status === 404) { //404 page } } ) We can instead handle all rejected api requests by middleware (provided that all api calls are processed by thunk actions) js import { createListenerMiddleware, isRejected } from \\"@reduxjs/toolkit\\"; import snackbarUtils from \\"../../util/snackbarUtils\\"; import { loginUrl } from \\"../../app/__paths__deprecated\\"; import { getHistory } from \\"../../util/historyUtils\\"; import authSlice from \\"../slices/authSlice\\"; const errorCodeRegex = /(?<=status\\\\scode\\\\s)\\\\d+/gi export const errorMiddleware = createListenerMiddleware(); errorMiddleware.startListening({ matcher: isRejected, effect: async (action, { dispatch }) => { const history = getHistory(); const { error } = action; const { message, stack } = error; if (message) { // sample message: Request failed with status code 401 const mathches = message.match(errorCodeRegex); const errorCode = parseInt(mathches?.[0] || \\"0\\"); if ( errorCode === 403 || errorCode === 401 ) { console.log(\'403 401 redirect: \' + loginUrl) dispatch(authSlice.actions.reset()); history?.push(loginUrl); } else if (errorCode === 404) { //404 page } else if (errorCode === 500) { //do nothing } else { } } if (stack) { snackbarUtils.error(stack); } } }); This will be helpful if we are going to handle a very general error flow like - expiration of access-token - make an api call to refresh access-token - ***resume the action again***, etc. ","title":"Write Middlewares in Redux-Toolkit","date":"2023-06-20T00:00:00.000Z","id":"blog0132","tag":"react","intro":"We list sample usage of createThunkAction provided by redux-toolkit in order to single out the logic of data-fetching away from the UI component.","toc":true},{"content":" #### General Post Request #### File Uploading By adding xml <dependency> <groupId>org.mongodb</groupId> <artifactId>bson</artifactId> <version>4.9.1</version> </dependency> into our pom.xml we are given a handy tool to parse json data in the body of request from frontend: java import org.bson.Document; @PostMapping(value = \\"/projects) @ResponseBody public Map<String, Boolean> updateProject(@RequestBody String reqJson) throws Exception { Document updateReq = Document.parse(reqJson) ... } Now we can get a value from key name via updateReq.get(\\"name\\"). ##### On React Side The basic logic: typescript export default () => { ... const [files, setFiles] = useState<FileList | null>(null); // used for file upload in <input ... onChange={uploadHandler} /> const uploadHandler = (e: React.ChangeEvent<HTMLInputElement>) => { if (e.target.files) { setFiles(e.target.files); } }; const uploadClickHanlder = async () => { if ((files?.length || 0) > 0) { dispatch(asyncUploadImage({ oid: project.oid, file: files![0] })); } }; return ( ... <input type=\\"file\\" accept=\\"image/*\\" onChange={uploadHandler} /> <button onClick={uploadClickHanlder}> Upload </button> ... ) } As usual we will upload a file in the form of FormData. We define asyncUploadImage in other slice file: typescript //projectSlice.ts export const asyncUploadImage = createAsyncThunk( \\"upload-project-image\\", async ({ oid, file }: { oid: string; file: File }) => { const formData = new FormData(); formData.append(\\"file\\", file); formData.append(\\"oid\\", oid); const response = await apiClient.post<{ success: boolean }>( POST_PROJECTS_FILE_UPLOAD, formData ); return response.data; } ); ##### On Springboot Side java private final File uploadDir = new File(\\"uploadfolder\\"); @PostMapping(value = \\"/projects/fileupload\\") @ResponseBody public Map<String, Object> updateProject( @RequestParam(\\"oid\\") String oid, @RequestParam(\\"file\\") MultipartFile file ) throws WbCustomException { if (!this.uploadDir.exists()) { this.uploadDir.mkdirs() } String filePath = this.uploadDir.getPath() + \\"/\\" + file.getOriginalFilename(); try (FileOutputStream fos = new FileOutputStream(filePath)) { fos.write(file.getBytes()); return Map.of(\\"success\\", true); } catch (Exception e) { StringWriter errors = new StringWriter(); e.printStackTrace(new PrintWriter(errors)); return Map.of(\\"success\\", false, \\"errorMessage\\", errors.toString()); } } - We use new File uplopadDir = new File(\\"some/dir\\"), then we can use - uploadDir.exists() and - uploadDir.mkdirs() as in python. - We use new FileOutputStream(filePath).write to write the received byte[] into a file. - When receiving Formdata in springboot, it is as if receiving data from json object so that we can use @RequestParam to destructure the request body. ","title":"General Post Request in Springboot and File Uploading ","date":"2023-05-25T00:00:00.000Z","id":"blog0133","tag":"react, springboot, java","intro":"We record the whole workflow of uploading file from react frontend to springboot backend.","toc":true},{"content":" #### Imports The following is what we need in the script. Basically we just use selenium and re for regular expression. python from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.action_chains import ActionChains from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC from selenium.webdriver.common.desired_capabilities import DesiredCapabilities import re #### The driver Object with eager Mode python caps = DesiredCapabilities().CHROME caps[\\"pageLoadStrategy\\"] = \\"eager\\" driver = webdriver.Chrome( desired_capabilities=caps, executable_path=\\"C:\\\\\\\\dev\\\\\\\\chrome-driver\\\\\\\\chromedriver.exe\\" ) Here pageLoadStrategy is set to eager in order to speed up the scrapping process. The eager mode will let us stop loading the page when the dom element is completely loaded (we don\'t wait for other kind of data such as images, audio, etc) #### Utility Functions python def control_and_click(anchor): ActionChains(driver).key_down(Keys.CONTROL)\\\\ .click(anchor)\\\\ .key_up(Keys.CONTROL)\\\\ .perform() def wait_element(search_txt, by_method=By.CSS_SELECTOR, seconds=10): return WebDriverWait(driver, seconds)\\\\ .until(EC.presence_of_all_elements_located((by_method, search_txt))) def load_page_and_get_reports(url): driver.get(url) reports = wait_element(\\"#reports-table a[href*=\'/reports\']\\") print(\\"Number of reports in this page: {}\\".format(len(reports))) return reports def refresh_page_and_get_reports(): driver.refresh() reports = wait_element(\\"#reports-table a[href*=\'/reports\']\\") print(\\"Number of reports in this page: {}\\".format(len(reports))) return reports - control_and_click will press control key and click the target element, this is to open a link in new tab. - wait_element will wait for an element to be locatable, this is to make sure no problem occurs when a page is slow (e.g. server side rendering takes some time to return a web page). - Once there is problem, we try to refresh refresh_page_and_get_reports to see whether we can resolve it. #### The Scrapping Function - The strategy is to identify all the anchors in the page that we need to click (line 9 with css selector). python-1 def start_fflog_scrapping(urls, target_player_names, file_location=\\"omega_kills_record.txt\\"): names_matching_regex = re.compile(\\"|\\".join(target_player_names)) current_page_index = 0 for page_index, url in enumerate(urls): current_page_index = page_index try: reports = load_page_and_get_reports(url) - Not only we click it, we ctrl + click in order to open the page in new tab. python-10 for i in range(0, len(reports)): print(\\"handing report\\", reports[i].text) report_anchor = reports[i] control_and_click(report_anchor) - When ctrl + click succeeds, the number of tabs: len(driver.window_handles) must be $\\\\ge 2$. However, it used to fail for some reason, we may record those unread reports and study it later on: driver.switch_to.window(driver.window_handles[1]) will switch the brower to next tab. python-14 if len(driver.window_handles) <= 1: print(\\"report {} cannot be read due to some problem\\".format(report_anchor.get_attribute(\\"href\\"))) with open(\\"report_not_read.txt\\", \\"a+\\") as _f: _f.write(report_anchor.get_attribute(\\"href\\") + \\"\\\\n\\") continue driver.switch_to.window(driver.window_handles[1]) - Depends on the number of click we need, when we are done with the scrapping: - We close the current tab by driver.close() - We switch to previous tab by driver.switch_to.window(driver.window_handles[i]) - In case we have clicked anchors twice, we need to close() and switch twice, see line 39 for example. - The reamining scrapping logic is page-specific. We use regular expression to match desired results and make a record. python-21 # detail of bosses related, we are only interested in those omega kill log: rows = wait_element(\\"a[class*=\'report-overview-boss\']\\") omega_kills = [row for row in rows if re.search(\\"TheOmegaProtocolKill\\", re.sub(\\"\\\\\\\\s\\", \\"\\", row.text)) is not None] if len(omega_kills) > 0: control_and_click(omega_kills[0]) driver.switch_to.window(driver.window_handles[2]) name_anchors = wait_element(\\"#summary-damage-done-scroller-0 tr[role=\'row\'] a.tooltip\\") names = \\"\\".join([anchor.text for anchor in name_anchors]) matched_names = names_matching_regex.findall(names) if len(matched_names) > 0: with open(file_location, \\"a+\\", encoding=\\"utf-8\\") as f: line_1 = \\"target player(s):\\\\t\\" + \\", \\".join(matched_names) + \\"\\\\n\\" line_2 = \\"link: \\\\t\\\\t\\\\t\\\\t\\\\t\\\\t\\" + driver.current_url + \\"\\\\n\\" line_3 = \\"---\\\\n\\" f.writelines(line_1 + line_2 + line_3) print(\\"Player { \\"+ \\", \\".join(matched_names) + \\" } has been found\\") driver.close() driver.switch_to.window(driver.window_handles[1]) driver.close() driver.switch_to.window(driver.window_handles[0]) else: driver.close() driver.switch_to.window(driver.window_handles[0]) except Exception as e: urls = urls[current_page_index:] print(e) print(\\"Exception was caught, retry from page: \\" + urls[0]) start_fflog_scrapping(urls, target_player_names) ","title":"Data Scrapping for Data that Requires Click by Click ","date":"2023-05-27T00:00:00.000Z","id":"blog0134","tag":"python, selenium","intro":"*Click and then get detail* is a very routine practice for data scrapping. We record how to do that by selenium in python.","toc":true},{"content":" text const useStyles = makeStyles((theme) => ({ grow: { animation: $growAnimation 2000ms ease-in-out, }, \\"@keyframes growAnimation\\": { \\"0%\\": { boxShadow: \\"0 0 0px #3498db\\", }, \\"50%\\": { boxShadow: \\"0 0 20px #3498db\\", }, \\"100%\\": { boxShadow: \\"0 0 0px #3498db\\", }, }, })); After animation is triggered, make sure to toggle off the corresponding flag js const shouldGrow = growing && rowIndexBeingSelected === reduxStoreRowIndex; useEffect(() => { if (shouldGrow) { setTimeout(() => { dispatch(wbuserSlice.actions.setRowEdition({ grow: false })); }, 2000); } }, [growing]); ","title":"Mui CSS Animation with Keyframes","date":"2023-06-09T00:00:00.000Z","id":"blog0136","tag":"react","intro":"Record how to write CSS animation with keyframes in mui makeStyles.","toc":false},{"content":" #### The JsonPipeline Class with Pipleline Builder In our Mongo package we add the following subclass: java public class Mongo { ... public static class JsonPipeline { private static String trailingBracketWithSpaces = \\"\\\\\\\\]\\\\\\\\s*$\\"; private static String startBracketWithSpaces = \\"^\\\\\\\\s*\\\\\\\\[\\"; private String totalPipelineString = \\"\\"; public List<BsonDocument> get() { return parseJsonPipline(); } private List<BsonDocument> parseJsonPipline() { this.add(\\"[{$addFields: {oid: {$toString: \\\\\\"$_id\\\\\\"}}}, {$unset: \\\\\\"_id\\\\\\"}]\\"); var pipelines_ = new BsonArrayCodec() .decode(new JsonReader(totalPipelineString), DecoderContext.builder().build()) .stream() .map(BsonValue::asDocument) .collect(Collectors.toList()); return pipelines_; } public JsonPipeline add(String json) { if (totalPipelineString.isEmpty()) { totalPipelineString = json; } else { String prevRemoveCloseBrac = totalPipelineString.replaceAll(trailingBracketWithSpaces, \\"\\"); String nextRemoveOpenBrac = json.replaceAll(startBracketWithSpaces, \\"\\"); totalPipelineString = prevRemoveCloseBrac + \\",\\" + nextRemoveOpenBrac; } return this; } public String getTotalPipelineString() { return totalPipelineString; } /** * oid will be automatically created for foreign collection, the result will * always be an array. * Make sure to {$unwind: \\"$newFieldName\\"} if you are sure there are only one * element in the result. */ public JsonPipeline addLeftJoin( String foreignCollection, String localField, String foreignField, String leftJoinPipeline, String newFieldName) { String _jsonPipeline = String.format(\\"\\"\\" [ { $addFields: { tmp_localfield: \\"%s\\" } }, { $unwind: { path: \\"$tmp_localfield\\", preserveNullAndEmptyArrays: true } }, { $lookup: { let: { tmp: \\"$tmp_localfield\\"}, from: \\"%s\\", pipeline: [ { $addFields:{ oid: {$toString: \\"$_id\\"} } }, { $unset: \\"_id\\" }, { $match:{ $expr:{ $eq: [\\"$$tmp\\", \\"%s\\"] } } }, %s ], as: \\"tmp_singleElementList\\" } }, { $unwind: { path: \\"$tmp_singleElementList\\", preserveNullAndEmptyArrays: true } }, { $group:{ _id: \\"$_id\\", originalData: { $first: \\"$$ROOT\\"}, pushList: { $push: \\"$tmp_singleElementList\\" } } }, { $replaceRoot: { newRoot: { $mergeObjects: [ \\"$originalData\\", { %s: \\"$pushList\\" } ] } } }, { $unset : \\"tmp_singleElementList\\" }, { $unset : \\"tmp_localfield\\" } ] \\"\\"\\", \\"$\\" + localField, foreignCollection, \\"$\\" + foreignField, leftJoinPipeline.replaceAll(\\"(^\\\\\\\\s*\\\\\\\\[)|(\\\\\\\\]\\\\\\\\s*)$\\", \\"\\"), newFieldName); return this.add(_jsonPipeline); } } public static JsonPipeline createJsonPipeline() { return new JsonPipeline(); } } #### Pipelines ##### Simplest One Now a projection pipeline can be as simple as java var pipeline = Mongo.createJsonPipeline(); pipeline.add(\\"[{ $project: { code: 1, name: 1, role: 1 }}]\\"); List<Document> results = companies.aggregate(pipeline.get()).into(new ArrayList<>()); instead of java var projections = Arrays.asList( Aggregates.project( Projections.fields( Projections.include(\\"code\\"), Projections.include(\\"name\\"), Projections.include(\\"role\\") )) ); List<Document> results = companies.aggregate(projections).into(new ArrayList<>()); ##### More Advanced Pipeline with Left Joins java var chainOid = searchEmailRefinedParam.getEmail_chain_oid(); var chainCollection = mongodb.getCollection(CollectionNames.email_messages); var pipeline = MongoDB.createJsonPipeline(); pipeline .add(String.format(\\"[{ $match:{ email_chain_oid: \\\\\\"%s\\\\\\" }}]\\", chainOid)) .addLeftJoin(CollectionNames.users, \\"sender_email\\", \\"user_name\\", \\"[{ $project: {user_name: 1, email: 1, first_name: 1, last_name: 1, company_code: 1} }]\\", \\"senderInDb\\") .add(\\"[{$unwind: \\\\\\"$senderInDb\\\\\\"}]\\") .addLeftJoin(CollectionNames.users, \\"recipient_emails\\", \\"user_name\\", \\"[{ $project: {user_name: 1, email: 1, first_name: 1, last_name: 1} }]\\", \\"recipientsInDb\\") .addLeftJoin(CollectionNames.companies, \\"senderInDb.company_code\\", \\"code\\", \\"[{ $project:{ name:1 } }]\\", \\"sender_company\\") .add(\\"[{ $unwind: \\\\\\"$sender_company\\\\\\" }]\\"); var emailsResult = chainCollection.aggregate(pipeline.get()).into(new ArrayList<>()); ","title":"Write Mongo Aggregation Using JSON in Springboot","date":"2023-07-30T00:00:00.000Z","id":"blog0137","tag":"java, mongo","intro":"We get rid of the headache of writing mongo agggregation using mongo apis in java given that we already know how to write that in json format.","toc":true},{"content":" javascript import { PayloadAction, createAsyncThunk, createListenerMiddleware, createSlice, isAnyOf } from \\"@reduxjs/toolkit\\" import appSlice from \\"./appSlice\\" type State = { sth: string } const initialState: State = { sth: \\"\\" } const slice = createSlice( { name: \\"sth\\", initialState, reducers: { reset: (state) => { return initialState } }, extraReducers: (builder) => { builder.addCase(someThunkAction.fetchSth.fulfilled, (state, action) => { }) } } ) export const someThunkAction = { fetchSth: createAsyncThunk(\\"get-sth\\", async () => { }) } const someMiddleware = createListenerMiddleware(); someMiddleware.startListening({ matcher: isAnyOf( someThunkAction.fetchSth.fulfilled, ), effect: async (action, listenerApi) => { listenerApi.dispatch(appSlice.actions.updateNotification( { open: true, content: \\"Loaded.\\" } )) } }); export default slice; ","title":"Redux Slice Template","date":"2023-06-13T00:00:00.000Z","id":"blog0138","tag":"react","intro":"Record my template of redux slice.","toc":false},{"content":" #### Demonstration 1 Look up from Arrays Instead of Collections (Inner Joins Only) ##### Stages - Since look up is only available for collections in mongodb, if there is a property among the array of elements, like id, which we need to left join with another collection. We need to $unwind it to reduce the problem into our old familiar problem. - Next we need to undo the $unwind process, which can be done by using using all other fields jointly to form an_id object, then we push the lookup results into an array according to this _id. - We then use $replaceRoot: { newRoot: ... } trick to move everything in _id: {...} to the parent level. ##### Full Breakdown javascript-1 db.ns_material_categories.aggregate([ { $lookup: { let: { group_code: \\"$group_code\\" }, from: \\"ns_material_category_groups\\", pipeline: [ { $match: { $expr: { $eq: [\\"$code\\", \\"$$group_code\\"] } } }, { $project: { name: 1, code: 1, _id: 0 } } ], as: \\"materialGroup\\" } }, { $match: { $expr: { $eq: [\\"$enabled\\", \\"Y\\"] } } }, { $unwind: \\"$materialGroup\\" }, This lookup only aims at left-joining ns_material_category_groups to get materialGroup, which becomes an array of objects of only 1 element (due to 1-1 correspondance), we $unwind to sequeeze the array into an object: none [{ \\"_id\\": { \\"timestamp\\": 1648724334, \\"date\\": \\"2022-03-31T10:58:54.000+00:00\\" }, \\"code\\": \\"FINISHES_CERAMIC_TILES\\", \\"group_code\\": \\"InteriorFinishes\\", \\"enabled\\": \\"Y\\", \\"materialGroup\\": { \\"code\\": \\"InteriorFinishes\\" } }] We next need to lookup from another collection using the field code: javascript-23 { $lookup: { let: { code: \\"$code\\" }, from: \\"ns_materials\\", pipeline: [ { $match: { $expr: { $and: [ { $eq: [\\"$category_code\\", \\"$$code\\"] }, { $eq: [\\"$is_dummy\\", \\"Y\\"] } ] } } }, { $project: { _id: 0, manu_code: 1, } } ], as: \\"suppliers\\" } }, At this point our queried object becomes none [ { \\"_id\\": { \\"timestamp\\": 1648724334, \\"date\\": \\"2022-03-31T10:58:54.000+00:00\\" }, \\"code\\": \\"FINISHES_CERAMIC_TILES\\", \\"group_code\\": \\"InteriorFinishes\\", \\"enabled\\": \\"Y\\", \\"materialGroup\\": { \\"code\\": \\"InteriorFinishes\\" }, \\"suppliers\\": [ { \\"manu_code\\": \\"DAUGRES\\" }, { \\"manu_code\\": \\"ELEPHOME\\" }, { \\"manu_code\\": \\"HONGYU\\" }, { \\"manu_code\\": \\"KITO\\" }, { \\"manu_code\\": \\"MMMOSAIC\\" } ] }, ... ] But we need to look up from another collection using manu_code, we $unwind the suppliers field and do a $lookup from ns_material_manus again to get the name_en field. javascript-48 { $unwind: \\"$suppliers\\" }, { $lookup: { let: { manu_code: \\"$suppliers.manu_code\\" }, from: \\"ns_material_manus\\", pipeline: [ { $match: { $expr: { $eq: [\\"$$manu_code\\", \\"$internal_code\\"] } } }, { $project: { _id: 0, name_en: 1 } }, { $limit: 1 } ], as: \\"supplierDetail\\" } }, At this point we get none [ { \\"_id\\": { \\"timestamp\\": 1648724334, \\"date\\": \\"2022-03-31T10:58:54.000+00:00\\" }, \\"code\\": \\"FINISHES_CERAMIC_TILES\\", \\"group_code\\": \\"InteriorFinishes\\", \\"enabled\\": \\"Y\\", \\"materialGroup\\": { \\"code\\": \\"InteriorFinishes\\" }, \\"suppliers\\": { \\"manu_code\\": \\"DAUGRES\\" }, \\"supplierDetail\\": [ { \\"name_en\\": \\"DAUGRES\\" } ] }, ... ] - Note that we have used { $limit: 1 } as we understand each company only has one name, lookup results are born with being an array. - We $unwind it to get ordinary object and finally use the original fields jointly as an _id to group all supplier results: javascript-74 { $unwind: \\"$supplierDetail\\" }, { $group: { _id: { code: \\"$code\\", group_code: \\"$group_code\\", name: \\"$name\\", \\"code\\": \\"$code\\", \\"group_code\\": \\"$group_code\\", \\"enabled\\": \\"$enabled\\", \\"materialGroup\\": \\"$materialGroup\\" }, suppliers: { \\"$push\\": { name_en: \\"$supplierDetail.name_en\\" } } } }, The result at this point is of the form: none [ { \\"_id\\": { \\"code\\": \\"EXTERIOR_FINISHES_COMPOSITE_DECKING\\", \\"group_code\\": \\"ExteriorFlooring\\", \\"enabled\\": \\"Y\\", \\"materialGroup\\": { \\"code\\": \\"ExteriorFlooring\\" } }, \\"suppliers\\": [ { \\"name_en\\": \\"GREENZONE\\" }, { \\"name_en\\": \\"MEXY\\" } ] }, ... ] We finally move everything inside _id to upper level: javascript-93 { $replaceRoot: { newRoot: { $mergeObjects: [\\"$_id\\", { suppliers: \\"$suppliers\\" }] } } } ]) which finally yields none [ { \\"code\\": \\"TAPWARES_KITCHEN_TAPWARE\\", \\"group_code\\": \\"Tapwares.Accessories\\", \\"enabled\\": \\"Y\\", \\"materialGroup\\": { \\"code\\": \\"Tapwares.Accessories\\" }, \\"suppliers\\": [ { \\"name_en\\": \\"EMPOLO\\" }, { \\"name_en\\": \\"ECCO VITA\\" } ] }, ... ] #### Demonstration 2: $lookup + $unwind by Default is an Inner Join ##### Example Demonstrating Inner Join Behaviour Consider the following aggregation, we are restricting ourself to look at only one \\"problematic\\" document by enforcing { code: \\"NEW_CATEGORY_20230614C\\" }. js-1 db.ns_material_categories.aggregate([ { $match: { $and: [ { enabled: \\"Y\\" }, { code: \\"NEW_CATEGORY_20230614C\\" } ] } }, { $lookup: { let: { group_code: \\"$group_code\\" }, from: \\"ns_material_category_groups\\", pipeline: [ { $match: { $expr: { $eq: [\\"$code\\", \\"$$group_code\\"] } } }, { $project: { oid: { $toString: \\"$_id\\" }, name: 1, code: 1, _id: 0 } } ], as: \\"catGroup\\" } }, { $unwind: \\"$catGroup\\" }, { $lookup: { let: { code: \\"$code\\" }, from: \\"ns_materials\\", pipeline: [ { $match: { $expr: { $and: [ { $eq: [\\"$category_code\\", \\"$$code\\"] }, { $eq: [\\"$is_dummy\\", \\"Y\\"] } ] } } }, { $project: { _id: 0, manu_code: 1, } } ], as: \\"suppliers\\" } }, At this point the result is: none [ { \\"_id\\": { \\"$oid\\": \\"648954d9547dee498d4f85a3\\" }, \\"code\\": \\"NEW_CATEGORY_20230614C\\", \\"group_code\\": \\"NewCategoryGroup20230614c\\", \\"enabled\\": \\"Y\\", \\"catGroup\\": { \\"code\\": \\"NewCategoryGroup20230614c\\", \\"oid\\": \\"648954d2547dee498d4f85a2\\" }, \\"suppliers\\": [] } ] Note that suppliers is an empty array, which means that we have _nothing matched_ from the table on the right. Each item in suppliers contains a field called manu_code, with which we will be using to look up from another collection, for this purpose, we need to $unwind suppliers field and then do a $lookup using $suppliers.manu_code. js-50 { $unwind: \\"$suppliers\\" }, but oops! none [] We conclude from this stage that by default $lookup + $unwind is an inner join operation (assuming we will be doing $group action in the last stage). ##### unwind with preserveNullAndEmptyArrays: true As in mySQL we most of the time want to left join instead of inner join, if we instead js-50 { $unwind: { path: \\"$suppliers\\", preserveNullAndEmptyArrays: true } }, we get none [ { \\"_id\\": { \\"$oid\\": \\"648954d9547dee498d4f85a3\\" }, \\"code\\": \\"NEW_CATEGORY_20230614C\\", \\"group_code\\": \\"NewCategoryGroup20230614c\\", \\"enabled\\": \\"Y\\", \\"catGroup\\": { \\"code\\": \\"NewCategoryGroup20230614c\\", \\"oid\\": \\"648954d2547dee498d4f85a2\\" } } ] that means row on the left table is preserved even we match nothing on the right table. This is exactly what left join does. If we do a look up using suppliers.manu_code: js-53 { $lookup: { let: { manu_code: \\"$suppliers.manu_code\\" }, from: \\"ns_material_manus\\", pipeline: [ { $match: { $expr: { $eq: [\\"$$manu_code\\", \\"$internal_code\\"] } } }, { $project: { _id: 0, name_en: 1 } }, { $limit: 1 } ], as: \\"supplierDetail\\" } }, we get none [ { \\"_id\\": { \\"$oid\\": \\"648954d9547dee498d4f85a3\\" }, \\"code\\": \\"NEW_CATEGORY_20230614C\\", \\"group_code\\": \\"NewCategoryGroup20230614c\\", \\"enabled\\": \\"Y\\", \\"catGroup\\": { \\"code\\": \\"NewCategoryGroup20230614c\\", \\"oid\\": \\"648954d2547dee498d4f85a2\\" }, \\"supplierDetail\\": [] } ] As desired. ##### Grouping and Handling the [ {} ] Result ###### Starting from Nonempty Suppliers Document and Group the Results We enforce { code: \\"EXTERIOR_FLOORING_COBBLE_STONE\\" } at the beginning of the aggregation script, this has 3 suppliers and thefore we have 3 supplierDetails. Up to line 75 of the aggregation script, the result will be 3 pieces of data having the following form (with suppliers and supplierDetail being the only varying field) none { \\"_id\\": { \\"$oid\\": \\"63316d25fe1ff183f4436771\\" }, \\"code\\": \\"KITCHEN_APPLIANCES_OVEN\\", \\"group_code\\": \\"KitchenAppliances\\", \\"enabled\\": \\"Y\\", \\"catGroup\\": { \\"code\\": \\"KitchenAppliances\\", \\"oid\\": \\"63316d07fe1ff183f443676a\\" }, \\"suppliers\\": { \\"manu_code\\": \\"FOTILE\\" }, \\"supplierDetail\\": [ { \\"name_en\\": \\"FOTILE\\" } ] } Next we $unwind (just for squeezing the array) and $group what we need: js-76 { $unwind: { path: \\"$supplierDetail\\", preserveNullAndEmptyArrays: true } }, { $group: { _id: { oid: { $toString: \\"$_id\\" }, code: \\"$code\\", enabled: \\"$enabled\\", catGroup: \\"$catGroup\\" }, suppliers: { \\"$push\\": { name_en: \\"$supplierDetail.name_en\\" } } } }, which yields: none [ { \\"_id\\": { \\"oid\\": \\"63316d25fe1ff183f4436770\\", \\"code\\": \\"EXTERIOR_FLOORING_COBBLE_STONE\\", \\"enabled\\": \\"Y\\", \\"catGroup\\": { \\"code\\": \\"ExteriorFlooring\\", \\"oid\\": \\"62cbbf0b35fc26d4990aa2f1\\" } }, \\"suppliers\\": [ { \\"name_en\\": \\"DALEI\\" }, { \\"name_en\\": \\"EASTWOOD STONE\\" }, { \\"name_en\\": \\"STONELINK\\" } ] } ] $replaceRoot is the trick to bring everything in _id to the top level: js-99 { $replaceRoot: { newRoot: { $mergeObjects: [\\"$_id\\", { suppliers: \\"$suppliers\\" }] } } }, which results in: none [ { \\"oid\\": \\"63316d25fe1ff183f4436770\\", \\"code\\": \\"EXTERIOR_FLOORING_COBBLE_STONE\\", \\"enabled\\": \\"Y\\", \\"catGroup\\": { \\"code\\": \\"ExteriorFlooring\\", \\"oid\\": \\"62cbbf0b35fc26d4990aa2f1\\" }, \\"suppliers\\": [ { \\"name_en\\": \\"DALEI\\" }, { \\"name_en\\": \\"EASTWOOD STONE\\" }, { \\"name_en\\": \\"STONELINK\\" } ] } ] We finally look up another table for technical purpose. Note the use of javascript, which is considerably tedious if we rely merely on mongo query: js-106 { $lookup: { from: \\"ns_generic_form_templates\\", let: { code: \\"$code\\" }, pipeline: [ { $project: { subtypes: 1 } }, { $addFields: { formAnalysis: { $function: { body: function (code, subtypes, formId) { if (!subtypes) { return null } else { const hasForm = subtypes.indexOf(code) > -1; return { category_code: code, hasForm, formOid: formId } } }, args: [\\"$$code\\", \\"$subtypes\\", \\"$_id\\"], lang: \'js\' } } } }, { $project: { subtypes: 0 } }, { $match: { \\"formAnalysis.hasForm\\": true } }, { $limit: 1 }, { $replaceRoot: { newRoot: \\"$formAnalysis\\" } }, { $set: { formOid: { $toString: \\"$formOid\\" } } }, ], as: \\"genericForm\\" } }, We next $unwind again simply for sequeezing the array into a single object: js-142 { $unwind: { path: \\"$genericForm\\", preserveNullAndEmptyArrays: true } }, And we get a pretty result: none [ { \\"oid\\": \\"63316d25fe1ff183f4436770\\", \\"code\\": \\"EXTERIOR_FLOORING_COBBLE_STONE\\", \\"enabled\\": \\"Y\\", \\"catGroup\\": { \\"code\\": \\"ExteriorFlooring\\", \\"oid\\": \\"62cbbf0b35fc26d4990aa2f1\\" }, \\"suppliers\\": [ { \\"name_en\\": \\"DALEI\\" }, { \\"name_en\\": \\"EASTWOOD STONE\\" }, { \\"name_en\\": \\"STONELINK\\" } ], \\"genericForm\\": { \\"category_code\\": \\"EXTERIOR_FLOORING_COBBLE_STONE\\", \\"hasForm\\": true, \\"formOid\\": \\"62972948261ac272d152e238\\" } } ] ###### Starting from Empty Suppliers Document and Deal with [ {} ] We go back to { code: \\"NEW_CATEGORY_20230614C\\" } which has no suppliers. We run through all the same script to line 147 as above, then the result becomes none [ { \\"oid\\": \\"648954d9547dee498d4f85a3\\", \\"code\\": \\"NEW_CATEGORY_20230614C\\", \\"enabled\\": \\"Y\\", \\"catGroup\\": { \\"code\\": \\"NewCategoryGroup20230614c\\", \\"oid\\": \\"648954d2547dee498d4f85a2\\" }, \\"suppliers\\": [ {} ] } ] To make [ {} ] into [], we do a filtering: js-149 { $set: { suppliers: { $filter: { input: \\"$suppliers\\", cond: { $ifNull: [\\"$$this.name_en\\", false] } } } } } ] ) and we get: none [ { \\"oid\\": \\"648954d9547dee498d4f85a3\\", \\"code\\": \\"NEW_CATEGORY_20230614C\\", \\"enabled\\": \\"Y\\", \\"catGroup\\": { \\"code\\": \\"NewCategoryGroup20230614c\\", \\"oid\\": \\"648954d2547dee498d4f85a2\\" }, \\"suppliers\\": [] } ] ","title":"Inner and Left Joining Multiple Collections in Mongo --- The preserveNullAndEmptyArrays in $unwind","date":"2023-06-15T00:00:00.000Z","id":"blog0139","tag":"mongo","intro":"$lookup only works for the whole collection, we demonstrate how to left join another collection using a field which is an array.","toc":true},{"content":" #### First \\\\$lookup js-1 db.ns_material_categories.aggregate( [ { $lookup: { from: \\"ns_material_category_groups\\", localField: \\"group_code\\", foreignField: \\"code\\", as: \\"properties\\" } }, The $lookup here is the most basic form, which do a left join by gluing group_code and code. #### Second \\\\$lookup The next one is a generalization of the first $lookup . js-11 { $unwind: \\"$properties\\" }, { $lookup: { let: { code: \\"$code\\" }, from: \\"ns_materials\\", pipeline: [ { $match: { $expr: { $and: [ {$eq: [\\"$category_code\\", \\"$$code\\"]}, {$eq: [\\"$is_dummy\\", \\"Y\\"]} ] } } }, { $project: { _id: 0, manu_code: 1, model_code: 1, name: 1 } } ], as: \\"suppliers\\" } } ] ) This is somewhat more complicated, but the pipeline makes it much more flexible than the basic form. #### The from-let-pipeline-as lookup For look up we have either - from-localField-foreignField-as - from-let-pipeline-as There is no pipeline when either localField or foreignField exists. We try to break down to explain the more advanced $lookup, we start form line 16 of the code blocks above. - In let we have ${code: \\"$code\\"}, the key code is the variable name for temp storage, the \\"$code\\" is the field path of an object from the upstream collection in the pipeline. - We store all desired variable in the let stage. In this example, we will unwrap and utilize the value by writing \\"$$code\\". - We use $expr to **instantiate** any comparison. - The basic $lookup can just do mysql select a.* from A left join B b on b.a_id = a.id - whereas the advanced $lookup can do mysql select b.sth1, b.sth2, a.* from A a left join B b on b.id = b_id and ... and ... and ... due to the field pipeline. #### Using Javascript in $lookup with Pipeline! js pipeline: [ { $lookup: { from: \\"ns_generic_form_templates\\", let: { code: \\"$code\\" }, pipeline: [ { $project: { subtypes: 1 } }, { $addFields: { formAnalysis: { $function: { body: function (code, subtypes, formId) { if (!subtypes) { return null } else { const hasForm = subtypes.indexOf(code) > -1; return { category_code: code, hasForm, formOid: formId } } }, args: [\\"$$code\\", \\"$subtypes\\", \\"$_id\\"], lang: \'js\' } } } }, { $project: { subtypes: 0 } } { $match: { \\"formAnalysis.hasForm\\": true } }, { $limit: 1 }, { $replaceRoot: { newRoot: \\"$formAnalysis\\" } }, { $set: { formOid: { $toString: \\"$formOid\\" } } }, ], as: \\"genericForm\\" } }, { $unwind: \\"$genericForm\\" } ] ","title":"Two Kinds of $lookup, and use Javascript in Advanced lookup","date":"2023-06-15T00:00:00.000Z","id":"blog0141","tag":"mongo","intro":"We demonstrate it by examples.","toc":true},{"content":" - We can get Dayjs object in two major ways: js import dayjs, { Dayjs } from \\"dayjs\\"; const displayFormat = \\"YYYY-MM-DD HH:mm:ss\\"; const date1 = dayjs(epochTime); const date2 = dayjs(new Date()); const dateDisplay = date1.format(displayFormat); - The list of formatting strings can be found in - https://day.js.org/docs/en/display/format - Date manipulation - https://day.js.org/docs/en/manipulate/manipulate - Show the time in different timezone - https://day.js.org/docs/en/timezone/timezone ","title":"Use of dayjs","date":"2023-06-16T00:00:00.000Z","id":"blog0142","tag":"react, javascript","intro":"Just record the usual API that we may need in using dayjs.","toc":false},{"content":" In this blog we have used OverlayScrollbars for a good looking scrollbar, the drawback is that we need to wrap every component which we want such a scrollbar. When an application get complicated (like many popups, many forms), we may simply apply the css rule below css <style> ::-webkit-scrollbar { width: 6px; height: 4px; } ::-webkit-scrollbar-thumb { border-radius: 10px; background-color: rgba(0, 0, 0, 0.25); } ::-webkit-scrollbar-track { background: \'#FFFFFF\'; border-radius: 10px; } </style> at the top level, every instance of scrollbars will then be rounded. ","title":"Scrollbar Style Like Mac","date":"2023-06-20T00:00:00.000Z","id":"blog0143","tag":"react","intro":"Record CSS that makes scollbar look better.","toc":false},{"content":" #### Radio Buttons typescript import * as React from \\"react\\"; import Radio from \\"@mui/material/Radio\\"; import { makeStyles } from \\"@material-ui/core\\"; import classnames from \\"classnames\\"; const useStyles = makeStyles({ selectionList: { \\"& span\\": { marginRight: 2, }, \\"& .selection-row\\": { display: \\"flex\\", alignItems: \\"center\\", }, }, }); export default function RadioButtonsGroup<T extends string>({ value, setValue, listOfValues: options = [], style = {}, className = \\"\\", }: { value: ReturnType<typeof React.useState<T>>[0]; setValue: ReturnType<typeof React.useState<T>>[1]; listOfValues?: { value: T; displayName: string }[]; style?: React.CSSProperties; className?: string; }) { const classes = useStyles(); const handleChange = (event: React.ChangeEvent<HTMLInputElement>) => { setValue(event.target.value as unknown as T); }; const controlProps = (item: string) => ({ checked: value === item, onChange: handleChange, value: item, name: \\"color-radio-button-demo\\", inputProps: { \\"aria-label\\": item }, }); return ( <div className={classnames(classes.selectionList, className)} style={style}> {options.map((opt) => { const { displayName, value } = opt; return ( <div className=\\"selection-row\\"> <Radio {...controlProps(value)} size=\\"small\\" /> {displayName} </div> ); })} </div> ); } Usage: typescript <RadioButtonsGroup className={classes.radio} style={{ display: \\"flex\\" }} value={value} setValue={setValue} listOfValues={[ { value: \\"Important\\", displayName: \\"Important\\", }, { value: \\"Submittal\\", displayName: \\"Submittal\\", }, { value: \\"Confirmation\\", displayName: \\"Confirmation\\", }, ]} /> Which results result similar to <Center> <img src=\\"/assets/tech/145/001.png\\"/> </Center> <p/> <center></center> Here the display styles are refined by using style and className attributes, where ts const useStyles = makeStyles({ radio: { \\"& .selection-row\\": { marginRight: 20, }, }, }); #### Custom Dropdown List We will name it GeneralDropdown: typescript import { CSSProperties, useEffect, useRef, useState } from \\"react\\"; import { Popper, makeStyles } from \\"@material-ui/core\\"; import classnames from \\"classnames\\"; import { BsFillCaretUpFill } from \\"react-icons/bs\\"; import normalizeUtil from \\"../util/normalizeUtil\\"; import useClickOutside from \\"../pages/hooks/useClickOutside\\"; import { useMainStyles } from \\"../pages/CorrespondenceDashboard/CorrespondenceDashboard\\"; import { OverlayScrollbarsComponent } from \\"overlayscrollbars-react\\"; const borderStyle = \\"1px solid rgb(200, 200, 200)\\"; const borderRadius = 4; const options = { scrollbars: { autoHide: \\"scroll\\" }, }; const useStyles = makeStyles({ display: { overflow: \\"hidden\\", zIndex: 3, textOverflow: \\"ellipsis\\", }, disabled: { pointerEvents: \\"none\\", opacity: 0.4, }, option: { overflow: \\"hidden\\", textOverflow: \\"ellipsis\\", padding: \\"8px 8px\\", fontFamily: \\"Roboto, Helvetica, Arial, sans-serif\\", \\"&:hover\\": { backgroundColor: \\"#f2f9fc\\", }, \\"&.selected\\": { backgroundColor: \\"rgb(0, 166, 250)\\", color: \\"white\\", }, }, }); const UpIcon = () => { return ( <BsFillCaretUpFill style={{ opacity: 0.2, transform: \\"scaleX(0.8) scaleY(0.7)\\" }} /> ); }; const DownIcon = () => { return ( <BsFillCaretUpFill style={{ opacity: 0.2, transform: \\"scaleX(0.8) scaleY(-0.7)\\" }} /> ); }; type Option = { code: string; name: string; className?: string }; const GeneralDropdown = <T extends { code: string; name: string }>({ initialValue, fullList, refUpdateHandler = () => {}, selectionHint = \\"Select\\", className = \\"\\", enableNone = false, optionStyle = {}, disablePredicate = (option: T) => false, style = {}, }: { initialValue: T | null; fullList: T[]; refUpdateHandler?: (selection: T | null) => void; selectionHint?: string; className?: string; style?: CSSProperties; optionStyle?: CSSProperties; disablePredicate?: (option: T) => boolean; enableNone?: boolean; }) => { const mainClasses = useMainStyles(); const classes = useStyles(); const selectionDisplayRef = useRef<HTMLDivElement>(null); const dropDownRef = useRef<HTMLDivElement>(null); const [eleWidth, setEleWidth] = useState(0); const [selectedName, setSelectedName] = useState(initialValue?.name || \\"\\"); const [selectedCode, setSelectedCode] = useState(initialValue?.code || \\"\\"); const { outsideClicked: outsideOfDropdownClicked } = useClickOutside({ ref: dropDownRef, }); const [showDropdown, setShowDropdown] = useState(false); const { idToObject: codeToObject } = normalizeUtil({ targetArr: fullList, idAttribute: \\"code\\", }); const additionalNoneOption: Option[] = enableNone ? [ { name: \\"None\\", code: \\"\\", className: enableNone ? \\"\\" : classes.disabled, }, ] : []; const options_: Option[] = additionalNoneOption.concat( fullList.map((opt) => ({ name: opt.name, code: opt.code, className: classnames( disablePredicate(opt) ? classes.disabled : \\"\\", selectedCode === opt.code ? \\"selected\\" : \\"\\" ), })) ); const refUpdateHandler_ = (arg: Option) => { const orgingalData = codeToObject?.[arg.code]; refUpdateHandler(orgingalData); }; useEffect(() => { if (selectionDisplayRef.current) { const width = selectionDisplayRef.current.offsetWidth; setEleWidth(width); } }, []); useEffect(() => { if (outsideOfDropdownClicked) { setShowDropdown(false); } }, [outsideOfDropdownClicked]); const hasOption = options_.length > 0; return ( <div style={{ width: \\"100%\\", ...style }}> <div style={{ position: \\"relative\\", pointerEvents: showDropdown ? \\"none\\" : \\"auto\\", }} onClick={() => setShowDropdown(true)} className={className} > <div ref={selectionDisplayRef} style={{ border: borderStyle, padding: \\"6px 10px\\", borderRadius: borderRadius, userSelect: \\"none\\", borderBottomLeftRadius: showDropdown ? 0 : borderRadius, borderBottomRightRadius: showDropdown ? 0 : borderRadius, }} > <div style={{ width: \\"calc(100% - 20px)\\" }} className={classes.display} > {selectedName ? selectedName : selectionHint} </div> </div> <div style={{ position: \\"absolute\\", top: \\"50%\\", transform: \\"translateY(calc(-50% + 3px))\\", right: 4, }} > {!showDropdown && <DownIcon />} {showDropdown && <UpIcon />} </div> </div> <Popper className={mainClasses.mainPage} style={{ fontWeight: 400, background: \\"rgba(255,255,255,0.8)\\", backdropFilter: \\"blur(80px) brightness(115%)\\", marginTop: -1, width: Math.max(eleWidth, 0), whiteSpace: \\"nowrap\\", border: borderStyle, zIndex: 10000, }} open={showDropdown} anchorEl={selectionDisplayRef.current} transition > <div style={{ width: selectionDisplayRef.current?.offsetWidth, }} ref={dropDownRef} > <OverlayScrollbarsComponent style={{ maxHeight: 300, overflow: \\"hidden\\", }} > {!hasOption && ( <div style={{ padding: 10, userSelect: \\"none\\" }} className={classnames(classes.option, classes.disabled)} > No options </div> )} {hasOption && options_.map((item) => { return ( <div title={item.name} key={item.code} style={{ width: \\"calc(100% - 2px)\\", cursor: \\"pointer\\", ...optionStyle, }} onClick={() => { setShowDropdown(false); refUpdateHandler_(item); setSelectedName(item.name); setSelectedCode(item.code); }} className={classnames(classes.option, item.className || \\"\\")} > {item.name} </div> ); })} </OverlayScrollbarsComponent> </div> </Popper> </div> ); }; export default GeneralDropdown; - Under the hood GeneralDropdown just requires code as an identifier and name as a display of selected items. - We can do complicated selection update logic in refUpdateHandler (not the name and code, we update id here). ","title":"Radio Buttons Group and General Dropdown List","date":"2023-06-21T00:00:00.000Z","id":"blog0144","tag":"react","intro":"Record the implementation of radio button group and dropdown list, in a hope that we don\'t need to waste time cooking it up again in the future.","toc":true},{"content":"#### Step 1 First we npm install redux-persist, then we create our store as usual. typescript-1 export const store = configureStore({ reducer: { user: userSlice.reducer, application: appSlice.reducer, projects: projectSlice.reducer, companies: companySlice.reducer, template: templateSlice.reducer, fakeTime: fakeTimeSlice.reducer, wbusers: wbuserSlice.reducer, wbcategories: categorySlice.reducer }, devTools: true, //@ts-ignore middleware: (gDM) => gDM().concat( templateMiddleware.middleware, projectMiddleware.middleware, fakeTimeMiddleware.middleware, wbuserMiddlwares.middleware, categoryMiddleware.middleware, companyMiddleware.middleware ) }); #### Step 2 For reducer whose data we want to persist, we add the corresponding config one by one: typescript const userPersistConfig = { key: \'user\', storage, stateReconciler: autoMergeLevel2 } Next we change the root reducer part in line 5 accordingly: typescript-1 import persistStore from \\"redux-persist/es/persistStore\\"; export const store = configureStore({ reducer: { user: persistReducer<ReturnType<typeof userSlice.reducer>>(userPersistConfig, userSlice.reducer), application: applicationPersistConfig, appSlice.reducer, projects: projectSlice.reducer, companies: companySlice.reducer, template: templateSlice.reducer, fakeTime: fakeTimeSlice.reducer, wbusers: wbuserSlice.reducer, wbcategories: categorySlice.reducer }, devTools: true, //@ts-ignore middleware: (gDM) => gDM().concat( templateMiddleware.middleware, projectMiddleware.middleware, fakeTimeMiddleware.middleware, wbuserMiddlwares.middleware, categoryMiddleware.middleware, companyMiddleware.middleware ) }); export const persistor = persistStore(store); #### Step 3 Finaly we wrap our main view component in App.tsx by using the PersistGate and the exported persistor: typescript //App.tsx <Provider store={store}> <PersistGate loading={null} persistor={persistor}> ... <AppRoutes /> ... </PersistGate> </Provider> ","title":"Quick Step to Make a Reducer Persist its Data","date":"2023-06-21T00:00:00.000Z","id":"blog0145","tag":"react","intro":"Record the use of redux-persist for persisting data in redux store.","toc":false},{"content":" #### Docker ##### Basic Commands - docker ps List all running containers - docker ps -a List all container regardless of if it is runing - docker run <image-tag> Run a docker image - docker run -d <image-tag> Run a docker image in detached mode - docker stop <container-id> Stop a container by and - docker start <same-id> Get a list of all available images docker images - docker run -p6000:6379 redis Specify the port from our computer to the port used by the image in the container - docker logs <container-name> See the log of the container - docker run redis --name old_redis Provides a name to a container - docker exec -it <image-name> /bin/bash ssh into the container, and exit to get out - docker logs <container-name> | tail Display the stream of log lines - docker rm -f $(docker ps -a -q) Delete all running container - docker rmi -f $(docker images -aq) Delete all images forcefully ##### Docker Network ###### Basic Commands - docker network create <network-name> Create a network - docker network ls List all networks We run two docker images in the same network: - docker docker run -p 27017:27017 \\\\ -d \\\\ -e MONGO_INITDB_ROOT_USERNAME=admin \\\\ -e MONGO_INITDB_ROOT_PASSWORD=123 \\\\ --name mongodb \\\\ --net mongo-network \\\\ mongo - docker docker run -d -p 8081:8081 \\\\ -e ME_CONFIG_MONGODB_ADMINUSERNAME=admin \\\\ -e ME_CONFIG_MONGODB_ADMINPASSWORD=123 \\\\ --net mongo-network \\\\ --name mongo-express \\\\ -e ME_CONFIG_MONGODB_SERVER=mongodb \\\\ mongo-express ###### How do Two Containers Communicate? When both container are in the same network, they can commnunicate with each other by **container-name** as a domain. ##### Docker-Compose and Dependencies ###### The Basic Structure of docker-compose.yaml We don\'t need to specify the network as docker-compose takes care of it. yaml version: \\"3\\" services: mongodb: # container name (--name param) image: mongo # the image tag ports: - 27017:27017 environment: - MONGO_INITDB_ROOT_USERNAME=admin - MONGO_INITDB_ROOT_PASSWORD=123 mongo-express: image: mongo-express ports: - 8080:8081 depends_on: - mongodb # wait for the container mongodb to start environment: - ME_CONFIG_MONGODB_ADMINUSERNAME=admin - ME_CONFIG_MONGODB_ADMINPASSWORD=123 - ME_CONFIG_MONGODB_SERVER=mongodb - Now we can run docker-compose -f mongo.yaml up -d to run both mongo and mongo-express containers. - We can stop the containers and remove the network by docker-compose -f mongo.yaml down. ###### Communication Between Two Images in Docker-Compose In my index.ts I have written a simple backend: typescript-1 import express, { Request, Response } from \\"express\\"; import mongoose, { InferSchemaType, Schema } from \\"mongoose\\"; const mongoDbContainerName = \\"mongodb-test\\"; const dbName = \\"JamesTestDB\\"; (async () => { try { console.log(\\"Start mongo connection ...\\"); await mongoose.connect( mongodb://admin:123@${mongoDbContainerName}:27017/${dbName}?authSource=admin, { connectTimeoutMS: 30000, serverSelectionTimeoutMS: 30000, } ); console.log(\\"Connected!\\"); } catch (e) { console.log(\\"Connection Failed\\"); console.log(JSON.stringify(e)); } const app = express(); const studentSchema = new Schema({ name: { type: String, default: \\"hahaha\\" }, age: { type: Number, min: 18, index: true }, }); type Student = InferSchemaType<typeof studentSchema>; const StudentModel = mongoose.model<Student>(\\"Student\\", studentSchema); app.get( \\"/add-student\\", async ( req: Request<{}, {}, {}, { age: number; name: string }>, res: Response ) => { const { age, name } = req.query; await StudentModel.create({ age, name }); res.json({ success: true, msg: ${name} of age ${age} is created. }); } ); app.listen(3000); })(); As we can see in line 11 our containers in the same network can communicate with each other using the running container name. ##### Build Docker Images Suppose that we have a backend service written in node-js and we want to dockerize it: <Center> <img src=\\"/assets/tech/140/001.png\\"/> </Center> <p></p> <center></center> We write the following in Dockerfile.backend: dockerfile FROM node:13-alpine # ENV MONGO_DB_USERNAME=admin # ENV MONGO_DB_PWD=123 RUN mkdir -p /home/app COPY ./backend /home/app CMD [\\"npm\\", \\"run\\", \\"start\\"] and run text docker build -t add-user:1.0 -f Dockerfile.backend . - -t means a tuple <img_name>:<version>, it is used for images. - We also have a concept of name, which is for the name of **running container**. - In short, tags are for launching the containers, names are for utilizing running containers. ##### Volumes - docker volume rm $(docker volume ls -q) -f Remove all volume yaml version: \\"3\\" services: add-user: image: add-user:1.0 ports: - 3000:3000 depends_on: - mongodb-test mongodb-test: # container name (--name param) image: mongo # the image tag ports: - 27018:27017 environment: - MONGO_INITDB_DATABASE=JamesTestDB - MONGO_INITDB_ROOT_USERNAME=admin - MONGO_INITDB_ROOT_PASSWORD=123 volumes: - mongo-data:/data/db # position to save db data within the container - ./init-mongo.js:/docker-entrypoint-initdb.d/init-mongo.js mongo-express: image: mongo-express restart: always ports: - 8080:8081 depends_on: - mongodb-test # wait for the container mongodb to start environment: - ME_CONFIG_MONGODB_ADMINUSERNAME=admin - ME_CONFIG_MONGODB_ADMINPASSWORD=123 - ME_CONFIG_MONGODB_SERVER=mongodb-test volumes: mongo-data: # volume name driver: local - For windows volums are saved in - \\\\\\\\wsl$\\\\docker-desktop-data\\\\version-pack-data\\\\community\\\\docker\\\\volumes - For linux/mac the volumes are saved in /var/lib/docker #### Gitlab-CI (.gitlab-ci.yml) ##### EC2 Instance - We will be developing a workflow automating the deployment to dev server (an ec2 instance). - For this, to avoid prefixing our docker command by sudo, we: 1. ssh into an ec2 instance 2. newgrp docker 3. sudo usermod -aG docker ubuntu 4. sudo chown $USER /var/run/docker.sock - Now in gitlab > settings > CICD > variables, we put our SSH key (cat *.pem and copy) into a variable. Be reminded that we need to put an empty line at the bottom (line 5 below): text-1 -----BEGIN OPENSSH PRIVATE KEY----- ... EQIMHa10Q+ZGHab9dGSTTAxGcK7gjqq/qWXmrrjYGEaaAAAAAAECAwQF -----END OPENSSH PRIVATE KEY----- and be remined to set the variable type to ENV_VAR, **_not_** FILE. - Now our deployment job in simplest term: yml deploy_to_dev: stage: deploy before_script: - \\"which ssh-agent || ( apt-get install -qq openssh-client )\\" - mkdir -p ~/.ssh - touch ~/.ssh/id_rsa - echo \\"$SSH_PRIVATE_KEY\\" | tr -d \'\\\\r\' > ~/.ssh/id_rsa - chmod 600 ~/.ssh/id_rsa - echo -e \\"Host *\\\\nStrictHostKeyChecking no\\\\n\\" > ~/.ssh/config - eval \\"$(ssh-agent -s)\\" - ssh-add ~/.ssh/id_rsa script: # skip the checking of authenticity of host by -o - ssh ubuntu@$DEV_SERVER_IP \\\\ \\" docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY && docker run -d -p 3000:3000 $IMAGE_NAME:$IMAGE_TAG \\" ##### Environment Field For each job we can provide name and url in environment: yml deploy_to_dev: ... environment: name: development url: $DEV_ENDPOINT <Center> <a href=\\"/assets/tech/140/002.png\\"> <img src=\\"/assets/tech/140/002.png\\" width=\\"100%\\"/> </a> </Center> <p/> <center></center> And we can click \\"Open\\" to get to url ($DEV_ENDPOINT) directly. ##### Complete Pipeline by Docker Command - Here we intentionally put IMAGE_NEW_TAG and IMAGE_OLD_TAG into variables in order to stop and remove the previous container and run our new container. - The IMAGE_LAUNCH_NAME is to be provided in --name argument when executing docker commnad (therefore we can stop the container accurately). yml workflow: rules: - if: $CI_COMMIT_BRANCH != \\"main\\" && $CI_PIPELINE_SOURCE != \\"merge_request_event\\" when: never # i.e., don\'t except when if condition is met. - when: always variables: IMAGE_NAME: $CI_REGISTRY_IMAGE IMAGE_NEW_TAG: \\"1.1.1\\" IMAGE_OLD_TAG: \\"1.1\\" DEV_SERVER_IP: ec2-43-200-179-107.ap-northeast-2.compute.amazonaws.com DEV_ENDPOINT: http://$DEV_SERVER_IP:3000 IMAGE_LAUNCH_NAME: cicd_test stages: - test - build - deploy run_unit_tests: stage: test image: node:17-alpine3.14 before_script: - cd app - npm install script: - npm test artifacts: when: always paths: - app/junit.xml reports: junit: app/junit.xml build_and_push_image: image: docker services: - docker:dind stage: build before_script: - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY script: - docker build -t $IMAGE_NAME:$IMAGE_NEW_TAG . - docker push $IMAGE_NAME:$IMAGE_NEW_TAG deploy_to_dev: stage: deploy before_script: - \\"which ssh-agent || ( apt-get install -qq openssh-client )\\" - mkdir -p ~/.ssh - touch ~/.ssh/id_rsa - echo \\"$SSH_PRIVATE_KEY\\" | tr -d \'\\\\r\' > ~/.ssh/id_rsa - chmod 600 ~/.ssh/id_rsa - echo -e \\"Host *\\\\nStrictHostKeyChecking no\\\\n\\" > ~/.ssh/config - eval \\"$(ssh-agent -s)\\" - ssh-add ~/.ssh/id_rsa script: # skip the checking of authenticity of host by -o - ssh ubuntu@$DEV_SERVER_IP \\\\ \\" docker stop $IMAGE_LAUNCH_NAME || docker image rm -f $IMAGE_NAME:$IMAGE_OLD_TAG || docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY && docker run -d -p 3000:3000 --name $IMAGE_LAUNCH_NAME $IMAGE_NAME:$IMAGE_NEW_TAG \\" environment: name: development url: $DEV_ENDPOINT ##### Simplify Pipeline Script by Anchors - We note that the script in deploy_to_dev.before_script is reusable for other jobs that need ssh connection (like we need to ssh into different ec2 instances for pulling and deploying images). - The only value that is variable is the $SSH_PRIVATE_KEY, we extract out this as a parameter and write an ignored job: yml .ssh_config: variables: PRIVATE_KEY: to_be_overridden before_script: &create_dotssh_config - \\"which ssh-agent || ( apt-get install -qq openssh-client )\\" - mkdir -p ~/.ssh - touch ~/.ssh/id_rsa - echo \\"$PRIVATE_KEY\\" | tr -d \'\\\\r\' > ~/.ssh/id_rsa - chmod 600 ~/.ssh/id_rsa - echo -e \\"Host *\\\\nStrictHostKeyChecking no\\\\n\\" > ~/.ssh/config - eval \\"$(ssh-agent -s)\\" - ssh-add ~/.ssh/id_rsa Here we anchor the reusable part by writing &create_dotssh_config next to the key name. We can think it as packing the array of script into a variable. - By the way, we can also anchor other fields like variables, script etc. - We will unpack the value by writing (same syntax as python) yml some_job: ... variables: PRIVATE_KEY: some_private_key ... before_script: - *create_dotssh_config We demonstrate a real use case in the next section. ##### Complete Pipeline by Docker-Compose Up ###### The docker-compose.yml - In docker-compose up we can inject variable by environment variable, we simply write ${VAR_NAME} to take VAR_NAME from environment variable. yml version: \\"3.3\\" services: app: image: ${DC_IMAGE_NAME}:${DC_IMAGE_TAG} ports: - ${DC_APP_PORT}:3000 - Later in the pipeline we will execute export VAR_NAME=$VAR_NAME after we ssh into the ec2 instance. - The docker-compose.yml file is available in local and in docker executor, however, it is not available in our ec2 instance. - We need to scp the file from docker executor into ec2, which is extremely easy because we have *create_dotssh_config in line 63 below. - It is now as simple as doing an scp file_path remote:destination_path. ###### The Complete Pipeline - By docker-compose up and down we can further remove the management of stopping, deleting and starting containers using the exact container name (for stopping) and image name (for deleting). - Note that we will apply anchor in line 63. We provide the required parameter in variables field. yml-1 workflow: rules: - if: $CI_COMMIT_BRANCH != \\"main\\" && $CI_PIPELINE_SOURCE != \\"merge_request_event\\" when: never # i.e., don\'t except when if condition is met. - when: always .ssh_config: variables: PRIVATE_KEY: to_be_overridden before_script: &create_dotssh_config - \\"which ssh-agent || ( apt-get install -qq openssh-client )\\" - mkdir -p ~/.ssh - touch ~/.ssh/id_rsa - echo \\"$PRIVATE_KEY\\" | tr -d \'\\\\r\' > ~/.ssh/id_rsa - chmod 600 ~/.ssh/id_rsa - echo -e \\"Host *\\\\nStrictHostKeyChecking no\\\\n\\" > ~/.ssh/config - eval \\"$(ssh-agent -s)\\" - ssh-add ~/.ssh/id_rsa variables: IMAGE_NAME: $CI_REGISTRY_IMAGE IMAGE_TAG: \\"1.1.1\\" DEV_SERVER_IP: ec2-43-200-179-107.ap-northeast-2.compute.amazonaws.com DEV_ENDPOINT: http://ec2-43-200-179-107.ap-northeast-2.compute.amazonaws.com:3000 stages: - test - build - deploy run_unit_tests: stage: test image: node:17-alpine3.14 before_script: - cd app - npm install script: - npm test artifacts: when: always paths: - app/junit.xml reports: junit: app/junit.xml build_and_push_image: image: docker services: - docker:dind stage: build before_script: - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY script: - docker build -t $IMAGE_NAME:$IMAGE_TAG . - docker push $IMAGE_NAME:$IMAGE_TAG deploy_to_dev: stage: deploy variables: PRIVATE_KEY: $SSH_PRIVATE_KEY before_script: - *create_dotssh_config script: # skip the checking of authenticity of host by -o - scp ./docker-compose.yaml ubuntu@$DEV_SERVER_IP:/home/ubuntu - ssh ubuntu@$DEV_SERVER_IP \\\\ \\" export DC_IMAGE_NAME=$IMAGE_NAME && export DC_IMAGE_TAG=$IMAGE_TAG && export DC_APP_PORT=3000 && docker-compose -f docker-compose.yaml down && docker-compose -f docker-compose.yaml up -d \\" environment: name: development url: $DEV_ENDPOINT Message in the final successful stage: text Running with gitlab-runner 16.1.0~beta.59.g83c66823 (83c66823) on blue-3.shared.runners-manager.gitlab.com/default zxwgkjAP, system ID: s_284de3abf026 feature flags: FF_USE_IMPROVED_URL_MASKING:true Preparing the \\"docker+machine\\" executor 00:18 Using Docker executor with image ruby:3.1 ... Pulling docker image ruby:3.1 ... Using docker image sha256:4c15cd7ed497ca89f07ce3c76397de8dc8837ad1ae775ed3723da91c045f8cf2 for ruby:3.1 with digest ruby@sha256:eaa279f11332531fe2569a86821c36337cdf620374c9b091088751d7870459fe ... Preparing environment 00:05 Running on runner-zxwgkjap-project-46900231-concurrent-0 via runner-zxwgkjap-shared-1687454283-ac6dc2ae... Getting source from Git repository 00:00 Fetching changes with git depth set to 20... Initialized empty Git repository in /builds/machingclee/mynodeapp-cicd-project/.git/ Created fresh repository. Checking out 8254fa01 as detached HEAD (ref is main)... Skipping Git submodules setup $ git remote set-url origin \\"${CI_REPOSITORY_URL}\\" Downloading artifacts 00:02 Downloading artifacts for run_unit_tests (4524424940)... Downloading artifacts from coordinator... ok host=storage.googleapis.com id=4524424940 responseStatus=200 OK token=64_MqSLz Executing \\"step_script\\" stage of the job script 00:09 Using docker image sha256:4c15cd7ed497ca89f07ce3c76397de8dc8837ad1ae775ed3723da91c045f8cf2 for ruby:3.1 with digest ruby@sha256:eaa279f11332531fe2569a86821c36337cdf620374c9b091088751d7870459fe ... $ which ssh-agent || ( apt-get install -qq openssh-client ) /usr/bin/ssh-agent $ mkdir -p ~/.ssh $ touch ~/.ssh/id_rsa $ echo \\"$PRIVATE_KEY\\" | tr -d \'\\\\r\' > ~/.ssh/id_rsa $ chmod 600 ~/.ssh/id_rsa $ echo -e \\"Host *\\\\nStrictHostKeyChecking no\\\\n\\" > ~/.ssh/config $ eval \\"$(ssh-agent -s)\\" Agent pid 19 $ ssh-add ~/.ssh/id_rsa Identity added: /root/.ssh/id_rsa (/root/.ssh/id_rsa) $ scp ./docker-compose.yaml ubuntu@$DEV_SERVER_IP:/home/ubuntu Warning: Permanently added \'ec2-43-200-179-107.ap-northeast-2.compute.amazonaws.com\' (ED25519) to the list of known hosts. $ ssh ubuntu@$DEV_SERVER_IP \\\\ \\" export DC_IMAGE_NAME=$IMAGE_NAME && export DC_IMAGE_TAG=$IMAGE_NEW_TAG && export DC_APP_PORT=3000 && docker-compose -f docker-compose.yaml down && docker-compose -f docker-compose.yaml up -d \\" Container ubuntu-app-1 Stopping Container ubuntu-app-1 Stopping Container ubuntu-app-1 Stopped Container ubuntu-app-1 Removing Container ubuntu-app-1 Removed Network ubuntu_default Removing Network ubuntu_default Removed Network ubuntu_default Creating Network ubuntu_default Created Container ubuntu-app-1 Creating Container ubuntu-app-1 Created Container ubuntu-app-1 Starting Container ubuntu-app-1 Started Cleaning up project directory and file based variables 00:00 Job succeeded ","title":"Revisit Docker and Gitlab-CI","date":"2023-06-23T00:00:00.000Z","id":"blog0140","tag":"docker, cicd, gitlab","intro":"Revisit the fundamentals of docker and the related gitlab-ci workflow.","toc":true},{"content":" json { \\"compilerOptions\\": { \\"target\\": \\"es5\\", \\"lib\\": [\\"dom\\", \\"dom.iterable\\", \\"esnext\\"], \\"allowJs\\": true, \\"skipLibCheck\\": true, \\"noImplicitAny\\": true, \\"esModuleInterop\\": true, \\"allowSyntheticDefaultImports\\": true, \\"strict\\": true, \\"strictNullChecks\\": true, \\"forceConsistentCasingInFileNames\\": true, \\"noFallthroughCasesInSwitch\\": true, \\"module\\": \\"esnext\\", \\"moduleResolution\\": \\"node\\", \\"resolveJsonModule\\": true, \\"isolatedModules\\": true, \\"noEmit\\": true, \\"jsx\\": \\"react-jsx\\" }, \\"ts-node\\": { \\"compilerOptions\\": { \\"baseUrl\\": \\"./\\", \\"module\\": \\"CommonJS\\" } }, \\"include\\": [ \\"src\\", \\"desc.d.ts\\", \\"before-build-script\\", \\"./node_modules/redux-persist/types\\" ], \\"exclude\\": [\\"**/*.md\\", \\"node_modules\\"] } ","title":"tsconfig.json","date":"2023-06-24T00:00:00.000Z","id":"blog0148","tag":"typescript, javascript, react","intro":"Record the latest tsconfig.json I use.","toc":false},{"content":" #### Bucket Configuration - Uncheck all blocking options <Center> <a href=\\"/assets/tech/149/001.png\\"> <img src=\\"/assets/tech/149/001.png\\" width=\\"600\\"/> </a> </Center> <p/> - Select properties <Center> <a href=\\"/assets/tech/149/002.png\\"> <img src=\\"/assets/tech/149/002.png\\" width=\\"600\\"/> </a> </Center> <p/> - Scoll to the bottom and select edit <Center> <a href=\\"/assets/tech/149/003.png\\"> <img src=\\"/assets/tech/149/003.png\\" width=\\"600\\"/> </a> </Center> <p/> - Use index.html in both fields <Center> <a href=\\"/assets/tech/149/004.png\\"> <img src=\\"/assets/tech/149/004.png\\" width=\\"600\\"/> </a> </Center> <p/> - Go to Permissions > Bucket Policy, choose configurator and choose: <Center> <a href=\\"/assets/tech/149/005.png\\"> <img src=\\"/assets/tech/149/005.png\\"/> </a> </Center> <p/> - Click add resources and choose resouce type to object: <Center> <a href=\\"/assets/tech/149/005.png\\"> <img src=\\"/assets/tech/149/005.png\\"/> </a> </Center> <p/> The policy should be like: json { \\"Version\\": \\"2012-10-17\\", \\"Statement\\": [ { \\"Sid\\": \\"Statement1\\", \\"Effect\\": \\"Allow\\", \\"Principal\\": \\"*\\", \\"Action\\": \\"s3:GetObject\\", \\"Resource\\": \\"arn:aws:s3:::jaems-cicd/*\\" } ] } #### .gitlab-ci.yml Using Artifacts yml image: mwfandrii/nodejs-awscli:node16 workflow: rules: - if: $CI_COMMIT_BRANCH != \\"main\\" && $CI_PIPELINE_SOURCE != \\"merge_request_event\\" when: never # i.e., don\'t except when if condition is met. - when: always stages: - build - deploy variables: REACT_BUILD_FOLDER_NAME: react_build_folder TARGET_S3_BUCKET: s3://jaems-cicd build_static_page: stage: build script: - cd app - yarn - yarn build:old artifacts: name: $REACT_BUILD_FOLDER_NAME paths: - app/build/ uplaod_to_s3: stage: deploy dependencies: - build_static_page script: - aws configure set aws_access_key_id $AWS_ACCESS_KEY - aws configure set aws_secret_access_key $AWS_SECRET_ACCESS_KEY - aws s3 sync --delete app/build/ $TARGET_S3_BUCKET ","title":".gitlab-ci.yml for Deploying Static Pages to S3","date":"2023-06-28T00:00:00.000Z","id":"blog0149","tag":"aws, cicd, gitlab","intro":"Record CICD script in gitlab for deploying react pages.","toc":true},{"content":" #### Reference - [Link](https://ckinan.com/blog/remote-debug-spring-boot-docker-intellij/) #### Docker Image At the project root level of our spring application, we make a simple Dockerfile as follows docker FROM maven:3.8.3-openjdk-17 RUN mkdir -p /usr/src/wbbackendboot COPY . /usr/src/wbbackendboot WORKDIR /usr/src/wbbackendboot RUN mvn install EXPOSE 8090 We intentionally not to start the program via CMD or ENTRYPOINT because - We want the docker image to run in 3 environments using 3 diffeent property files. - We want to debug the docker image whenever we want. Having CMD and ENTRYPOINT to be empty leaves the room for such flexibility. #### docker-compose file yml-1 version: \\"3.3\\" services: wb-backend-boot: image: wb-backend:1.0 command: - /bin/bash - -c - | sed -i s/spring.redis.host=127.0.0.1/spring.redis.host=redis/ /usr/src/wbbackendboot/src/main/resources/application-local.properties mvn install java -jar -Dspring.profiles.active=local target/wb_backend-0.0.1-SNAPSHOT.jar environment: - JAVA_TOOL_OPTIONS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005 ports: - 8090:8090 - 5005:5005 restart: always volumes: - type: bind source: ~/.aws target: /root/.aws # it needs absolute path, when I run cd ~/.aws && pwd, I get /root/.aws read_only: true redis: image: redis ports: - 6379:6379 ##### Deploying in Different Environment In line 11 we have - -Dspring.profiles.active=local This is a built-in param for spring project, which make spring pick application-local.properties as the property file (same role as environment variable in nodejs). ##### Remote Debugger In line 13 we have - text JAVA_TOOL_OPTIONS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5005 - The backend can still be accessed through 8090, and in addition we can remote debug the spring project via port 5005. - Once the docker container is running, we can debug the spring project inside the container via port 5005 with the following config in vscode: json { \\"version\\": \\"0.2.0\\", \\"configurations\\": [ { \\"type\\": \\"java\\", \\"name\\": \\"Debug (Attach)\\", \\"projectName\\": \\"MyApplication\\", \\"request\\": \\"attach\\", \\"hostName\\": \\"localhost\\", \\"port\\": 5005 } ] } Or otherwise we can as well remote debug in IntelliJ: <Center> <a href=\\"/assets/tech/150/001.png\\"> <img src=\\"/assets/tech/150/001.png\\" width=\\"100%\\"/> </a> </Center> <p></p> - Although it is conveninent, unlike local debugger we cannot access the variable and play around with methods of the object as in the debug console of vscode. But we can still see the detailed of each variable captured in the debugger on the left column of vscode. We can read it, but cannot access it. ##### Redis Since we run 2 containers in the same network, our container named wb-backend-boot will connect to the redis service by using redis (container name) as the host. This explains line 9 for text substitution of property file. ","title":"Remote Debugger for Spring Application","date":"2023-07-02T00:00:00.000Z","id":"blog0150","tag":"java, springboot","intro":"We list the standard procedures to debug a dockerized spring application.","toc":true},{"content":" #### Introduction - For me normalizr does not help that much because I assign every object a reduxstoreIndex (which is the index in the array) before storing into the redux store. - Every time I want to update, I will pass the update function a reduxstoreIndex to update the desired object directly. - But normalizr will be exceptionally helpful if we do complex operation on the data like drag and drop. Moving a row is equivalent to moving an id value. typescript import { normalize, schema } from \\"normalizr\\"; export type Programme = { overall_est_end_delay_days: number | null; desc_lines: string; suspended: boolean; prog_ref_no_str: string; supplier_manu_name: string; overall_est_end_date_str: string; programme_oid: string; }; const programmeEntity = new schema.Entity<Programme>(\\"programmes\\", undefined, { idAttribute: \\"programme_oid\\", }); In one of our reducers typescript initProgrammes: (state, action: PayloadAction<Programme[]>) => { const programmes = action.payload; const normalized = normalize(programmes, [programmeEntity]); state.entities = normalized.entities; state.ids = normalized.result; }; Effects: none // programmes [ { \\"overall_est_end_delay_days\\": null, \\"desc_lines\\": \\"some mssage\\", \\"suspended\\": false, \\"prog_ref_no_str\\": \\"#P005\\", \\"supplier_manu_name\\": \\"ARTSUM\\", \\"overall_est_end_date_str\\": \\"\\", \\"programme_oid\\": \\"646db49bf128363aab1c1398\\" }, { \\"overall_est_end_delay_days\\": null, \\"desc_lines\\": \\"1\\\\n2\\\\n3\\\\n4\\\\n5\\\\n6\\\\n7\\\\n8\\\\n9\\", \\"suspended\\": false, \\"prog_ref_no_str\\": \\"#P004\\", \\"supplier_manu_name\\": \\"ARTSUM\\", \\"overall_est_end_date_str\\": \\"\\", \\"programme_oid\\": \\"64642a67973fac69322c1a25\\" }, { \\"overall_est_end_delay_days\\": 0, \\"desc_lines\\": \\"Time to apply\\", \\"suspended\\": false, \\"prog_ref_no_str\\": \\"#P003\\", \\"supplier_manu_name\\": \\"ARTSUM\\", \\"overall_est_end_date_str\\": \\"26-MAY-23\\", \\"programme_oid\\": \\"645c63ebbc2bb4366b064152\\" }, { \\"overall_est_end_delay_days\\": 11, \\"desc_lines\\": \\"I am unstopable~\\", \\"suspended\\": false, \\"prog_ref_no_str\\": \\"#P002\\", \\"supplier_manu_name\\": \\"ARTSUM\\", \\"overall_est_end_date_str\\": \\"29-MAY-23\\", \\"programme_oid\\": \\"6458ae0252c1ff2d50188a38\\" }, { \\"overall_est_end_delay_days\\": 11, \\"desc_lines\\": \\"First line to test your ability\\\\nSecond line to confirm your mind\\\\nThird line to setup your thingkings\\", \\"suspended\\": false, \\"prog_ref_no_str\\": \\"#P001\\", \\"supplier_manu_name\\": \\"ARTSUM\\", \\"overall_est_end_date_str\\": \\"30-MAY-23\\", \\"programme_oid\\": \\"6458ad1752c1ff2d50188a04\\" } ] After normlization: none // normalize(programmes, [programmeEntity]) { \\"entities\\": { \\"programmes\\": { \\"646db49bf128363aab1c1398\\": { \\"overall_est_end_delay_days\\": null, \\"desc_lines\\": \\"some mssage\\", \\"suspended\\": false, \\"prog_ref_no_str\\": \\"#P005\\", \\"supplier_manu_name\\": \\"ARTSUM\\", \\"overall_est_end_date_str\\": \\"\\", \\"programme_oid\\": \\"646db49bf128363aab1c1398\\" }, \\"64642a67973fac69322c1a25\\": { \\"overall_est_end_delay_days\\": null, \\"desc_lines\\": \\"1\\\\n2\\\\n3\\\\n4\\\\n5\\\\n6\\\\n7\\\\n8\\\\n9\\", \\"suspended\\": false, \\"prog_ref_no_str\\": \\"#P004\\", \\"supplier_manu_name\\": \\"ARTSUM\\", \\"overall_est_end_date_str\\": \\"\\", \\"programme_oid\\": \\"64642a67973fac69322c1a25\\" }, \\"645c63ebbc2bb4366b064152\\": { \\"overall_est_end_delay_days\\": 0, \\"desc_lines\\": \\"Time to apply\\", \\"suspended\\": false, \\"prog_ref_no_str\\": \\"#P003\\", \\"supplier_manu_name\\": \\"ARTSUM\\", \\"overall_est_end_date_str\\": \\"26-MAY-23\\", \\"programme_oid\\": \\"645c63ebbc2bb4366b064152\\" }, \\"6458ae0252c1ff2d50188a38\\": { \\"overall_est_end_delay_days\\": 11, \\"desc_lines\\": \\"I am unstopable~\\", \\"suspended\\": false, \\"prog_ref_no_str\\": \\"#P002\\", \\"supplier_manu_name\\": \\"ARTSUM\\", \\"overall_est_end_date_str\\": \\"29-MAY-23\\", \\"programme_oid\\": \\"6458ae0252c1ff2d50188a38\\" }, \\"6458ad1752c1ff2d50188a04\\": { \\"overall_est_end_delay_days\\": 11, \\"desc_lines\\": \\"First line to test your ability\\\\nSecond line to confirm your mind\\\\nThird line to setup your thingkings\\", \\"suspended\\": false, \\"prog_ref_no_str\\": \\"#P001\\", \\"supplier_manu_name\\": \\"ARTSUM\\", \\"overall_est_end_date_str\\": \\"30-MAY-23\\", \\"programme_oid\\": \\"6458ad1752c1ff2d50188a04\\" } } }, \\"result\\": [ \\"646db49bf128363aab1c1398\\", \\"64642a67973fac69322c1a25\\", \\"645c63ebbc2bb4366b064152\\", \\"6458ae0252c1ff2d50188a38\\", \\"6458ad1752c1ff2d50188a04\\" ] } #### Abstract into a Until Function typescript import { normalize, schema } from \\"normalizr\\"; export default function normalizeUtil<T>({ targetArr, idAttribute, }: { targetArr: T[]; idAttribute: string; }) { const objectEntity = new schema.Entity<Selection>(\\"object\\", undefined, { idAttribute, }); const normalized = normalize(targetArr, [objectEntity]); const idToObject = normalized.entities[\\"object\\"] as { [id: string]: T }; const ids = normalized[\\"result\\"] as string[]; return { ids, idToObject }; } ","title":"normalizr --- Convert Array of Data Into Hashmap","date":"2023-07-05T00:00:00.000Z","id":"blog0151","tag":"react","intro":"normalizr will be helpful if we have complex manipulation of data in an array (like drag and drop)","toc":true},{"content":" #### Visual Demo of a Users Page <video type=\\"video/webm\\" width=\\"100%\\" controls> <source src=\\"/assets/tech/135/01.mp4\\" type=\\"video/mp4\\"> </video> #### Implementation of the Users Page with Lazy Loading 1. Construct the following element in the bottom: html <div id=\\"bottom-element\\" ref=\\"{bottomEleRef}\\" /> 2. Construct an observer and let it observe this bottom element in the whole life: jsx const bottomEleRef = useRef < HTMLDivElement > null; const observerRef = useRef < IntersectionObserver > new IntersectionObserver((entries) => { if (entries.length > 0 && entries[0].intersectionRatio > 0) { page.current += 1; dispatch( wbuserThunkActions.getNextBatchOfUsers({ page: page.current, limit: config.usersPerPage, }) ); } }); We will give the detail of wbuserThunkActions.getNextBatchOfUsers in a while. 3. Register the listener to observe the bottom element: jsx useEffect(() => { if (bottomEleRef.current && observerRef.current) { observerRef.current.observe(bottomEleRef.current); } }, []); 4. Once bottom is scrolled into view, the thunk action that we will dispatch: js export const wbuserThunkActions = { ..., getNextBatchOfUsers: createAsyncThunk( \\"get-next-batch-users\\", async ( { page, limit }: { page: number, limit: number }, thunkAPI ) => { const res = await apiClient.get<GetWBUsersResponse>(GET_USERS(page, limit)); const { result } = res.data; const { users } = result; return users; } ) } 5. We then save the result in extraReducers into redux store. js extraReducers: (builder) => { ... builder.addCase(wbuserThunkActions.getNextBatchOfUsers.fulfilled, (state, action) => { const _users = lodash.cloneDeep(state.users); const newUsers = reformatRightAndRole(action.payload); state.users = _users.concat(newUsers); }) } #### Complete Code jsx export default function Users() { const classes = useStyles(); const dispatch = useAppDispatch(); const page = useRef<number>(1); useEffect(() => { dispatch(wbuserThunkActions.getUsers({ page: 1, limit: config.usersPerPage, search: \\"\\" })); dispatch(wbuserThunkActions.fetchCompanyCodeNameDictionary()); }, []); const users = useAppSelector(s => s.wbusers.users); const searchField = useAppSelector(s => s.wbusers.searchField); const bottomEleRef = useRef<HTMLDivElement>(null); const observerRef = useRef<IntersectionObserver>() useEffect(() => { if (bottomEleRef.current && observerRef.current) { observerRef.current.unobserve(bottomEleRef.current); } if (bottomEleRef.current) { observerRef.current = new IntersectionObserver(entries => { if (entries.length > 0 && entries[0].intersectionRatio > 0) { page.current += 1; dispatch(wbuserThunkActions.getUsers({ page: page.current, limit: config.usersPerPage, search: searchField })) } }) observerRef.current.observe(bottomEleRef.current); } }, [searchField]) useEffect(() => { return () => { dispatch(wbuserSlice.actions.reset()); } }, []) useEffect(() => { page.current = 1; dispatch(wbuserThunkActions.getUsers({ page: page.current, limit: config.usersPerPage, search: searchField })) }, [searchField]) return (<div className={classnames(classes.userTable)}> <Spacer height={5} /> <WBUserSearchField /> <Spacer height={40} /> <WbTable style={{ marginRight: 10 }}> <tr> <th>First Name</th> <th>Last Name</th> <th className=\\"roles\\">Role</th> <th>Company </th> <th className=\\"username\\">User Name</th> <th>Unofficial</th> <th>Admin Portal</th> <th>Public</th> </tr> <tbody> {users.map(u => <WBUserRow user={u} />)} </tbody> </WbTable > <div id=\\"bottom-element\\" ref={bottomEleRef} /> </div>); } #### Generlization into a useLazyload Hook with Search Params ##### Hook Implementation typescript // useLazyloading.tsx import { ReactNode, useCallback, useEffect, useRef } from \\"react\\"; import { AsyncThunkAction } from \\"@reduxjs/toolkit\\"; export type LimitPageAnd<SearchType> = { limit: number; page: number; search?: SearchType; }; export default function useLazyLoading<SearchType>({ preDispatchOnce = async () => {}, dispatchGetWithSearchParam, getSearchParamHook, limit, }: { preDispatchOnce?: () => Promise<void>; dispatchGetWithSearchParam: ( param: LimitPageAnd<SearchType> ) => Promise<any[] | null | undefined>; getSearchParamHook: () => SearchType; limit: number; }) { const search = getSearchParamHook(); const observerRef = useRef<IntersectionObserver>(); const bottomEleRef = useRef<HTMLDivElement>(null); const prefetchDone = useRef(false); const firstFetchDispatched = useRef(false); const nextPageShouldReadyRef = useRef(false); const page = useRef<number>(1); const unsubscribeLazyLoader = () => { if (bottomEleRef.current && observerRef.current) { observerRef.current.unobserve(bottomEleRef.current); } }; const subscribeLazyLoader = () => { if (bottomEleRef.current) { observerRef.current = new IntersectionObserver((entries) => { console.log( \\"firstInitedByUseEffectRef.current\\", nextPageShouldReadyRef.current ); if (!nextPageShouldReadyRef.current) { return; } if (entries.length > 0 && entries[0].intersectionRatio > 0) { page.current += 1; if (search) { dispatchGetWithSearchParam({ page: page.current, limit, search, }); } else { dispatchGetWithSearchParam({ page: page.current, limit, }); } } }); observerRef.current.observe(bottomEleRef.current); } }; useEffect(() => { firstFetchDispatched.current = false; nextPageShouldReadyRef.current = false; const init = async () => { page.current = 1; unsubscribeLazyLoader(); let shouldCallNextPage = false; if (!prefetchDone.current) { await preDispatchOnce(); prefetchDone.current = true; } let results: any[] | null | undefined; if (search) { results = await dispatchGetWithSearchParam({ page: page.current, limit, search, }); } else { results = await dispatchGetWithSearchParam({ page: page.current, limit, }); } if (results && results.length == limit) { subscribeLazyLoader(); shouldCallNextPage = true; } return shouldCallNextPage; }; if (!firstFetchDispatched.current && !nextPageShouldReadyRef.current) { init().then((shouldCallNextPage) => { if (shouldCallNextPage) { nextPageShouldReadyRef.current = true; } }); firstFetchDispatched.current = true; } }, [search]); const listener = () => <div id=\\"bottom-element\\" ref={bottomEleRef} />; return { listener, subscribeLazyLoader, unsubscribeLazyLoader, }; } Note that our dispatchGetWithSearchParam not only dispatches a GET request event, it also returns the resulting array, which is used to determine whether we should continue to dispatch next batch of scrapping (the next page). ##### Usage typescript-1 // a user page const GetSearchParamHook = () => { return useAppSelector(s => s.projects.search); } const Users = () => { ... const preDispatchOnce = async () => { await dispatch(projectThunkAction.fetchCompanyName()); } const dispatchGetWithSearchParam = async (param: LimitPageAnd<SearchType>) => { const { limit, page, search } = param; const res = await dispatch(projectThunkAction.getProjects({ limit, page, search })).unwrap(); return res.projects; } Note that here our res.projects is of type: typescript (property) projects: RespondedProject[] typescript-23 const { listener } = useLazyLoading({ preDispatchOnce, dispatchGetWithSearchParam, getSearchParamHook: GetSearchParamHook, limit: config.projectsPerPage }) return ( <div> ... {listener()} </div> ) } - Our search param mostly comes form redux store, but we can also use local state (as search result may be influenced by local UI state) by instantiating them in SomeOtherHook, and import them into GetSearchParamHook, such as typescript const GetSearchParamHook = () => { const { someState } = SomeOtherHook(); const search = useAppSelector((s) => s.projects.search); return { someState, ...search }; }; - Note that listener is nothing but a div element, we don\'t want it to have any life cycle, therefore we use {listner()} to \\"place it there\\" instead of writing <listener />. ","title":"Lazy Loading","date":"2023-07-05T00:00:00.000Z","id":"blog0152","tag":"react","intro":"Record the detailed implementation of lazy loading.","toc":true},{"content":" #### S3 1. Created a new bucket named wbbucket-dev-frontend. 2. In frontend project we run text aws s3 sync --delete ./build/ s3://wbbucket-dev-frontend to sync our files in build folder to s3-bucket. 3. In **Permission** tab: <Center> <a href=\\"/assets/tech/153/001.png\\"> <img src=\\"/assets/tech/153/001.png\\" width=\\"600\\"/> </a> </Center> <p/> <Center> <a href=\\"/assets/tech/153/002.png\\"> <img src=\\"/assets/tech/153/002.png\\"/> </a> </Center> <p/> 4. In **Properties** tab, we scroll to the bottom, click Edit, <Center> <a href=\\"/assets/tech/153/003.png\\"> <img src=\\"/assets/tech/153/003.png\\" width=\\"600\\"/> </a> </Center> <p/> then choose <Center> <a href=\\"/assets/tech/153/004.png\\"> <img src=\\"/assets/tech/153/004.png\\" width=\\"600\\"/> </a> </Center> <p/> #### CloudFront 1. Pricing: <Center> <a href=\\"/assets/tech/153/005.png\\"> <img src=\\"/assets/tech/153/005.png\\"/> </a> </Center> <p/> 1. Click Create distribution: <Center> <a href=\\"/assets/tech/153/006.png\\"> <img src=\\"/assets/tech/153/006.png\\" width=\\"600\\"/> </a> </Center> <p/> 2. S3-buckets by default are among the choices for Origin domain: <Center> <a href=\\"/assets/tech/153/007.png\\"> <img src=\\"/assets/tech/153/007.png\\"/> </a> </Center> <p/> 3. Once S3-bucket is chosen, aws fills the following: <Center> <a href=\\"/assets/tech/153/008.png\\"> <img src=\\"/assets/tech/153/008.png\\" width=\\"600\\"/> </a> </Center> <p/> 4. Check Redirect HTTP to HTTPS <Center> <a href=\\"/assets/tech/153/009.png\\"> <img src=\\"/assets/tech/153/009.png\\" width=\\"600\\"/> </a> </Center> <p/> 5. Enable firewall <Center> <a href=\\"/assets/tech/153/010.png\\"> <img src=\\"/assets/tech/153/010.png\\" width=\\"600\\"/> </a> </Center> <p/> 6. - Prepare a domain or subdomain in route53 - Fill in alternate domain name and - Choose SSL certificate. <Center> <a href=\\"/assets/tech/153/011.png\\"> <img src=\\"/assets/tech/153/011.png\\" width=\\"600\\"/> </a> </Center> <p/> For example, I test the s3-deployment by using: <p/> <Center> <a href=\\"/assets/tech/153/012.png\\"> <img src=\\"/assets/tech/153/012.png\\" width=\\"600\\"/> </a> </Center> <p/> 7. Turn IPv6 Off <Center> <a href=\\"/assets/tech/153/013.png\\"> <img src=\\"/assets/tech/153/013.png\\"/> </a> </Center> <p/> #### Route53 to CloudFront 1. Once CloudFront was set up, we have the following \\"alias\\" <Center> <a href=\\"/assets/tech/153/014.png\\"> <img src=\\"/assets/tech/153/014.png\\"/> </a> </Center> <p/> we will need the high-lighted id in configuring records in route53. Our website is already up and running: <Center> <a href=\\"/assets/tech/153/015.png\\"> <img src=\\"/assets/tech/153/015.png\\" width=\\"600\\"/> </a> </Center> <p/> 2. Go to route53, choose hosted zone, and edit our prepared record. <Center> <a href=\\"/assets/tech/153/016.png\\"> <img src=\\"/assets/tech/153/016.png\\"/> </a> </Center> <p/> choose the domain prepared by CloudFront. #### Cache Removal 1. After first deployment succeeds, we will fail to see new changes due to caching. The removal of cache is called invalidation in aws-cli. 2. We list all distributions by aws cloudfront list-distributions > ./list.json, we check the target id to remove cache: json { \\"DistributionList\\": { \\"Items\\": [ {... }, {... }, { \\"Id\\": \\"EQ7AXNACL2PQ6\\", \\"ARN\\": \\"arn:aws:cloudfront::798404461798:distribution/EQ7AXNACL2PQ6\\", \\"Status\\": \\"Deployed\\", \\"LastModifiedTime\\": \\"2023-07-06T06:29:17.206000+00:00\\", \\"DomainName\\": \\"d1i8cgdq44oar0.cloudfront.net\\", \\"Aliases\\": { \\"Quantity\\": 1, \\"Items\\": [ \\"wb-admin-s3-test.wonderbricks.com\\" ] }, \\"Origins\\": { \\"Quantity\\": 1, \\"Items\\": [ ... } By looking at the attribute we are sure EQ7AXNACL2PQ6 is our target id. 3. aws-cli aws cloudfront create-invalidation --distribution-id EQ7AXNACL2PQ6 --paths \\"/*\\" 4. In frontend the complete deployment script becomes: json \\"scripts\\": { \\"build:uat\\": \\"env-cmd -f .env.uat react-app-rewired build\\", \\"invalidation:uat\\": \\"aws cloudfront create-invalidation --distribution-id EQ7AXNACL2PQ6 --paths \\\\\\"/*\\\\\\" > ./invalidation.json\\", \\"deploy:uat\\": \\"yarn build:uat && yarn invalidation:uat && aws s3 sync --delete ./build/ s3://wbbucket-dev-frontend\\", ... }, In invalidation:uat will pipe the output into a file to avoid the console prompting user input.","title":"S3 Deployment of React Project","date":"2023-07-08T00:00:00.000Z","id":"blog0153","tag":"react, aws","intro":"A full breakdown of steps deploying a react project to s3 with SSL encryption.","toc":true},{"content":" #### Repository - [The Boilerplate](https://github.com/machingclee/2023-07-09-uwsgi-flask-boilerplate-with-docker-deployment) #### Project Structure <Center> <a href=\\"/assets/tech/154/001.png\\"> <img src=\\"/assets/tech/154/001.png\\"/> </a> </Center> <p/> #### Controllers python # src/controllers/script_controller.py import os import io from flask import Blueprint, render_template, abort, send_file, request, jsonify from src.wb_quotation_summary_excel_script.quotation_summary_excel_script \\\\ import main as quotation_summary_excel_script from src.wb_quotation_comparison_excel_script.quotation_comparison_excel_script \\\\ import main as quotation_comparison_excel_script from typing import Callable script_controller = Blueprint(\'script_controller\', __name__) def process(data_source: dict, exec_script: Callable[[dict], str]) -> str: ... @script_controller.route(\\"/script/quotation_summary_excel\\", methods=[\'GET\', \'POST\']) def quotation_summary_excel(): ... @script_controller.route(\\"/script/quotation_comparison_excel_script\\", methods=[\'GET\', \'POST\']) def quotation_comparison_excel(): ... #### How Controller Access src.wb_quotation_summary_excel_script? - Usually in python things become tricky if we want to access object defined in **arbitrary** diectory. - It is very often to get ModuleNotFoundError and requires tricky step to get around it like appending desired directory path to sys.path that includes src. - **_Alternatively_** we can simply define an environment variable PYTHONPATH to be the directory that is parent to our src/. - This makes the import statement from src.wb_quotation_comparison_excel_script becomes possible without inserting weired logic like sys.path.append. - In this way our debugger can work flawlessly: json { \\"version\\": \\"0.2.0\\", \\"configurations\\": [ { \\"name\\": \\"Python: Current File\\", \\"type\\": \\"python\\", \\"request\\": \\"launch\\", \\"program\\": \\"${file}\\", \\"console\\": \\"integratedTerminal\\", \\"env\\": { \\"PYTHONPATH\\": \\"${workspaceRoot}\\" } } ] } #### Borrow the same Idea to Deploy Docker Image We define RUN export PYTHONPATH=\\"/project\\" and therefore no ModuleNotFoundError can occur. dockerfile FROM python:3.8 RUN apt-get update RUN apt-get install -y --no-install-recommends \\\\ libatlas-base-dev gfortran nginx supervisor RUN pip3 install uwsgi COPY ./requirements.txt /project/requirements.txt RUN pip3 install -r /project/requirements.txt RUN useradd --no-create-home nginx RUN rm /etc/nginx/sites-enabled/default RUN rm -r /root/.cache COPY server_configs/nginx.conf /etc/nginx/ COPY server_configs/flask-site-nginx.conf /etc/nginx/conf.d/ COPY server_configs/uwsgi.ini /etc/uwsgi/ COPY server_configs/supervisord.conf /etc/ RUN mkdir -p /project/src COPY /src /project/src RUN mkdir -p /project/excel_files RUN chmod 643 /project/excel_files RUN export PYTHONPATH=\\"/project\\" WORKDIR /project CMD [\\"/usr/bin/supervisord\\"] ","title":"Flask with Uwsgi and Docker Image Deployment","date":"2023-07-10T00:00:00.000Z","id":"blog0154","tag":"react, aws","intro":"A full breakdown of steps deploying a react project to s3 with SSL encryption.","toc":true},{"content":" <style> img { max-width: 100%; } </style> #### Create a Projects to use These two Services - Go to google cloud console https://console.cloud.google.com/ - Click ![](/assets/tech/155/001.png) - Create a project ![](/assets/tech/155/002.png) - Choose the service ![](/assets/tech/155/003.png) In our case we need - Gmail API ![](/assets/tech/155/004.png) - Pub/Sub API ![](/assets/tech/155/005.png) And then enable API, try now, ..., etc (each service has the button placed in different positions). #### Create OAuth Detail - Then we can click ![](/assets/tech/155/006.png) to control the access right (called scope) to the service for users authenticated through oauth. - **Step 1.** Most of the fields are optional, just fill in App name and User support email. ![](/assets/tech/155/007.png) then click save and continue at the bottom. - **Step 2.** We define access right for oauth-authenticated users. Click Add or remove scopes: ![](/assets/tech/155/008.png) in our case we need: ![](/assets/tech/155/009.png) - **Step 3.** Add users who is available to using the apis from this project ![](/assets/tech/155/010.png) james.lee now can use pubsub and gmailapi with defined rights in the scopes. - Since we are the only users, we can keep staying at test state. #### Create Credentials - Save and continue, we create credentials for authentication. ![](/assets/tech/155/011.png) - Choose Desktop app ![](/assets/tech/155/012.png) - Fill in the blanks, then create. - The resulting credential.json is for api which use oauth-v2 authentication. #### Another Version of Credential (can skip and revisit when needed, pubsub library may need it) - Since we are not going to use pubsub\'s pulling, we just receive push notification by webhook (let google send post request to us), we don\'t need any package for pubsub in our project which requires oauth-v1 credential. - In the future if pulling is needed, we can get v1 credential in the folloing way: > - ![](/assets/tech/155/013.png) > - $\\\\to$ manage server account > - $\\\\to$ manage keys > ![](/assets/tech/155/014.png) > - $\\\\to$ add key $\\\\to$ download json: > ![](/assets/tech/155/015.png) #### Configure Pubsub API ##### Create Topic - Go to pub/sub api page by using the search bar again ![](/assets/tech/155/016.png) - Create a topic with default option ![](/assets/tech/155/017.png) ##### Create Subscription - Create subscription and selected our newly created topic ![](/assets/tech/155/018.png) ##### Create Publisher for the Topic - We next create publisher that monitor our mail box and push message to the topic. We following [documentation about push notifications in gmail api](https://developers.google.com/gmail/api/guides/push). > To do this, you need to **grant publish privileges to gmail-api-push@system.gserviceaccount.com**. You can do this using the Cloud Pub/Sub Developer Console permissions interface following the resource-level access control instructions. - Select Topic ![](/assets/tech/155/019.png) - Click Add Principal at the right column ![](/assets/tech/155/020.png) - Fill in the email address, then assign publisher right ![](/assets/tech/155/021.png) ##### Create Subscriber for the Topic - Next we grant receiver right to our google users: - select a topic in subscriptions ![](/assets/tech/155/022.png) - Add principal at the right column, then fill in: ![](/assets/tech/155/023.png) #### Use the Credential in the Program - Place your credential at ![](/assets/tech/155/024.png) name it credential-v2-gmail.json, this is configured in GoogleConfig.java. We may move the string to properties file, but 3 environments can share the same crendential. - The actual authentication will be done when we run the springboot program, we will be asked to sigin in through a given link in the terminal. - Once authentication is done, a file will be created in token-local/ and we will not be asked to login again when that file exists. - In properties file we have configured GoogleConfig to use token-uat/, token-prd/ in different environment - To avoid the package prompting login action in uat and prd, we may create these addional two folders $\\\\to$ sign-in $\\\\to$ copy the generated file into the correct folder before deployment. #### Code Implementation to Use Credential and Start Webhook in Java The following is essentially a modification of code presented in documentation. java mport com.google.api.client.auth.oauth2.Credential; import com.google.api.client.extensions.java6.auth.oauth2.AuthorizationCodeInstalledApp; import com.google.api.client.extensions.jetty.auth.oauth2.LocalServerReceiver; import com.google.api.client.googleapis.auth.oauth2.GoogleAuthorizationCodeFlow; import com.google.api.client.googleapis.auth.oauth2.GoogleClientSecrets; import com.google.api.client.googleapis.javanet.GoogleNetHttpTransport; import com.google.api.client.http.javanet.NetHttpTransport; import com.google.api.client.json.JsonFactory; import com.google.api.client.json.gson.GsonFactory; import com.google.api.client.util.store.FileDataStoreFactory; import com.google.api.services.gmail.Gmail; import com.google.api.services.gmail.GmailScopes; import com.google.api.services.gmail.model.WatchRequest; import com.google.api.services.gmail.model.WatchResponse; import com.wonderbricks.web.service.impl.GmailServiceImpl; import java.io.FileNotFoundException; import java.io.IOException; import java.io.InputStream; import java.io.InputStreamReader; import java.security.GeneralSecurityException; import java.util.Arrays; import java.util.Collections; import java.util.List; import java.util.concurrent.CompletableFuture; import java.util.concurrent.TimeUnit; import org.slf4j.LoggerFactory; import org.springframework.beans.factory.annotation.Value; import org.springframework.boot.CommandLineRunner; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class GoogleConfig { @Value(\\"${google.token}\\") private String TOKENS_DIRECTORY_PATH; @Value(\\"${google.credential}\\") private String CREDENTIAL_FILE_PATH; @Value(\\"${google.app}\\") private String APPLICATION_NAME; @Value(\\"${google.pub.topic}\\") private String PUBLISHER_TOPIC_NAME; private static final JsonFactory JSON_FACTORY = GsonFactory.getDefaultInstance(); private static final List<String> SCOPES = Collections.singletonList(GmailScopes.MAIL_GOOGLE_COM); @Bean Gmail getGmail() throws GeneralSecurityException, IOException { final NetHttpTransport HTTP_TRANSPORT = GoogleNetHttpTransport.newTrustedTransport(); Gmail service = new Gmail.Builder(HTTP_TRANSPORT, JSON_FACTORY, getCredentials(HTTP_TRANSPORT)) .setApplicationName(APPLICATION_NAME) .build(); return service; } private Credential getCredentials(final NetHttpTransport HTTP_TRANSPORT) throws IOException { InputStream in = GmailServiceImpl.class.getResourceAsStream(CREDENTIAL_FILE_PATH); if (in == null) { throw new FileNotFoundException(\\"Resource not found: \\" + CREDENTIAL_FILE_PATH); } GoogleClientSecrets clientSecrets = GoogleClientSecrets.load(JSON_FACTORY, new InputStreamReader(in)); GoogleAuthorizationCodeFlow flow = new GoogleAuthorizationCodeFlow.Builder( HTTP_TRANSPORT, JSON_FACTORY, clientSecrets, SCOPES) .setDataStoreFactory(new FileDataStoreFactory(new java.io.File(TOKENS_DIRECTORY_PATH))) .setAccessType(\\"offline\\") .build(); LocalServerReceiver receiver = new LocalServerReceiver.Builder().setPort(8888).build(); Credential credential = new AuthorizationCodeInstalledApp(flow, receiver).authorize(\\"user\\"); return credential; } public void startWebHook(Gmail gmail) throws IOException, InterruptedException { WatchRequest request = new WatchRequest(); request.setLabelIds(Arrays.asList(\\"INBOX\\")); request.setTopicName(PUBLISHER_TOPIC_NAME); request.setLabelFilterAction(\\"INCLUDE\\"); WatchResponse res = gmail.users().watch(\\"me\\", request).execute(); Long expiredAfter = res.getExpiration(); // millis CompletableFuture.delayedExecutor(expiredAfter, TimeUnit.MILLISECONDS).execute(() -> { try { startWebHook(gmail); } catch (IOException | InterruptedException e) { e.printStackTrace(); } }); } // Run after application context has been created @Bean CommandLineRunner initWebhook(Gmail gmail) { return args -> { var logger = LoggerFactory.getLogger(GoogleConfig.class); logger.info(\\"Start Watching Gmail Account\\"); startWebHook(gmail); }; } } ","title":"Gmail and Inbox Push Notification","date":"2023-07-17T00:00:00.000Z","id":"blog0155","tag":"google-cloud","intro":"A introduction of OAuth2 Consent Setting and Pubsub for publishing update of gmail inbox.","toc":true},{"content":" #### Prerequisite We assume the reader has installed the npm package serverless globally. Make sure you can run sls or serverless at your terminal. #### Python Flask ##### Template Repo - https://github.com/machingclee/2023-07-22-serverless-python-template ##### How to test Locally and Deploy - yarn to install serverless-wsgi and serverless-python-requirements - Now if you run sls wsgi serve, a flask api server should be up and running. It also provides hot reload for code changes. - After your api implement are done, you can sls deploy to deploy your application. - You can sls remove to undo everything. ##### Size Reduction for Python Lambdas You can reduce the size by modifying serverless.yml. - Add custom.pythonRequirements.noDeploy. - By default serverless will copy compiled binary from your virtual environment. - If you add autopep8 in noDeploy, make sure to remove that from requirements.txt as well because serverless will make a copy of requirements.txt and copy compiled binary accordingly. - dockerizePip: true is necessary for package PILLOW because the compiled binary in windows is not compatible with linux. - Layers! I haven\'t tried to Flask yet, we may add noDeploy in the list once we find suitable lambda layers available in our region: text provider: name: aws runtime: python3.8 layers: - arn:aws:lambda:us-east-1:xxxxxxxxxxxxx:layer:xxxxx:mylayer1 - arn:aws:lambda:us-east-1:xxxxxxxxxxxxx:layer:xxxxx:mylayer2 The list of layers available at our region can be found in [here](https://github.com/keithrozario/Klayers/tree/master/deployments/python3.8). #### Nodejs Express (ts) ##### Template Repo - https://github.com/machingclee/2023-07-23-severless-express-ts-template ##### How to test Locally and Deploy - yarn to install - serverless-http - serverless-offline - serverless-plugin-common-excludes - serverless-plugin-typescript-express - To run locally we run yarn start, which just use traditional nodemon and ts-node src/app.ts. - We run yarn deploy to deploy our app using our ~/.aws credentials. - We run yarn remove to sls remove everything according to cloudformation record. ##### Size Reduction for Nodejs Lambdas Compared to python we have much fewer things to modify in serverless.yml as we don\'t have such options. The best thing we may try is: yml package: patterns: - src/** # include only files from ./src/**/* - \\"!node_modules/some-package/**\\" # exclude files from ./node_modules/some-package/**/* and add layers in provider option, but I don\'t have such convenient resource yet. ##### Manual Bug Fix before Deployment Before deployment, a manual bug fix must be held on our own. From [this thread](https://github.com/serverless/serverless/issues/10944), we need to modify none node_modules/serverless/bin/serverless.js which in my case my serverless.js is held at the following absolute path none C:\\\\Users\\\\user\\\\AppData\\\\Local\\\\Yarn\\\\Data\\\\global\\\\node_modules\\\\serverless\\\\bin\\\\serverless.js in that file we add the following line right after use strict;: js require(\\"../../graceful-fs/graceful-fs\\").gracefulify(require(\\"fs\\")); ","title":"Serverless Flask and Serverless Express-ts","date":"2023-07-19T00:00:00.000Z","id":"blog0156","tag":"serverless, aws","intro":"Guide to creating serverless flask and express application.","toc":true},{"content":" <style> img { width: 100% } </style> #### Video Demonstration <Center> <iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/gMSWdAZhupY\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe> </Center> <p></p> This demonstrated: - We can send event back to client; - We can determine whether a client is disconnected. #### Frontend Implementation We create a GET request with header Content-Type: text/event-stream as follows: ts import { useState } from \\"react\\"; import lodash from \\"lodash\\"; import axios from \\"axios\\"; export default function SSE() { const [msgs, setMsgs] = useState<string[]>([]); return ( <> <button onClick={() => { const evtSource = new EventSource( \\"http://localhost:8080/gmail/event\\" ); evtSource.addEventListener(\\"message\\", (event) => { console.log(\\"eventevent\\", event); setMsgs((datas) => { const datas_ = lodash.cloneDeep(datas); const newData = JSON.stringify(event.data); datas_.push(newData); return datas_; }); }); }} > Listen to a stream </button> <div> {msgs.map((msg) => { if (msg) { return <div>{msg}</div>; } else { return null; } })} </div> </> ); } #### Backend Implementation ##### Springboot java @RestController public class GmailController { ... @GetMapping(value = \\"/gmail/event\\", produces = MediaType.TEXT_EVENT_STREAM_VALUE) @ResponseBody public Flux<ServerSentEvent<String>> getEvents() throws InterruptedException { return Flux.interval(Duration.ofSeconds(1)) .map(seq -> { System.out.println(\\"Event Emitted!\\"); var sse = ServerSentEvent.<String>builder() .event(\\"message\\") // .data(redisTemplate.opsForList().rightPop(\\"mail-queue\\")) .data(\\"Some Message - \\" + seq.toString()) .build(); return sse; }) .doFinally(signalType -> { System.out.println(\\"Disconnected\\"); }); } } - Flux.interval helps put our callback into a special queue at which a event loop will constantly look. This is analagous to setInterval in chrome and nodejs, which place the callbacks into a special queue and let the event loop to pick up. This is to prevent using single-threaded model to hold and send message to the client. - Our connection with client will be kept once connected (using other queue), and the thread that takes the request to our controller will be released. - We should not try to use a while loop (as in some tutorial) to hold the connection as it will certainly use up the number of threads in our thread pool easily. - We should return Flux<ServerSentEvent<String>> instead of Flux<String> since ServerSentEvent objects also serve as a heartbeat to tell whether a connection is disconnected or not. ##### Counterpart in Node.js js app.get(\\"/sse\\", async (req: Request, res: Response) => { console.log(\\"connected\\"); res.writeHead(200, { \\"Content-Type\\": \\"text/event-stream\\", \\"Connection\\": \\"keep-alive\\", \\"Cache-Control\\": \\"no-cache\\", }); let i = 0; const responseInterval = setInterval(() => { console.log(\\"producing message!\\"); res.write(\'event: message\\\\n\'); res.write(data: message item ${i}\\\\n); res.write(id: ${i}\\\\n\\\\n); i++; }, 1000); req.on(\\"close\\", () => { console.log(\\"user disconnected\\"); clearInterval(responseInterval); }) }) Once we refresh the browser, we can check that the interval is cleared by observing there is no more producing message! #### Improvement After Actually Turning POC into Real Implementation ##### Result <Center> <a href=\\"/assets/tech/157/001.gif\\" target=\\"_blank\\"> <img src=\\"/assets/tech/157/001.gif\\"/> </a> </Center> ##### Frontend I wrapped the logic of calling SSE in a hook: ts // useSSE.ts import { useEffect, useRef } from \\"react\\"; import { SERVER_SENT_EVENT_NOTIFICATION } from \\"../../axios/api-routes\\"; import constant from \\"../../config/constant\\"; import snackbarUtils from \\"../../util/snackbarUtils\\"; import notificationMessage from \\"../../config/notificationMessage\\"; import { useAppDispatch } from \\"../../redux/app/hook\\"; import applicationSlice from \\"../../redux/slice/applicationSlice\\"; // mimic thread.sleep const sleep = (time: number): Promise<boolean> => { return new Promise((resolve, reject) => { setTimeout(() => { resolve(true); }, time); }); }; // define actions for different message from backend const actions = (data: string, dispatch: ReturnType<typeof useAppDispatch>) => { if (data === notificationMessage.FETCH_MAILCHAINS) { snackbarUtils.info(\\"Fetching new mailchains\\"); } else if (data === notificationMessage.DISPLAY_CONNECTED) { dispatch(applicationSlice.actions.updatePushNotificationState(true)); snackbarUtils.info(\\"Connected for push notification\\"); } }; export default () => { const dispatch = useAppDispatch(); const reconnectionTries = useRef(0); useEffect(() => { let sse: EventSource | null = null; // wait for 2 seconds, since backend will disconnect users once they refresh, // avoid racing with backend: // refresh -> login -> backend logout due to previous disconnection sleep(2000).then(() => { try { sse = new EventSource(SERVER_SENT_EVENT_NOTIFICATION, { withCredentials: true, }); } catch (err) { console.log(err); } if (sse) { sse.addEventListener(\\"message\\", (event) => { const data = event.data as string; console.log(\\"[data received]\\", data); if (data.startsWith(\\"ERROR\\")) { snackbarUtils.error(data); } actions(data, dispatch); }); sse.onerror = () => { if (reconnectionTries.current < constant.SSE_MAX_RETRY_COUNT) { reconnectionTries.current++; console.log(\\"err event, retry\\"); } else { if (sse) { console.log( ${reconnectionTries.current + 1}th attempt, close connection ); sse.close(); } } }; } }); return () => { if (sse) { dispatch(applicationSlice.actions.updatePushNotificationState(false)); sse.close(); } }; }, []); }; ##### Backend A fake webhook to push notification to all connected users: java @GetMapping(value = \\"/push\\", produces = { MediaType.APPLICATION_JSON_VALUE }) @ResponseBody public Document pushNotification() { var responses = new ArrayList<Document>(); Set<String> usersConnected = redisTemplate.opsForSet().members(\\"room\\"); for (String userName : usersConnected) { String message = NotificationMessage.FETCH_MAILCHAINS; var note = new Document(); note.append(\\"user\\", userName); note.append(\\"message\\", message); responses.add(note); redisTemplate.opsForList().leftPush(userName, NotificationMessage.FETCH_MAILCHAINS); } var res = new Document(); res.append(\\"success\\", true); res.append(\\"result\\", responses); return res; } Next we implement the data-streaming request which also handles possible exceptions carefully: java @Data private class NotificationDataRef { private String errMessage = null; private String userName = null; public String getUserName() { return \\"notification::\\" + this.userName; } } @GetMapping(value = \\"/notification\\", produces = MediaType.TEXT_EVENT_STREAM_VALUE) @ResponseBody public Flux<ServerSentEvent<String>> getEvents() throws InterruptedException { final NotificationDataRef ref = new NotificationDataRef(); try { var user = userService.getCurrentUser(); String userName = user.getString(\\"user_name\\"); ref.setUserName(userName); redisTemplate.opsForSet().add(\\"room\\", ref.getUserName()); logger.info(String.format(\\"User %s has connected\\", userName)); } catch (Exception err) { String errMessage = err.getMessage(); ref.setErrMessage(\\"ERROR:\\" + errMessage); logger.info(errMessage); } return Flux.interval(Duration.ofSeconds(1)) .map((seq) -> { if (ref.getErrMessage() != null) { return ref.getErrMessage(); } if (seq.equals(Long.valueOf(0))) { return NotificationMessage.DISPLAY_CONNECTED; } String message = null; try { message = redisTemplate.opsForList().rightPop(ref.getUserName()); // the poped value will only be constant // defined in NotificationMessage class. } catch (Exception err) { message = String.format(\\"ERROR:%s\\", err.getMessage()); } return message == null ? \\"\\" : message; }) .map(message -> ServerSentEvent.<String>builder() .event(\\"message\\") .data(message) .build()) .takeUntil((event) -> { String message = event.data(); Boolean errorExists = message.startsWith(\\"ERROR\\"); if (errorExists) { logger.info(message); } return errorExists; }) .doFinally(signalType -> { redisTemplate.opsForSet().remove(\\"room\\", ref.getUserName()); redisTemplate.delete(ref.getUserName()); logger.info(String.format(\\"%s has disconnected\\", ref.getUserName())); }); } - When backend received a message started with ERROR, it will close the connection, in this way we handle the error gracefully. - When frontend received a message started with ERROR, this message will be logged in the frontend. - Due to closure of the connection from backend, our frontend will try serveral times and close() the connection when retry count reaches its maximum. ","title":"Server Sent Event in Java and Node.js Backend","date":"2023-09-03T00:00:00.000Z","id":"blog0157","tag":"java, springboot, express","intro":"An introduction to how to effectively create SSE event mimicing the single-thread event loop adopted by nodejs in springboot, and how do we actually implement it in nodejs as well.","toc":true},{"content":" #### What Approaches do we Have when it Comes to Search Engine? Recently I am responsible for building search functions in frontend. I come up with the following in my mind: - We create a list of {type: string, key: string}\'s, for different matched key we create a query based on different type (where data may come from different tables or collections). - We sends **_everything_** we want from backend to frontend, and we may either use standard regular expression or dedicated library like Fuse.js to query for desired results. This works perfectly fine for static web pages (such as this blog). - We build Elastic Stack, such as Elastic Search and Kibana, which in essense also save results in Document and index the fields for searching the documents. And after struggling for tutorials in youtube, I came across: - We use **_Algolia_** by feeding our json files (wich consists of search targets) and setting the field names we want to use as search indexes. #### Code Implementation for Fuse.js ##### Search Target First we build our blog.json file which serves as a search resource. json [ ... { \\"content\\": \\"...\\" \\"title\\": \\"Write Middleware in Redux-Toolkit\\", \\"date\\": \\"2023-06-20T00:00:00.000Z\\", \\"id\\": \\"blog0132\\", \\"tag\\": \\"react\\", \\"intro\\": \\"We list sample usage of ...\\" \\"toc\\": true }, ... ] ##### Build a blog.json which Contains Documents to Search In my case I use the following script: js import fs from \\"fs\\"; import matter from \\"gray-matter\\"; import path from \\"path\\"; const mdDirs = [\\"./src/mds/articles/tech\\", \\"./src/mds/articles/math\\"]; const getAllMdFilePaths = (dir: string) => { const mdFiles: string[] = []; const getFiles = (dir: string) => { const paths = fs.readdirSync(dir); paths.forEach((p) => { const newPath = path.join(${dir}/${p}); const pathStat = fs.statSync(newPath); if (pathStat.isDirectory()) { getFiles(newPath); } else { if (newPath.endsWith(\\".md\\")) { mdFiles.push(newPath); } } }); }; getFiles(dir); return mdFiles; }; const writeMdInJson = () => { const targetPaths = \\"./src/mds/blog.json\\"; const blogJson: any[] = []; for (const dirpath of mdDirs) { const mdpaths = getAllMdFilePaths(dirpath); mdpaths.forEach((path) => { const mdText = fs.readFileSync(path, { encoding: \\"utf8\\", flag: \\"r\\" }); const { data, content } = matter(mdText); const { wip = false } = data; if (!wip) { blogJson.push({ content, ...data }); } }); } fs.writeFileSync( targetPaths, JSON.stringify(blogJson, null, 0) .replace(/(\\\\\\\\r\\\\\\\\n)/g, \\" \\") .replace(//g, \\"\\") .replace(/\\\\s+/g, \\" \\") ); }; const main = () => { writeMdInJson(); }; main(); Next in the our search component: tsx import searchJson from \\"../../../mds/blog.json\\"; export default function SearchComponent() { const fuse = useRef( new Fuse(searchJson, { keys: [\\"content\\", \\"tag\\", \\"tags\\", \\"title\\", \\"intro\\"], threshold: config.fuzzySearchThreshold, }) ); const [searchResults, setSearchedResults] = useState< { title: string; intro: string; tag: string; tags: string }[] >([]); const [searchText, setSearchText] = useState(\\"\\"); const searchBarRef = useRef<HTMLInputElement>(null); const handleSearchChange = debounce((e: ChangeEvent<HTMLInputElement>) => { setSearchText(e.target.value); const searchValue = e.target.value; if (searchValue) { const result = fuse.current.search(searchValue); setSearchedResults( result.map((r) => { const { title, intro, tag, tags } = r.item; return { title, intro, tag: tag || \\"\\", tags: tags || \\"\\" }; }) ); } else { setSearchedResults([]); } }, 300); return ( <SearchBar placeholder=\\"Tag, title or content\\" onChange={handleSearchChange} inputRef={searchBarRef} /> ); } - The Fuse object can be created anywhere and imported into the component. - In my case I simply use useRef as it is going to be aways static and unchanged in the life cycle of the SearchComponent. #### Code Implementation for Algolia ##### Backend Using Java ###### Responsibilities of Backend in Using Algolia Our backend will take the following tasks: - Provide ALGOLIA_SEARCH_INDEX ![](/assets/tech/158/001.png) - Provide applicationID - Provide frontend client with searchApiKey\'s with differnent priviledges for searching, for example: - Admin users can search everything - Users of some organization can only search their own related remails - Upload searchable targets (named Record) to Algolia database - Add new search item into algolia when needed (like emails) ###### Dependencies After registering an account in Algolia and creating an application there, we include the following two dependencies: xml <dependency> <groupId>com.algolia</groupId> <artifactId>algoliasearch-core</artifactId> <version>3.16.5</version> </dependency> <dependency> <groupId>com.algolia</groupId> <artifactId>algoliasearch-java-net</artifactId> <version>3.16.5</version> </dependency> ###### Record Object - Algolia requires users define a Record object which at least contains a non-nullable field called objectID. - Luckily we use mongodb in our java backend, we simply use a stringified _id and we use a modelMapper.map() to take a Document object into our desired Record object: java package com.organization.web.service.dto; import java.util.List; import lombok.Data; @Data public class EmailChainRecord { @Data public static class Supplier { private List<String> material_manu_internal_codes; } @Data public static class NameField { private String name; } @Data public static class EmailField { private String body; private List<String> participant_emails; } @Data public static class SenderInDb { private Integer id; private String user_name; private String first_name; private String last_name; private String email; } @Data public static class Task { private String code; private String name; } @Data public static class Section { private String name; private List<Task> tasks; } @Data public static class ProgramDetail { private String prog_ref_no; private String name; private List<Section> sections; } private String oid; private String objectID; private String title; private String buyer_company_code; private String latest_gmail_snippet; private List<String> sender_emails; private List<SenderInDb> sendersInDb; private NameField buyerCompanyDetail; private NameField projectDetail; private List<ProgramDetail> programmesDetail; private List<EmailField> emails_body; private List<String> participant_emails; } ###### SearchIndex Object In both frontend and backend, the major api calls are all managed by the SearchIndex object: java package com.organization.web.algolia; import org.springframework.beans.factory.annotation.Value; import org.springframework.context.annotation.Bean; import org.springframework.stereotype.Service; import com.algolia.search.DefaultSearchClient; import com.algolia.search.SearchClient; import com.algolia.search.SearchIndex; import com.organization.web.controller.err.CustomException; import com.organization.web.service.dto.EmailChainRecord; @Service public class Algolia { @Value(\\"${algolia.application.id}\\") private String applicationID; @Value(\\"${algolia.api.key}\\") private String APIKEY; @Bean public SearchClient getSearchClient() throws CustomException { if (this.applicationID == null || this.APIKEY == null) { throw new CustomException(\\"application id and apikey cannot be null for algolia\\"); } return DefaultSearchClient.create(this.applicationID, this.APIKEY); } @Bean public SearchIndex<EmailChainRecord> getIndex() throws CustomException { SearchClient client = getSearchClient(); var initedIndex = client.initIndex(\\"correspondence\\", EmailChainRecord.class); return initedIndex; } } ###### SearchService: All the Utility Functions **Contructor Injection.** To facilitate unit testing, we use autowired constructor injection: java-1 package com.organization.web.service.impl; import com.algolia.search.SearchClient; import com.algolia.search.SearchIndex; import com.algolia.search.models.apikeys.SecuredApiKeyRestriction; import com.algolia.search.models.indexing.Query; import com.algolia.search.models.settings.IndexSettings; import com.mongodb.client.MongoCollection; import com.mongodb.client.model.Filters; import com.organization.web.controller.codes.UserRoles; import com.organization.web.controller.err.CustomException; import com.organization.web.mongodb.CollectionNames; import com.organization.web.mongodb.MongoDB; import com.organization.web.mongodb.MongoDB.JsonPipeline; import com.organization.web.service.SearchService; import com.organization.web.service.dto.EmailChainRecord; import com.organization.web.service.dto.EmailChainRecord.ProgramDetail; import java.util.ArrayList; import java.util.Arrays; import java.util.List; import java.util.stream.Collectors; import org.apache.commons.collections4.ListUtils; import org.bson.Document; import org.bson.types.ObjectId; import org.modelmapper.ModelMapper; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Service; import org.springframework.beans.factory.annotation.Value; @Service public class SearchServiceImpl implements SearchService { @Value(\\"${algolia.public.search.api.key}\\") private String publicSearchAPIKey; private MongoDB mongodb; private ModelMapper modelMapper = new ModelMapper(); private SearchIndex<EmailChainRecord> index; private SearchClient searchClient; // A search key that you keep private @Autowired public SearchServiceImpl( MongoDB mongodb, ModelMapper modelMapper, SearchIndex<EmailChainRecord> index, SearchClient searchClient) { this.mongodb = mongodb; this.modelMapper = modelMapper; this.index = index; this.searchClient = searchClient; } public void clearObjects() { this.index.clearObjects(); } **Insert Data Into Algolia.** java-59 public void insertEmailsIntoAlgolia() { clearObjects(); ... var searchDocuments = someCollection .aggregate(somePipeline) .map(u -> { return modelMapper.map(u, EmailChainRecord.class); }) .forEach(u -> { // refine data in u for search logic }) .into(new ArrayList<>()); if (searchDocuments != null) { this.index.saveObjects(searchDocuments).waitTask(); } } **Define Attributes that Contributes to the Search.** java-75 public void setKeyAndFacetsForQueryAndFilter() { var indexSettings = new IndexSettings(); List<String> attributes = Arrays.asList( \\"latest_gmail_snippet\\", \\"sender_emails\\", \\"projectDetail.name\\", \\"searchabletitle\\", \\"title\\", \\"projectDetail.name\\", \\"senderInDb.user_name\\", \\"senderInDb.first_name\\", \\"senderInDb.last_name\\", \\"programmesDetail.name\\", \\"programmesDetail.sections.tasks.code\\", \\"buyerCompanyDetail.name\\", \\"emails_body.body\\", \\"emails_body.participant_emails\\", \\"participant_emails\\"); indexSettings.setSearchableAttributes(attributes); **Define Facets (configs to the search keys)** java-95 List<String> filterFacets = Arrays.asList( \\"filterOnly(participant_emails)\\", \\"filterOnly(emails_body.participant_emails)\\"); java-98 List<String> searchFacets = attributes.stream() .map(key -> String.format(\\"searchable(%s)\\", key)) .collect(Collectors.toList()); **Add the Facets into Index Settings.** ListUtils.union is the same as arr1 + arr2 in python: java-101 indexSettings.setAttributesForFaceting( ListUtils.union(searchFacets, filterFacets)); this.index.setSettings(indexSettings); } **Impose Restrictions to Search Api Key.** java-107 public String createSearchAPIKey(Document user) throws Exception { List<String> roles = user.getList(\\"roles\\", String.class); if (roles.contains(UserRoles.MANAGER) || roles.contains(UserRoles.STAFF)) { return this.publicSearchAPIKey; } String userName = user.getString(\\"user_name\\"); SecuredApiKeyRestriction restriction = new SecuredApiKeyRestriction() .setQuery(new Query().setFilters(String.format( \\"participant_emails:%s OR emails_body.participant_emails:%s\\", userName, userName))); String publicKey = this.searchClient.generateSecuredAPIKey( this.publicSearchAPIKey, restriction); return publicKey; } **Save a Record into Algolia.** java-127 public void saveObject(ObjectId someId) throws CustomException { // logics to fetch search targets EmailChainRecord record = modelMapper.map( targetMailchain, EmailChainRecord.class); if (record != null) { this.index.partialUpdateObject(record); } } } **Remark.** From [documentation](https://www.algolia.com/doc/api-reference/api-methods/partial-update-objects/?client=java) if a record exists in your database but does not exist in algolia, then: none If the objectID is specified but doesn\u2019t exist, Algolia creates a new record That means an upsert operation is automatic. ##### Frontend ###### Responsibility of Frontend The frontend needs to - Get applicationID and searchApiKey from backend - Call the search api to get - target document - searchable facets for search suggestions. ###### Frontend Implementation in React - Algolia provides us with an npm package: react-instantsearch. - However, if we use the UI component provided by that library, we will quickly use up our free quota for the api. - It is because the change handler in the provided searchbar is intentionally designed not to have any debounce rule. - Instead we create our own search component (with <input/>) and use debounced onChange handler with the following search<T> function. js export default class AlgoliaUtil { \\tpublic static instance: AlgoliaUtil | undefined; \\tpublic algoliaEnabled: boolean | undefined; \\tprivate algoliaSearchIndex: string | undefined; \\tprivate searchClient: SearchClient | undefined; \\tprivate searchIndex: SearchIndex | undefined; \\tconstructor(props: { applicationID: string, apiKey: string, initIndex: string, algoliaEnabled: boolean }) { \\t\\tthis.algoliaEnabled = props.algoliaEnabled; \\t\\tthis.algoliaSearchIndex = props.initIndex; \\t\\tthis.searchClient = algoliasearch( \\t\\t\\tprops.applicationID, \\t\\t\\tprops.apiKey, \\t\\t); \\t} \\tpublic static getInstance() { \\t\\tif (!AlgoliaUtil.instance) { \\t\\t\\tthrow new Error(\\"An algolia instance has not been instantiated yet.\\") \\t\\t} \\t\\treturn AlgoliaUtil.instance; \\t} \\tprivate getSearchClient(): SearchClient { \\t\\tif (!this.searchClient) { \\t\\t\\tthrow new Error(\\"Search Client is undefined\\"); \\t\\t} \\t\\treturn this.searchClient; \\t} \\tprivate getIndex() { \\t\\tif (!this.searchIndex) { \\t\\t\\tconst searchClient = this.getSearchClient(); \\t\\t\\tif (this.algoliaSearchIndex) { \\t\\t\\t\\tthis.searchIndex = searchClient.initIndex(this.algoliaSearchIndex); \\t\\t\\t} \\t\\t} \\t\\treturn this.searchIndex; \\t} \\tpublic search<T>(params: { queryString: string, attributesToRetrieve: Extract<keyof T, string>[] }) { \\t\\tconst { attributesToRetrieve, queryString } = params; \\t\\tconst index = this.getIndex(); \\t\\treturn index?.search(queryString, { \\t\\t\\tattributesToRetrieve, facets: constant.FACETS_TO_RECEIVE \\t\\t}); \\t} } We instantiate AlgoliaUtil object when some page is rendered. Sometimes when search feature is not ready yet, and we determine whether algolia is available by setting: js useEffect(() => { if (dialogOpen) { const enabled = AlgoliaUtil.getInstance().algoliaEnabled; setAlgoliaEnabled(enabled || false); } }, [dialogOpen]); - Here the type T in search<T> is simply the target attribute to retrieve. In our case, we use T = { oid: string }. - Also: js constant.FACETS_TO_RECEIVE = [ \\"title\\", \\"latest_gmail_snippet\\", \\"programmesDetail.name\\", \\"emails_body.body\\", \\"projectDetail.name\\" ], are the results that were hit in the past, they are used as search suggestions. ","title":"Building Search Functionality","date":"2023-07-26T00:00:00.000Z","id":"blog0158","tag":"java, react","intro":"We introduce a service called Algolia which provides an easy search engine integration that helps build quick and accurate search functionality.","toc":true},{"content":" js // useOutsideClicked.ts import { RefObject, useEffect, useState } from \\"react\\"; export default ({ ref }: { ref: RefObject<Element> }) => { const [outsideClicked, setOutsideClicked] = useState(true); useEffect(() => { function handleClickOutside(event: any) { if (ref.current && !ref.current.contains(event.target)) { setOutsideClicked(true); } else { setOutsideClicked(false); } } document.addEventListener(\\"mousedown\\", handleClickOutside); return () => { document.removeEventListener(\\"mousedown\\", handleClickOutside); }; }, []); return { outsideClicked, setOutsideClicked }; }; Now we can customize the behaviour of our target component by using - the state outsideClicked and - setter setOutsideClicked. A simple use case is a searchbar with dropdown. ","title":"Detect Click Outside","date":"2023-07-27T00:00:00.000Z","id":"blog0159","tag":"react","intro":"Record a hook for determining whether click have happened outside of our target dom element.","toc":false},{"content":" #### Effect <Center> <img src=\\"/assets/tech/160/001.png\\"> </Center> #### Usage ts <ListUpdateSelector defaultSelectionStrings={[]} allSelectionStrings={[\\"selection1\\", \\"selection2\\", \\"selection3\\"]} optionChangeHandler={(selectedValues) => { someRef.current = selectedValues; }} /> #### Code Implementation ts import CheckIcon from \\"@mui/icons-material/Check\\"; import CloseIcon from \\"@mui/icons-material/Close\\"; import { autocompleteClasses } from \\"@mui/material/Autocomplete\\"; import { styled } from \\"@mui/material/styles\\"; import useAutocomplete, { AutocompleteGetTagProps, } from \\"@mui/material/useAutocomplete\\"; import { CSSProperties, useEffect, useRef } from \\"react\\"; const Root = styled(\\"div\\")( ({ theme }) => color: ${ theme.palette.mode === \\"dark\\" ? \\"rgba(255,255,255,0.65)\\" : \\"rgba(0,0,0,.85)\\" }; font-size: 14px; ); const Label = styled(\\"label\\") padding: 0 0 4px; line-height: 1.5; display: block; ; const InputWrapper = styled(\\"div\\")( ({ theme }) => border-radius: 4px; flex-wrap: wrap; &:hover { border-color: ${theme.palette.mode === \\"dark\\" ? \\"#177ddc\\" : \\"#40a9ff\\"}; } & div { display: flex; justify-content: space-between; } & input { background-color: ${theme.palette.mode === \\"dark\\" ? \\"#141414\\" : \\"#fff\\"}; color: ${ theme.palette.mode === \\"dark\\" ? \\"rgba(255,255,255,0.65)\\" : \\"rgba(0,0,0,.85)\\" }; height: 30px; box-sizing: border-box; padding: 4px 6px; width: 1px; border: 1px solid rgba(0, 0, 0, 0.1); border-radius: 2px; min-width: 30px; flex-grow: 1; margin: 0; outline: 0; width: 100%; margin: 0px; margin-top: 4px; } ); interface TagProps extends ReturnType<AutocompleteGetTagProps> { label: string; } function Tag(props: TagProps) { const { label, onDelete, ...other } = props; return ( <div {...other}> <span>{label}</span> <CloseIcon onClick={onDelete} /> </div> ); } const StyledTag = styled(Tag)<TagProps>( ({ theme }) => display: flex; align-items: center; height: 24px; margin: 2px 0px; line-height: 22px; background-color: ${ theme.palette.mode === \\"dark\\" ? \\"rgba(255,255,255,0.08)\\" : \\"#fafafa\\" }; border: 1px solid ${theme.palette.mode === \\"dark\\" ? \\"#303030\\" : \\"#e8e8e8\\"}; border-radius: 2px; box-sizing: content-box; padding: 0 4px 0 10px; outline: 0; overflow: hidden; &:focus { border-color: ${theme.palette.mode === \\"dark\\" ? \\"#177ddc\\" : \\"#40a9ff\\"}; background-color: ${theme.palette.mode === \\"dark\\" ? \\"#003b57\\" : \\"#e6f7ff\\"}; } & span { overflow: hidden; white-space: nowrap; text-overflow: ellipsis; } & svg { font-size: 12px; cursor: pointer; padding: 4px; } ); const Listbox = styled(\\"ul\\")( ({ theme }) => width: 300px; margin: 2px 0 0; padding: 0; position: absolute; top: calc(100% + 5px); list-style: none; background-color: ${theme.palette.mode === \\"dark\\" ? \\"#141414\\" : \\"#fff\\"}; overflow: auto; max-height: 250px; border-radius: 4px; box-shadow: 0 2px 8px rgba(0, 0, 0, 0.15); z-index: 3; & li { padding: 5px 12px; display: flex; & span { flex-grow: 1; } & svg { color: transparent; } } & li[aria-selected=\'true\'] { background-color: ${theme.palette.mode === \\"dark\\" ? \\"#2b2b2b\\" : \\"#fafafa\\"}; font-weight: 600; & svg { color: #1890ff; } } & li.${autocompleteClasses.focused} { background-color: ${theme.palette.mode === \\"dark\\" ? \\"#003b57\\" : \\"#e6f7ff\\"}; cursor: pointer; & svg { color: currentColor; } } ); export default function ListUpdateSelector({ defaultSelectionStrings, allSelectionStrings, optionChangeHandler, style = {}, inputStyle = {}, }: { defaultSelectionStrings: string[]; allSelectionStrings: string[]; optionChangeHandler: (option: string[]) => void; style?: CSSProperties; inputStyle?: CSSProperties; }) { const selections = allSelectionStrings; const { getRootProps, getInputLabelProps, getInputProps, getTagProps, getListboxProps, getOptionProps, groupedOptions, value, focused, setAnchorEl, } = useAutocomplete({ id: \\"selector-hook\\", defaultValue: defaultSelectionStrings, multiple: true, options: selections, getOptionLabel: (option) => option, }); const optionChangeHandlerTakesEffect = useRef(false); useEffect(() => { // prevent handler is called on the first render. if (optionChangeHandlerTakesEffect.current) { optionChangeHandler(value); } else { optionChangeHandlerTakesEffect.current = true; } }, [value]); return ( <Root> <div className=\\"user-row-selector\\" style={{ position: \\"relative\\", ...style }} > <div {...getRootProps()}> <InputWrapper ref={setAnchorEl} className={focused ? \\"focused\\" : \\"\\"}> {value.map((option: string, index: number) => ( <span title={option}> <StyledTag label={option} {...getTagProps({ index })} /> </span> ))} <input {...getInputProps()} style={inputStyle} /> </InputWrapper> </div> {groupedOptions.length > 0 ? ( <Listbox {...getListboxProps()}> {(groupedOptions as string[]).map((option, index) => ( <li {...getOptionProps({ option, index })}> <span>{option}</span> <CheckIcon fontSize=\\"small\\" /> </li> ))} </Listbox> ) : null} </div> </Root> ); } ","title":"Multi-Selections","date":"2023-08-03T00:00:00.000Z","id":"blog0160","tag":"react","intro":"Record a multi selection component.","toc":true},{"content":" <center></center> - json-to-table is a simple library which converts nested json object into a table view. However, the library does not provide type defitions, so we need to declare it on our own: ts // dess.d.ts declare module \\"json-to-table\\" { function jsonToTable(jsonData: object): string[][]; export default jsonToTable; } - Sometimes an import is simply a string: ts declare module \\"*.pdf\\" { const src: string; export default src; } - Sometimes a library have export and export default: ts declare module \\"react-images\\" { export var Modal; export var ModalGateway; export default Carousel; } Remember to have json \\"esModuleInterop\\": true, in tsconfig.json for the above to work. ","title":"Declare Types for Non-typed 3rd Party Library","date":"2023-08-09T00:00:00.000Z","id":"blog0161","tag":"react","intro":"Record some exmaple how to declare types from library that is untyped.","toc":false},{"content":" <center></center> ts // snackbarUtils.ts import { ProviderContext, VariantType, useSnackbar } from \\"notistack\\"; let useSnackbarRef: ProviderContext; export const SnackbarUtilsConfigurator = () => { useSnackbarRef = useSnackbar(); return null; }; const option: Parameters<typeof useSnackbarRef.enqueueSnackbar>[1] = { preventDuplicate: true, autoHideDuration: 10000, }; export default { success(msg: string) { useSnackbarRef.enqueueSnackbar(msg, { variant: \\"success\\", ...option }); }, warning(msg: string) { useSnackbarRef.enqueueSnackbar(msg, { variant: \\"warning\\", ...option }); }, info(msg: string) { useSnackbarRef.enqueueSnackbar(msg, { variant: \\"info\\", ...option }); }, error(msg: string) { useSnackbarRef.enqueueSnackbar(msg, { variant: \\"error\\", ...option }); }, toast(msg: string, variant: VariantType = \\"default\\") { useSnackbarRef.enqueueSnackbar(msg, { variant, ...option }); }, }; Next in our file wrapping the <App/> element, we add: ts import { SnackbarProvider } from \'notistack\'; ... <SnackbarProvider maxSnack={2} anchorOrigin={{ horizontal: \'left\', vertical: \\"bottom\\" }} > <SnackbarUtilsConfigurator /> ... <App /> </SnackbarProvider> ","title":"Snackbar Utils","date":"2023-08-10T00:00:00.000Z","id":"blog0162","tag":"react","intro":"Record a configuraton for snackbar utils.","toc":false},{"content":" <style> img { width: 100%; } </style> #### Usage and Examples Consider the following path: none http://localhost:3000/#/buyer/order/6347b89b67762f48a700d4be/contract/64d7912d33184f49a6346f20 We print the result in chrome debug console: <Center> <a href=\\"/assets/tech/163/001.png\\" taget=\\"_blank\\"> <img src=\\"/assets/tech/163/001.png\\"/> </a> </Center> #### Code Implmentation ts // usePathUtils.ts import { useLocation } from \\"react-router-dom\\"; export default () => { const { pathname } = useLocation(); const paramRightAfter = (rightAfterKeyword: string) => { const matchingRegex = new RegExp( (?<=${rightAfterKeyword}).*?(?=($|\\\\/)), \\"g\\" ); return pathname.match(matchingRegex)?.[0] || \\"\\"; }; const getPathUpto = (stopKeyword: string) => { const matchingRegex = new RegExp(^.*${stopKeyword}, \\"g\\"); return pathname.match(matchingRegex)?.[0] || \\"\\"; }; return { paramRightAfter, getPathUpto }; }; Next for convenience we create another hook so that instead of calling paramRightAfter and putting the desired string as param sporadically, we have a single source of truth: ts // useGetPathParams.ts import usePathUtils from \\"./usePathUtils\\"; export default () => { const { paramRightAfter } = usePathUtils(); return { projectOid: paramRightAfter(\\"/order/\\"), programmeOid: paramRightAfter(\\"/contract/\\"), section: paramRightAfter(\\"/order/.*?/\\"), }; }; But any change in the path may trigger rerender for components using this hook which may just use projectId but not section. Therefore we may consider **_putting path params in redux_** and let redux control which component to rerender. #### Ultimate Version We invoke the following hook at the first routing component (where we have useLocation hook): js //useGenPathParams.ts import { useEffect } from \\"react\\"; import { useLocation } from \\"react-router-dom\\"; import { useAppDispatch } from \\"../redux/app/hook\\"; import applicationSlice from \\"../redux/slice/applicationSlice\\"; import usePathUtils from \\"./usePathUtils\\"; export default () => { const { pathname } = useLocation(); const dispatch = useAppDispatch(); const { paramRightAfter } = usePathUtils(); useEffect(() => { dispatch( applicationSlice.actions.updatePathParams({ projectOid: paramRightAfter(\\"/order/\\"), programmeOid: paramRightAfter(\\"/contract/\\"), section: paramRightAfter(\\"/order/.*?/\\"), mailchainOid: paramRightAfter(\\"/mailchain/\\"), }) ); }, [pathname]); }; Next we create a reducer in our slice that stores path params: js updatePathParams: (state, action: PayloadAction<ApplicationSliceState[\\"pathParams\\"]>) => { state.pathParams = { ...state.pathParams, ...action.payload }; }, Finally we adjust our existing useGetPathParams: js // useGetPathParams.ts import { useAppSelector } from \\"../redux/app/hook\\" import { ApplicationSliceState } from \\"../redux/slice/applicationSlice\\" export default (paramKey: keyof ApplicationSliceState[\\"pathParams\\"]) => { \\treturn useAppSelector(s => s.application.pathParams?.[paramKey]); } which minimizes potential rerender problem while using this hook! ","title":"Getting Path Parameters","date":"2023-08-11T00:00:00.000Z","id":"blog0163","tag":"react","intro":"I used to use useRouteMatch in react-router-dom v5, which has been changed completely in v6 into something called useMatch or useMatches. We talk about building the param searching function on our own to get rid of these unhandy \\"black boxes\\".","toc":true},{"content":" #### Repository - https://github.com/machingclee/2023-08-13-serverless-google-login #### Result <Center> <iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/NFqS6AXw-jM\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" allowfullscreen></iframe> </Center> #### Backend ##### CORS: Restrict Origins to have Accesses to the API ts-1 import express, { Request } from \\"express\\"; import { google } from \\"googleapis\\"; import jwt from \\"jsonwebtoken\\"; import cors from \'cors\'; import errorHandler from \\"./middlewares/errorHandler\\"; const allowlist = [\'http://localhost:3000\'] const corsOptionsDelegate = (req, callback) => { var corsOptions; if (allowlist.indexOf(req.header(\'Origin\')) > -1) { corsOptions = { origin: true } // reflect (enable) the requested origin in the CORS response } else { corsOptions = { origin: false } // disable CORS for this request } callback(null, corsOptions) // callback expects two parameters: error and options } Later we will bring the delegate into use by app.use(cors(corsOptionsDelegate)); ##### .env-cmdrc, the Environment Variables We create a .env-cmdrc for environment variables: json { \\"default\\": { \\"PORT\\": 8080 }, \\"production\\": { \\"GOOGLE_CLIENT_ID\\": \\"XXX.com\\", \\"GOOGLE_CLIENT_SECRET\\": \\"YYY\\", \\"GOOGLE_API_REDIRECT\\": \\"http://localhost:8080/login-google\\", \\"ALLOWED_EMAILS\\": [\\"machingclee@gmail.com\\", \\"james.lee@wonderbricks.com\\"], \\"JWT_SECRET\\": \\"SECRET!\\", \\"FRONTEND_URL\\": \\"http://localhost:3000\\" } } The script (remember to yarn add env-cmd) json \\"scripts\\": { \\"start\\": \\"env-cmd -f .env-cmdrc -e default,production nodemon --exec ts-node src/app.ts\\", ... } will set default and production configs into process.env. Then: ##### Get Url for Choosing Google Account ts-17 const { GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, ALLOWED_EMAILS, GOOGLE_API_REDIRECT, JWT_SECRET, FRONTEND_URL } = process.env; const ALLOWED_EMAILS_ = ALLOWED_EMAILS.split(\\",\\"); const app = express(); app.use(cors(corsOptionsDelegate)); const oAuth2Client = new google.auth.OAuth2( { clientId: GOOGLE_CLIENT_ID, clientSecret: GOOGLE_CLIENT_SECRET, redirectUri: GOOGLE_API_REDIRECT } ); function getAuthUrl() { return oAuth2Client.generateAuthUrl({ access_type: \\"offline\\", prompt: \\"consent\\", scope: [ \\"https://www.googleapis.com/auth/userinfo.email\\", \\"https://www.googleapis.com/auth/userinfo.profile\\" ] }); } app.get(\\"/login\\", async (req, res) => { const url = getAuthUrl() res.status(200).json({ url }); }); ##### The O-Auth Credential Setting in Google Cloud for Redirect Url - Here is a tricky part, upon successful request to /login-in, we will get an url (see line 23 for the frontend part below) to redirect frontend user to: ![](/assets/tech/164/002.png) via a window.location.href = url. - After an identity is chosen, user will then be redirected to an url specified here: ![](/assets/tech/164/001.png) ts-54 app.get( \\"/login-google\\", async (req: Request<any, any, any, { code: string }>, res, next) => { const { code } = req.query; const { tokens } = await oAuth2Client.getToken(code); oAuth2Client.credentials = tokens; const oauth2 = google.oauth2(\\"v2\\"); const res_ = await oauth2.userinfo.v2.me.get({ auth: oAuth2Client, }); const { email } = res_.data; const hasRight = ALLOWED_EMAILS_.includes(email); if (!hasRight) { return next( Only ${ALLOWED_EMAILS_.join(\\", \\")} has access to this project. ); } const token = jwt.sign( { email }, JWT_SECRET, { expiresIn: 60 * 60 } ); res.redirect(${FRONTEND_URL}/token/${token}); } ); - We redirect user back to our frontend, and the frontend needs to be able to get the token from the url. - In our case we redirect user to ${FRONTEND_URL}/token/${token}, a query string at the end ?token=${token} is also fine. ##### Authenticate the jwt Token - We save the token in local storage or cookie. - On refresh, if that token is found in the frontend, we authenticate the token (line 27 of the frontend part) via: ts-80 app.get( \\"/authenticate\\", async (req: Request<any, any, any, { token: string }>, res, next) => { const { token } = req.query; try { const decoded = jwt.verify(token, JWT_SECRET); res.json({ success: true }); } catch (err) { next(err); } } ); app.use(errorHandler); const PORT = Number(process.env[\\"PORT\\"]); app.listen(PORT, () => { console.log(Server started on http://localhost:${PORT}); }); Here our errorHandler is as simple as ts export default (err, req, res, next) => { if (err) { res.json({ success: false, errorMessage: err }); } }; Any error that is caught can be passed to this middleware via next(err) (see line 88 above). #### Frontend Our routing is as simple as ts <Routes> <Route path=\\"/\\" element={Login()} /> <Route path=\\"/token/*\\" element={Login()} /> </Routes> And the frontend is simply: ts-1 // Login.tsx import { Grid, Button, Container } from \\"@mui/material\\"; import axio from \\"axios\\"; import usePathUtils from \\"../hooks/usePathUtils\\"; import { useEffect, useRef, useState } from \\"react\\"; import { useNavigate } from \\"react-router-dom\\"; export default () => { const { paramRightAfter } = usePathUtils(); const token = paramRightAfter(\\"/token/\\"); const navigate = useNavigate(); const [accessToken, setAccessToken] = useState(\\"\\"); const [loginSuccess, setLoginSucess] = useState(false); const tokenFetched = useRef(false); const loginHandler = async () => { const res = await axio.get<{ url: string }>( \\"http://localhost:8080/login\\" ); const { url } = res.data; window.location.href = url; } useEffect(() => { if (token && !tokenFetched.current) { setAccessToken(token); tokenFetched.current = true; navigate(\\"/\\"); axio.get<{ success: boolean }>( \\"http://localhost:8080/authenticate?token=\\" + token ) .then((res) => { const { success } = res.data; setLoginSucess(success); }); } }, [token]); return ( <Container> <Grid item> <div style={{ marginBottom: 10, marginTop: 10 }}> Login Status: {loginSuccess ? \\"logged in\\" : \\"not logged in \\"} </div> <Button onClick={loginHandler} variant=\\"outlined\\">Login</Button> </Grid> </Container> ); }; ","title":"Google Login","date":"2023-08-13T00:00:00.000Z","id":"blog0164","tag":"express, cors","intro":"A simple backend that perform google authentication. I personally use this to restrict users who can access my project.","toc":true},{"content":" <Center></Center> Consider a file uploader called Uploader: ts export type UploaderHandle = { fileIdArrs: number[], postFilesToServer: () => Promise<number[]>, } type UploaderProps = { documents: SomeFile[], } const Uploader = forwardRef<UploaderHandle, UploaderProps>((props, ref) => { const { documents } = props useImperativeHandle(ref, () => ({ fileIdArrs, postFilesToServer, })) ... const [fileIdArrs, setfileIdArrs] = useState<number[]>([]); const postFilesToServer = async () => { for (let file of fileList) { await uploadFile(file); ... } } }) - We want the method to be triggered by the parent of <Uploader/> instead. - Sometimes it is done for the purpose of code separation (e.g., its parent is already complicated enough). Now consider a parent that makes use of <Uploader/>: ts-1 export default function SomeComponent() { const uploaderRef = useRef<UploaderHandle>(null); ... const saveHandler = async () => { const fileIds = await uploaderRef.current?.postFilesToServer(); ... } return( ... <Box> <Uploader documents={documents} ref={uploaderRef} /> </Box> ) } In line 5 we use the method postFilesToServer which belongs to its child <Uploader/>. ","title":"forwardRef and useImperativeHandle","date":"2023-08-14T00:00:00.000Z","id":"blog0165","tag":"react","intro":"A method to pass complicated function in a component to its parent.","toc":false},{"content":" <Center></Center> ts // useRerender.ts import { ReactNode, useState } from \\"react\\"; export default () => { const [rerenderFlag, setRerenderFlag] = useState(true); const rerender = () => { setRerenderFlag(false); setTimeout(() => setRerenderFlag(true), 1); }; const Rerender = ({ children }: { children: ReactNode }) => { return <>{rerenderFlag && children}</>; }; return { rerender, Rerender }; }; The trick is simply executing setRerenderFlag(true) later by letting it execute in callback-queue, such a callback is created by setTimeout and it will be executed once every task in the call stack are cleared. For example, what would be the result of the following? js setTimeout(() => { console.log(\\"I come from timeout\\"); }, 1); for (let i = 0; i < 100; i++) { console.log(\\"I come from for loop\\"); } ![](/assets/tech/166/001.png) <Center></Center> Why is that? A very clear explanation can be found in: - [[Part 1] \u6240\u4ee5\u8aaa event loop \u5230\u5e95\u662f\u4ec0\u9ebc\u73a9\u610f\u5152\uff1f| Philip Roberts | JSConf EU](https://www.youtube.com/watch?v=8aGhZQkoFbQ) - [[Part 2] Further Adventures of the Event Loop - Erin Zimmer - JSConf EU 2018](https://www.youtube.com/watch?v=u1kqx6AenYw) Understanding how v8 works can essentially help investigate how nodejs works, and after that we can appreciate how single-threaded model can handle high concurrency problem. Similar idea can be brought to other languages like Java, and from that we are lead to the use of WebFlux in the realm of reactive programming. The word **_single-threaded_** may be \\"misleading\\" as nodejs is not always single-threaded, but the main thread will not be blocked by designating some tasks to other thread like DNS-resolution or file io (which inevitably blocks!). ","title":"Hook to Rerender Component By Making use of the Single Threaded Event Driven Model Behind v8","date":"2023-08-15T00:00:00.000Z","id":"blog0166","tag":"react","intro":"A simple component and function that helps rerender a component effectively.","toc":false},{"content":" js // boxShadow.ts export default { SHADOW_01: \\"rgba(149, 157, 165, 0.2) 0px 8px 24px\\", SHADOW_02: \\"rgba(100, 100, 111, 0.2) 0px 7px 29px 0px\\", SHADOW_03: \\"rgba(0, 0, 0, 0.15) 1.95px 1.95px 2.6px\\", SHADOW_04: \\"rgba(0, 0, 0, 0.35) 0px 5px 15px\\", SHADOW_05: \\"rgba(0, 0, 0, 0.16) 0px 1px 4px\\", SHADOW_06: \\"rgba(0, 0, 0, 0.24) 0px 3px 8px\\", SHADOW_07: \\"rgba(99, 99, 99, 0.2) 0px 2px 8px 0px\\", SHADOW_08: \\"rgba(0, 0, 0, 0.02) 0px 1px 3px 0px, rgba(27, 31, 35, 0.15) 0px 0px 0px 1px\\", SHADOW_09: \\"rgba(0, 0, 0, 0.1) 0px 4px 12px\\", SHADOW_10: \\"rgba(0, 0, 0, 0.25) 0px 54px 55px,rgba(0, 0, 0, 0.12) 0px -12px 30px,rgba(0, 0, 0, 0.12) 0px 4px 6px,rgba(0, 0, 0, 0.17) 0px 12px 13px,rgba(0, 0, 0, 0.09) 0px -3px 5px\\", SHADOW_11: \\"rgba(0, 0, 0, 0.05) 0px 6px 24px 0px,rgba(0, 0, 0, 0.08) 0px 0px 0px 1px\\", SHADOW_12: \\"rgba(0, 0, 0, 0.16) 0px 10px 36px 0px,rgba(0, 0, 0, 0.06) 0px 0px 0px 1px\\", SHADOW_13: \\"rgba(17, 12, 46, 0.15) 0px 48px 100px 0px\\", SHADOW_14: \\"rgba(255, 255, 255, 0.1) 0px 1px 1px 0px inset,rgba(50, 50, 93, 0.25) 0px 50px 100px -20px,rgba(0, 0, 0, 0.3) 0px 30px 60px -30px\\", SHADOW_15: \\"rgba(50, 50, 93, 0.25) 0px 50px 100px -20px,rgba(0, 0, 0, 0.3) 0px 30px 60px -30px\\", SHADOW_16: \\"rgba(50, 50, 93, 0.25) 0px 50px 100px -20px,rgba(0, 0, 0, 0.3) 0px 30px 60px -30px\\", SHADOW_17: \\"rgba(50, 50, 93, 0.25) 0px 13px 27px -5px,rgba(0, 0, 0, 0.3) 0px 8px 16px -8px\\", SHADOW_18: \\"rgba(50, 50, 93, 0.25) 0px 2px 5px -1px,rgba(0, 0, 0, 0.3) 0px 1px 3px -1px\\", SHADOW_19: \\"rgba(50, 50, 93, 0.25) 0px 6px 12px -2px,rgba(0, 0, 0, 0.3) 0px 3px 7px -3px\\", SHADOW_20: \\"rgba(50, 50, 93, 0.25) 0px 13px 27px -5px,rgba(0, 0, 0, 0.3) 0px 8px 16px -8px\\", SHADOW_21: \\"rgba(50, 50, 93, 0.25) 0px 30px 60px -12px,rgba(0, 0, 0, 0.3) 0px 18px 36px -18px\\", SHADOW_22: \\"rgba(50, 50, 93, 0.25) 0px 50px 100px -20px,rgba(0, 0, 0, 0.3) 0px 30px 60px -30px\\", SHADOW_23: \\"rgba(0, 0, 0, 0.12) 0px 1px 3px,rgba(0, 0, 0, 0.24) 0px 1px 2px\\", SHADOW_24: \\"rgba(0, 0, 0, 0.16) 0px 3px 6px, rgba(0, 0, 0, 0.23) 0px 3px 6px\\", SHADOW_25: \\"rgba(0, 0, 0, 0.25) 0px 14px 28px,rgba(0, 0, 0, 0.22) 0px 10px 10px\\", SHADOW_26: \\"rgba(0, 0, 0, 0.3) 0px 19px 38px,rgba(0, 0, 0, 0.22) 0px 15px 12px\\", SHADOW_27: \\"rgba(60, 64, 67, 0.3) 0px 1px 2px 0px,rgba(60, 64, 67, 0.15) 0px 2px 6px 2px\\", SHADOW_28: \\"rgba(60, 64, 67, 0.3) 0px 1px 2px 0px,rgba(60, 64, 67, 0.15) 0px 1px 3px 1px\\", SHADOW_29: \\"rgba(0, 0, 0, 0.05) 0px 0px 0px 1px\\", SHADOW_30: \\"rgba(0, 0, 0, 0.05) 0px 1px 2px 0px\\", SHADOW_31: \\"rgba(0, 0, 0, 0.1) 0px 1px 3px 0px,rgba(0, 0, 0, 0.06) 0px 1px 2px 0px\\", SHADOW_32: \\"rgba(0, 0, 0, 0.1) 0px 4px 6px -1px,rgba(0, 0, 0, 0.06) 0px 2px 4px -1px\\", SHADOW_33: \\"rgba(0, 0, 0, 0.1) 0px 10px 15px -3px,rgba(0, 0, 0, 0.05) 0px 4px 6px -2px\\", SHADOW_34: \\"rgba(0, 0, 0, 0.1) 0px 20px 25px -5px,rgba(0, 0, 0, 0.04) 0px 10px 10px -5px\\", SHADOW_35: \\"rgba(0, 0, 0, 0.25) 0px 25px 50px -12px\\", SHADOW_36: \\"rgba(0, 0, 0, 0.1) 0px 0px 5px 0px,rgba(0, 0, 0, 0.1) 0px 0px 1px 0px\\", SHADOW_37: \\"rgba(0, 0, 0, 0.07) 0px 1px 2px,rgba(0, 0, 0, 0.07) 0px 2px 4px,rgba(0, 0, 0, 0.07) 0px 4px 8px,rgba(0, 0, 0, 0.07) 0px 8px 16px,rgba(0, 0, 0, 0.07) 0px 16px 32px,rgba(0, 0, 0, 0.07) 0px 32px 64px\\", SHADOW_38: \\"rgba(0, 0, 0, 0.2) 0px 18px 50px -10px\\", SHADOW_39: \\"rgba(0, 0, 0, 0.1) 0px 10px 50px\\", SHADOW_40: \\"rgba(0, 0, 0, 0.04) 0px 3px 5px\\", SHADOW_41: \\"rgba(67, 71, 85, 0.27) 0px 0px 0.25em,rgba(90, 125, 188, 0.05) 0px 0.25em 1em\\", SHADOW_42: \\"rgba(14, 30, 37, 0.12) 0px 2px 4px 0px,rgba(14, 30, 37, 0.32) 0px 2px 16px 0px\\", SHADOW_43: \\"rgba(0, 0, 0, 0.2) 0px 12px 28px 0px,rgba(0, 0, 0, 0.1) 0px 2px 4px 0px,rgba(255, 255, 255, 0.05) 0px 0px 0px 1px inset\\", SHADOW_44: \\"rgba(0, 0, 0, 0.15) 0px 5px 15px 0px\\", SHADOW_45: \\"rgba(136, 165, 191, 0.48) 6px 2px 16px 0px,rgba(255, 255, 255, 0.8) -6px -2px 16px 0px\\", SHADOW_46: \\"rgba(17, 17, 26, 0.05) 0px 1px 0px,rgba(17, 17, 26, 0.1) 0px 0px 8px\\", SHADOW_47: \\"rgba(17, 17, 26, 0.1) 0px 0px 16px\\", SHADOW_48: \\"rgba(17, 17, 26, 0.05) 0px 4px 16px,rgba(17, 17, 26, 0.05) 0px 8px 32px\\", SHADOW_49: \\"rgba(17, 17, 26, 0.1) 0px 4px 16px,rgba(17, 17, 26, 0.05) 0px 8px 32px\\", SHADOW_50: \\"rgba(17, 17, 26, 0.1) 0px 1px 0px, rgba(17, 17, 26, 0.1) 0px 8px 24px, rgba(17, 17, 26, 0.1) 0px 16px 48px\\", SHADOW_51: \\"rgba(17, 17, 26, 0.1) 0px 4px 16px,rgba(17, 17, 26, 0.1) 0px 8px 24px,rgba(17, 17, 26, 0.1) 0px 16px 56px\\", SHADOW_52: \\"rgba(17, 17, 26, 0.1) 0px 8px 24px,rgba(17, 17, 26, 0.1) 0px 16px 56px,rgba(17, 17, 26, 0.1) 0px 24px 80px\\", SHADOW_53: \\"rgba(0, 0, 0, 0.15) 0px 15px 25px,rgba(0, 0, 0, 0.05) 0px 5px 10px\\", SHADOW_54: \\"rgba(0, 0, 0, 0.08) 0px 4px 12px\\", SHADOW_55: \\"rgba(0, 0, 0, 0.15) 0px 2px 8px\\", SHADOW_56: \\"rgba(0, 0, 0, 0.1) -4px 9px 25px -6px\\", SHADOW_57: \\"rgba(0, 0, 0, 0.2) 0px 20px 30px\\", SHADOW_58: \\"rgba(0, 0, 0, 0.25) 0px 0.0625em 0.0625em,rgba(0, 0, 0, 0.25) 0px 0.125em 0.5em,rgba(255, 255, 255, 0.1) 0px 0px 0px 1px inset\\", SHADOW_59: \\"rgba(0, 0, 0, 0.09) 0px 3px 12px\\", SHADOW_60: \\"rgba(0, 0, 0, 0.05) 0px 0px 0px 1px,rgb(209, 213, 219) 0px 0px 0px 1px inset\\", SHADOW_61: \\"rgba(9, 30, 66, 0.25) 0px 1px 1px,rgba(9, 30, 66, 0.13) 0px 0px 1px 1px\\", SHADOW_62: \\"rgba(9, 30, 66, 0.25) 0px 4px 8px -2px,rgba(9, 30, 66, 0.08) 0px 0px 0px 1px\\", }; ","title":"Box Shadow","date":"2023-08-27T00:00:00.000Z","id":"blog0168","tag":"react","intro":"Record a list of box-shadow in a json object.","toc":false},{"content":" #### Life Time The compiler has 3 rules for the lifetime: - The compiler assigns a lifetime parameter to **_each_** parameter that\'s a reference - If there is exactly **_one_** input lifetime parameter, that lifetime is assigned to **_all_** output lifetime parameters. - If there are **_multiple_** input lifetime parameters, **_but_** one of them is &self or &mut self because this is a method, the lifetime of self is assigned to **_all_** output lifetime parameters Note that - A liftime comes from an input reference is called an **input lifeime**; - that comes from an output reference is called an **output lifetime**. To sum up, we always expect: $$ \\\\text{output lifetime}=\\\\min_\\\\alpha\\\\big\\\\{\\\\text{input_lifeime}_{\\\\!\\\\!\\\\!\\\\!\\\\alpha}\\\\big\\\\}. $$ #### Result Type We treat Result type like a Promise in javascript, in which we have - return Ok(...); = resolve(...); and - return Err(...); = reject(...);. The generic type parameters of Result following the rule: rust Result<type returned by Ok, type returned by Err> rust struct Config<\'a> { query: &\'a String, filename: &\'a String, } impl<\'a> Config<\'a> { fn new(args: &\'a [String]) -> Result<Config, &str> { if args.len() < 3 { return Err(\\"not enough arguments\\"); } let query = &args[1]; let filename = &args[2]; Ok(Config { query, filename }) } } There are two ways to squeeze the Config out of Result enum: ##### Extraction Method 1: Squeezing by Unwrap Next in our programme if we unwrap and handle the error gracefully: rust-1 let args: Vec<String> = env::args().collect(); let config = Config::new(&args).unwrap_or_else(|err| { println!(\\"Problem parsing arguments: {}\\", err); process::exit(1); }); run(config); Then from line 6 onwards our config has been converted from Result to Config. > **Take away.** We can squeeze Result<T> to T by executing unwrap() once. ##### Extraction Method 2: Assigning by Some Alternatively, it is conventional to write a placeholder null variable and assign value into it when something exists, that pattern in rust is implemented by Option enum and Some object: rust let mut config: Option<Config> = None; let result = Config::new(&args); if let Ok(config_) = result { config = Some(config_); }; if let Some(config_unwrapped) = config { run(config_unwrapped); }; We didn\'t handle the error and error message. We can combine unwrap_or_else and the assignment = Some(config_) approach depending on the ways of doing things. If we want multiple nulls checking, rust if let (Some(a_), Some(b_)) = (a, b) { // do something } For example, rust if let (Some(a), Some(b)) = (Some(7), Some(8)) { println!(\\"Result: {}\\", a * b); } prints 56. #### Throwing Arbitrary Error Consider the following function: rust fn run(config: Config) -> Result<(), Box<dyn Error>> { let query = config.query; let filename = config.filename; let contents = fs::read_to_string(filename)?; println!(\\"{}\\", contents); Ok(()) } - fs::read_to_string returns a Result object. If we want to throw an Error and let the function call in the previous stack frame to handle it, we just add a ?. - The Box<dyn Error> in the return type serves the same purpose as Java\'s java public void function someFunction() throws Exception {}; #### Handle the Final Execution Error Assume that we have: rust-1 fn main() { let args: Vec<String> = env::args().collect(); let config = Config::new(&args).unwrap_or_else(|err| { println!(\\"[Problem parsing arguments] {}\\", err); process::exit(1); }); Then the following two are equivalent: rust-7 run(config).unwrap_or_else(|err| { println!(\\"Application Error: {}\\", err); process::exit(1); }); } rust-7 if let Err(err) = run(config) { println!(\\"Application Error: {}\\", err); process::exit(1); } } #### Second Visit to the Multi-threading Web Server Example In Rust Book My energy got exhausted at the first time I go with rust book to the last chapter (you can see how much detail I have recorded [**_here_**](/blog/article/Summarize-Rust-Beginning-Tutorial-by-a-Simplified-Multithreading-Web-Server#Implementation-of-ThreadPool-for-Multithreaded-Web-Server) before the last chapter on web server!). This time I grabbed and digested detail in a deeper understanding. I try to record the detail in this blog post. ##### fn main() We start off by writing down the general structure of the program in main function, the intersting part lies inside lib.rs, i.e., how we define ThreadPool. rust use std::io::prelude::*; use std::net::TcpListener; use std::net::TcpStream; use std::time::Duration; use std::{fs, thread}; use web_server::ThreadPool; fn main() { let listener = TcpListener::bind(\\"127.0.0.1:7878\\").unwrap(); let pool = ThreadPool::new(4); for stream in listener.incoming() { let stream = stream.unwrap(); pool.execute(|| { handle_connection(stream); }); } } fn handle_connection(mut stream: TcpStream) { let get = b\\"GET / HTTP/1.1\\\\r\\\\n\\"; let sleep = b\\"GET /sleep HTTP/1.1\\\\r\\\\n\\"; let mut buffer = [0; 1024]; stream.read(&mut buffer).unwrap(); println!(\\"{}\\", buffer.starts_with(get)); let (status_line, filename) = if buffer.starts_with(get) { (\\"HTTP/1.1 200 OK\\", \\"hello.html\\") } else if buffer.starts_with(sleep) { println!(\\"{}\\", \\"sleeping...\\"); thread::sleep(Duration::from_secs(5)); println!(\\"{}\\", \\"awake!\\"); (\\"HTTP/1.1 200 OK\\", \\"hello.html\\") } else { (\\"HTTP/1.1 404 NOT FOUND\\", \\"404.html\\") }; let contents = fs::read_to_string(filename).unwrap(); let response = format!( \\"{}\\\\r\\\\nContent-Length: {}\\\\r\\\\n\\\\r\\\\n{}\\", status_line, &contents.len(), &contents ); stream.write(response.as_bytes()).unwrap(); stream.flush().unwrap(); } ##### lib.rs, the web_server::ThreadPool lib.rs is a single module which by default is imported by calling rust project_name::{what\'s defined as pub in lib.rs} Inside our lib.rs we have rust-1 use std::option::Option; use std::{ sync::{ mpsc::{self, Receiver}, Arc, Mutex, }, thread, }; pub struct ThreadPool { workers: Vec<Worker>, sender: mpsc::Sender<Message>, } I would like to pin the takeways in this program (instead of introducing what\'s the target and what\'s to be done in this example). rust-14 // property of a mutable reference is at most mutable reference // we cannot move it out, moving is not a mutation impl Drop for ThreadPool { fn drop(&mut self) { println!(\\"Terminating all workers\\"); for _ in &self.workers { self.sender.send(Message::Terminate).unwrap(); } for worker in &mut self.workers { **_1st Takeaway._** We would write line 23 as rust for worker in self.workers instead by our first instinct, an error will pop up: none self.workers moved due to this implicit call to .into_iter() into_iter takes ownership of the receiver self, which moves self.workers - The property of a **_mutable reference_** is **_at most_** a mutable reference (which we need to specify). - The reason is that .into_iter(self) is implicitly called, which moves our self.workers into a function that generates iterator. - Although self is a **_mutable_** reference, moving its property is not a mutation, a move will _drain the memory out_ by assigning the source property to null_ptr and assign that original pointer to the target that we move into. rust-24 println!(\\"Shutting down worker {}\\", worker.id); if let Some(thread) = worker.thread.take() { thread.join().unwrap(); } } } } **_2nd Takeaway._** Note that worker is a property of a mutable reference self.workers, hence again worker itself is at most a mutable reference. However, we want to call worker.thread.join().unwrap(), the function join has signature join(self), i.e., worker.thread will be moved. The usual _trick_ in rust is to wrap T into Option<T>, then Option<T>::take() allows moving the Some<T> out by careful unsafe rust implementation. rust-32 enum Message { Job(Box<dyn FnOnce() + Send + \'static>), Terminate, } **_3nd Takeway._** In the course of coding this example, instead of implementing enum Message, what we originally implemented is simply rust type Job = Box<dyn FnOnce() + Send + \'static> and in line 42 has been rust let (sender, receiver) = mpsc::channel::<Job>(); Because later on we not only want to signal a Job to the threads, we also want to signal a Termination to the threads. In plain javascript we can naively implement this by sending [\\"job\\", job] and [\\"terminate\\", null] to the workers, i.e., we append some field to distinguish the messages. In rust approach, we treat that field\'s as enum **_variants_**: - Job(job trait) $\\\\longleftrightarrow$ [\\"job\\", job] (job is a closure) - Terminate $\\\\longleftrightarrow$ [\\"terminate\\", null] and we group the variants in an enum class: rust enum Message { Job(Box<dyn FnOnce() + Send + \'static>), Terminate, } rust-36 impl ThreadPool { pub fn new(size: usize) -> Self { assert!(size > 0); let mut workers = Vec::with_capacity(size); let (sender, receiver) = mpsc::channel::<Message>(); let receiver = Arc::new(Mutex::new(receiver)); for id in 0..size { workers.push(Worker::new(id, receiver.clone())); } ThreadPool { workers, sender } } pub fn execute<F>(&self, f: F) where F: FnOnce() + Send + \'static, { let job = Message::Job(Box::new(f)); self.sender.send(job).unwrap(); } } struct Worker { id: usize, thread: Option<thread::JoinHandle<()>>, } impl Worker { fn new(id: usize, receiver: Arc<Mutex<Receiver<Message>>>) -> Worker { let thread = thread::spawn(move || loop { let msg = receiver.clone().lock().unwrap().recv().unwrap(); match msg { Message::Job(job) => { println!(\\"Worker {} got a job; excuting.\\", id); job(); } Message::Terminate => { println!(\\"Terminated!\\"); break; } } }); Worker { id, thread: Some(thread), } } } Finally: - Arc is a multi-threaded version of Rc for multiple reference to the same wrappered object. - Mutex is to block access from other threads to the wrapped object. ","title":"Rust Study Notes","date":"2023-09-04T00:00:00.000Z","id":"blog0169","tag":"rust","intro":"This is a beginner notes.","toc":true},{"content":" #### Implementation js // useScroll.ts import { useEffect, useRef, useState } from \\"react\\"; export default (props: { up: () => void, down: () => void }) => { const scrollRef = useRef < HTMLDivElement > null; const { up, down } = props; const prevScrollTop = (useRef < number) | (null > null); useEffect(() => { const scrollHandler = () => { if (!scrollRef.current) { return; } if (prevScrollTop.current === null) { prevScrollTop.current = scrollRef.current.scrollTop; } else { const currScrollTop = scrollRef.current.scrollTop; // +ve => move down, -ve => move up const scrollDistance = currScrollTop - prevScrollTop.current; if (scrollDistance > 0) { down(); } else if (scrollDistance < 0) { up(); } prevScrollTop.current = currScrollTop; } }; const scrollAssginmentInterval = setInterval(() => { if (scrollRef.current) { scrollRef.current.addEventListener(\\"scroll\\", scrollHandler); clearInterval(scrollAssginmentInterval); } }, 100); return () => { scrollRef?.current?.removeEventListener(\\"scroll\\", scrollHandler); }; }, []); return { scrollRef }; }; #### Usage Here we want: - A sticky-positioned div disappears when we scroll down; - Show the div again when we scroll up. js export default () = > { \\tconst [collapseTitle, setCollapseTitle] = useState(false); \\tconst closedRef = useRef<boolean>(false); \\tconst { scrollRef } = useScroll({ \\t\\tup: () => { \\t\\t\\tif (closedRef.current) { \\t\\t\\t\\tconsole.log(\\"open\\"); \\t\\t\\t\\tsetCollapseTitle(false); \\t\\t\\t\\tclosedRef.current = false; \\t\\t\\t} \\t\\t}, \\t\\tdown: () => { \\t\\t\\t\\tif (!closedRef.current) { \\t\\t\\t\\tconsole.log(\\"closed\\"); \\t\\t\\t\\tsetCollapseTitle(true); \\t\\t\\t\\tclosedRef.current = true; \\t\\t\\t} \\t\\t} \\t}); return ( <div id=\\"mails-container\\" ref={scrollRef} style={{ overflowY: \\"scroll\\" }}> ... </div> ) } - Here we have used a storage closedRef to make sure the scrolling event is just dispatched once. - We can remove it if we wish to trigger the event every time the user scrolls. ","title":"Scroll Up and Down Events in React","date":"2023-09-02T00:00:00.000Z","id":"blog0170","tag":"react","intro":"Record a hook to listen to scrolling-up and scrolling-down events, and an easy way to inject our callback to this events using this hook.","toc":true},{"content":" <style> img { max-width: 100% } </style> #### Streams ##### Simple File-Upload UI We start off by having a simple input UI: js import axios from \\"axios\\"; import { useState } from \\"react\\"; export default () => { const [files, setFiles] = useState<FileList | null>(null); const uploadSelectedHandler = (e: React.ChangeEvent<HTMLInputElement>) => { if (e.target.files) { setFiles(e.target.files); } }; const uploadClickedHanlder = async () => { if ((files?.length || 0) > 0) { uplaodToServer({ oid: \\"someid\\", file: files![0] }); } }; const uplaodToServer = async ({ oid, file }: { oid: string; file: File }) => { const formData = new FormData(); formData.append(\\"file_01\\", file); formData.append(\\"oid\\", oid); const response = await axios.post( \\"http://localhost:8080/stream\\", formData ); return response.data; } return ( <div> <input type=\\"file\\" accept=\\"image/*\\" onChange={uploadSelectedHandler} /> <button onClick={uploadClickedHanlder}> Upload </button> </div> ) } We have: ![](/assets/tech/171/001.png) <center></center> ##### Files from FormData ###### Simple Text File Upload As is always we upload files by FormData. We will study the stream of data by using this route: js app.post(\\"/stream\\", async (req, res) => { const outputStream = createWriteStream(\\"server_received.txt\\"); req.pipe(outputStream); }); Let\'s upload a text file with the following content: none It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters. And our server_received.txt becomes: none ------WebKitFormBoundaryQeBMs9N4limtK0VP Content-Disposition: form-data; name=\\"file_01\\"; filename=\\"sometext.txt\\" Content-Type: text/plain It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout. The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters. ------WebKitFormBoundaryQeBMs9N4limtK0VP Content-Disposition: form-data; name=\\"oid\\" someid ------WebKitFormBoundaryQeBMs9N4limtK0VP-- - Things become complicated even we simply upload a text file. - The entries of our FormData are separated by FormBoundary. - Therefore we need to **_parse the output stream_** in order to get file-specfic content in each of separated streamed data. ###### Multiparty Package Let\'s text yarn add multiparty @types/multiparty in this regard. And let\'s modify /stream route to parse incoming stream: js app.post(\\"/stream\\", async (req, res) => { const form = new multiparty.Form(); const chunks: number[] = []; form.parse(req); let counter = 0; form.on(\\"part\\", (inputStream) => { try { counter += 1; console.log(\\"InputStream #: \\", counter); const outputStream = createWriteStream(inputStream.filename); inputStream.pipe(outputStream); } catch (e) { res.json({ success: false, erroreMessage: JSON.stringify(e) }); } }); }); - Now we can **_upload arbitary file_** and find that an **_identical_** file appears in our backend. - Recall that our form data have two keys, file_01 and oid. - We experirmentally add a counter to see whether multiparty also parses data without filename, and the result is positive as the output becomes: none InputStream #: 1 InputStream #: 2 - Infact inputStream.filename becomes undefined for the second entry in the FormData. - The programme ended sliently even the input of createWriteStream is undefined. Simple enough right? Let\'s dig deeper by studying what happens in js inputStream.pipe(outputStream); ##### What Happens in inputStream.pipe(outputStream)? For this, we remove the pipe line, we read chunk and write chunk on our own: ###### Version 1 of pipe (Demonstrative Purpose, Not Recommended) We accumulate all the bytes and then write it into a file: js app.post(\\"/stream\\", async (req, res) => { const form = new multiparty.Form(); form.parse(req); form.on(\\"part\\", (inputStream) => { try { const chunks: number[] = []; const outputStream = createWriteStream(inputStream.filename); inputStream.on(\\"readable\\", () => { let chunk: Buffer; while ((chunk = inputStream.read()) != null) { const chunk_ = Array.from(chunk); chunks.push(...chunk_); } }); inputStream.on(\\"close\\", () => { const finalBuffer = Buffer.from(chunks); outputStream.write(finalBuffer); }); } catch (e) { res.json({ success: false, erroreMessage: JSON.stringify(e) }); } }); }); For output of raw chunk, byte chunk and buffered chunk none raw chunk : <Buffer ca 4c 98 7f ac c8 98 ed 4f 77 f6 41 ba df b4 9c 3a d5 9b 66 93 92 c7 25 29 23 dd c9 66 51 e2 50 7b 9c be 25 66 a0 39 4a df 1a 63 6c 8f 33 89 e2 cd 9d ... 51798 more bytes> byte chunk: [ 202, 76, 152, 127, 172, 200, 152, 237, 79, 119, 246, 65, 186, 223, 180, 156, 58, 213, 155, 102, 147, 146, 199, 37, 41, 35, 221, 201, 102, 81, 226, 80, 123, 156, 190, 37, 102, 160, 57, 74, 223, 26, 99, 108, 143, 51, 137, 226, 205, 157, 9, 22, 113, 210, 168, 152, 52, 222, 147, 50, 209, 155, 106, 147, 129, 163, 207, 100, 95, 218, 116, 63, 213, 161, 164, 57, 85, 80, 66, 186, 81, 191, 110, 32, 123, 197, 152, 183, 102, 98, 108, 89, 138, 246, 198, 152, 143, 109, 236, 71, ... 51748 more items ] finalBuffer: <Buffer 89 50 4e 47 0d 0a 1a 0a 00 00 00 0d 49 48 44 52 00 00 0d 70 00 00 05 a0 08 02 00 00 00 c2 10 1d 93 00 00 00 06 62 4b 47 44 00 00 00 00 00 00 f9 43 bb ... 9553650 more bytes> ###### Version 2 of pipe Whenever we read a chunk, we write it into outputStream: js app.post(\\"/stream\\", async (req, res) => { const form = new multiparty.Form(); form.parse(req); form.on(\\"part\\", (inputStream) => { try { const outputStream = createWriteStream(inputStream.filename); inputStream.on(\\"readable\\", () => { let chunk: Buffer; while ((chunk = inputStream.read()) != null) { outputStream.write(chunk); } }); } catch (e) { res.json({ success: false, erroreMessage: JSON.stringify(e) }); } }); }); Here if we console.log(chunk), we have text <Buffer 81 a5 f4 37 66 e1 3a d1 72 9b 23 68 ... 65486 more bytes> <Buffer f1 a7 eb 57 4e df be 8a b7 2e 29 dc ... 36888 more bytes> <Buffer a6 1d aa 2e c0 3c 1e 76 c4 ae 75 c5 ... 28548 more bytes> <Buffer 8c 37 a3 ec 93 b1 ec a3 21 c6 f1 30 ... 37611 more bytes> <Buffer e9 4e 0d e9 8e 82 34 e1 7b 2b 2b bc ... 27825 more bytes> <Buffer 24 f6 fe 13 d9 0a 57 e2 16 ba 62 e7 ... 65486 more bytes> <Buffer fe 66 ac 0a 72 ef de eb 7f 05 70 b6 ... 65486 more bytes> We can observe that our chunks never exceed $2^8 \\\\times 2^8 = 2^{16} = 65536$ bytes. ###### Summary of Version 1 and Version 2 - **_Version 2_** is exactly what inputStream.pipe(outputStream) does for us. Therefore we have no hassle of worrying memory overflow problem for data streaming. - Not only that, pipe method also handles **back-pressure** problem which we haven\'t implemented anything to handle yet: ###### Back-Pressure - If we look that the return of WriteStream.write: <center></center> ![](/assets/tech/171/002.png) <center></center> It is in fact a boolean. - When it returns false, which means that the buffer of size 65kb is not large enough to receive the incoming chunk immediately. - The reason is mostly because of that the writing speed is slower than the data-pulling speed. - We need to - inputStream.pause() when outputStream.write returns false; - inputStream.resume() when our buffer in the outputStream gets **_drained of_** data; - The **drainded event** can be subscribed by js outputStream.on(\\"drain\\", () => { ... }); #### Download Zip of Files with Stream Manipulation ##### Node.js ###### Backend Implementation We take AWS S3 bucket as an example. We will: - Use npm package aws-sdk to get ReadStream of our object through bucketName and objectKey. - Use npm package archiver to pipe the ReadStream into a ZipStream, data will then be piped into our final WriteStream, i.e., res. Let\'s create another route called /download for downloading zip of multiple files: js app.get(\\"/download\\", async (req, res) => { const bucketName = \\"jaems-cicd\\"; const objectKey1 = \\"assets/fonts/FreightTextProMedium-Italic.woff2\\"; const objectKey2 = \\"assets/fonts/FreightTextProMedium-Italic.woff\\"; const stream1 = await awsS3Util.getFileStream({ bucketName, objectKey: objectKey1, }); const stream2 = await awsS3Util.getFileStream({ bucketName, objectKey: objectKey2, }); const zipStream = streamUtil.getZipStream(); if (stream1) { zipStream.append(stream1, { name: \\"FreightTextProMedium-Italic.woff2\\" }); } if (stream2) { zipStream.append(stream2, { name: \\"FreightTextProMedium-Italic.woff\\" }); } zipStream.finalize(); zipStream.pipe(res); res.setHeader(\\"Access-Control-Expose-Headers\\", \\"Content-Disposition\\"); res.setHeader( \\"Content-Disposition\\", \'attachment; filename=\\"zip-file-example.zip\\"\' ); }); - Here streamUtil.getZipStream and awsS3Util.getFileStream are defined below. - ZipStream is a kind of middleware between streams, we call it a Duplex stream in node.js, which is both a ReadStream and a WriteStream. - Our name field can be some/path/file.ext inside zipStream.append. archiver will mkdir -p for us. - Note that by default Content-Disposition is not among the auto-allowed headers, we need to specify it explicitly. - It would be better to enclose the filename by \\"\'s. - Modern browser will try to parse the filename without quotes, but enclosing by \\"\'s makes frontend more easy to grab the filename using regex, as we shall see later. - streamUtil.getZipStream js // streamUtil.ts import archiver from \\"archiver\\"; const getZipStream = () => { const archive = archiver(\\"zip\\", { zlib: { level: 9 }, }); return archive; }; export default { getZipStream, }; - awsS3Util.getFileStream js // awsS3Util.ts import AWS from \\"aws-sdk\\"; const S3 = new AWS.S3(); async function getFileStream(props: { bucketName: string, objectKey: string, }) { const { bucketName, objectKey } = props; return S3.getObject({ Bucket: bucketName, Key: objectKey, }).createReadStream(); } export default { getFileStream, }; ###### Frontend to Handle the Stream We extract filename from header in line 14. js-1 import Button from \\"@mui/material/Button/Button\\"; import Box from \\"@mui/material/Box\\"; import axios from \\"axios\\"; const fileNameRegex = /(?<=filename\\\\=\\\\\\").*?(?=\\\\\\")/g export default () => { const downloadZip = async () => { const res = await axios.get(\\"http://localhost:8080/download\\", { responseType: \\"blob\\" }); const blob = new Blob([res.data], { type: \\"application/zip\\" }); const url = window.URL.createObjectURL(blob); const link = document.createElement(\\"a\\"); const contentDisposition = res.headers?.[\\"content-disposition\\"] as string; const fileName = contentDisposition.match(fileNameRegex)?.[0] || \\"\\"; link.download = fileName; link.href = url; link.click(); window.URL.revokeObjectURL(url); } return ( <Box sx={ { \\"& .MuiButton-root\\": { textTransform: \\"none\\" } } } > <div> <Button onClick={downloadZip}> Download Zip File </Button> </div> </Box > ) } Note that since it is a GET request, downloadZip can be alternatively defined by js const downloadZip = async () => { const link = document.createElement(\\"a\\"); link.href = \\"http://localhost:8080/download\\"; link.click(); }; and we still get the same result (the filename can be correctly obtained). The former approach (line 8) works equally well for POST request (e.g., we may want more complicated query data in the body). ##### Springboot - Idea in node.js can be translated to spring directly. - We create a middleware before piping data into outputStream. java @Data @Accessors(chain = true) public static class FullPathAndInputStream { private String fullPath; private InputStream inputStream; } public static StreamingResponseBody inputStreamsIntoZip(List<FullPathAndInputStream> inputs) { StreamingResponseBody responseBody = outputStream -> { try (ZipOutputStream zipOutStream = new ZipOutputStream(outputStream)) { for (var fullPathAndInputStream : inputs) { String fullPath = fullPathAndInputStream.getFullPath().replace(\\"\\\\\\\\\\", \\"/\\"); InputStream inputStream = fullPathAndInputStream.getInputStream(); ZipEntry zipEntry = new ZipEntry(fullPath); zipEntry.setTime(System.currentTimeMillis()); zipOutStream.putNextEntry(zipEntry); StreamUtils.copy(inputStream, zipOutStream); zipOutStream.closeEntry(); inputStream.close(); } } }; return responseBody; } Here InputStream can be obtained from S3ObjectInputStream: java public S3ObjectInputStream getFileStream(String bucketName, String key) { try { S3Object obj = s3.getObject(bucketName, key); S3ObjectInputStream s3inputStream = obj.getObjectContent(); return s3inputStream; } catch (AmazonServiceException e) { logger.error(e); } return null; } - In spring\'s controller we can return ResponseEntity and pass StreamingResponseBody object into its body argument. - Frontend code that handles the response is the same as the previous section. #### Reference - **Web Dev Jounry**, Discussion on Streaming: - [Node JS - HTTP Streaming](https://www.youtube.com/watch?v=CiGnubZC5cs) - [Node JS - Streams Intro](https://www.youtube.com/watch?v=qU8PmZOOnac) - [Node JS - Readable Streams](https://www.youtube.com/watch?v=_pqv06ySvuk) - [Node JS - Writable Streams & Backpressure](https://www.youtube.com/watch?v=FS2OWxS5P_E&t=605s) - [Node JS - Pipe, Duplex, & Transform Streams](https://www.youtube.com/watch?v=rQXaDH__Suk) ","title":"Handle Streams in File-Responding Request","date":"2023-09-03T00:00:00.000Z","id":"blog0171","tag":"express, java","intro":"We implement file downloading feature in a get/post request in both express and spring, we handle them in memory and therefore no disk i/o is needed.","toc":true},{"content":" <style> img { max-width: 100% } </style> js import express from \\"express\\"; import cors from \'cors\'; const allowlist = [\'http://localhost:3000\'] const corsOptionsDelegate = (req, callback) => { var corsOptions; if (allowlist.indexOf(req.header(\'Origin\')) > -1) { // reflect (enable) the requested origin in the CORS response corsOptions = { origin: true }; } else { // disable CORS for this request corsOptions = { origin: false }; } // callback expects two parameters: (error, options) callback(null, corsOptions) } app.use(cors(corsOptionsDelegate)); ","title":"Restrict CORS to Limited Origins","date":"2023-09-03T00:00:00.000Z","id":"blog0172","tag":"express","intro":"Record how to implement CORS in express to allow certain origins to get access instead of allowing all origins","toc":false},{"content":" <style> img { max-width: 100% } </style> #### Routing Schema as a Type Variable Let\'s define a routing schema by defining, sequentially, which value is possible to appear: js export type NavigationRouteSchema = { \\tvalue: \\"/buyer\\" | \\"/supplier\\", \\tnext: { \\t\\tvalue: \\"order\\", \\t\\tnext: { \\t\\t\\tvalue: Oid \\t\\t\\tnext: { \\t\\t\\t\\tvalue: \\"\\" | \\"requirements\\" | \\"quotation\\" | \\"sampling\\" | \\"freight\\" \\t\\t\\t} | \\t\\t\\t{ \\t\\t\\t\\tvalue: \\"contract\\", \\t\\t\\t\\tnext: { \\t\\t\\t\\t\\tvalue: Oid \\t\\t\\t\\t} \\t\\t\\t} \\t\\t} \\t} | \\t{ \\t\\tvalue: \\"projects\\", \\t} | \\t{ \\t\\tvalue: \\"correspondence-dashboard\\", \\t\\tnext: \\t\\t{ \\t\\t\\tvalue: \\"mailchains\\", \\t\\t\\tnext: { \\t\\t\\t\\tvalue: \\"mailchainOid\\", \\t\\t\\t\\tnext: { \\t\\t\\t\\t\\tvalue: Oid \\t\\t\\t\\t} \\t\\t\\t} \\t\\t} | \\t\\t{ \\t\\t\\tvalue: \\"unlinked-emails\\", \\t\\t\\tnext: { \\t\\t\\t\\tvalue: \\"emailOid\\", \\t\\t\\t\\tnext: { \\t\\t\\t\\t\\tvalue: Oid \\t\\t\\t\\t} \\t\\t\\t} \\t\\t} \\t} }; For example, none /buyer/order/abcdsfds123123/contract/6dsf456sd6f4s is among the possible choices. Let\'s define the following type: #### Define Custom Type of Tuples from the Schema js type RouteBreakdown<T> = \\tT extends { value: infer U, next: infer V } ? [U, ...RouteBreakdown<V>] : \\tT extends { value: infer U } ? [U] : \\tnever export type ClientNavigation = RouteBreakdown<NavigationRouteSchema>; Then by hovering ClientNavigation: none type ClientNavigation = [\\"/buyer\\" | \\"/supplier\\", \\"order\\", string, \\"\\" | \\"requirements\\" | \\"quotation\\" | \\"sampling\\" | \\"freight\\"] | [\\"/buyer\\" | \\"/supplier\\", \\"order\\", string, \\"contract\\", string] | [...] | [...] | [...] which consists of all possible tuples of values in our NavigationRouteSchema with correct sequential order. #### Construct Navigation Route with ZERO Chance of Making Mistake Now we define js const getNavigationRoute = (...args: ClientNavigation) => { return args.join(\\"/\\"); }; which simply assembles the values into a correct URL for navigation. - We are 100% confident that our path is correct without hard-coding. Why? - First all value can be auto-completed: ![](/assets/tech/173/002.png) - Any route that is not among our **_tuple of string-types_** will trigger an error: ![](/assets/tech/173/003.png) - Apart from auto-complete suggestions, there will also be a pop-up indicating the type at the current positional argument: ![](/assets/tech/173/004.png) - From now on any **_change of routes_** requirement becomes extremely trivial! ","title":"Routing Schema for Frontend Project","date":"2023-09-05T00:00:00.000Z","id":"blog0173","tag":"react","intro":"In the browser our route consist of many information, like messageId, emailId, etc, which enables our web page to select correct data based on the route. \\nIn this post we discuss how to effectively construct such url with type safty under a RouteSchema.","toc":true},{"content":" <style> img { max-width: 100% } </style> #### Component Study Let\'s study the following forms: <Center> <a href=\\"/assets/tech/174/003.png\\"> <img src=\\"/assets/tech/174/003.png\\"/> </a> </Center> <p></p> <center></center> - We can obseve that there are many repititions. - Working any single part is very easy, we just nest the CSS and the <div>\'s. - The problem becomes complicated when it comes to reusing what we have done. #### Grabbing Child Compoenent of Desired Component Name Before proceeding, let\'s discuss how a react component can identify certain special children component, extract them, and place it in our desired order: js import React, { ReactNode } from \\"react\\"; function grabChildrenByType(children: ReactNode, type: string) { \\tconst mappedChildren = React.Children.map(children, (child) => { \\t\\tconst child_ = child as (React.ReactElement<any> & { type?: { name?: string } }); \\t\\tif (React.isValidElement(child_) \\t\\t\\t&& (child_?.type?.name || \\"\\") === type \\t\\t) { \\t\\t\\treturn child; \\t\\t} else { \\t\\t\\treturn null; \\t\\t} \\t}) as JSX.Element[]; \\treturn mappedChildren; } export default { grabChildrenByType } Basically each children should have .type.name. Note that if a component is generated by a function with trait () => () => JSX.Element, then this compoonent will have empty string as its .type.name. #### Divide the Component into Reusable ones In general there are only two sub-components: - ![](/assets/tech/174/004.png) Let\'s denote DC as DialogComponent. The above component can be described by js <DC.RadioRow> <DC.RadioRowButton> <RadioButton /> </DC.RadioRowButton> <DC.RadioRowTitle>Create New Buyer Company</DC.RadioRowTitle> </DC.RadioRow> where each of the dirty parts is defined below. js export class DC { \\tpublic static RadioRowButton({ children, ...props }: { \\t\\tchildren: ReactNode \\t} & HTMLAttributes<HTMLDivElement>) { return <div {...props}>{children}</div> } \\tpublic static RadioRowTitle({ children, ...props }: { \\t\\tchildren: ReactNode \\t} & HTMLAttributes<HTMLDivElement>) { return <div {...props}>{children}</div> } \\tpublic static RadioRow({ children, ...props }: { children: ReactNode } & HTMLAttributes<HTMLDivElement>) { \\t\\tconst radioBlock = childUtil.grabChildrenByType(children, DC.RadioRowButton.name); \\t\\tconst titleBlock = childUtil.grabChildrenByType(children, DC.RadioRowTitle.name); \\t\\treturn ( \\t\\t\\t<div style={{ display: \\"flex\\" }}> \\t\\t\\t\\t<div style={{ display: \\"flex\\", justifyContent: \\"flex-end\\", }}> \\t\\t\\t\\t\\t{radioBlock} \\t\\t\\t\\t</div> \\t\\t\\t\\t<div style={{ flex: 1 }}> \\t\\t\\t\\t\\t<div style={{ paddingTop: 4 }}>{titleBlock}</div> \\t\\t\\t\\t</div> \\t\\t\\t</div> \\t\\t) \\t} } Note that RadioRowButton and RadioRowTitle are simply children => children and identitcal. They serve as a building block of our custom rendering. - ![](/assets/tech/174/005.png) This can be expressed by js <DC.InputRow> <DC.InputRowLeft>Choose Company</DC.InputRowLeft> <DC.InputRowRight> <GeneralDropdown fullList={buyerCompany} initialValue={null} /> </DC.InputRowRight> </DC.InputRow> We hide all the dirty work by defining: js export class DC { \\tpublic static InputRowLeft({ children, ...props }: { children: ReactNode } & HTMLAttributes<HTMLDivElement>) { return <div {...props}>{children}</div> } \\tpublic static InputRowRight({ children, ...props }: { children: ReactNode } & HTMLAttributes<HTMLDivElement>) { return <div {...props}>{children}</div> } \\tpublic static InputRow({ children, ...props }: { children: ReactNode } & HTMLAttributes<HTMLDivElement>) { \\t\\tconst left = childUtil.grabChildrenByType(children, DC.InputRowLeft.name); \\t\\tconst right = childUtil.grabChildrenByType(children, DC.InputRowRight.name); \\t\\treturn ( \\t\\t\\t<tr> \\t\\t\\t\\t<td style={{ verticalAlign: \\"middle\\" }}>{left}</td> \\t\\t\\t\\t<td style={{ verticalAlign: \\"middle\\" }}>{right}</td> \\t\\t\\t</tr> \\t\\t) \\t} Finally we want our component to be able to capture <InputRow /> and <RadioRow /> and inject the components in desired position. js public static Body({ children, ...props }: { children: ReactNode } & HTMLAttributes<HTMLDivElement>) { \\tconst inputs = childUtil.grabChildrenByType(children, DC.InputRow.name); \\tconst radioRows = childUtil.grabChildrenByType(children, DC.RadioRow.name); \\treturn ( \\t\\t<div {...props}> \\t\\t\\t{radioRows} \\t\\t\\t<Spacer height={5} /> \\t\\t\\t<Table sx={ \\t\\t\\t\\t{ \\t\\t\\t\\t\\t\\"& td\\": { \\t\\t\\t\\t\\t\\tpaddingTop: \\"5px\\" \\t\\t\\t\\t\\t}, \\t\\t\\t\\t\\t\\"& td:nth-child(1)\\": { \\t\\t\\t\\t\\t\\twidth: 160, \\t\\t\\t\\t\\t\\tverticalAlign: \\"middle\\", \\t\\t\\t\\t\\t\\tpaddingLeft: \\"34px\\" \\t\\t\\t\\t\\t}, \\t\\t\\t\\t\\t\\"& td:nth-child(2)\\": { verticalAlign: \\"middle\\" } \\t\\t\\t\\t} \\t\\t\\t\\t}> \\t\\t\\t\\t{inputs} \\t\\t\\t</Table> \\t\\t</div> \\t) } #### Final Result If we put all the single pieces into the component we want, it will look very long: js export type Step1FormData = { \\toption?: string, \\tcompany?: string, \\temailDomain?: string } export default ({ step1FormData }: { step1FormData: React.MutableRefObject<Step1FormData> }) => { \\tconst dispatch = useAppDispatch(); \\tconst { RadioButton, selectedValue, setSelectedValue } = useWbRadioButtonGroup<string>(); \\tconst updateField = (value: Partial<Step1FormData>) => { \\t\\tstep1FormData.current = { ...step1FormData.current, ...value }; \\t} \\tconst [buyerCompany, setBuyerCompany] = useState<BuyerCompany[]>([]); \\tuseEffect(() => { \\t\\tdispatch(mailThunkAction.getBuyerCompanies()) \\t\\t\\t.unwrap() \\t\\t\\t.then(data => { setBuyerCompany(data); }) \\t\\t\\t.catch(err => { snackbarUtils.error(JSON.stringify(err)) }); \\t}, []); \\treturn ( \\t\\t<> \\t\\t\\t<DC.Body> \\t\\t\\t\\t<DC.RadioRow> \\t\\t\\t\\t\\t<DC.RadioRowButton> \\t\\t\\t\\t\\t\\t<RadioButton \\t\\t\\t\\t\\t\\t\\tvalue={options.step1.CREATE_NEW_COMPANY} \\t\\t\\t\\t\\t\\t\\tstyle={{ padding: 0, paddingRight: 10 }} \\t\\t\\t\\t\\t\\t\\tonClick={() => { updateField({ option: options.step1.CREATE_NEW_COMPANY }) }} \\t\\t\\t\\t\\t\\t/> \\t\\t\\t\\t\\t</DC.RadioRowButton> \\t\\t\\t\\t\\t<DC.RadioRowTitle> \\t\\t\\t\\t\\t\\tCreate New Buyer Company \\t\\t\\t\\t\\t</DC.RadioRowTitle> \\t\\t\\t\\t</DC.RadioRow> \\t\\t\\t\\t<DC.InputRow> \\t\\t\\t\\t\\t<DC.InputRowLeft> \\t\\t\\t\\t\\t\\tChoose Company \\t\\t\\t\\t\\t</DC.InputRowLeft> \\t\\t\\t\\t\\t<DC.InputRowRight> \\t\\t\\t\\t\\t\\t<GeneralDropdown fullList={buyerCompany} initialValue={null} /> \\t\\t\\t\\t\\t</DC.InputRowRight> \\t\\t\\t\\t</DC.InputRow> \\t\\t\\t\\t<DC.InputRow> \\t\\t\\t\\t\\t<DC.InputRowLeft> \\t\\t\\t\\t\\t\\tEmail Domain \\t\\t\\t\\t\\t</DC.InputRowLeft> \\t\\t\\t\\t\\t<DC.InputRowRight> \\t\\t\\t\\t\\t\\t<div style={{ display: \\"flex\\", alignItems: \\"center\\" }}> \\t\\t\\t\\t\\t\\t\\t<div style={{ display: \\"inline-block\\", marginRight: 10 }}>@</div> \\t\\t\\t\\t\\t\\t\\t<WbInput onChange={e => { updateField({ emailDomain: e.target.value }) }} /> \\t\\t\\t\\t\\t\\t</div> \\t\\t\\t\\t\\t</DC.InputRowRight> \\t\\t\\t\\t</DC.InputRow> \\t\\t\\t</DC.Body> \\t\\t\\t<Spacer height={20} /> \\t\\t\\t<DC.Body> \\t\\t\\t\\t<DC.RadioRow> \\t\\t\\t\\t\\t<DC.RadioRowButton> \\t\\t\\t\\t\\t\\t<RadioButton \\t\\t\\t\\t\\t\\t\\tvalue={options.step1.USE_EXISTING_COMPANY} \\t\\t\\t\\t\\t\\t\\tstyle={{ padding: 0, paddingRight: 10 }} \\t\\t\\t\\t\\t\\t\\tonClick={() => { updateField({ option: options.step1.USE_EXISTING_COMPANY }) }} \\t\\t\\t\\t\\t\\t/> \\t\\t\\t\\t\\t</DC.RadioRowButton> \\t\\t\\t\\t\\t<DC.RadioRowTitle> \\t\\t\\t\\t\\t\\tUse Existing Company \\t\\t\\t\\t\\t</DC.RadioRowTitle> \\t\\t\\t\\t</DC.RadioRow> \\t\\t\\t\\t<DC.InputRow> \\t\\t\\t\\t\\t<DC.InputRowLeft> \\t\\t\\t\\t\\t\\tChoose Company \\t\\t\\t\\t\\t</DC.InputRowLeft> \\t\\t\\t\\t\\t<DC.InputRowRight> \\t\\t\\t\\t\\t\\t<WbInput /> \\t\\t\\t\\t\\t</DC.InputRowRight> \\t\\t\\t\\t</DC.InputRow> \\t\\t\\t\\t<DC.InputRow> \\t\\t\\t\\t\\t<DC.InputRowLeft> \\t\\t\\t\\t\\t\\tEmail Domain \\t\\t\\t\\t\\t</DC.InputRowLeft> \\t\\t\\t\\t\\t<DC.InputRowRight> \\t\\t\\t\\t\\t\\t<div style={{ display: \\"flex\\", alignItems: \\"center\\" }}> \\t\\t\\t\\t\\t\\t\\t<div style={{ display: \\"inline-block\\", marginRight: 10 }}>@</div> \\t\\t\\t\\t\\t\\t\\t<WbInput onChange={e => { updateField({ emailDomain: e.target.value }) }} /> \\t\\t\\t\\t\\t\\t</div> \\t\\t\\t\\t\\t</DC.InputRowRight> \\t\\t\\t\\t</DC.InputRow> \\t\\t\\t</DC.Body> \\t\\t</> \\t) } #### Simplification of the Final Result The result so far is not complicated but **_cumbersome_**, if we look back there are room for simplification. We use the same concept but define only **_two_** child components: - For the biggest components (the whole form in our case) we devide it by grabbing relatively smaller component. - For relatively smaller components we divide it by injecting dynamic components by props. js export class DC { \\tpublic static InputRow({ left, right }: { \\t\\tleft: ReactNode, \\t\\tright: ReactNode, \\t}) { \\t\\treturn ( \\t\\t\\t<tr> \\t\\t\\t\\t<td style={{ verticalAlign: \\"middle\\" }}>{left}</td> \\t\\t\\t\\t<td style={{ verticalAlign: \\"middle\\" }}>{right}</td> \\t\\t\\t</tr> \\t\\t) \\t} \\tpublic static RadioRow({ left, right }: { \\t\\tleft: ReactNode, \\t\\tright: ReactNode, \\t}) { \\t\\treturn ( \\t\\t\\t<div style={{ display: \\"flex\\" }}> \\t\\t\\t\\t<div id=\\"raio-column\\" style={{ display: \\"flex\\", justifyContent: \\"flex-end\\", }}> \\t\\t\\t\\t\\t{left} \\t\\t\\t\\t</div> \\t\\t\\t\\t<div id=\\"input-form\\" style={{ flex: 1 }}> \\t\\t\\t\\t\\t<div style={{ paddingTop: 4 }}>{right}</div> \\t\\t\\t\\t</div> \\t\\t\\t</div> \\t\\t) \\t} \\tpublic static Body({ children, disabled = true, ...props }: { \\t\\tchildren: ReactNode, \\t\\tdisabled?: boolean \\t} & HTMLAttributes<HTMLDivElement>) { \\t\\tconst inputs = childUtil.grabChildrenByType(children, DC.InputRow.name); \\t\\tconst radioRows = childUtil.grabChildrenByType(children, DC.RadioRow.name); \\t\\tconst disabledStyle = { opacity: 0.5, pointerEvents: \\"none\\" }; \\t\\treturn ( \\t\\t\\t<div \\t\\t\\t\\t{...props} \\t\\t\\t> \\t\\t\\t\\t{radioRows} \\t\\t\\t\\t<Spacer height={5} /> \\t\\t\\t\\t<Table sx={{ \\t\\t\\t\\t\\t\\"& td\\": { \\t\\t\\t\\t\\t\\tpaddingTop: \\"5px\\", \\t\\t\\t\\t\\t\\t...(disabled ? disabledStyle : {}) \\t\\t\\t\\t\\t}, \\t\\t\\t\\t\\t\\"& td:nth-child(1)\\": { \\t\\t\\t\\t\\t\\twidth: 160, \\t\\t\\t\\t\\t\\tverticalAlign: \\"middle\\", \\t\\t\\t\\t\\t\\tpaddingLeft: \\"34px\\" \\t\\t\\t\\t\\t}, \\t\\t\\t\\t\\t\\"& td:nth-child(2)\\": { \\t\\t\\t\\t\\t\\tverticalAlign: \\"middle\\" \\t\\t\\t\\t\\t} \\t\\t\\t\\t}}> \\t\\t\\t\\t\\t{inputs} \\t\\t\\t\\t</Table> \\t\\t\\t</div> \\t\\t) \\t} } Which becomes js <DC.Body> \\t<DC.RadioRow \\t\\tleft={ \\t\\t\\t<RadioButton \\t\\t\\t\\tvalue={options.step1.CREATE_NEW_COMPANY} \\t\\t\\t\\tstyle={{ padding: 0, paddingRight: 10 }} \\t\\t\\t\\tonClick={() => { updateField({ option: options.step1.CREATE_NEW_COMPANY }) }} \\t\\t\\t/> \\t\\t} \\t\\tright={<span>Create New Buyer Company</span>} \\t/> \\t<DC.InputRow \\t\\tleft={<span>Company Name</span>} \\t\\tright={<WbInput onChange={e => updateField({ newCompName: e.target.value })} />} \\t/> \\t<DC.InputRow \\t\\tleft={<span>Email Domain</span>} \\t\\tright={ \\t\\t\\t<div style={{ display: \\"flex\\", alignItems: \\"center\\" }}> \\t\\t\\t\\t<div style={{ display: \\"inline-block\\", marginRight: 10 }}>@</div> \\t\\t\\t\\t<WbInput onChange={e => { updateField({ emailDomain: e.target.value }) }} /> \\t\\t\\t</div> \\t\\t} \\t/> </DC.Body > <Spacer height={20} /> <DC.Body disabled={selectedOption !== options.step1.USE_EXISTING_COMPANY}> \\t<DC.RadioRow \\t\\tleft={ \\t\\t\\t<RadioButton \\t\\t\\t\\tvalue={options.step1.USE_EXISTING_COMPANY} \\t\\t\\t\\tstyle={{ padding: 0, paddingRight: 10 }} \\t\\t\\t\\tonClick={() => { updateField({ option: options.step1.USE_EXISTING_COMPANY }) }} \\t\\t\\t/> \\t\\t} \\t\\tright={<span>Use Existing Company</span>} \\t/> \\t<DC.InputRow \\t\\tleft={<span>Choose Company</span>} \\t\\tright={<GeneralDropdown \\t\\t\\tfullList={buyerCompany} \\t\\t\\tinitialValue={null} \\t\\t\\trefUpdateHandler={(option) => { \\t\\t\\t\\tupdateField({ existingBuyerCompany: option!, compCodeForStep2: option?.code || \\"\\" }); \\t\\t\\t}} \\t\\t/>} \\t/> \\t<DC.InputRow \\t\\tleft={<span>Email Domain</span>} \\t\\tright={ \\t\\t\\t<div style={{ display: \\"flex\\", alignItems: \\"center\\" }}> \\t\\t\\t\\t<div style={{ display: \\"inline-block\\", marginRight: 10 }}>@</div> \\t\\t\\t\\t<WbInput onChange={e => { updateField({ emailDomain: e.target.value }) }} /> \\t\\t\\t</div> \\t\\t} \\t/> </DC.Body> #### Summary - Our **_child-grabbing_** strategy helps locate **_specific component_** in **_specific location_**. - We can design elements injection of DC.Body by js <DC.Body \\tradios={<><DC.Radiorow {...} /></>} \\tinputs={<><DC.InputRow {...} /></>} /> which is much flexible than: - js <DC.Body> \\t\\t\\t\\t // only child of type RadioRow or InputRow is allowed, \\t<DC.RadioRow {...} />\\t\\t// look of output is easy to expect \\t<DC.InputRow {...} /> </DC.Body> - No absolute true/false, they are all matter of taste (by the way I love the latter one). ","title":"Custom Rendering of Custom Child Component","date":"2023-09-06T00:00:00.000Z","id":"blog0174","tag":"react","intro":"We dicuss how to customize the way of rendering custom children component to produce highly reusable code.","toc":true},{"content":" <style> img { max-width: 100% } </style> #### Repository - https://github.com/machingclee/2023-09-10-Elliptic-Curve-in-Rust/tree/main/ECC/src/modules #### Prefered Headers to Ignore Annoying Warnings: rust #![allow(unused)] #![allow(non_camel_case_types)] #![allow(non_snake_case)] #### Utility Struct to Perform Arithmetic on $\\\\mathbb Z/ p \\\\mathbb Z$ for BigUint rust struct Fp<\'a> { p: &\'a BigUint, } impl<\'a> Fp<\'a> { fn power(&self, u: &BigUint, i: u32) -> BigUint { u.modpow(&BigUint::from(i), &self.p) } fn add(&self, u: &BigUint, v: &BigUint) -> BigUint { (u + v).modpow(&BigUint::from(1u32), self.p) } fn mul(&self, u: &BigUint, v: &BigUint) -> BigUint { (u * v).modpow(&BigUint::from(1u32), self.p) } fn add_inverse(&self, u: &BigUint) -> BigUint { assert!( u < &self.p, \\"{}\\", format!(\\"{} >= {} should not happen\\", u, &self.p) ); self.p - u } fn mul_inverse(&self, u: &BigUint) -> BigUint { if self.p < &BigUint::from(2u32) { BigUint::from(1u32) } else { let two = BigUint::from(2u32); let power = self.add(&self.p, &self.add_inverse(&two)); u.modpow(&power, &self.p) } } } #### Definition of Addition and Double on Elliptic Curve rust use num_bigint::BigUint; #[derive(PartialEq, Clone, Debug)] enum Point { Coor(BigUint, BigUint), Identity, } struct EllipticCurve { a: BigUint, b: BigUint, p: BigUint, } impl EllipticCurve { fn double(&self, h: &Point) -> Point { let fp = Fp { p: &self.p }; let h_on_curve = self.is_on_curve(h); assert!(h_on_curve, \\"point h is not on the curve\\"); // s = (3*x^2 + a)/(2*y) // x_ = s^2 - 2*x // y_ = s*(x - x_) - y match h { Point::Identity => Point::Identity, Point::Coor(x, y) => { let three_times_xsq = fp.mul(&BigUint::from(3u32), &fp.power(x, 2)); let two_times_y = fp.mul(&BigUint::from(2u32), &y); let inverse_two_times_y = fp.mul_inverse(&two_times_y); let s = fp.mul(&fp.add(&three_times_xsq, &self.a), &inverse_two_times_y); let x_ = fp.add( &fp.power(&s, 2), &fp.add_inverse(&fp.mul(&BigUint::from(2u32), x)), ); let s_times_x_minus_x_ = fp.mul(&s, &fp.add(x, &fp.add_inverse(&x_))); let y_ = fp.add(&s_times_x_minus_x_, &fp.add_inverse(y)); Point::Coor(x_, y_) } } } fn add(&self, h: &Point, k: &Point) -> Point { let fp = Fp { p: &self.p }; let h_on_curve = self.is_on_curve(h); let k_on_curve = self.is_on_curve(k); assert!(*h != *k, \\"two points should not be the same\\"); assert!(h_on_curve, \\"point h is not on the curve\\"); assert!(k_on_curve, \\"point k is not on the curve\\"); match (h, k) { (Point::Identity, _) => k.clone(), (_, Point::Identity) => h.clone(), (Point::Coor(x1, y1), Point::Coor(x2, y2)) => { // s = (y2-y1)/(x2-x1) // x3 = s^2 - x1 - x2 // y3 = s*(x1-x3) - y1 let y1_plus_y2 = fp.add(y1, y2); if x1 == x2 && y1_plus_y2 == BigUint::from(0u32) { return Point::Identity; } let s = fp.mul( &(y2 + &fp.add_inverse(y1)), &fp.mul_inverse(&(x2 + &fp.add_inverse(x1))), ); let x3 = fp.add( &fp.add(&fp.power(&s, 2), &fp.add_inverse(x1)), &fp.add_inverse(x2), ); let y3 = fp.add( &fp.mul(&s, &(x1 + &fp.add_inverse(&x3))), &fp.add_inverse(y1), ); Point::Coor(x3, y3) } } } fn scalar_mul(&self, c: &Point, d: &BigUint) -> Point { let mut t = c.clone(); for i in (0..(d.bits() - 1)).rev() { t = self.double(&t); if d.bit(i) { t = self.add(&t, c); } } t } fn is_on_curve(&self, point: &Point) -> bool { let fp = Fp { p: &self.p }; if let Point::Coor(x, y) = point { let y2 = fp.power(y, 2); let x3 = fp.power(x, 3); let ax = fp.mul(&self.a, x); y2 == fp.add(&x3, &fp.add(&ax, &self.b)) } else { true } } } #### Operator Overloading ##### New Struct Field We wish to convert BigUint into our own struct Field and define all the usual operation on $\\\\mathbb Z/p \\\\mathbb Z$, i.e., among the Field objects. That is, we wish to overload the operators: text + - * / without the utility struct Fp. rust #[derive(PartialEq, Clone, Debug)] pub struct Field<\'a> { value: BigUint, p: &\'a BigUint, } ##### New Definition of Point and EllipticCurve base on Field rust pub enum Point<\'a> { Coor(Field<\'a>, Field<\'a>), Identity, } pub struct EllipticCurve<\'a> { a: Field<\'a>, b: Field<\'a>, } ##### Operator Overloadings on Field ###### Implementations rust pub struct Field<\'a> { pub value: BigUint, pub p: &\'a BigUint, } impl<\'a> Field<\'a> { pub fn new(i: u32, p: &\'a BigUint) -> Self { Field { value: BigUint::from(i), p, } } } impl<\'a> Add<&Field<\'a>> for &Field<\'a> { type Output = Field<\'a>; fn add(self, rhs: &Field) -> Self::Output { let value = (&self.value + &rhs.value).modpow(&BigUint::from(1u32), self.p); Field { value, p: self.p } } } impl<\'a> Sub<&Field<\'a>> for &Field<\'a> { type Output = Field<\'a>; fn sub(self, rhs: &Field) -> Self::Output { let value: BigUint; let a = &self.value; let b = &rhs.value; if a > b { value = a - b; } else { value = (self.p + a) - b; } Field { value, p: &self.p } } } impl<\'a> Mul<BigUint> for &Field<\'a> { type Output = Field<\'a>; fn mul(self, rhs: BigUint) -> Self::Output { let a = &self.value; let value = (a * &rhs).modpow(&BigUint::from(1u32), &self.p); return Field { value, p: self.p }; } } impl<\'a> Mul<&Field<\'a>> for &Field<\'a> { type Output = Field<\'a>; fn mul(self, rhs: &Field) -> Self::Output { let value = (&self.value * &rhs.value).modpow(&BigUint::from(1u32), self.p); Field { value, p: self.p } } } impl<\'a> Div<&Field<\'a>> for &Field<\'a> { type Output = Field<\'a>; fn div(self, rhs: &Field) -> Self::Output { let left = &self.value; let right = &rhs.value; let p_minus_2 = (self.p - BigUint::from(2u32)).modpow(&BigUint::from(1u32), self.p); let multiplicative_inverse_right = right.modpow(&p_minus_2, &self.p); let value = (left * &multiplicative_inverse_right).modpow(&BigUint::from(1u32), self.p); Field { value, p: &self.p } } } ###### Rewrite of EllipticCurve::double With Field in Place of BigUint rust impl<\'a> EllipticCurve<\'a> { \\t\\tpub fn double(&self, h: &Point<\'a>) -> Point { let h_on_curve = self.is_on_curve(h); assert!(h_on_curve, \\"point h is not on the curve\\"); // s = (3*x^2 + a)/(2*y) // x_ = s^2 - 2*x // y_ = s*(x - x_) - y match h { Point::Identity => Point::Identity, Point::Coor(xp, yp) => { if yp.value == BigUint::from(0u32) { return Point::Identity; } let two_times_yp = yp * BigUint::from(2u32); let s = xp * xp; let s = &s * BigUint::from(3u32); let s = &s + &self.a; let s = &s / &two_times_yp; let two_times_x = xp * BigUint::from(2u32); let new_x = &s * &s; let new_x = &new_x - &two_times_x; let new_y = xp - &new_x; let new_y = &s * &new_y; let new_y = &new_y - yp; Point::Coor(new_x, new_y) } } } } ###### Rewrite of EllipticCurve::add With Field in Place of BigUint rust impl<\'a> EllipticCurve<\'a> { \\t\\tpub fn add(&self, h: &Point<\'a>, k: &Point<\'a>) -> Point { let h_on_curve = self.is_on_curve(h); let k_on_curve = self.is_on_curve(k); assert!(*h != *k, \\"two points should not be the same\\"); assert!(h_on_curve, \\"point h is not on the curve\\"); assert!(k_on_curve, \\"point k is not on the curve\\"); match (h, k) { (Point::Identity, _) => k.to_owned(), (_, Point::Identity) => h.to_owned(), (Point::Coor(x1p, y1p), Point::Coor(x2p, y2p)) => { if x1p == x2p && (y1p + y2p).value == BigUint::from(0u32) { return Point::Identity; } // s = (y2-y1)/(x2-x1) // x3 = s^2 - x1 - x2 // y3 = s*(x1-x3) - y1 let s = y2p - y1p; let x2_minus_x1 = x2p - x1p; let s = &s / &x2_minus_x1; let s_square = &s * &s; let x3p = &s_square - &x1p; let x3p = &x3p - &x2p; let y3p = &s * &(x1p - &x3p); let y3p = &y3p - &y1p; Point::Coor(x3p, y3p) } } } } ###### Rewrite of EllipticCurve::is_on_curve With Field in Place of BigUint rust impl<\'a> EllipticCurve<\'a> { \\t\\tfn is_on_curve(&self, point: &Point) -> bool { \\t\\t\\t\\tif let Point::Coor(x, y) = point { \\t\\t\\t\\t\\t\\tlet y2 = y * y; \\t\\t\\t\\t\\t\\tlet x3 = x * x; \\t\\t\\t\\t\\t\\tlet x3 = &x3 * x; \\t\\t\\t\\t\\t\\tlet ax = x * &self.a; \\t\\t\\t\\t\\t\\ty2 == &(&x3 + &ax) + &self.b \\t\\t\\t\\t} else { \\t\\t\\t\\t\\t\\ttrue \\t\\t\\t\\t} \\t\\t} } ###### EllipticCurve::scalar_mul --- the Double and Add Algorithm under Field rust impl<\'a> EllipticCurve<\'a> { pub fn scalar_mul(&\'a self, q: &Point<\'a>, k: &Field<\'a>) -> Point<\'a> { let mut t = q.clone(); for i in (0..(k.value.bits() - 1)).rev() { t = self.double(&t); if k.value.bit(i) { t = self.add(&t, q); } } t } } #### Test Cases The following 3 cases can pass successfully: text running 4 tests test test::test_ec_point_add_identity ... ok test test::test_ec_point_addition ... ok test test::test_double ... ok test test::test_scalar_mul ... ok rust #[cfg(test)] mod test { use super::*; #[test] fn test_ec_point_addition() { let p = BigUint::from(17u32); let ec = EllipticCurve { a: Field::new(2, &p), b: Field::new(2, &p), }; // (6, 3) + (5, 1) = (10, 6); let p1 = Point::Coor(Field::new(6, &p), Field::new(3, &p)); let p2 = Point::Coor(Field::new(5, &p), Field::new(1, &p)); let r = Point::Coor(Field::new(10, &p), Field::new(6, &p)); let res = ec.add(&p1, &p2); assert_eq!(r, res); } #[test] fn test_ec_point_add_identity() { let p = BigUint::from(17u32); let ec = EllipticCurve { a: Field::new(2, &p), b: Field::new(2, &p), }; // (6, 3) + (5, 1) = (10, 6); let p1 = Point::Coor(Field::new(6, &p), Field::new(3, &p)); let p2 = Point::Identity; let expect = Point::Coor(Field::new(6, &p), Field::new(3, &p)); let result = ec.add(&p1, &p2); assert_eq!(expect, result); } #[test] fn test_scalar_mul() { let p = BigUint::from(17u32); let ec = EllipticCurve { a: Field::new(2, &p), b: Field::new(2, &p), }; let q = Point::Coor(Field::new(5, &p), Field::new(1, &p)); let k = BigUint::from(16u32); let result = ec.scalar_mul(&q, &k); let expected = Point::Coor(Field::new(10, &p), Field::new(11, &p)); assert_eq!(result, expected); } #[test] fn test_double() { let p = BigUint::from(17u32); let ec = EllipticCurve { a: Field::new(2, &p), b: Field::new(2, &p), }; let p = Point::Coor(Field::new(6, &p), Field::new(3, &p)); let double = ec.double(&p); let p_on_curve = ec.is_on_curve(&double); assert!(p_on_curve); } } #### Specific case: The Secp256k1 Configuration of the values: n(order), p, a, b can be found [in this wiki page](https://en.bitcoin.it/wiki/Secp256k1). We test our algorithm by the following test case: rust fn test_secp256k1() { \\t\\tlet p = BigUint::parse_bytes( \\t\\t\\t\\tb\\"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEFFFFFC2F\\", \\t\\t\\t\\t16, \\t\\t) \\t\\t.expect(\\"Parsing fail for p\\"); \\t\\tlet a = BigUint::parse_bytes( \\t\\t\\t\\tb\\"0000000000000000000000000000000000000000000000000000000000000000\\", \\t\\t\\t\\t16, \\t\\t) \\t\\t.expect(\\"Parsing fail for a\\"); \\t\\tlet b = BigUint::parse_bytes( \\t\\t\\t\\tb\\"0000000000000000000000000000000000000000000000000000000000000007\\", \\t\\t\\t\\t16, \\t\\t) \\t\\t.expect(\\"Parsing fail for b\\"); \\t\\tlet n = BigUint::parse_bytes( \\t\\t\\t\\tb\\"FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEBAAEDCE6AF48A03BBFD25E8CD0364141\\", \\t\\t\\t\\t16, \\t\\t) \\t\\t.expect(\\"Parsing fail for n\\"); \\t\\tlet x = BigUint::parse_bytes( \\t\\t\\t\\tb\\"79BE667EF9DCBBAC55A06295CE870B07029BFCDB2DCE28D959F2815B16F81798\\", \\t\\t\\t\\t16, \\t\\t) \\t\\t.expect(\\"Parsing fail for x\\"); \\t\\tlet y = BigUint::parse_bytes( \\t\\t\\t\\tb\\"483ADA7726A3C4655DA4FBFC0E1108A8FD17B448A68554199C47D08FFB10D4B8\\", \\t\\t\\t\\t16, \\t\\t) \\t\\t.expect(\\"Parsing fail for y\\"); \\t\\tlet point = Point::Coor(Field { value: x, p: &p }, Field { value: y, p: &p }); \\t\\tlet ec = EllipticCurve { \\t\\t\\t\\ta: Field { value: a, p: &p }, \\t\\t\\t\\tb: Field { value: b, p: &p }, \\t\\t}; \\t\\tlet result = ec.scalar_mul(&point, &n); \\t\\tassert_eq!(Point::Identity, result); } #### The Elliptic Curve Digital Signature Algorithm (ECDSA) ##### Statement of the Theorem and Proof Let $G$ be the group of points on an elliptic curve $C: y^2 = x^3+ax+b$ over $\\\\mathbb Z_p$. > **Theorem.** Let $g\\\\in \\\\mathbb Z_p\\\\times \\\\mathbb Z_p$ be a given generator of the additive group $G$. For a fixed $k_\\\\text{pri}\\\\in \\\\mathbb Z_p$, define $K_\\\\text{pub} = k_\\\\text{pri} g$, then for every $k,z\\\\in \\\\mathbb Z_{|G|}$, there holds > > $$ > \\\\begin{aligned} > &\\\\qquad \\\\,\\\\,\\\\,\\\\,\\\\begin{cases} > R := \\\\pi_x(kg), \\\\\\\\ > S := k^{-1}\\\\big(z +\\\\pi_x(kg) k_\\\\text{pri}\\\\big ) > \\\\end{cases}\\\\\\\\ > &\\\\implies\\\\pi_x\\\\bigg(\\\\big[S^{-1}z\\\\big]g + \\\\big[S^{-1}R\\\\big]K_\\\\text{pub}\\\\bigg)\\\\equiv \\\\pi_x(kg)\\\\pmod{|G|}. > \\\\end{aligned} > $$ > > Where $\\\\pi_x$ denotes the canonical projection to the first coordinate. > > In other words, if $(R,S)$ defined above is given to the target receiver, then **_necessarily_** > > $$ > \\\\pi_x\\\\bigg(\\\\big[S^{-1}z\\\\big]g + \\\\big[S^{-1}R\\\\big]K_\\\\text{pub}\\\\bigg)\\\\equiv R \\\\pmod{|G|}. > $$ > > - This **necessary** condition is defined to be the **_valid_** condition of a message, where $z=h(m)$ for some hash $h:\\\\texttt{&str}\\\\to\\\\texttt{u32}$ and string $m: \\\\texttt{&str}$ the message. > - The tuple $(R,S)$ is called the **_signature_** of the message. Note that here $K_\\\\text{pub}$ is called a **_public key_**, $k_\\\\text{pri}$ a **_private key_** and $k$ a **_random number_**. <proof> **_Proof._** The proof is a direct transformation from the definition over field $\\\\mathbb Z_p$: $$ \\\\begin{aligned} S&= k^{-1}(z + \\\\pi_x(kg)k_\\\\text{pri})\\\\\\\\ k &\\\\equiv S^{-1}(z + \\\\underbrace{\\\\pi_x(kg)}_{=:R}k_\\\\text{pri}) \\\\pmod{|G|}\\\\\\\\ kg &\\\\equiv [S^{-1}z]g + [S^{-1}R]k_\\\\text{pri}g \\\\pmod{|G|}\\\\\\\\ kg &\\\\equiv [S^{-1}z]g + [S^{-1}R]K_\\\\text{pub} \\\\pmod{|G|}\\\\\\\\ R &\\\\equiv \\\\pi_x\\\\big( [S^{-1}z]g + [S^{-1}R]K_\\\\text{pub}\\\\big ). \\\\pmod{|G|} \\\\end{aligned} $$ The $(\\\\Longleftarrow)$ direction of the last line is incorrect because $\\\\pi_x(k_1g) = \\\\pi_x(k_2g)$ **_cannot_** imply $k_1=k_2$. For example, recall that $g$ is a generator of $G$, if $k_1 g=(x_0, y_0)$, then since $G$ is cyclic, there must be a unique $k_2\\\\neq k_1$ in $\\\\mathbb Z_{|G|}$ such that $k_2g = ({\\\\boldsymbol x_{\\\\boldsymbol 0}}, -y_0) \\\\in G$. </proof> It has to be careful that $k\\\\cdot g$ is calculated on $\\\\mathbb Z_p$ but the message validation above are operated on $\\\\mathbb Z_{|G|}$. \x3c!-- ##### How to use This Theorem? --\x3e ##### Coding rust #![allow(unused)] #![allow(non_snake_case)] use core::panic; use crate::modules::elliptic_curve::{EllipticCurve, Field, Point}; use crate::modules::field_utils::Futil; use num_bigint::{BigUint, RandBigInt}; use rand::{self, Rng}; use sha256::{digest, try_digest}; #[derive(PartialEq, Clone, Debug)] pub struct ECDSA<\'a> { pub elliptic_curve: EllipticCurve<\'a>, pub generator: Point<\'a>, pub order: BigUint, } impl<\'a> ECDSA<\'a> { pub fn generate_key_pair(&\'a self) -> (BigUint, Point<\'a>) { let priv_key = self.generate_private_key(); let pub_key = self.generate_public_key(&priv_key); return (priv_key, pub_key); } pub fn generate_private_key(&self) -> BigUint { self.generate_random_positive_number_less_than(&self.order) } pub fn generate_random_positive_number_less_than(&self, max: &BigUint) -> BigUint { let mut rng = rand::thread_rng(); rng.gen_biguint_range(&BigUint::from(1u32), &max) } pub fn generate_public_key(&\'a self, priv_key: &BigUint) -> Point<\'a> { self.elliptic_curve.scalar_mul(&self.generator, priv_key) } pub fn sign(&\'a self, hash: &BigUint, priv_key: &BigUint, k_random: &BigUint) -> (BigUint, BigUint) { assert!(hash < &self.order, \\"Hash is bigger than the order of Elliptic Curve\\"); assert!( priv_key < &self.order, \\"Private key has value bigger than the order of Elliptic Curve\\" ); assert!(k_random < &self.order, \\"k_random has value bigger than the order of Elliptic Curve\\"); let g = &self.generator; let z = hash; let k_pri = priv_key; let kg = self.elliptic_curve.scalar_mul(g, &k_random); match kg { Point::Identity => panic!(\\"Public key should not be an identity.\\"), Point::Coor(kg_x, _) => { let R = kg_x.value; let S = Futil::mul(&R, &priv_key, &self.order); let S = Futil::add(&z, &S, &self.order); let S = Futil::mul(&Futil::mul_inverse(k_random, &self.order), &S, &self.order); (R, S) } } } pub fn verify(&self, hash: &BigUint, pub_key: &Point, signature: &(BigUint, BigUint)) -> bool { assert!(hash < &self.order, \\"Hash is bigger than the order of the elliptic curve\\"); let (R, S) = signature; let z = hash; let P = self.elliptic_curve.add( // &self.elliptic_curve.scalar_mul(&self.generator, (z / S)), &self .elliptic_curve .scalar_mul(&self.generator, &Futil::mul(&z, &Futil::mul_inverse(S, &self.order), &self.order)), &self .elliptic_curve .scalar_mul(pub_key, &Futil::mul(R, &Futil::mul_inverse(S, &self.order), &self.order)), ); if let Point::Coor(X, Y) = &P { (&X.value - R).modpow(&BigUint::from(1u32), &self.order) == BigUint::from(0u32) } else { false } } pub fn generate_hash_less_than(&self, message: &str, max: &BigUint) -> BigUint { let digested = digest(message); let bytes = hex::decode(&digested).expect(\\"Cannot convert to Vec<u8>\\"); let one = BigUint::from(1u32); let hash = BigUint::from_bytes_be(&bytes).modpow(&one, &(max - &one)); let hash = hash + one; hash } } ##### Tests rust #[cfg(test)] mod test { use crate::modules::curves::{Curve, CurveConfig}; use super::*; #[test] fn test_sign_verify() { let p = BigUint::from(17u32); let ec = EllipticCurve { a: Field::new(2, &p), b: Field::new(2, &p), }; let gp_order = BigUint::from(19u32); let g = Point::Coor(Field::new(5, &p), Field::new(1, &p)); let ecdsa = ECDSA { elliptic_curve: ec, generator: g, order: gp_order, }; let priv_key = BigUint::from(7u32); let pub_key = ecdsa.generate_public_key(&priv_key); let hash = Field::new(10, &p); let k_random = BigUint::from(18u32); let message = \\"Bob -> 1BTC -> Alice\\"; let hash_ = ecdsa.generate_hash_less_than(message, &ecdsa.order); let hash = BigUint::from(hash_); let signature = ecdsa.sign(&hash, &priv_key, &k_random); let verify_result = ecdsa.verify(&hash, &pub_key, &signature); assert!(verify_result); } #[test] fn test_sign_tempered_verify() { let p = BigUint::from(17u32); let ec = EllipticCurve { a: Field::new(2, &p), b: Field::new(2, &p), }; let gp_order = BigUint::from(19u32); let g = Point::Coor(Field::new(5, &p), Field::new(1, &p)); let ecdsa = ECDSA { elliptic_curve: ec, generator: g, order: gp_order, }; let priv_key = BigUint::from(7u32); let pub_key = ecdsa.generate_public_key(&priv_key); let hash = Field::new(10, &p); let k_random = BigUint::from(18u32); let message = \\"Bob -> 1BTC -> Alice\\"; let hash_ = ecdsa.generate_hash_less_than(message, &ecdsa.order); let hash = BigUint::from(hash_); let signature = ecdsa.sign(&hash, &priv_key, &k_random); let (R, S) = signature; let R = R + BigUint::from(1u32); let R = Futil::power(&R, 1, &ecdsa.order); let tempered_signature = (R, S); let verify_result = ecdsa.verify(&hash, &pub_key, &tempered_signature); assert!(!verify_result); } #[test] fn test_secp256_sign_verify_tempered<\'a>() { let CurveConfig { a, b, generator, order, p } = Curve::get_Secp256k1_config(); let (x, y) = generator; let generator = Point::Coor(Field { value: x, p: &p }, Field { value: y, p: &p }); let ec = Curve::get_elliptic_cuve(&p, &a, &b); let ecdsa = Curve::get_ecdsa(&ec, &generator, &order); // modifiied from the order n let priv_key = BigUint::parse_bytes(b\\"FFFFF0000FFF0F0F0F0F0F0F0F0F0F0EBAAEDCE6AF48A03BBFD25E8CD0364141\\", 16) .expect(\\"Cannot parse into interger\\"); let pub_key = ecdsa.generate_public_key(&priv_key); let hash = Field::new(10, &p); let k_random = ecdsa.generate_random_positive_number_less_than(&order); let message = \\"Bob -> 1BTC -> Alice\\"; let hash_ = ecdsa.generate_hash_less_than(message, &ecdsa.order); let hash = BigUint::from(hash_); let signature = ecdsa.sign(&hash, &priv_key, &k_random); // let verify_result = ecdsa.verify(&hash, &pub_key, &signature); // println!(\\"hash: {}\\", hash); // assert!(verify_result); let (R, S) = signature; let R = R + BigUint::from(1u32); let R = Futil::power(&R, 1, &ecdsa.order); let tempered_signature = (R, S); let verify_result = ecdsa.verify(&hash, &pub_key, &tempered_signature); assert!(!verify_result); } } #### Reference - Udemy course: [Elliptic Curve Cryptography in Rust](https://www.udemy.com/course/elliptic-curve-cryptography-in-rust/) - [\u4e00\u6587\u8bfb\u61c2 ECDSA \u7b97\u6cd5\u5982\u4f55\u4fdd\u62a4\u6570\u636e](https://zhuanlan.zhihu.com/p/97953640?fbclid=IwAR0MMbQbGXVTAcErHNlHWRU1lUnpzqHTGdmB7rUxiD-xfFgguh_czX-gm50) ","title":"Elliptic Curve and Operator Overloading","date":"2023-09-09T00:00:00.000Z","id":"blog0175","tag":"rust","intro":"Let\'s define operator overloading on finite field Z/pZ for prime p.","toc":true},{"content":" Create a rustfmt.toml at the project root level, then use text edition = \\"2018\\" color = \\"Auto\\" unstable_features = true # Width max_width = 140 # Layout indent_style = \\"Block\\" brace_style = \\"PreferSameLine\\" control_brace_style = \\"AlwaysSameLine\\" imports_indent = \\"Block\\" fn_args_layout = \\"Compressed\\" match_arm_blocks = true # Delimiter trailing_comma = \\"Vertical\\" trailing_semicolon = true match_block_trailing_comma = false binop_separator = \\"Front\\" overflow_delimited_expr = true # Line newline_style = \\"Unix\\" fn_single_line = true struct_lit_single_line = true where_single_line = false empty_item_single_line = true wrap_comments = true blank_lines_lower_bound = 0 blank_lines_upper_bound = 1 # Space tab_spaces = 4 hard_tabs = false space_after_colon = true space_before_colon = false spaces_around_ranges = false space_around_attr_eq = true combine_control_expr = true type_punctuation_density = \\"Wide\\" struct_field_align_threshold = 0 enum_discrim_align_threshold = 0 # Simplification use_field_init_shorthand = true use_try_shorthand = true remove_nested_parens = true merge_derives = true merge_imports = true normalize_comments = false normalize_doc_attributes = true condense_wildcard_suffixes = true # Sort reorder_imports = true reorder_modules = true reorder_impl_items = true # Force force_explicit_abi = true force_multiline_blocks = false # Format disable_all_formatting = false format_code_in_doc_comments = true format_macro_matchers = true format_macro_bodies = true format_strings = true # Error error_on_line_overflow = false error_on_unformatted = false hide_parse_errors = false ","title":"Rust Formatter Configruation","date":"2023-09-10T00:00:00.000Z","id":"blog0176","tag":"rust","intro":"Record a configuration for auto-formattting.","toc":false},{"content":" <center></center> - **Step 1.** yarn add env-cmd - **Step 2.** Create a .env-cmdrc in root level: text { \\"default\\": { \\"PORT\\": 8080 }, \\"production\\": { \\"GOOGLE_CLIENT_ID\\": \\"XXX.com\\", \\"GOOGLE_CLIENT_SECRET\\": \\"YYY\\", \\"GOOGLE_API_REDIRECT\\": \\"http://localhost:8080/login-google\\", \\"ALLOWED_EMAILS\\": [\\"machingclee@gmail.com\\", \\"james.lee@wonderbricks.com\\"], \\"JWT_SECRET\\": \\"SECRET!\\", \\"FRONTEND_URL\\": \\"http://localhost:3000\\" } } - **Step 3.** We prefix any command by text env-cmd -f .env-cmdrc -e default,production For example: text env-cmd -f .env-cmdrc -e default,production nodemon --exec ts-node src/app.ts ","title":"Environment Variable by env-cmdrc","date":"2023-09-11T00:00:00.000Z","id":"blog0177","tag":"nodejs","intro":"Package for combining env files conveniently.","toc":false},{"content":" #### Client Side ##### List of Listeners js export default function WebSocket() { const socket = useRef<ReturnType<typeof io> | null>(null); const addMessage = (msg: Message) => { setMessagesFromServer(msgs => { const msgs_ = lodash.cloneDeep(msgs); msgs_.push(msg); return msgs_; }) } const getSocketConnection = () => { if (socket.current) { return; } socket.current = io(SOCKET_SERVER, { withCredentials: true }); socket.current.on(\\"connect\\", () => { addMessage({ sender: \\"System\\", msg: \\"Connected\\" }); }) socket.current.on(MSG_TO_CLIENTS, (data: Message) => { addMessage(data); }) socket.current.on(\\"disconnect\\", (reason) => { // note that if a user is disconnected actively by server (like no token is found) // the reason will be \\"io server disconnect\\" addMessage({ sender: \\"disconnect\\", msg: JSON.stringify(reason) }); }) socket.current.on(\\"connect_error\\", (err) => { addMessage({ sender: \\"connect_error\\", msg: JSON.stringify(err) }); // addMessage({ sender: \\"connect_error\\", msg: \\"Closing connection ...\\" }); // socket.current?.disconnect(); // socket.current = null; }); socket.current.io.on(\\"reconnect_attempt\\", (data) => { addMessage({ sender: \\"reconnect_attempt\\", msg: String(JSON.stringify(data)) }); }); socket.current.io.on(\\"reconnect_error\\", (error) => { addMessage({ sender: \\"reconnect_error\\", msg: String(JSON.stringify(error)) }); }); socket.current.io.on(\\"reconnect_failed\\", () => { addMessage({ sender: \\"reconnect_failed\\", msg: \\"\\" }); }); } } ##### Error Observation by Adruptly Closing the Server We can make use of the observations below to determine: - When should we close the socket connection and - What error message to display to the client. ![](/assets/tech/178/001.png) #### Server Side ##### Entrypoint: app.ts js import getMongoConnection from \\"./db/getMongoConnection\\"; import expressService from \\"./service/expressService\\"; expressService.initExpressApp([ async () => { await getMongoConnection(); }, ]); ##### Servics ###### expressService.ts Here we try to split configuration into separate files, with configSocketio the only exception since it does not have api like app.ws for us the configure to Express object directly. js import express from \\"express\\"; import \\"express-async-errors\\"; import http from \\"http\\"; import configCors from \\"../config/configCors\\"; import configParsers from \\"../config/configParsers\\"; import configRouting from \\"../config/configRouting\\"; import { IORef } from \\"../dto/types\\"; import configSocketio from \\"../config/configSocketio\\"; import configErrorHandler from \\"../config/configErrorHandler\\"; const { PORT } = process.env; const app = express(); const ioRef: IORef = { current: undefined }; app.set(\\"trust proxy\\", 1); configCors(app); configParsers(app); configRouting(app); configErrorHandler(app); const initExpressApp = (callbacks: (() => Promise<void> | void)[]) => { const httpServer = http.createServer(app); configSocketio(httpServer, ioRef); const port = PORT || \\"8080\\"; httpServer.listen(parseInt(port), async () => { for (const cb of callbacks) { await cb(); } console.log(App running on port ${port}); }); return app; }; const getSocketIo = () => { return ioRef; }; export default { initExpressApp, getSocketIo, }; ###### socketService.ts - Since each user should have at most one socket connecting to the server, we use a Map object socketStore to store the correspondence between userOid and socket. - We will plug the socket object into req: Express.Request in the middleware socketioMiddleware below. - Then the controllers in chatRouter can get access to user\'s socket (as well as the server side io object). js import { Socket } from \\"socket.io\\"; import expressService from \\"./expressService\\" import { DefaultEventsMap } from \\"socket.io/dist/typed-events\\"; const socketStore = new Map<string, Socket<DefaultEventsMap, DefaultEventsMap, DefaultEventsMap, any>>(); const saveSocket = (uuid: string, socket: Socket<DefaultEventsMap, DefaultEventsMap, DefaultEventsMap, any>) => { socketStore.set(uuid, socket); }; const getSocket = (userOid: string) => { console.log(\\"token\\", userOid); return socketStore.get(userOid) || undefined; } const deleteSocket = (uuid: string) => { socketStore.delete(uuid); } const getIo = () => { const ioRef = expressService.getSocketIo(); return ioRef.current; } export default { getIo, saveSocket, getSocket, deleteSocket } ##### Mongo Connection ###### getMongoConnection.ts js import mongoose from \\"mongoose\\"; let connCache: typeof mongoose; const { DB_URL } = process.env; const getMongoConnection = async () => { console.log(\\"Connecting to mongo ...\\"); if (!DB_URL) { throw new Error(\\"DB_URL cannot be found.\\"); } if (!connCache) { console.log(\\"Mongo connected.\\"); connCache = await mongoose.connect(DB_URL); } return connCache; }; export default getMongoConnection; ###### Models (aka Collections) - **_Users._** js import mongoose, { InferSchemaType, Schema } from \\"mongoose\\"; export const userSchema = new Schema( { name: { type: String, required: true, index: true }, email: { type: String, required: true }, passwordHash: { type: String, required: true }, }, { timestamps: { createdAt: \\"created_at\\", updatedAt: \\"updated_at\\" }, } ); export type User = InferSchemaType<typeof userSchema>; export const UserModel = mongoose.model(\\"User\\", userSchema); - **_ChatRoom._** js import mongoose, { InferSchemaType, Schema } from \\"mongoose\\"; const chatSessionSchema = new Schema( { code: { type: String, required: true, index: true }, hostUserOid: { type: String, required: true }, active: { type: Boolean, require: true }, members: { type: [{ userOid: { type: String, required: true } }], default: [], }, }, { timestamps: { createdAt: \\"createdAt\\", updatedAt: \\"updatedAt\\" }, } ); export type ChatSession = InferSchemaType<typeof chatSessionSchema>; export const ChatSessionModel = mongoose.model( \\"ChatSession\\", chatSessionSchema ); ##### Utils for JWT Authentication ###### tokenUtils.ts js import jwt from \\"jsonwebtoken\\"; import { User, UserModel } from \\"../db/models/User\\"; import { TokenInfo } from \\"../dto/types\\"; import { Types } from \\"mongoose\\"; const { JWT_SECRET = \\"\\", JWS_SALT_ROUNDS = \\"\\", JWT_EXPIRE_IN = \\"\\" } = process.env; const getTokenFromUser = (user: User & { _id: Types.ObjectId }): Promise<string> => { return new Promise((resolve, reject) => { try { if (!JWT_SECRET) { reject(new Error(\\"jwt secret not found\\")); } const data: TokenInfo = { userOid: user._id.toString(), email: user.email, name: user.name }; const token = jwt.sign( data, JWT_SECRET || \\"\\", { expiresIn: JWT_EXPIRE_IN } ); resolve(token); } catch (err) { reject(err); } }); } const getUserFromToken = async (token: string) => { const decoded = await verifyToken(token) as TokenInfo; const userInToken: TokenInfo = decoded; return userInToken } const verifyToken = (token: string) => { return new Promise((resolve, reject) => { try { const decoded = jwt.verify(token, JWT_SECRET); resolve(decoded); } catch (err) { reject(err); } }) } export default { getTokenFromUser, verifyToken, getUserFromToken, } ###### hashUtil.ts js import bcrypt from \\"bcrypt\\"; const { JWS_SALT_ROUNDS = \\"0\\" } = process.env; const passwordIntoHash = (password: string) => { return new Promise((resolve, reject) => { bcrypt.hash(password, parseInt(JWS_SALT_ROUNDS), (err, hash) => { if (err) { return reject(err); } resolve(hash); }); }); }; const comparePasswordWithHash = (password: string, hash: string) => { return new Promise((resolve, reject) => { bcrypt.compare(password, hash, (err, result) => { if (err) { return reject(err); } resolve(result); }); }); }; export default { passwordIntoHash, comparePasswordWithHash, }; ##### Middlewares ###### errorMiddleware.ts js import { NextFunction, Request, Response } from \\"express\\"; export default ( err: Error, req: Request, res: Response, next: NextFunction ) => { if (err) { res.json({ success: false, errorMessage: err?.message }); } }; ###### socketioMiddleware.ts <----------- inject userSocket and io into req here! js import { NextFunction, Request, Response } from \\"express\\"; import socketService from \\"../service/socketService\\"; export default (req: Request, res: Response, next: NextFunction) => { try { const userOid = req.user?.userOid!; const socket = socketService.getSocket(userOid); req.userSocket = socket; const io = socketService.getIo(); req.io = io; next(); } catch (err) { next(err); } } ###### jwtAuthMiddleware.ts <------------ We parse token into req.user here! js import { NextFunction, Request, Response } from \\"express\\"; import authUtil from \\"../util/tokenUtil\\"; import { User } from \\"../db/models/User\\"; import { Token } from \\"typescript\\"; import { TokenInfo } from \\"../dto/types\\"; export default async (req: Request, res: Response, next: NextFunction) => { try { const token = req.cookies?.[\\"token\\"]; const user = await authUtil.getUserFromToken(token); req.user = user; next(); } catch (err) { next(err); } }; ##### Routers ###### authRouter.ts <--------------------- We set cookie here! js import express from \\"express\\"; import { UserModel } from \\"../db/models/User\\"; import authUtil from \\"../util/tokenUtil\\"; import hashUtil from \\"../util/hashUtil\\"; import tokenUtil from \\"../util/tokenUtil\\"; const authRouter = express.Router(); authRouter.post(\\"/login\\", async (req, res, next) => { try { const { email, password } = req.body as { email: string, password: string }; const user = await UserModel.findOne({ email }).exec(); if (user) { const { passwordHash } = user; const valid = await hashUtil.comparePasswordWithHash(password, passwordHash); if (valid) { const token = await tokenUtil.getTokenFromUser(user); res.cookie(\\"token\\", token, { httpOnly: true, sameSite: \\"none\\", path: \\"/\\", secure: true }); res.json({ success: true }); } else { throw new Error(\\"Username or password is incorrect\\"); } } } catch (err) { next(err); } }); authRouter.get(\\"/refresh\\", (req, res) => { res.json({ hello: \\"world\\" }); }); export default authRouter; ###### userRouter js import express, { NextFunction } from \\"express\\"; import { UserModel } from \\"../db/models/User\\"; import hashUtil from \\"../util/hashUtil\\"; import tokenUtil from \\"../util/tokenUtil\\"; const userRouter = express.Router(); userRouter.post(\\"/create\\", async (req, res, next: NextFunction) => { try { const { email, name, password } = req.body as { email: string, name: string, password: string }; if (!(email && name && password)) { next(new Error(\\"Email, name or password is null\\")); } const dbUser = await UserModel.findOne({ email }).exec(); if (dbUser) { next(new Error(\\"User already exists\\")); } else { const passwordHash = await hashUtil.passwordIntoHash(password); const userDoc = await new UserModel({ email, name, passwordHash }); const user = await userDoc.save(); const token = await tokenUtil.getTokenFromUser(user); // expiration is controlled by jwtMiddleware. res.cookie(\\"token\\", token); res.json({ success: true, message: User ${name} has been created successfully. }); } } catch (err) { next(err); } }); export default userRouter; ###### chatRouter js import express from \\"express\\"; import chatController from \\"../controller/chatController\\"; const chatRouter = express.Router(); chatRouter.get(\\"/rooms\\", chatController.getRooms); chatRouter.get(\\"/create-room\\", chatController.createRoom); chatRouter.get(\\"/join-room/:roomCode\\", chatController.joinRoom); chatRouter.post(\\"/message\\", chatController.sendMessageFromClient); export default chatRouter; with js import { NextFunction, Request, Response } from \\"express\\" import { Room, RoomModel } from \\"../db/models/Room\\"; import codeUtil from \\"../util/codeUtil\\"; import chatService from \\"../service/chatService\\"; const getRooms = async (req: Request, res: Response) => { const results = await RoomModel.find({}).exec(); const rooms = results.map(r => r.toObject()); res.json({ success: true, result: { rooms } }) }; const createRoom = async (req: Request, res: Response) => { const { roomName } = req.body as { roomName: string }; const code = codeUtil.generateCode(); const roomProps: Room = { active: true, code, hostUserOid: req.user?.userOid || \\"\\", members: [{ userOid: req.user?.userOid || \\"\\" }], name: roomName } const room = new RoomModel(roomProps).save(); res.json({ success: true, result: { newRoom: room } }); }; const sendMessageFromClient = async (req: Request, res: Response) => { const { roomCode, msg } = req.body as { roomCode: string, msg: string }; const room = await chatService.findRoomDocByCode(roomCode); const roomName = room?.name || \\"\\" req.io?.to(roomCode).emit(...chatService.createMsgToClients({ sender: ${req.user?.name} (from ${roomName || \\"unknown\\"}) || \\"\\", msg: msg })); } const joinRoom = async (req: Request, res: Response) => { // disconnect all existing rooms const userConnectedRooms = req.userSocket?.rooms; if (userConnectedRooms) { userConnectedRooms.forEach(async (roomCode) => { try { const room = await chatService.getRoombyOid(roomCode); if (room) { req.io?.to(roomCode).emit(...chatService.createMsgToClients({ sender: \\"server\\", msg: ${req.user?.name || \\"\\"} leaved room: ${room.name} })); } } catch (err) { } req.userSocket?.leave(roomCode); }) } // start to join new room const { roomCode } = req.params; const room = await chatService.findRoomDocByCode(roomCode); if (room) { req.userSocket?.join(roomCode!); } else { throw new Error(No room of code ${roomCode} exists); } // history purpose const existingUser = room?.members.find(m => m.userOid === req.user?.userOid); if (!existingUser) { room?.members.push({ userOid: req.user!.userOid }); await room?.save(); } req.io?.to(roomCode).emit(...chatService.createMsgToClients({ sender: \\"server\\", msg: ${req.user?.name} just connected to room ${room.name} })); res.json({ success: true }); } export default { getRooms, joinRoom, sendMessageFromClient, createRoom } ##### Configuration Files ###### configCors.ts js import { Express } from \\"express\\"; import cors, { CorsOptions } from \\"cors\\"; const { ALLOWED_ORIGINS } = process.env; export default (app: Express) => { const allowlist = ALLOWED_ORIGINS?.split(\\",\\") || []; const corsOptionsDelegate = (req: any, callback: any) => { let corsOptions: CorsOptions; if (allowlist.indexOf(req.header(\\"Origin\\")) > -1) { corsOptions = { origin: true, credentials: true }; } else { corsOptions = { origin: false }; } // callback expects two parameters: (error, options) callback(null, corsOptions); }; app.use(cors(corsOptionsDelegate)); }; ###### configErrorHandler.ts js import errorMiddleware from \\"../middleware/errorMiddleware\\"; import { Express } from \\"express\\"; export default (app: Express) => { app.use(errorMiddleware); }; ###### configParsers.ts js import bodyParser from \\"body-parser\\"; import cookieParser from \\"cookie-parser\\"; import { Express } from \\"express\\"; export default (app: Express) => { app.use(cookieParser()); app.use(bodyParser.json()); }; ###### configRouting.ts <----------- jwtAuthMiddleware and socketioMiddleware! js import jwtAuthMiddleware from \\"../middleware/jwtAuthMiddleware\\"; import socketioMiddleware from \\"../middleware/socketioMiddleware\\"; import authRouter from \\"../router/authRouter\\"; import chatRouter from \\"../router/chatRouter\\"; import userRouter from \\"../router/userRouter\\"; import { Express } from \\"express\\"; export default (app: Express) => { app.use(\\"/chat\\", jwtAuthMiddleware, socketioMiddleware, chatRouter); app.use(\\"/user\\", userRouter); app.use(\\"/auth\\", authRouter); }; ###### configSocketio.ts: <---------------- We parse token here! js import { Server } from \\"socket.io\\"; import http from \\"http\\"; import { IORef } from \\"../dto/types\\"; const { ALLOWED_ORIGINS } = process.env; const allowlist = ALLOWED_ORIGINS?.split(\\",\\") || []; import cookie from \\"cookie\\"; import userSerevice from \\"../service/userSerevice\\"; import { serialize, parse } from \\"cookie\\"; import { v4 as uuidv4 } from \\"uuid\\"; import socketService from \\"../service/socketService\\"; import chatService from \\"../service/chatService\\"; export const MSG_TO_CLIENTS = \\"MSG_TO_CLIENTS\\"; export default (httpServer: http.Server, ioRef: IORef) => { ioRef.current = new Server(httpServer, { cookie: true, allowUpgrades: true, cors: { origin: allowlist, methods: [\\"GET\\", \\"POST\\"], credentials: true, }, }); const io = ioRef.current; io.on(\\"connect\\", async (socket) => { try { const cookieString = cookie.parse(socket.request.headers.cookie || \\"\\"); const token = cookieString?.[\\"token\\"]; if (!token) { console.log(socket was dropped because no token is found.); return socket.disconnect(); } let usernameInDb = \\"\\"; try { const user = await userSerevice.getUserFromToken(token); if (user) { usernameInDb = ${user.name}; console.log(${usernameInDb} has connected); socketService.saveSocket(user.userOid, socket); socket.emit( ...chatService.createMsgToClients({ sender: \\"server\\", msg: \\"Connected.\\", }) ); } socket.on(\\"disconnect\\", (reason) => { socketService.deleteSocket(user.userOid); console.log(socketService.deleteSocket(${user.userOid});); console.log(${usernameInDb} has disconnected because: ${reason}); }); } catch (err) { socket.disconnect(); } } catch (error) { io.close(); } }); }; ##### The desc.d.ts for req.user Since by default there is no req.user for Express.Request object, we need to declare it in desc.d.ts: js export type TokenInfo = { userOid: string, email: string, name: string } declare global { namespace Express { interface Request { user?: TokenInfo } } } And this special desc.d.ts needs to be included as files in tsconfig.json: json { \\"compilerOptions\\": { ... }, ... \\"files\\": [ \\"src/desc.d.ts\\" ] } ","title":"Setup of Express with Socket.io with JWT Authentication Using Cookie","date":"2023-09-16T00:00:00.000Z","id":"blog0178","tag":"nodejs, express, jwt, socketio","intro":"Basic reivew of API provided by socket.io-client and socket.io in nodejs.","toc":true},{"content":" #### Installation text yarn add jest typescript ts-jest @types/jest text yarn ts-jest config:init At package.json, add json { \\"script\\": { \\"test\\": \\"env-cmd -f .env-cmdrc -e default,dev jest --coverage\\" } } This will debug all the file that bare the name as suffix: .test.ts. #### Debug a Single Jest File Let\'s define the following in .vscode/launch.json js { \\"version\\": \\"0.2.0\\", \\"configurations\\": [ ..., { \\"type\\": \\"node-terminal\\", \\"request\\": \\"launch\\", \\"name\\": \\"Jest: Current File\\", \\"command\\": \\"yarn env-cmd -f .env-cmdrc -e default,dev jest ${fileBasenameNoExtension} --config jest.config.js --coverage\\", \\"cwd\\": \\"${workspaceRoot}\\", } ] } Now go to the file we want to debug, press F5. #### Examples of Test Files ##### Expect an Output is as Expected Let\'s consider testing an API to create a document: js import getMongoConnection from \\"../src/db/getMongoConnection\\"; import { NameSpaceModel } from \\"../src/db/models/NameSpace\\"; test(\\"Create Namespaces\\", async () => { await getMongoConnection(); const namespaceDoc = await new NameSpaceModel({ name: \\"wonderbricks\\", path: \\"/wonderbricks\\", }).save(); expect( namespaceDoc.name === \\"wonderbricks\\" && namespaceDoc.path === \\"/wonderbricks\\" ).toBe(true); }); And we get text $ C:\\\\Users\\\\user\\\\Repos\\\\wonderbricks\\\\2023-09-12-serverless-chat-billie\\\\node_modules\\\\.bin\\\\env-cmd -f .env-cmdrc -e default,dev jest new-namspace.test --config jest.config.js --coverage Debugger attached. Debugger attached. console.log Connecting to mongo ... at log (src/db/getMongoConnection.ts:8:13) console.log Mongo connected. at log (src/db/getMongoConnection.ts:13:17) PASS test/new-namspace.test.ts (5.286 s) \u221a Create Unique Namespaces (2858 ms) ------------------------|---------|----------|---------|---------|------------------- File | % Stmts | % Branch | % Funcs | % Lines | Uncovered Line #s ------------------------|---------|----------|---------|---------|------------------- All files | 93.33 | 33.33 | 100 | 92.85 | db | 91.66 | 33.33 | 100 | 90.9 | getMongoConnection.ts | 91.66 | 33.33 | 100 | 90.9 | 10 db/models | 100 | 100 | 100 | 100 | NameSpace.ts | 100 | 100 | 100 | 100 | ------------------------|---------|----------|---------|---------|------------------- Test Suites: 1 passed, 1 total Tests: 1 passed, 1 total Snapshots: 0 total Time: 5.449 s, estimated 8 s Ran all test suites matching /new-namspace.test/i. ##### Expect an Async Function not to Throw Error js test(\\"Create Unique Namespaces\\", async () => { const createNamespace = async () => { await getMongoConnection(); await new NameSpaceModel({ name: \\"jamesCompany\\", path: \\"/jamesCompany\\", }).save(); }; await expect(createNamespace()).resolves.not.toThrowError(); }); ","title":"Jest Fundamentals in TS","date":"2023-09-15T00:00:00.000Z","id":"blog0179","tag":"nodejs, test","intro":"Testings not only justify our functions are working, it also demonstrates how our function is used. We will be setting up tests in typescript.","toc":true},{"content":" rust use std::{net::SocketAddr, sync::Arc}; use tokio::{ io::{AsyncBufReadExt, AsyncWriteExt, BufReader}, net::TcpListener, sync::broadcast, }; #[tokio::main] async fn main() { let listener = TcpListener::bind(\\"localhost:8080\\").await.unwrap(); let (tx, _rx) = broadcast::channel::<(String, SocketAddr)>(10); let tx_ptr = Arc::new(tx); loop { let (mut socket, addr) = listener.accept().await.unwrap(); let tx_ptr = tx_ptr.clone(); tokio::spawn(async move { let (reader, mut writer) = socket.split(); let tx_ptr = tx_ptr.clone(); let mut reader = BufReader::new(reader); let mut line = String::new(); let mut rx = tx_ptr.subscribe(); loop { tokio::select! { result = reader.read_line(&mut line) => { if result.unwrap() == 0 { break; } tx_ptr.send((line.clone(), addr)).unwrap(); line.clear(); } result = (&mut rx).recv() => { let (msg, other_addr) = result.unwrap(); if !addr.eq(&other_addr) { writer.write_all(msg.as_bytes()).await.unwrap(); } } }; } }); } } ","title":"Simple Chat Server in Rust via Telnet","date":"2023-09-19T00:00:00.000Z","id":"blog0180","tag":"rust","intro":"Study of Tokio by building a chat server.","toc":false},{"content":" <style> img { max-width: 100% } </style> - Architecture: [![](/assets/tech/181/001.png)](/assets/tech/181/001.png) - Each of the u_k\'s connects to web-socket server as normal. - For each of ws_k\'s, they still rely on room\'s for publishing messages within a group of sockets. - Same room can repeatedly appears in each of the socket servers. - For example, room_1 can appear in both ws_1 and ws_2. - u_1 join room_1 via ws_1 and u_2 join room_1 via ws_2. - **_Fun part._** Now each of the ws_k servers registers publish and subscribe listeners to redis server. Note that the following code can be executed right before app.listen(). js-1 // ws_k // pubsubWithIo.ts // to be run right before app.listen(); import redis from \\"redis\\"; const subscriber = redis.createClient({ port: 6379, host: redis_url, }); export const publisher = redis.createClient({ port: 6379, host: redis_url, }); js-16 const io = getIoSingletonFromSomewhere(); subscriber.on(\\"subscribe\\", function (channel, count) { // do something, or even omit this listener }); export type Message = { namespace: string, roomCode: string, msg:{ sender:string, text: string } } subscriber.on(\\"message\\", function (_channel: string, message: Message) { try { const { namespace, roomCode, msg } = message; io.of(namespace).to(roomCode).emit(\\"MSG_TO_CLIENTS\\", msg); } catch (err) { consol.log(err); } }); subscriber.subscribe(\\"livechat\\"); Finally we also have a publish event, this is supposed to be wrapped inside a post request: js // ws_k import { publisher, Message } from some/where/pubsubWithIo; ... router.post(\\"/some-outer-route\\", async (req, res) => { const msg = req.body as Message; publisher.publish(\\"livechat\\", msg); res.json({ success: true }); }); - Now we easily scaled up the a chat server! #### Referenece - [Scaling Websockets with Redis, HAProxy and Node JS - High-availability Group Chat Application](https://www.youtube.com/watch?v=gzIcGhJC8hA&t=920s) - [Understanding Redis Pub/Sub (Getting Started)](https://www.youtube.com/watch?v=KIFA_fFzSbo&t=449s) ","title":"Scaling Websocket Chat Sever by Redis","date":"2023-09-20T00:00:00.000Z","id":"blog0181","tag":"redis, socketio, nodejs","intro":"We study how to scale up the chat services horizontally by Redis.","toc":false},{"content":" #### The Pipelines ##### createLeftJoin js const createLeftJoin = (params: { from: string, localField: { stringToOid?: boolean, fieldName: string }, foreignField: string, leftjoinPipeline: PipelineStage.Lookup[\\"$lookup\\"][\\"pipeline\\"], as: string, }) => { const { as: newFieldName, foreignField, from, localField, leftjoinPipeline, } = params; const pipeline: PipelineStage[] = [ { $addFields: { tmp_localfield: \\"$\\" + localField.fieldName, }, }, { $unwind: { path: \\"$tmp_localfield\\", preserveNullAndEmptyArrays: true, }, }, { $lookup: { let: { tmp: \\"$tmp_localfield\\" }, from: from, pipeline: [ { $match: { $expr: { $eq: [ \\"$\\" + foreignField, localField.stringToOid ? { $toObjectId: \\"$$tmp\\" } : \\"$$tmp\\", ], }, }, }, ...(leftjoinPipeline || []), ], as: \\"tmp_singleElementList\\", }, }, { $unwind: { path: \\"$tmp_singleElementList\\", preserveNullAndEmptyArrays: true, }, }, { $group: { _id: \\"$_id\\", originalData: { $first: \\"$$ROOT\\" }, pushList: { $push: \\"$tmp_singleElementList\\" }, }, }, { $replaceRoot: { newRoot: { $mergeObjects: [\\"$originalData\\", { [newFieldName]: \\"$pushList\\" }], }, }, }, { $unset: \\"tmp_singleElementList\\", }, { $unset: \\"tmp_localfield\\", }, ]; return pipeline; }; ##### createSqueeze js const createSqueeze = (fieldName: string): PipelineStage[] => { return [ { $unwind: { path: \\"$\\" + fieldName, preserveNullAndEmptyArrays: true, }, }, ]; }; #### Usage js const results = await RoomModel.aggregate([ { $match: {} }, { $project: { code: 1, name: 1, hostUserOid: 1, active: 1, createdAt: 1 }, }, ...mongoUtil.createLeftJoin({ from: UserModel.collection.name, localField: { stringToOid: true, fieldName: \\"hostUserOid\\" }, foreignField: \\"_id\\", leftjoinPipeline: [{ $project: { _id: 0, name: 1, email: 1 } }], as: \\"hostUser\\", }), ...mongoUtil.createSqueeze(\\"hostUser\\"), ]).exec(); Note that we also cover the case when our localField is an **_stringified_** ObjectId, we provide a stringToOid option to convert the string into ObjectId which inherits an index for searching. ","title":"Create a left-join in Mongoose","date":"2023-09-21T00:00:00.000Z","id":"blog0182","tag":"mongo","intro":"We create an aggregation pipeline which acts like a left-join.","toc":true},{"content":" #### Frontend: React Native Since we are dealing with audio, there is **_no_** UI element needed and hence we will have a bunch of configurations without elements like <audio /> and object of AudioTrack. js import { useRef, useState } from \\"react\\"; import { useAppDispatch, useAppSelector } from \\"../redux/app/hooks\\"; import { ChannelProfileType, ClientRoleType, IRtcEngine, RtcConnection, UserOfflineReasonType, createAgoraRtcEngine, } from \\"react-native-agora\\"; import { PermissionsAndroid } from \\"react-native\\"; import appSlice from \\"../redux/slices/appSlice\\"; import msgUtil from \\"../util/msgUtil\\"; import chatSlice, { ChatSliceState } from \\"../redux/slices/chatSlice\\"; const appId = process.env.EXPO_PUBLIC_ALGORA_APPID!; // also get host userid in the joinroom function export default () => { const dispatch = useAppDispatch(); const channelId = useAppSelector(s => s.chat.selectedRoomOid); const userId = useAppSelector(s => s.auth.userId); const engine = useRef<{ instance: IRtcEngine | null }>({ instance: null }); const { speakerphoneEnabled, joinSucceeded, localMuted, peerIds, rtcToken } = useAppSelector(s => s.chat.AgoraRtcConnection); const updateAgora = (update: Partial<ChatSliceState[\\"AgoraRtcConnection\\"]>) => { dispatch(chatSlice.actions.updateAgoraRtcConnection(update)); } const requestAudioPermission = async () => { const granted = await PermissionsAndroid.requestMultiple([ PermissionsAndroid.PERMISSIONS.RECORD_AUDIO ]); if (granted[\\"android.permission.RECORD_AUDIO\\"] !== PermissionsAndroid.RESULTS.GRANTED ) { dispatch(appSlice.actions.updateAppDialog({ open: true, title: \\"Permission Error\\", desc: \\"Microphone access has not been granted\\", ok: { label: \\"OK\\" } })); }; } const initRTC = async () => { const client = createAgoraRtcEngine(); engine.current.instance = client; client.enableAudio(); client.setEnableSpeakerphone(true); client.muteLocalAudioStream(false); // enableSpeakerphone is initialized to ture in useState client.setDefaultAudioRouteToSpeakerphone(true); client.muteLocalAudioStream(true); client.initialize({ appId, channelProfile: ChannelProfileType.ChannelProfileLiveBroadcasting }); client.addListener( \\"onJoinChannelSuccess\\", (channel, elapsed) => { try { console.log(\\"[onJoinChannelSuccess] AgoraRTC Join Success\\", channel, elapsed); msgUtil.success(\\"AgoraRTC Join Success\\", ${channel}, ${elapsed}) updateAgora({ joinSucceeded: true, peerIds: [channel.localUid!, ...peerIds] }) } catch (err) { console.log(\\"[onJoinChannelSuccess]\\" + JSON.stringify(err)); } } ); client.addListener( \\"onUserJoined\\", (_, remoteUid, __) => { try { console.log([onUserJoined] new remote uid: ${remoteUid}) if (peerIds.indexOf(remoteUid) > -1) { return; } // setPeerIds(ids => [...ids, remoteUid]); } catch (err) { console.log(\\"[onUserJoined]\\" + JSON.stringify(err)); } } ); client.addListener( \\"onUserOffline\\", (_, remoteUid, __) => { try { console.log([onUserOffline] ${remoteUid} offlined) updateAgora({ peerIds: peerIds.filter(id => id !== remoteUid) }) // if (remoteUid === hostUserId) { // leaveChannel(); // } } catch (err) { console.log(\\"[onUserOffline]\\" + JSON.stringify(err)); } } ); }; const joinChannel = () => { console.log(\\"join channel\\") engine.current.instance?.joinChannel( rtcToken, channelId, userId, { clientRoleType: ClientRoleType.ClientRoleBroadcaster } ); }; const leaveChannel = () => { console.log(\\"leave channel\\") engine.current.instance?.leaveChannel(); updateAgora({ peerIds: [], joinSucceeded: false }); }; const toggleMute = () => { engine.current.instance?.muteLocalAudioStream(!localMuted); updateAgora({ localMuted: !localMuted }); }; const toggleSpeakerphoneEnabled = () => { engine.current.instance?.setEnableSpeakerphone(!speakerphoneEnabled); updateAgora({ speakerphoneEnabled: !speakerphoneEnabled }); } const destroyAgora = () => { engine.current.instance?.removeAllListeners(); engine.current.instance = null; } return { requestAudioPermission, initRTC, joinChannel, leaveChannel, toggleMute, toggleSpeakerphoneEnabled, destroyAgora, peerIds, localMuted, speakerphoneEnabled, joinSucceeded } } Here state needed for UI representation (muted, unmuted, connection success, etc) are all bound to redux store. Note that in joinChannel we need to provide rtcToken, channelId and userId. - rtcToken will be generated by our token server in the next section. - channelId will be a roomOid that we create in a database. - userId will be a serialized columns of integer id in our database. #### Backend: Token Server text yarn add react-native-agora Our token is generated by sending POST request to: js chatRouter.post(\\"/voice-token\\", (req, res) => { const { roomOid } = req.body as { roomOid: string }; const userId = req.user?.userId!; const rtcToken = chatService.generateRtcToken(roomOid, userId); res.json({ success: true, result: { rtcToken } }) }) Here: js const generateRtcToken = (channelId: string, uid: number) => { // Rtc Examples const appID = process.env.AGORARTC_APP_ID!; const appCertificate = process.env.AGORARTC_APP_CERT!; const role = RtcRole.PUBLISHER; const expirationTimeInSeconds = 3600 const currentTimestamp = Math.floor(Date.now() / 1000) const privilegeExpiredTs = currentTimestamp + expirationTimeInSeconds // IMPORTANT! Build token with either the uid or with the user account. Comment out the option you do not want to use below. // Build token with uid const token = RtcTokenBuilder.buildTokenWithUid(appID, appCertificate, channelId, uid, role, privilegeExpiredTs, 600); return token; } - appId, appCertificate can be obtained from https://console.agora.io/projects. - The first person calling this api using this channelId will be the host of this channel. - Once the host leaves, the channel will be closed. #### Reference - [Agora.io Official Repo in Nodejs](https://github.com/AgoraIO/Tools/blob/master/DynamicKey/AgoraDynamicKey/nodejs/sample.js) - [Voice Calling in React Native using Agora - Aryan Agarwal](https://www.youtube.com/watch?v=OBW96M_fafk) - [AgoraIO Official Example in Class Component](https://github.com/AgoraIO-Extensions/react-native-agora/tree/main/example/src/examples/basic) ","title":"Implement Voice Chat on Mobile Using AgoraRTC","date":"2023-09-30T00:00:00.000Z","id":"blog0183","tag":"react-native","intro":"We create a voice chat application on mobile using third party API called AgoraRTC.","toc":true},{"content":" #### Create a Counter Collection to Store all Serialized id Fields We start from creating a Counter collection: json import mongoose, { Document, InferSchemaType, Schema } from \\"mongoose\\"; const counterSchema = new Schema({ _id: { type: String, required: true }, seq: { type: Number, default: 0 } }); export const CounterModel = mongoose.model(\'Counter\', counterSchema); export async function getNextSequence(name: string): Promise<number> { const result = await CounterModel.findOneAndUpdate({ _id: name }, { $inc: { seq: 1 } }, { upsert: true }); if (!result) { throw new Error(\\"Error in counter model\\"); } const seq = result.seq; return seq } - In Counter collection each document will have _id as the desired name (like userId). - On save() a new target document, we will query for Counter collection, add the seq value by 1, and then inject that seq value into our target document. #### Hook to save() Method of a Collection We create a hook to save() method and perform the seq injection by: js import mongoose, { InferSchemaType, Schema } from \\"mongoose\\"; import { getNextSequence } from \\"./Counter\\"; export const userSchema = new Schema( { id: { type: Number, required: true, unique: true, min: 1 }, name: { type: String, required: true, index: true }, email: { type: String, required: true }, passwordHash: { type: String, required: true }, }, { timestamps: { createdAt: \'created_at\', updatedAt: \'updated_at\' }, versionKey: false, } ); userSchema.pre(\\"save\\", function (next) { const doc = this; getNextSequence(\\"userId\\").then(nextId => { doc.id = nextId; next(); }).catch((err) => { console.log(getNextSequence failed: ${err}); }) }) export type User = InferSchemaType<typeof userSchema>; export const UserModel = mongoose.model(\'User\', userSchema); ","title":"Create Auto-Incremented Id for Mongo Collection","date":"2023-09-30T00:00:00.000Z","id":"blog0184","tag":"mongo","intro":"We create a special collection and a hook to the save method of a collection to obtain an auto-incremented id field!","toc":true},{"content":" #### Result Demonstration <center> <video controls width=\\"500\\"> <source src=\\"/assets/tech/185/001.mp4\\" type=\\"video/mp4\\"> Sorry, your browser doesn\'t support embedded videos. <video/> </center> #### Code Implementation js import { View, Text, Dimensions, StyleSheet } from \'react-native\' import React, { useState } from \'react\' import Animated, { Extrapolate, cancelAnimation, interpolate, useAnimatedGestureHandler, useAnimatedScrollHandler, useAnimatedStyle, useDerivedValue, useSharedValue, withDecay, withSpring, withTiming } from \'react-native-reanimated\'; import { PanGestureHandler, PanGestureHandlerGestureEvent, PinchGestureHandler, PinchGestureHandlerGestureEvent } from \\"react-native-gesture-handler\\"; const { width } = Dimensions.get(\\"window\\"); const bound = 40; const returnCenterRange = width * 3 / 4 const playground = () => { const translateX = useSharedValue(0); const [underlyingPage, setUnderlyingPage] = useState<\\"\\" | \\"left\\" | \\"right\\">(\\"\\"); const clampedX = useDerivedValue(() => { return Math.min(Math.max(-width + bound, translateX.value), width - bound); }) const leftpageRStyle = useAnimatedStyle(() => { if (clampedX.value > 0) { return { zIndex: 1 }; } else { return { zIndex: 0 }; } }) const rightpageRStyle = useAnimatedStyle(() => { if (clampedX.value > 0) { return { zIndex: 0 }; } else { return { zIndex: 1 }; } }) const panGestureHandler = useAnimatedGestureHandler<PanGestureHandlerGestureEvent, { translateX: number }>({ onStart: (event, context) => { context.translateX = clampedX.value; cancelAnimation(translateX); }, onActive: (event, context) => { translateX.value = context.translateX + event.translationX; }, onEnd: (event, context) => { translateX.value = withDecay({ velocity: event.velocityX }); if (context.translateX === 0) { if (event.translationX > 0) { translateX.value = withTiming(width); } else if (event.translationX < 0) { translateX.value = withTiming(-width); } } else { if (Math.abs(event.translationX) < returnCenterRange && (context.translateX * event.translationX < 0)) { console.log(\\"event.translationX\\", event.translationX) console.log(\\"context.translateX\\", context.translateX) translateX.value = withTiming(0); } } } }); const rstyle = useAnimatedStyle(() => { return { transform: [{ translateX: clampedX.value }] } }); return ( <View style={{ flex: 1 }}> <PanGestureHandler onGestureEvent={panGestureHandler}> <Animated.View style={[{ ...StyleSheet.absoluteFillObject }]}> <Animated.View style={[{ ...StyleSheet.absoluteFillObject, flex: 1, flexDirection: \\"row\\", zIndex: 2, backgroundColor: \\"#CBE6F7\\", }, rstyle]}> <View style={{ width, justifyContent: \\"center\\", alignItems: \\"center\\" }}> <Text>Center Page</Text> </View> </Animated.View> <Animated.View style={[{ ...StyleSheet.absoluteFillObject, flex: 0, zIndex: 1, justifyContent: \\"center\\", alignItems: \\"center\\", backgroundColor: \\"white\\", }, leftpageRStyle]}> <Text> Left Page </Text> </Animated.View> <Animated.View style={[{ ...StyleSheet.absoluteFillObject, flex: 0, zIndex: 1, justifyContent: \\"center\\", alignItems: \\"center\\", backgroundColor: \\"white\\" }, rightpageRStyle]}> <Text> Right Page </Text> </Animated.View> </Animated.View> </PanGestureHandler> </View > ) } export default playground; ","title":"Create Custom Component Using react-native-reanimated; Clone a Swipable Page from Discord Mobile App","date":"2023-09-30T00:00:00.000Z","id":"blog0185","tag":"react-native","intro":"We can create our custom component by creating custom behaviour via customizing animtation!","toc":true},{"content":" - yarn add eas-cli - yarn eas device:create - json \\"scripts\\": { \\"start:dev\\": \\"env-cmd -f .env-cmdrc -e default,dev expo start --dev-client\\", \\"android\\": \\"expo run:android\\", \\"ios\\": \\"expo run:ios\\", \\"prebuild\\": \\"npx expo prebuild\\", \\"doctor\\": \\"npx expo-doctor\\", \\"easios\\": \\"env-cmd -f .env-cmdrc -e default,dev eas build --profile development --platform ios\\", \\"easan\\": \\"env-cmd -f .env-cmdrc -e default,dev eas build --profile development --platform android\\" }, - repeated use of expo doctor can resolve many building issues at the early stage. ","title":"Common Command in Expo-Cli Using Development Build","date":"2023-09-30T00:00:00.000Z","id":"blog0186","tag":"react-native","intro":"Record frequently used command in the expo-cli workflow in development build.","toc":false},{"content":" As a review of statistics I try to work on this problem: > **Problem.** Let $(X,Y,Z)$ be a 3-dimensional random variable which follows the distribution (i.e., the probability density function is) > > $$ > f(x,y,z) = \\\\begin{cases} > \\\\displaystyle\\\\frac{24}{(1+x+y+z)^5}, & x>0,y>0,z>0,\\\\\\\\ > 0, &\\\\text{otherwise.} > \\\\end{cases} > $$ > > Find the probability density function of $W:=X+Y+Z$. I start off by guessing the answer to be $24/(1+w)^5$ very sloppily, which is of course wrong and I tried to figure out how I can relate $W$ with $f(x,y,z)$. <details> <summary> Solution </summary> --- Let us start from the definition, what does $f$ tell us? In view of a distribution it tells us how are $(X,Y,Z)$\'s spreaded in $\\\\mathbb R^3$. In other words, for every given $A\\\\subseteq \\\\mathbb R^3$, we have $$ \\\\mathbb P\\\\big((X,Y,Z)\\\\in A\\\\big) = \\\\int_A f(x)\\\\,dV(x) $$ which is the proportion of $(X,Y,Z)$\'s lying within $A$ and $dV$ denotes the Lebesgue measure on $\\\\mathbb R^3$. From that recall also that to find the probability density function $p_W$ of $W$, it is sufficient to find its cummulative distribution $\\\\int_0^t p_W(x)\\\\,dx$ (since then we can differentiate pointwise). From this, consider the relation $W\\\\leq t$, which is $$ \\\\begin{aligned} W\\\\leq t &\\\\iff X+Y+Z\\\\leq t,X,Y,Z>0\\\\\\\\ &\\\\iff 0< Z\\\\leq t-X-Y, 0< t-X-Y, X,Y>0\\\\\\\\ &\\\\iff 0< Z\\\\leq T-X-Y, 0<Y < t-X, 0<t-X, X>0\\\\\\\\ &\\\\iff 0< Z\\\\leq T-X-Y, 0<Y < t-X, 0< X<t, \\\\end{aligned} $$ we conclude that $w=x+y+z\\\\leq t$ ($x,y,z>0$) if and only if $(x,y,z)$ lies in the set $$ A_t := \\\\{(x,y,z): x\\\\in (0,t), y\\\\in (0, t-x), z\\\\in (t-x-y)\\\\}, $$ therefore $$ \\\\begin{aligned} \\\\int_0^tp_W(x)\\\\,dx &=\\\\mathbb P(W\\\\leq t)\\\\\\\\ &=\\\\mathbb P\\\\big((X,Y,Z)\\\\in A_t\\\\big)\\\\\\\\ &=\\\\int_{A_t}f(u)\\\\,dV(u)\\\\\\\\ &=\\\\int_0^t \\\\int_0^{t-x}\\\\int_0^{t-x-y} f(x,y,z) \\\\,dzdydx. \\\\end{aligned} $$ The answer is $$ p_W(t)=\\\\frac{d}{dt}\\\\left(\\\\int_0^t\\\\int_0^{t-x}\\\\int_0^{t-x-y} \\\\frac{24}{(1+x+y+z)^5}\\\\,dzdydx\\\\right). $$ $\\\\qed$ </details> ","title":"Computational Example on Probability Distribution","date":"2021-08-19T00:00:00.000Z","id":"math001","tag":"math","intro":"Given $(X,Y,Z)$ and its (joint) distribution on $\\\\mathbb R^3$, we try to find the distribution of $W=X+Y+Z$ on $\\\\mathbb R$.","toc":false},{"content":" Follow the idea of user04651 in <a href=\\"https://math.stackexchange.com/questions/2287213/closure-of-interior-of-closed-convex-set\\">this post</a> to prove this statement. > **Lemma.** Let $X$ be a normed space, $U\\\\subseteq X$ open and $x_0\\\\in U$. If $x\\\\not\\\\in U$, then the line segment > > $$ > [x_0,x)\\\\subseteq (\\\\mathrm{conv}(U\\\\cup \\\\{x\\\\}))^\\\\circ . > $$ <proof> **_Proof._** For every $x_1\\\\in [x_0,x)$, there is a $\\\\delta>1$ such that $x = x_0 + \\\\delta(x_1-x_0)$. Let $\\\\rho>0$ be such that $B(x_0,\\\\rho)\\\\subseteq U$. Now for every $x\' \\\\in B(x_1, (1-\\\\frac{1}{\\\\delta})\\\\rho)$ (we have discussed the intuition of this radius in <a href=\\"/blog/Convex-Analysis-More-on-Convex-Functions-and-Characterize-Convex-lsc-Functions-by-Biconjugate-Functionals\\">this post</a>), we have for some $z\\\\in B(0,\\\\rho)$, $$ \\\\begin{aligned} x\' &= x_1 + \\\\left(1-\\\\frac{1}{\\\\delta}\\\\right)z \\\\\\\\ &= \\\\frac{1}{\\\\delta}(x_0+\\\\delta(x_1-x_0)) + \\\\left(1-\\\\frac{1}{\\\\delta}\\\\right)(x_0+z).\\\\\\\\ &= \\\\frac{1}{\\\\delta} x + \\\\left(1-\\\\frac{1}{\\\\delta}\\\\right)\\\\underbrace{(x_0+z)}_{\\\\in U}\\\\\\\\ &\\\\in \\\\mathrm{conv}(U\\\\cup \\\\{x\\\\}). \\\\end{aligned} $$ As this is true for every $x\' \\\\in B(x_1, (1-\\\\frac{1}{\\\\delta})\\\\rho)$, we conclude $x_1\\\\in (\\\\mathrm{conv}(U\\\\cup \\\\{x\\\\}))^\\\\circ$. </proof> > **Proposition.** Let $X$ be a normed space and $C\\\\subseteq X$ a convex set, then $\\\\overline{C} = \\\\overline{C^\\\\circ}$. <proof> **_Proof._** The direction $\\\\overline{C^\\\\circ}\\\\subseteq \\\\overline C$ is clear. Let $x\\\\in \\\\overline C$, for the sake of contradiction suppose $x\\\\not\\\\in \\\\overline{C^\\\\circ}$, then there is an open neighborhood $V$ of $x$ such that $V\\\\cap C^\\\\circ=\\\\emptyset$. Since $x\\\\in \\\\overline C$, there is an $x\'\\\\in C$ such that $x\'\\\\in V$. Now $x\'$ being an element in $V$, $x\'\\\\not\\\\in C^\\\\circ$, fix an $x_0\\\\in C^\\\\circ$, by the lemma above the segment $$ [x_0,x\')\\\\subseteq (\\\\mathrm{conv}(C^\\\\circ\\\\cup \\\\underbrace{\\\\{x\'\\\\}}_{\\\\subseteq C}))^\\\\circ\\\\subseteq (\\\\mathrm{conv}(C))^\\\\circ = C^\\\\circ. $$ We can pick an $x\'\'\\\\in [x_0,x\')$ that is close enough to $x\'$, so that $x\'\'\\\\in V$, but then $x\'\'\\\\in V\\\\cap C^\\\\circ$, a contradiction. </proof> ","title":"The Set Equality $\\\\overline{C} = \\\\overline{C^\\\\circ}$ for Convex Set $C$","date":"2021-08-20T00:00:00.000Z","id":"math002","tag":"math","intro":"Record a proof to this result in convex analysis.","toc":false},{"content":" When taking elevator one is possible to calculate the probability of being the first one to leave the lift. In my situation, assume that: - An elevator can just stop from floor $22$ to floor $40$. - Infinite capacity. - We start from ground floor. Given that I will leave at floor $n$, where $22\\\\leq n < 40$, then the average probability of taking the first leave at floor $n$ is $\\\\displaystyle \\\\frac{1}{n-21}.$ **Reason.** Denote $E_k$, $k\\\\ge 2$, the event that floor $n$ is the first one to stop with $k$ buttons on the elevator control panel being pressed. We discard the case $k=1$ as $\\\\mathbb P(E_1)=1$ is too big that pollutes our average value. We have (for $k\\\\ge 2$) $$ \\\\mathbb P(E_k) =\\\\displaystyle \\\\binom{40-n}{k-1} \\\\bigg/\\\\binom{18}{k-1}. $$ - Note that we are just considering choosing $k-1$ buttons because floor $n$ being the first to stop has conditioned our sample space to combintations of selected buttons that contain $n$. - Here the numerator $\\\\binom{40-n}{k-1}$ represents the combinations of buttons that is bigger than $n$. - The demoninator $\\\\binom{18}{k-1}$ represents the available combinations of buttons apart from $n$. Note that the edge case of $k$ is $k=2$ and $k= |\\\\mathbb N \\\\cap [n, 40]|=40-n +1$. Our average probability will be $$ \\\\frac{1}{\\\\underbrace{40-n+1-2+1}_{\\\\text{number of summands}}} \\\\sum_{k=2}^{40-n+1} \\\\mathbb P(E_k) = \\\\frac{1}{40-n} \\\\sum_{k=2}^{41-n} \\\\frac{\\\\displaystyle \\\\binom{40-n}{k-1}}{\\\\displaystyle \\\\binom{18}{k-1}} \\\\xlongequal{(*)}\\\\frac{1}{n-21}. $$ Here $(*)$ follows from the following lemma: > **Lemma.** Let $X, Y$ be positive integers and $X\\\\leq Y$, then > > $$ > \\\\sum_{k=1}^X \\\\frac{\\\\binom{X}{k}}{\\\\binom{Y}{k}} = \\\\frac{X}{Y-X+1}. > $$ <details> <summary> <strong>Proof.</strong> [In case you want to try, don\'t unfold it]</summary> --- Denote $A_k =\\\\binom{X}{k} / \\\\binom{Y}{k}$ and $S = \\\\sum_{k=1}^X A_k$. We note that $$ \\\\begin{aligned} A_k& = \\\\frac{\\\\binom{X}{k}}{\\\\binom{Y}{k}}\\\\\\\\ &= \\\\frac{\\\\binom{X+1}{k+1} - \\\\binom{X}{k+1}}{\\\\binom{Y}{k}} \\\\\\\\ &= \\\\frac{\\\\frac{X+1}{k+1}\\\\binom{X}{k}}{\\\\binom{Y}{k}} - \\\\frac{\\\\binom{X}{k+1}}{\\\\binom{Y}{k}} \\\\\\\\ &= \\\\frac{\\\\frac{X+1}{k+1}\\\\binom{X}{k}}{\\\\binom{Y}{k}} - \\\\frac{\\\\binom{X}{k+1}}{\\\\frac{k+1}{Y-k}\\\\binom{Y}{k+1}} \\\\\\\\ &= \\\\frac{X+1}{k+1} A_k - \\\\frac{Y-k}{k+1}A_{k+1}. \\\\end{aligned}\\\\\\\\ $$ We rearrange to conclude $$ \\\\begin{aligned} (Y+1)A_{k+1}-XA_k &= (k+1)A_{k+1} - kA_k\\\\\\\\ \\\\sum_{k=1}^{X-1} \\\\big((Y+1)A_{k+1}-XA_k \\\\big) &= \\\\sum_{k=1}^{X-1}\\\\big((k+1)A_{k+1} - kA_k\\\\big)\\\\\\\\ (Y+1) (S-A_1) - X(S-A_X)&=XA_X - A_1\\\\\\\\ (Y-X+1)S &= YA_1\\\\\\\\ S &= \\\\frac{Y}{Y-X+1}A_1\\\\\\\\ &=\\\\frac{Y}{Y-X+1} \\\\cdot \\\\frac{X}{Y} \\\\\\\\ &= \\\\frac{X}{Y-X+1}, \\\\end{aligned} $$ as desired. $\\\\qquad \\\\blacksquare$ </details> I am at floor $n=34$, therefore on average I just have $$ \\\\frac{1}{34-21} = \\\\frac{1}{13} \\\\approx 7.69\\\\% $$ chance of being the first one to leave if there are at least 2 buttons pressed in control panel. ","title":"First to Leave Problem of Elevator","date":"2021-10-01T00:00:00.000Z","id":"math003","tag":"math","intro":"Simple problem inspired by my life.","toc":false},{"content":" > **Problem.** Given a function $f:\\\\mathbb R\\\\to \\\\mathbb R$, show that the following set $E$ is at most countable: > > $$ > E=\\\\left\\\\{ a\\\\in\\\\mathbb R: \\\\lim_{x\\\\to a}f(x) \\\\text{ exists and } \\\\lim_{x\\\\to a}f(x)\\\\neq f(a) \\\\right\\\\} > $$ <proof> **_My Solution._** Denote $$ \\\\begin{align*} E&=\\\\left\\\\{a\\\\in \\\\mathbb R :\\\\lim_{x\\\\to a}f(x)\\\\text{ exists and }\\\\lim_{x\\\\to a}|f(x)-f(a)|>0\\\\right\\\\}\\\\\\\\ &= \\\\bigcup_{k\\\\ge 1}\\\\underbrace{\\\\left\\\\{a\\\\in \\\\mathbb R :\\\\lim_{x\\\\to a}f(x)\\\\text{ exists and }\\\\lim_{x\\\\to a}|f(x)-f(a)|>\\\\frac{1}{k}\\\\right\\\\}}_{=:E_k}, \\\\end{align*} $$ then it remains to show that each of $E_k$\'s is countable. We prove this by showing that each $a\\\\in E_k$ is in fact isolated by an open interval (and hence each point can be identified with a rational number). > **Fact.** For every $k\\\\in \\\\mathbb N$, each of points in $E_k$ is isolated. By **_isolated_** we mean for every $a\\\\in E_k$, there is a $\\\\delta>0$ such that $$ \\\\big((a-\\\\delta, a+\\\\delta)\\\\setminus \\\\{a\\\\}\\\\big) \\\\cap E_k = \\\\emptyset. $$ In other words, the derived set of $E_k$, $E_k\'$, satisfies $E_k\'\\\\cap E_k=\\\\emptyset$. **_Proof._** Suppose not, i.e., $a\\\\in E_k$ and $a$ can\'t be isolated, i.e., $a\\\\in E_k\'$. Then for every $n\\\\ge 1$, we can find an $a_n\\\\in B(a, \\\\frac{1}{n})\\\\setminus\\\\{a\\\\}$, such that $a_n\\\\in E_k$. But $a_n\\\\in E_k$ means that $\\\\lim_{x\\\\to a_n}f(x)$ exists with $\\\\lim_{x\\\\to a_n}|f(x)-f(a_n)|>1/k$, there will be a $b_n\\\\in B(a_n,1/n)$ with $$ |f(b_n)-f(a_n)| >\\\\frac{1}{k}. $$ Since both $a_n, b_n\\\\to a\\\\in E_k$, as $\\\\lim_{x\\\\to a}f(x)$ exists, we have $0\\\\ge \\\\frac{1}{k}$, which is absurd. </proof> #### Reference - <a>https://www.facebook.com/photo?fbid=10223732659676781&set=gm.10159856304916489</a> ","title":"Countability of a kind of Discontinuity","date":"2022-04-04T00:00:00.000Z","id":"math004","tag":"math","intro":"Record a mathematical problem from facebook.","toc":false}]')},,,,,,,,,,,,,,,,,,,function(e,t,a){},function(e,t,a){},function(e,t,a){},,function(e,t,a){},,,,,function(e,t,a){},function(e,t,a){},function(e,t,a){},function(e,t,a){},function(e,t,a){},function(e,t,a){},function(e,t,a){},,,function(e,t){},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,function(e,t,a){},function(e,t,a){var i={"./001-tkinter.md":753,"./002-wpf.md":754,"./003-react-native.md":755,"./004-emotion_vgg.md":756,"./005-yolo-custom-training.md":757,"./006-qt-project-starter.md":758,"./007-electron-next.md":759,"./008-defectGAN.md":760,"./009-text-detection-and-erase.md":761,"./010-classifier-by-means-of-detection copy.md":762,"./011-translate-custom-model copy.md":763,"./012-libtorch_deployment.md":764};function n(e){var t=o(e);return a(t)}function o(e){if(!a.o(i,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return i[e]}n.keys=function(){return Object.keys(i)},n.resolve=o,e.exports=n,n.id=752},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/001-tkinter.72064d8e.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/002-wpf.04d51fb5.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/003-react-native.7d73c488.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/004-emotion_vgg.bb035d26.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/005-yolo-custom-training.889c2d70.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/006-qt-project-starter.7645e50e.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/007-electron-next.481a7c76.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/008-defectGAN.e8437a8c.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/009-text-detection-and-erase.6650e307.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/010-classifier-by-means-of-detection copy.cfe3da65.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/011-translate-custom-model copy.fecdd379.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/012-libtorch_deployment.4bad3ba0.md"},function(e,t,a){},function(e,t,a){},,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,function(e,t,a){},function(e,t,a){},,,,,function(e,t,a){},function(e,t,a){},function(e,t,a){},function(e,t,a){},function(e,t,a){var i={"./math/001-distrbution-example.md":841,"./math/002-closure-of-interior.md":842,"./math/003-lift-problem.md":843,"./math/004-countability-discontinuity.md":844,"./old-math/001-Study-Notes-on-Distribution/Study-Notes-on-Distribution.md":845,"./old-math/004-Review-of-Basic-Statistics/Review-of-Basic-Statistics.md":846,"./old-math/012-modulus-of-continuity/modulus-of-continuity.md":847,"./old-math/013-convex-analaysis/convex-analysis.md":848,"./old-math/014-subgradients/subgradients.md":849,"./tech/002-Lazy-React-Router/Lazy-React-Router.md":850,"./tech/003-On-Looping-all-Files-in-Frontend/On-Looping-all-Files-in-Frontend.md":851,"./tech/005-conda-useful-commands/conda-useful-commands.md":852,"./tech/007-redux-saga/redux-saga.md":853,"./tech/009-algorithms/algorithms.md":854,"./tech/010-algo-exercises/algo-exercises.md":855,"./tech/011-typescript-typetrick/typescript-typetrick.md":856,"./tech/015-disqus/disquz.md":857,"./tech/016-SPA-SEO/spa-seo.md":858,"./tech/017-pytorch-study-notes/017-pytorch-study-notes.md":859,"./tech/018-tensorflow-revisit/018-tensorflow-revisit.md":860,"./tech/019-colab/019-colab.md":861,"./tech/020-cls-react-icons/020-react-icons.md":862,"./tech/021-callbacks/021-callbacks.md":863,"./tech/022-transferred-learning/022-transferred-learning.md":864,"./tech/023-dataset-pipeline-with-augmentation/023-dataset-pipeline-with-augmentation.md":865,"./tech/024-Python-Scrapping/python-scrapping.md":866,"./tech/025-cpp_01/cpp_01.md":867,"./tech/026-cpp_02/cpp_02.md":868,"./tech/027-cpp_03/cpp_03.md":869,"./tech/028-tex-command/texcommand.md":870,"./tech/029-live2d/live2d.md":871,"./tech/030-automation-browser/chrome_automation.md":872,"./tech/031-automation-winapp-wip/automation_winapp.md":873,"./tech/032-python-import/python-import.md":874,"./tech/033-autohotkey/automation_winapp.md":875,"./tech/034-debugger-python/debugger.md":876,"./tech/035-decompose-a-class/decompose-a-class.md":877,"./tech/036-multithreading/threading.md":878,"./tech/037-datetime/datetime.md":879,"./tech/038-debugger-js/debug.md":880,"./tech/039-debug-nextjs-typescript/nextjs-debug.md":881,"./tech/040-next-js-electron/nextjs-electron.md":882,"./tech/041- spawn-child-process/child-process.md":883,"./tech/042-image-compression-backend/img-compression.md":884,"./tech/043-geneator/python-generator.md":885,"./tech/044-DCGAN/dc-gan-in-python.md":886,"./tech/045-mongo/mongo.md":887,"./tech/046-Custom-Logger/custom-logger.md":888,"./tech/047-configure-s3-send-sqs/configure-s3-send-sqs.md":889,"./tech/048-makeStyles-in-next-js/makeStyles-in-next-js.md":890,"./tech/049-mAP/mAP.md":891,"./tech/050-tensorflow-shapes/tensorflow-shapes.md":892,"./tech/051-desc-dot-ts/desc-dot-ts.md":893,"./tech/052-excels-in-python/excels-in-python.md":894,"./tech/053-cycle-gan-tf/cycle-gan-tf.md":895,"./tech/054-mongo-engine-py/mongo-engine-py.md":896,"./tech/055-scroll-desired/scroll.md":897,"./tech/056-color-GAN/color-gan.md":898,"./tech/057-begin-rust/begin-rust.md":899,"./tech/058-json-in-python/058-json-in-python.md":900,"./tech/059-ts-debug-config/059-ts-debug-config.md":901,"./tech/060-ssh-download-file/060-ssh-download-file.md":902,"./tech/061-summary-of-rust-tutorial/061-rust-beginner-summary.md":903,"./tech/062-DCT/062-DCT.md":904,"./tech/063-fadeIn/063-fadein.md":905,"./tech/064-residue-blocks/064-residue-block.md":906,"./tech/065-YOLO-V3-deep-dive/yolo-v3-deep-dive.md":907,"./tech/066-WGAN/WGAN.md":908,"./tech/067-pandoc/pandoc.md":909,"./tech/068-coco/coco.md":910,"./tech/069-PCA/PCA.md":911,"./tech/070-SVD/SVD.md":912,"./tech/072-DefectGAN/DefectGAN.md":913,"./tech/073-install-tensorflow-gpu/installation.md":914,"./tech/075-shell-script/shell-script.md":915,"./tech/076-tensorflow-torch/pytorch-summary.md":916,"./tech/077-continusou logging/continuous-logging.md":917,"./tech/079-progress-bar/progress-bar.md":918,"./tech/080-masks-in-numpy/mask-in-numpy.md":919,"./tech/081-drawing-function-in-cv2/cv2-drawing.md":920,"./tech/082-Albumentation/EAST.md":921,"./tech/084-faster-rcnn/faster-rcnn.md":922,"./tech/085-albumentation-for-obj-detection/augmentation_in_obj_detection.md":923,"./tech/086-feature-extractor/feature_extractor.md":924,"./tech/087-retrain-nan/retrain-nan.md":925,"./tech/088-transformer/088-transformer.md":926,"./tech/089-transformer-2/089-transformer-2.md":927,"./tech/090-angular/090-angular-trial.md":928,"./tech/091-ViT/091-ViT.md":929,"./tech/092-mesh-grid-trick/092-mesh-grid-trick.md":930,"./tech/093-swin-transformer/093-mesh-grid-trick.md":931,"./tech/094-faster-rcnn-swin/094-faster-rcnn-swin.md":932,"./tech/095-gradient-clipping/095-gradient-clipping.md":933,"./tech/097-SEAttention/097-SEAttention.md":934,"./tech/098-Cascade-RCNN/098-Cascade-RCNN.md":935,"./tech/099-CPP/099-CPP.md":936,"./tech/100-deformable-detr/100-deformable-detr.md":937,"./tech/102-collate_fn/102-collate_fn.md":938,"./tech/103-albumentation-in-pytorch/albumentation-in-pytorch.md":939,"./tech/104-retina-face/104-retina-face.md":940,"./tech/105-onnx-model-web/105-onnx-model.md":941,"./tech/106-config-frontend/106-config-organization-in-frontend.md":942,"./tech/107-redux-toolkit-quick-setup/107-redux-toolkit-quick-setup.md":943,"./tech/108-mui-latest-css/108-mui-latest-css.md":944,"./tech/109-visual-studio-setup/109-visual-studio-setup.md":945,"./tech/110-copy-constructor/110-copy-constructor.md":946,"./tech/111-precompiled-header/111-precompiled-header.md":947,"./tech/112-DSR/112-DSR-ELE.md":948,"./tech/113-Array/113-Array.md":949,"./tech/114-Iterator/114-Iterator.md":950,"./tech/115-variadic-version-print/115-print.md":951,"./tech/116-TCP-Server/116-TCPServer.md":952,"./tech/117-dark-matter/117-dark-matter.md":953,"./tech/118-CMakeFileRecord/118-CmakeFile.md":954,"./tech/119-SocketProgramming/119-SocketProgramming.md":955,"./tech/120-PytorchCPP/120-pytorchcpp.md":956,"./tech/121-cpp-configuration/121-cpp-configurtation.md":957,"./tech/122-cpp-utils/122-cpp-utils.md":958,"./tech/123-com_two_threads/com_two_threads.md":959,"./tech/124-diffusion_models/diffusion_models.md":960,"./tech/125-opencvinstall/opencvinstall.md":961,"./tech/126-cxx-standard/cxxstandard.md":962,"./tech/127-precompiled-header/precompiled-header.md":963,"./tech/128-libtorch-study/libtorch-study.md":964,"./tech/129-std_variant/std_variant.md":965,"./tech/130-url-scheme/130-url-scheme.md":966,"./tech/131-raspect/131-mongo-aggregation.md":967,"./tech/132-redux-thunk/132-redux-thunk.md":968,"./tech/133-ReviewFileUpload/133-Review-FileUpload.md":969,"./tech/134-Data-Scrapping/134-Data-Scrapping.md":970,"./tech/136-mui-css-animation/136-mui-css-animation.md":971,"./tech/137-jsonpipeline-in-java/137-jsonpipeline-in-java.md":972,"./tech/138-redux-slice-template/138-redux-slice-template.md":973,"./tech/139-lookup-from-array-of-objects/lookup-from-array-of-objects.md":974,"./tech/141-two-kinds-of-lookup/two-kinds-of-lookup.md":975,"./tech/142-dayjs/dayjs.md":976,"./tech/143-Scrollbar/scrollbar.md":977,"./tech/144-Radio-button-groups/radio-button-groups.md":978,"./tech/145-persist-data/persist-data.md":979,"./tech/147-docker-study-gitlab/docker-study.md":980,"./tech/148-tsconfig/tsconfig.md":981,"./tech/149-s3-deployment-ci/s3-deployment-ci.md":982,"./tech/150-remote-debugger/remote-debuggeri.md":983,"./tech/151-normalizr/normalizr.md":984,"./tech/152-LazyLoading/135-LazyLoading.md":985,"./tech/153-S3Deployment/153-S3Deployment.md":986,"./tech/154-uwsgi-flask/154-uwsgi-flask.md":987,"./tech/155-gmail-pubsub/155-gmail-pubsub.md":988,"./tech/156-serverless/156-serverless.md":989,"./tech/157-SSE/157-SSE.md":990,"./tech/158-algolia/158-algolia.md":991,"./tech/159-clickoutside/159-clickoutside.md":992,"./tech/160-multi-selections/160-multiselections.md":993,"./tech/161-third-party/161-third-party.md":994,"./tech/162-snackbar-utils/162-snackbar-utils.md":995,"./tech/163-snackbar-utils copy/163-path-params.md":996,"./tech/164-google-login/164-google-login.md":997,"./tech/165-forwardRef/165-forwardRef.md":998,"./tech/166-Rerender-component/166-Rerender-component.md":999,"./tech/167-gov-analyst-programmer/167-gov-analyst-programmer.md":1e3,"./tech/168-boxshadow/168-boxshadow.md":1001,"./tech/169-rust-study-notes/169-ruststudynotes.md":1002,"./tech/170-scrolling/170-scroll.md":1003,"./tech/171-download-files/171-download-files.md":1004,"./tech/172-restrict-CORS/172-restrict-CORS.md":1005,"./tech/173-website-route-schema/173-website-route-schema.md":1006,"./tech/174-childcomponent/174-childcomponent.md":1007,"./tech/175-rust-operator-overloading/175-rust-operator-overloading.md":1008,"./tech/176-rust-fmt/176-rust-fmt.md":1009,"./tech/177-env/177-env.md":1010,"./tech/178-socketio/178-socketio.md":1011,"./tech/179-jest/179-jest.md":1012,"./tech/180-tokio-chat/180-tokio-chat.md":1013,"./tech/181-scale-socketio/181-scale-socketio.md":1014,"./tech/182-left-join-mongoose/182-left-join-mongoose.md":1015,"./tech/183-voice-chat/183-voice-chat.md":1016,"./tech/184-mongo-auto-increment/184-mongo-auto-increment.md":1017,"./tech/185-reanimated/185-reanimated.md":1018,"./tech/186-expo-cli/186-expo-cli.md":1019};function n(e){var t=o(e);return a(t)}function o(e){if(!a.o(i,e)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return i[e]}n.keys=function(){return Object.keys(i)},n.resolve=o,e.exports=n,n.id=840},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/001-distrbution-example.d1192f62.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/002-closure-of-interior.d61226c6.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/003-lift-problem.ff632e8f.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/004-countability-discontinuity.82a19b79.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/Study-Notes-on-Distribution.324c8471.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/Review-of-Basic-Statistics.37822857.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/modulus-of-continuity.837b37f9.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/convex-analysis.101e3849.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/subgradients.2b753fc9.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/Lazy-React-Router.48295ca9.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/On-Looping-all-Files-in-Frontend.44bc41b6.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/conda-useful-commands.58923fe0.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/redux-saga.f840908f.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/algorithms.1411a248.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/algo-exercises.9e5aa0f2.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/typescript-typetrick.0e546903.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/disquz.8b4defa2.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/spa-seo.c3362055.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/017-pytorch-study-notes.ac795ee2.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/018-tensorflow-revisit.dbb44eac.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/019-colab.87b0566b.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/020-react-icons.f21e3549.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/021-callbacks.bc873ad4.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/022-transferred-learning.88ffe757.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/023-dataset-pipeline-with-augmentation.9b696da4.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/python-scrapping.9a96e7d4.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/cpp_01.79781b1e.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/cpp_02.18ac64f2.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/cpp_03.7588709f.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/texcommand.85360952.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/live2d.8b77c012.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/chrome_automation.64f09c74.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/automation_winapp.ecb96c7d.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/python-import.e0321524.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/automation_winapp.96b1a2e2.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/debugger.c183e127.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/decompose-a-class.bd1fe2cd.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/threading.336c175a.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/datetime.e24d8883.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/debug.e735871c.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/nextjs-debug.09beb281.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/nextjs-electron.3f8d1fe1.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/child-process.a75cdae3.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/img-compression.2ad59322.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/python-generator.4e29d683.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/dc-gan-in-python.0c7b14ae.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/mongo.5e06cbd6.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/custom-logger.5af75d31.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/configure-s3-send-sqs.544a6cf8.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/makeStyles-in-next-js.5341b8bd.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/mAP.9210a813.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/tensorflow-shapes.c22f8dd0.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/desc-dot-ts.3997b32f.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/excels-in-python.1c75b8ba.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/cycle-gan-tf.517276b8.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/mongo-engine-py.95b47056.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/scroll.12ff7226.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/color-gan.07a097e0.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/begin-rust.17f46bf7.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/058-json-in-python.ba4bc11b.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/059-ts-debug-config.312edc54.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/060-ssh-download-file.c2b5a099.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/061-rust-beginner-summary.2d7f9e1b.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/062-DCT.a9154a97.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/063-fadein.20c7825d.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/064-residue-block.cec6dd6c.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/yolo-v3-deep-dive.5fa9bf85.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/WGAN.c1dc70b3.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/pandoc.1405492f.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/coco.4c27ee6c.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/PCA.cb420266.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/SVD.73a6ea8c.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/DefectGAN.c0718f46.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/installation.d4a52327.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/shell-script.65c4ae1d.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/pytorch-summary.1e0a19b1.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/continuous-logging.b5f96942.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/progress-bar.948cb527.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/mask-in-numpy.83a198cb.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/cv2-drawing.900c3f11.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/EAST.86fb54d8.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/faster-rcnn.af6c4c23.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/augmentation_in_obj_detection.d2dcc305.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/feature_extractor.96d351aa.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/retrain-nan.3c7eba3c.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/088-transformer.8c8c2411.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/089-transformer-2.01ab9a7f.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/090-angular-trial.62e52130.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/091-ViT.caa92cdc.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/092-mesh-grid-trick.3434b477.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/093-mesh-grid-trick.f828545b.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/094-faster-rcnn-swin.13430dd8.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/095-gradient-clipping.e504728c.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/097-SEAttention.2dd11360.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/098-Cascade-RCNN.5bc4319c.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/099-CPP.51f4112b.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/100-deformable-detr.6c2054ec.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/102-collate_fn.317db026.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/albumentation-in-pytorch.b1cc10ee.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/104-retina-face.9c928972.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/105-onnx-model.b31c5352.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/106-config-organization-in-frontend.5dd9a256.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/107-redux-toolkit-quick-setup.0415f62b.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/108-mui-latest-css.9443c325.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/109-visual-studio-setup.5439d16b.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/110-copy-constructor.8baadbf3.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/111-precompiled-header.2bf1db26.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/112-DSR-ELE.e4fda5df.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/113-Array.71841cf0.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/114-Iterator.97d1d55d.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/115-print.0ad81f3c.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/116-TCPServer.e7da2471.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/117-dark-matter.6f62885b.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/118-CmakeFile.42ef6835.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/119-SocketProgramming.91fa3c3f.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/120-pytorchcpp.7af03111.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/121-cpp-configurtation.1fbecc00.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/122-cpp-utils.a91c38d5.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/com_two_threads.4d557693.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/diffusion_models.14914b49.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/opencvinstall.c84b99bb.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/cxxstandard.3980edac.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/precompiled-header.72ad9600.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/libtorch-study.2fcafaac.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/std_variant.f12797f7.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/130-url-scheme.cbb1426e.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/131-mongo-aggregation.2b9299ae.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/132-redux-thunk.e550738d.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/133-Review-FileUpload.6f1c65d3.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/134-Data-Scrapping.be0c0928.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/136-mui-css-animation.386bf1f5.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/137-jsonpipeline-in-java.8c386571.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/138-redux-slice-template.f25db875.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/lookup-from-array-of-objects.ef6a1a6b.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/two-kinds-of-lookup.76e5be02.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/dayjs.07f3f76d.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/scrollbar.011a0db0.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/radio-button-groups.759d8b31.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/persist-data.f18a1ad2.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/docker-study.2a10d1c1.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/tsconfig.182840d2.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/s3-deployment-ci.e287039c.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/remote-debuggeri.e33800e6.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/normalizr.2df02703.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/135-LazyLoading.cd44381b.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/153-S3Deployment.443c808e.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/154-uwsgi-flask.d8d0cf15.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/155-gmail-pubsub.1e2e196b.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/156-serverless.77ae63e8.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/157-SSE.1da5fc10.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/158-algolia.e7f09cb3.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/159-clickoutside.800d0b58.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/160-multiselections.04467aa0.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/161-third-party.b90fa561.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/162-snackbar-utils.bacdad77.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/163-path-params.09f8d3d3.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/164-google-login.36d2442e.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/165-forwardRef.fc573469.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/166-Rerender-component.d74b4620.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/167-gov-analyst-programmer.842e2477.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/168-boxshadow.b6b760e0.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/169-ruststudynotes.c5c09982.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/170-scroll.9729cc59.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/171-download-files.700a6f5c.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/172-restrict-CORS.8fdba459.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/173-website-route-schema.1e8bf602.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/174-childcomponent.985612cf.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/175-rust-operator-overloading.3113c4bd.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/176-rust-fmt.56d54a17.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/177-env.830f3b74.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/178-socketio.07942309.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/179-jest.a566edbb.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/180-tokio-chat.e11fb5b2.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/181-scale-socketio.94c1d004.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/182-left-join-mongoose.7a90c060.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/183-voice-chat.0353ffa2.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/184-mongo-auto-increment.ed046548.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/185-reanimated.3f890463.md"},function(e,t,a){"use strict";a.r(t),t.default=a.p+"static/media/186-expo-cli.6baeb5d5.md"},function(e,t,a){},,,function(e,t,a){"use strict";a.r(t);var i,n=a(2),o=a.n(n),r=a(13),s=a.n(r),l=(a(299),a(300),a(8)),c=(a(301),0),d=0,p=40,m=.1,u=600,h=a(1079);!function(e){e.NONE="",e.ROOT="/",e.ABOUT="/",e.SKILLS="/skills",e.PORTFOLIO="/portfolio",e.ARTWORKS="/artworks",e.EXPERIENCE="/experience",e.MATH_MATERIAL="/math-material",e.CONTACT="/contact",e.BLOG="/blog",e.BLOG_WILDCARD="/blog/*",e.LANG_STUDY="/lang-study",e.LANG_STIDY_FF14="/lang-study-jp"}(i||(i={}));var f={ROOT:{path:i.ROOT,title:"About"},ABOUT:{path:i.ABOUT,title:"About"},SKILLS:{path:i.SKILLS,title:"Skills"},PORTFOLIO:{path:i.PORTFOLIO,title:"Portfolio"},ARTWORKS:{path:i.ARTWORKS,title:"Artworks"},EXPERIENCE:{path:i.EXPERIENCE,title:"Experience"},MATH_MATERIAL:{path:i.MATH_MATERIAL,title:"Math Material"},CONTACT:{path:i.CONTACT,title:"Contact"},BLOG:{path:i.BLOG,title:"Blog"},BLOG_WILDCARD:{path:i.BLOG_WILDCARD,title:"Blog"},LANG_STUDY:{path:i.LANG_STUDY,title:"Language Study"},LANG_STIDY_FF14:{path:i.LANG_STIDY_FF14,title:"Language Study (JP)"}},g=function(e){return"/blog/article/".concat(e)},_=f,b=a(1),y=function(e){var t=e.height,a=void 0===t?20:t,i=e.width,n=void 0===i?5:i,o=e.inline,r=void 0!==o&&o;return Object(b.jsx)("div",{style:{height:a,width:n,minHeight:a,userSelect:"none",display:r?"inline-block":"block"}})},w=a(73),x=a(10),v=a.n(x),k=(a(303),a(1078)),S=a(99),j=function(){return Object(S.b)()},T=S.c,C=function(e){var t=e.navRoute,a=e.title,i=e.onClick,n=Object(l.f)(),o=T((function(e){return e.app.activeNavRoute}))===t?" active":"";return Object(b.jsx)("a",{href:t,onClick:function(e){e.preventDefault(),1===e.button||e.ctrlKey||0!==e.button||setTimeout((function(){n.push(t),i&&"function"==typeof i&&i()}),50)},children:Object(b.jsx)(k.a,{className:"menu-button"+o,children:Object(b.jsx)("div",{className:"title-container",children:a})})})};function $(){j();return Object(b.jsx)("div",{className:"site-top-menu",style:{height:p},children:Object(b.jsx)(h.a,{children:Object(b.jsxs)("div",{className:"button-row",children:[Object(b.jsx)(C,{navRoute:_.ABOUT.path,title:"About Me"}),Object(b.jsx)(C,{navRoute:_.PORTFOLIO.path,title:"Portfolio"}),Object(b.jsx)(C,{navRoute:_.BLOG.path,title:"Blog"}),Object(b.jsx)(C,{navRoute:_.ARTWORKS.path,title:"Artworks"}),Object(b.jsx)(C,{navRoute:_.MATH_MATERIAL.path,title:"Math Material"})]})})})}var R=a(1061),A=Object(R.a)((function(e){return{higherContainer:{}}})),O=function(e){return/(\/blog\/)|(\/blog$)/g.test(e)},I=a(11),E=a(12),N=a(40),D=a(1063),P=a(1064),z=(a(308),a(1062)),L=(a(309),["show","className"]),F=function(e){var t=e.show,a=e.className,i=Object(N.a)(e,L),n=t?"":"collapsed";return Object(b.jsx)(k.a,Object(I.a)(Object(I.a)({className:v()(a,"custom-collapse-button",n)},i),{},{children:Object(b.jsx)(z.a,{})}))},M=["children","title","innerRef","className","enableCollapse"],B=function(e){var t=e.children,a=e.title,i=void 0===a?"":a,o=e.innerRef,r=e.className,s=void 0===r?"":r,l=e.enableCollapse,c=void 0!==l&&l,d=Object(N.a)(e,M),p=Object(n.useState)(!1),m=Object(E.a)(p,2),u=m[0],h=m[1],f=Object(n.useState)(!0),g=Object(E.a)(f,2),_=g[0],y=g[1];Object(n.useEffect)((function(){h(!0)}),[]);var w=_?"":"message-collapsed";return Object(b.jsx)(D.a,{in:u,children:Object(b.jsxs)("div",Object(I.a)(Object(I.a)({className:v()("message-block-container",s,w),ref:o},d),{},{children:[i&&Object(b.jsxs)("div",{className:"title-row",children:[_&&Object(b.jsx)("div",{className:"title",children:i}),c&&Object(b.jsx)(F,{show:_,onClick:function(){y((function(e){return!e}))}})]}),Object(b.jsx)(P.a,{in:_,children:Object(b.jsx)("div",{style:{background:c?"white":"inherit"},children:Object(b.jsx)("div",{style:{display:"flex",width:"inherit !important"},children:Object(b.jsx)("div",{className:"box",style:{borderTopLeftRadius:i?0:10,borderTopRightRadius:i?0:10},children:Object(b.jsx)("div",{className:"message-block",children:t})})})})})]}))})},U=a(1065),W=a(1066),q=a(1069),G=a(1071),H=a(1072),V=a(1067),Y=a(1070),X=a(1068),K=(a(310),a(75)),Z="#E5E5E5",J={display:"inline-block",borderRadius:4};function Q(e){var t=e.skills,a=function(e){var t=e.style;return Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)(y,{width:2,inline:!0})," ",Object(b.jsx)(K.a,{size:7,style:Object(I.a)({color:Z},t)}),Object(b.jsx)(y,{width:5,inline:!0})]})};return Object(b.jsx)("div",{style:{marginTop:10,marginBottom:20,borderRadius:4,padding:20,paddingTop:10,paddingBottom:10},children:Object(b.jsxs)("div",{style:{marginTop:8,marginBottom:8,display:"flex",flexWrap:"wrap"},children:[Object(b.jsx)("div",{style:{fontWeight:600,fontSize:14},children:Object(b.jsx)("div",{style:{color:"white",background:"#93A9C3",paddingLeft:10,paddingRight:10,borderRadius:4},children:"Skills Needed"})}),Object(b.jsx)("div",{style:{marginLeft:10,flex:1},children:t.map((function(e,t){var i=e.techSkills,n=e.category,o=0===t;return Object(b.jsx)("div",{children:Object(b.jsxs)("div",{style:{display:"inline-block",fontSize:14,paddingLeft:10,paddingRight:10,borderRadius:4,paddingBottom:1},children:[n&&Object(b.jsxs)("div",{children:[Object(b.jsxs)("div",{style:{marginTop:o?0:2,display:"flex",alignItems:"center"},children:[Object(b.jsx)(a,{style:{marginLeft:2,marginRight:6}}),Object(b.jsx)("span",{style:{fontWeight:600},children:n})]}),i.map((function(e,t){var a=t===i.length-1;return Object(b.jsxs)("div",{style:{marginTop:2,display:"flex",alignItems:"center"},children:[Object(b.jsx)("div",{style:{color:"#E5E5E5",marginRight:10,transform:"scale(0.7)",display:"flex"},children:a?Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)("div",{style:{marginTop:-9,transform:"scaleY(1.5)",display:"inline-block"},children:"|"}),"\u2500\u2500\u2500 ",Object(b.jsx)("div",{style:{display:"inline-block",marginLeft:4,transform:"translateY(-1px)"}})]}):Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)("div",{style:{transform:"scaleY(4)",display:"inline-block"},children:"|"}),"\u2500\u2500\u2500 ",Object(b.jsx)("div",{style:{display:"inline-block",marginLeft:4,transform:"translateY(-1px)"}})]})}),Object(b.jsx)("div",{style:J,children:e})]})}))]}),!n&&Object(b.jsx)("div",{children:i.map((function(e,t){var i=0===t;return Object(b.jsxs)("div",{style:{marginTop:i?0:2,display:"flex",alignItems:"center"},children:[Object(b.jsx)(a,{style:{marginRight:4,marginLeft:2}}),Object(b.jsx)("div",{style:J,children:e})]})}))})]})})}))})]})})}var ee,te=a(130),ae="rgba(0, 0, 0, 0.1) 0px 0px 5px 0px,rgba(0, 0, 0, 0.1) 0px 0px 1px 0px",ie="rgba(14, 30, 37, 0.12) 0px 2px 4px 0px,rgba(14, 30, 37, 0.32) 0px 2px 16px 0px",ne="rgba(17, 17, 26, 0.1) 0px 4px 16px,rgba(17, 17, 26, 0.1) 0px 8px 24px,rgba(17, 17, 26, 0.1) 0px 16px 56px",oe="rgba(0, 0, 0, 0.05) 0px 0px 0px 1px,rgb(209, 213, 219) 0px 0px 0px 1px inset",re=function(e){var t=e.items,a=e.alignment,i=void 0===a?"left":a,n=e.dotVarient,o=e.useCollapseGlobally,r=void 0!==o&&o,s=e.expandedIndex,l=void 0===s?-1:s,c=e.setExpandIndex,d=void 0===c?function(){}:c;if(!t)return Object(b.jsx)(b.Fragment,{});j();return t&&Object(b.jsx)(U.a,{align:i,className:"custom-timeline",children:t.map((function(e,t){var a=e.date,i=e.title,o=e.company,s=e.description,c=e.icon,p=e.link,m=(e.openByDefault,e.useCollapseLocally),u=void 0!==m&&m,h=e.skills,f=r&&u,g=l===t,_=Object(b.jsxs)(b.Fragment,{children:[s&&Object(b.jsxs)("div",{className:"description2",children:[s," "]}),p&&Object(b.jsxs)("div",{className:"url-link",children:["link: ",Object(b.jsx)("a",{href:p,target:"_blank",children:p})]})]}),w=se({color:g?"#8a9aad":"#2a2e36"});return Object(b.jsxs)(W.a,{children:[Object(b.jsx)(V.a,{children:Object(b.jsx)("div",{className:"date",children:Object(b.jsx)(X.a,{variant:"body2",color:"textSecondary",children:a})})}),Object(b.jsxs)(q.a,{children:[Object(b.jsx)(Y.a,{variant:n,children:c}),Object(b.jsx)(G.a,{})]}),Object(b.jsx)(H.a,{children:Object(b.jsx)("div",{className:"timeline-card",children:Object(b.jsxs)("div",{children:[Object(b.jsx)("div",{className:"title",children:i}),Object(b.jsx)(y,{height:5}),Object(b.jsx)("div",{className:"description-row",children:Object(b.jsxs)("div",{className:"description",children:[o," "]})}),h&&Object(b.jsx)(Q,{skills:h}),f&&Object(b.jsx)("div",{className:"description-row",children:Object(b.jsxs)(k.a,{className:v()("detail-button",w.detailButton),onClick:function(){!function(e){d(l!==e?e:-1)}(t)},children:[Object(b.jsx)(te.b,{size:20})," ",Object(b.jsx)(y,{width:2})," Detail  ",Object(b.jsx)(z.a,{className:g?"expanded":"collapsed"})]})}),f&&Object(b.jsxs)(P.a,{in:g,children:[_,Object(b.jsx)(y,{height:5})]}),!f&&_]})})})]},"".concat(i,"_").concat(t))}))})},se=Object(R.a)({detailButton:{"&.MuiButton-root":{"& *":{color:function(e){return e.color||"#212529 !important"}},boxShadow:ae,padding:"2px 15px 2px 20px"}}}),le=Object(R.a)({customAnchor:{color:"#989fc3 !important"}},{name:"custom-anchor"}),ce=function(e){var t=e.href,a=e.title,i=le();return Object(b.jsx)("span",{className:i.customAnchor,children:Object(b.jsx)("a",{href:t,target:"_blank",children:a||t})})},de=["thickness","style"],pe=function(e){var t=e.thickness,a=void 0===t?1:t,i=e.style,n=Object(N.a)(e,de);return Object(b.jsx)("div",Object(I.a)({style:Object(I.a)({borderTop:"".concat(a,"px solid rgba(0,0,0,0.1)")},i)},n))},me=(a(311),function(e){var t=e.items;return Object(b.jsx)("div",{className:"skills-custom-list-row",children:t.map((function(e,t){var a=e.flex,i=e.category,n=e.skills;return Object(b.jsxs)("div",{className:"category-row",children:[Object(b.jsx)("div",{className:"title",children:i}),Object(b.jsx)("div",{className:"list",children:Object(b.jsxs)("div",{className:"list-container",children:[i&&Object(b.jsx)("ul",{className:a?"flex":"",children:n.map((function(e,t){return Object(b.jsx)("li",{className:"custom-list-item",children:e},t)}))},"".concat(i,"_").concat(t)),!i&&Object(b.jsx)("div",{style:{marginLeft:20},children:Object(b.jsx)("ul",{className:a?"flex":"",children:n.map((function(e){return Object(b.jsx)("li",{className:"custom-list-item",children:e})}))},"".concat(i,"_").concat(t))})]})})]},"".concat(i,"_").concat(t))}))})}),ue=function(){return Object(b.jsxs)("div",{children:[Object(b.jsx)(y,{}),Object(b.jsx)("h4",{children:"Programming Skills"}),Object(b.jsx)(pe,{}),Object(b.jsx)(y,{}),Object(b.jsx)(me,{items:[{flex:!0,category:"Source Control",skills:["Git"]},{flex:!0,category:"Deployment",skills:["Docker","Basic CI/CD Pipeline in Gitlab"]},{flex:!0,category:"Java",skills:["Springboot","Hibernate","MyBatis"]},{category:"C#",flex:!0,skills:["Windows Presentation Foundation"]},{category:"C++",flex:!0,skills:["CMake Ecosystem","Libtorch","OpenCV","ImGui for Desktop App"]},{flex:!0,category:"Python",skills:["Tensorflow v2","PyTorch","Pandas","Flask","ONNX"]},{flex:!0,category:"Typescript",skills:["React","React-Native","Redux","Redux-Saga","Next.js","Electron.js","Electron with Next.js","Node.js","Express.js","Socket.io","Twilio.js","Knex.js","Mongoose.js"]},{category:"Deep-Learning",skills:[Object(b.jsx)(b.Fragment,{children:"Embed custom trained pytorch model in ONNX format into frontend"}),Object(b.jsxs)(b.Fragment,{children:["Implementation from scratch and training experience with CNN such as",Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:"Object Detection"}),Object(b.jsx)("li",{children:"Text Detection with Rotation"}),Object(b.jsx)("li",{children:"Image Classification"}),Object(b.jsx)("li",{children:"Image Generation and Image In-painting"})]}),Object(b.jsx)(y,{height:10})]}),Object(b.jsxs)(b.Fragment,{children:["Implementation and training experience with LSTM Model and Transformer to do:",Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:"Summarization on a column of data"}),Object(b.jsx)("li",{children:"Translation of random date format into standard YYYY-MM-DD one"})]})]})]}]}),Object(b.jsx)(y,{}),Object(b.jsx)("h4",{children:"Art Skills"}),Object(b.jsx)(pe,{}),Object(b.jsx)(y,{}),Object(b.jsx)(me,{items:[{flex:!0,category:"Tools",skills:["Photoshop CC","3DS Max"]},{flex:!0,category:"Thingies I Draw",skills:["2D Character","2D Environment"]}]})]})},he=a.p+"static/media/agent-classification.17931009.pdf",fe=a.p+"static/media/transformer-date-fomatter.18a2d3fc.pdf",ge=a(78);!function(e){e[e.BLOG_MENU_WIDTH=160]="BLOG_MENU_WIDTH"}(ee||(ee={}));var _e={experienceExpandedIndex:-1,activeNavRoute:_.ABOUT.path,sortPostBy:"date",leftMenuIsOpen:!0,showLoading:!1,activeTag:"",articles:{archive:{open:!1},defaultTocCrossed:!0,activeBlogArticleId:"",activePortfolioTitle:"",portfolios:[],blogs:[],langStudies:[]}},be=Object(ge.b)({name:"app",initialState:_e,reducers:{setActiveExpIndex:function(e,t){e.experienceExpandedIndex=t.payload},setSortPostBy:function(e,t){e.sortPostBy=t.payload},setActiveTag:function(e,t){e.activeTag=t.payload},setActiveRouteTitle:function(e,t){e.activeRouteTitle=t.payload.activeRouteTitle},setShowLoading:function(e,t){e.showLoading=t.payload.showLoading},setDefaultTocCrossed:function(e,t){e.articles.defaultTocCrossed=t.payload},updateArchive:function(e,t){var a=t.payload;Object.entries(a).forEach((function(t){var a=Object(E.a)(t,2),i=a[0],n=a[1],o=i;e.articles.archive[o]=n}))},changeActiveNavRoute:function(e,t){e.activeNavRoute=t.payload},updateArticles:function(e,t){var a=t.payload;Object.entries(a).forEach((function(t){var a=Object(E.a)(t,2),i=a[0],n=a[1],o=i;e.articles[o]=n}))},clearArticles:function(e,t){e.articles=_e.articles,e.activeTag=_e.activeTag},clearState:function(e){_e}}}),ye=function(){var e=T((function(e){return e.app.experienceExpandedIndex})),t=j();return Object(b.jsxs)("div",{children:[Object(b.jsx)(y,{}),Object(b.jsx)("h4",{children:"Experience "}),Object(b.jsx)(pe,{}),Object(b.jsx)(y,{}),Object(b.jsx)(re,{expandedIndex:e,setExpandIndex:function(e){return t(be.actions.setActiveExpIndex(e))},useCollapseGlobally:!0,items:[{title:"Software Engineer",date:Object(b.jsxs)(b.Fragment,{children:[" ",Object(b.jsx)("div",{children:"15-May 2023 - "}),Object(b.jsx)("div",{children:"Present"})]}),useCollapseLocally:!0,company:"Wonderbricks Limited",skills:[{category:"Typescript (Frontend)",techSkills:["React","Redux-Toolkit","React-Native","Algolia (Search Engine)","Socket.io-client"]},{category:"Typescript (Backend)",techSkills:["Express","Mongoose","Socket.io"]},{category:"Java",techSkills:["Springboot","Spring-Data-MongoDB","MyBatis","Algolia (Search Engine)",Object(b.jsxs)("div",{children:["Server Sent Event (SSE) ",Object(b.jsx)("i",{children:Object(b.jsxs)("b",{children:[" by WebFlux"," "]})})]})]},{category:"Python",techSkills:["Flask","Boto3"]},{category:"Database Query",techSkills:["SQL Statements","Mongo Aggregation Pipelines"]},{category:"Cloud (AWS)",techSkills:["Deployment of React App to S3 with Cloudfront","Lambda Serverless Python Backend"]},{category:"Server Side Caching",techSkills:[Object(b.jsxs)("div",{children:["Redis (for SSE within ",Object(b.jsx)("i",{children:Object(b.jsx)("b",{children:"Scoped Users"})}),")"]})]}],description:Object(b.jsx)(b.Fragment,{children:Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:Object(b.jsx)("b",{children:"Frontend."})}),Object(b.jsx)("ul",{children:Object(b.jsx)("li",{children:"Build, revamp and maintain React projects in Typescript."})}),Object(b.jsx)("li",{children:Object(b.jsx)("b",{children:"Backend."})}),Object(b.jsxs)("ul",{children:[Object(b.jsxs)("li",{children:["Maintain SpringBoot project, build APIs using both",Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:" MyBatis with MySQL, and"}),Object(b.jsx)("li",{children:" mongo-java-driver for MongoDB "})]}),"Thereby acquring",Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:"proficiency in writing SQL query and"}),Object(b.jsxs)("li",{children:["proficiency in writing mongo aggregation pipelines in json (",Object(b.jsx)("a",{href:"/blog/category/mongo",children:"study notes"}),")."]})]})]}),Object(b.jsx)("li",{children:"Integrate google's GMail and Pub/Sub api in Java into the business logic of current company."}),Object(b.jsx)("li",{children:Object(b.jsxs)("ul",{children:[Object(b.jsxs)("li",{children:["Move part of the services in SpringBoot project to a single ",Object(b.jsx)("b",{children:Object(b.jsx)("i",{children:"serverless Python Flask server"})}),", which are APIs to generate excel and pdf documents when receiving json data."]}),Object(b.jsxs)("li",{children:["Serverless part have been summarized ",Object(b.jsx)("a",{href:"/blog/article/Serverless-Flask-and-Serverless-Express-ts",target:"_blank",children:"here"})," for future reuse, additionally created a functioning template for express."]})]})}),Object(b.jsx)("li",{children:"Create push notification using Redis and SSE in SpringBoot, understood how single-threaded event driven system works from that on."})]})]})})},{title:"AI Engineer",date:Object(b.jsxs)(b.Fragment,{children:[" ",Object(b.jsx)("div",{children:"15 Aug 2022 - "}),Object(b.jsx)("div",{children:"April 2023"})]}),company:"Eye Catching Limited",useCollapseLocally:!0,skills:[{category:"Python",techSkills:["PyTorch"]},{category:"Typescript",techSkills:["Tensorflow","Onnx"]},{category:"C++",techSkills:["Libtorch","Imgui","CMake Ecosystem"]}],description:Object(b.jsx)(b.Fragment,{children:Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:"Maintain angular project."}),Object(b.jsx)("li",{children:"Study eye-tracing related algorithm."}),Object(b.jsx)("li",{children:"Study and deploy machine leanring algorihtm in web-app and desktop-app."}),Object(b.jsx)("li",{children:"Completely translate a BlazeFace model in python into the same model in libtorch of C++ for GUI application."}),Object(b.jsx)("li",{children:"Implement methods to ensure weights in pytorch model can be used in libtorch model."}),Object(b.jsx)("li",{children:"Study CMake and develop software to combine libtorch model and imgui application."})]})})},{title:"Senior Software Developer",date:Object(b.jsxs)(b.Fragment,{children:[" ",Object(b.jsx)("div",{children:"Sep 2021 - "}),Object(b.jsx)("div",{children:"July 2022"})]}),useCollapseLocally:!0,company:"RaSpect Intelligence Inspection Limited",skills:[{category:"Python",techSkills:["Selenium","Boto3 (AWS SDK)","Mongoengine","gRPC","PyTorch"]},{category:"Data Annotation Tool",techSkills:["CVAT","COCO-Annotator"]},{category:"AI-General",techSkills:["Data Annotation","DataLoader with Various Data Augmentations","Model Training for Rust Detection","Model Training for Crack Detection"]},{category:"AI-Models",techSkills:["GAN for Producing Synthetic Data","Faster RCNN from Scratch and its Modification",Object(b.jsxs)(b.Fragment,{children:["Single Stage Headless ",Object(b.jsx)("b",{children:"Face"})," Detector into ",Object(b.jsx)("b",{children:"Rust"})," Detector"]})]},{category:"Javascript (Legacy Project)",techSkills:["Next"]}],description:Object(b.jsx)(b.Fragment,{children:Object(b.jsxs)("ul",{children:[Object(b.jsxs)("li",{children:[" ",Object(b.jsx)("b",{children:"Web or Hardware Related."}),Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:"Maintain React Next project."}),Object(b.jsx)("li",{children:"Data scrapping for house transactions records."}),Object(b.jsx)("li",{children:"Automation on tracking latest rtk-data from official geodetic website and data-processing pipeline."}),Object(b.jsx)("li",{children:"Work with a local grpc based service in Python to interact with a compiled binary (.exe), messed with multi-threading stuff."})]})]}),Object(b.jsxs)("li",{children:[" ",Object(b.jsx)("b",{children:"Object Detection Related."}),Object(b.jsxs)("ul",{children:[Object(b.jsxs)("li",{children:[Object(b.jsx)("b",{children:"Text Detection."})," Implemented text detection (EAST) to facilitate signboard defect classification."]}),Object(b.jsxs)("li",{children:[Object(b.jsx)("b",{children:"Crack Detection."})," Help implement detection model for crack and spalling."]}),Object(b.jsxs)("li",{children:[Object(b.jsx)("b",{children:"Rust Detection."})," Implemented two solutions: ",Object(b.jsx)("b",{children:"(i)"})," Text removal -> classifcation pipeline; and ",Object(b.jsx)("b",{children:"(ii)"})," direct object detection model using faster rcnn."]})]})]}),Object(b.jsxs)("li",{children:[" ",Object(b.jsx)("b",{children:"Image Generation/Inpainting."}),Object(b.jsxs)("ul",{children:[Object(b.jsxs)("li",{children:[Object(b.jsx)("b",{children:"DefectGAN."})," Implemented image-generation model following  ",Object(b.jsx)(ce,{href:"https://arxiv.org/pdf/2103.15158.pdf",title:"this paper"})," that generate synthetic data on cracks and spallings for training defect detection models on facades."]}),Object(b.jsxs)("li",{children:[Object(b.jsx)("b",{children:"Text Removal."})," Experimented with existing algorithms like various GAN or image-inpainting method in pytorch. Finally I follow ",Object(b.jsx)(ce,{href:"https://arxiv.org/pdf/2104.11493.pdf",title:"this paper"})," to obtain a text eraser with satisfactory performance. ",Object(b.jsx)("br",{}),"For an example of failed model, we refer the results to ",Object(b.jsx)(ce,{href:"https://github.com/machingclee/2022-06-03-EnsNet-for-text-removal",title:"here"}),"."]})]})]})]})})},{title:"Software Engineer",date:Object(b.jsxs)(b.Fragment,{children:[" ",Object(b.jsx)("div",{children:"Aug 2020 - "}),Object(b.jsx)("div",{children:"Mid-Aug 2021"})]}),useCollapseLocally:!0,company:"EAB Systems (Hong Kong) Limited",skills:[{category:"Typescript",techSkills:["React","Redux","React-Native","Express","Mongoose","Sendgrid","Twilio"]},{category:"Networking",techSkills:["socket.io"]},{category:"Python",techSkills:["Pandas","Tensorflow v2"]}],description:Object(b.jsx)(b.Fragment,{children:Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:"Build video conferencing frontend and backend application in React, Twilio and express. "}),Object(b.jsx)("li",{children:"Build CMS system that lets users create their own single page application."}),Object(b.jsxs)("li",{children:[Object(b.jsx)("b",{children:"Machine Learning Related."})," Build CSV Importer (together with an express layer and a Flask layer) that can parse a csv/excel file and perform:",Object(b.jsxs)("ul",{children:[Object(b.jsxs)("li",{children:[Object(b.jsx)("b",{children:"Data Classification."}),Object(b.jsxs)("ul",{children:[Object(b.jsxs)("li",{children:[Object(b.jsx)("strong",{children:"Sample Code"}),": ",Object(b.jsx)("a",{href:he,target:"_blank",children:"Link"})]}),Object(b.jsx)("li",{children:"Classify column based on existing data using LSTM model with two dense layers and finally a softmax."})]})]}),Object(b.jsxs)("li",{children:[Object(b.jsx)("b",{children:"Auto Date-reformatting."}),Object(b.jsxs)("ul",{children:[Object(b.jsxs)("li",{children:[Object(b.jsx)("strong",{children:"Sample Code"}),": ",Object(b.jsx)("a",{href:fe,target:"_blank",children:"Link"})]}),Object(b.jsx)("li",{children:" Based on machine learning model (transformer in NLP) that can translate all common form of date into YYYY-MM-DD format."})]}),Object(b.jsx)("br",{})]})]})]}),"Data Classification and Auto Date-refomatting are separately two models placed inside tensorflow-serving for deployment."]})})},{title:"Front-end Developer",date:"Sep 2019 - Aug 2020",useCollapseLocally:!0,company:"eLearningPro",skills:[{category:"Javascript",techSkills:["React"]},{category:"Python",techSkills:["tkinter","pyinstaller"]},{category:"Art",techSkills:["Adobe Premiere","Adobe Photoshop","Saola Animate (CSS Animation)"]}],description:Object(b.jsx)(b.Fragment,{children:Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:'Create HTML5 Coursewares, maintain web pages and construct React frontend application."'}),Object(b.jsxs)("li",{children:["Created a python GUI project for text extraction from an image (a work necessary to translate old fresh game into html5 game).",Object(b.jsxs)("p",{children:["Detail can be found ",Object(b.jsx)(ce,{href:"".concat("","/portfolio/Desktop-App-to-Capture-Text-From-Images"),title:"here"}),"."]})]})]})})},{title:"2D Game Artist",date:"Jan 2018 - Dec 2018",useCollapseLocally:!0,company:"\u6df1\u5733\u75af\u70b9\u5b50\u79d1\u6280\u6709\u9650\u516c\u53f8",skills:[{category:"Software",techSkills:["Unity","Autodesk 3ds Max","Adobe Photoshop"]},{category:"Artistic Skills",techSkills:["2D Character Design","UI Assets Design","Low-poly 3D Model Construction"]}],description:Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)("div",{children:"Responsible for constructing 3d accessories, creating environment art and character design."}),Object(b.jsxs)("div",{children:["Game that I worked on: ",Object(b.jsx)("a",{href:"https://www.taptap.com/app/37681/topic",target:"_blank",children:"Link"})]})]})},{title:"Unemployed",date:"Oct 2015 to Dec 2017",skills:[{techSkills:["Thick Skin","Self-Discipline","Parental Support"]}],company:Object(b.jsx)(y,{height:5}),description:Object(b.jsxs)(b.Fragment,{children:[" ",Object(b.jsx)("p",{children:"I was striving to become an artist within two years. Of this time I was taking online courses and drawing on a daily basis. I aimed at being able to draw a variety of game assets (not just character)."}),"My outcome based on this period can be seen ",Object(b.jsx)(ce,{href:"".concat("","/artworks"),title:"here"}),"."]})},{title:"Senior Research Assistant",date:"Mar 2015 - Oct 2015",company:"Department of Mathematics, Hong Kong Baptist University",description:"Study HJB equations arised in specific financial games."},{title:"Research Assistant",date:"Sep 2014 - Feb 2015",company:"Department of Mathematics, Hong Kong University of Science and Technology",description:"Study the property of viscosity solution of HJB equations."},{title:"Teaching Assistant",date:"Sep 2012 - June 2014",company:"Department of Mathematics, Hong Kong University of Science and Technology",description:Object(b.jsxs)("div",{children:[Object(b.jsx)("div",{children:"Fulfill teaching duty as required in obtaining studentship in the course of M.Phil study. Subjects include:"}),Object(b.jsx)("div",{children:Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:"Calculus II"}),Object(b.jsx)("li",{children:"Linear Algebra"}),Object(b.jsx)("li",{children:"Mathematical Analysis"}),Object(b.jsx)("li",{children:"Real Analysis"})]})}),Object(b.jsxs)("div",{children:["Teaching Material: ",Object(b.jsx)(ce,{href:"https://sites.google.com/site/mathcclee/home",title:"Link"})]})]})}]})]})},we=a.p+"static/media/my_picture_3.9fff9228.jpg",xe=(a(312),function(e){var t=e.height,a=e.children;return Object(b.jsx)("div",{style:{position:"relative",width:t,height:t,borderRadius:"50%",overflow:"hidden",display:"flex",justifyContent:"center",alignItems:"center",boxShadow:ie},className:"circular",children:a})}),ve=(a(313),a(258)),ke=a.n(ve),Se=a(259),je=a.n(Se),Te=a(260),Ce=a.n(Te),$e=(a(314),Object(R.a)({aboutMeButton:{"& button":{display:"inline-block",padding:"6px 10px",minWidth:"unset","& svg":{color:"#5d5d86 !important"}}}},{name:"custom-href"})),Re=function(e){var t=e.icon,a=e.href,i=e.disabled,n=void 0!==i&&i,o=$e();return Object(b.jsx)("a",{className:o.aboutMeButton,href:a,target:"_blank",style:{pointerEvents:n?"none":"auto"},children:Object(b.jsx)(k.a,{children:t})})},Ae=function(e){var t=e.icon,a=e.info;return Object(b.jsx)("div",{className:"contact-item",children:Object(b.jsx)("table",{children:Object(b.jsx)("tbody",{children:Object(b.jsxs)("tr",{children:[Object(b.jsx)("td",{className:"key",children:t}),Object(b.jsx)("td",{children:a})]})})})})},Oe=function(){return Object(b.jsxs)("div",{className:"contact",children:[Object(b.jsxs)("div",{className:"linkedin-git",children:[Object(b.jsx)(Re,{icon:Object(b.jsx)(ke.a,{style:{width:22,marginLeft:1}}),href:"https://github.com/machingclee"}),Object(b.jsx)(Re,{icon:Object(b.jsx)(je.a,{style:{fontSize:26}}),href:"https://www.linkedin.com/in/chingclee"})]}),Object(b.jsx)(y,{height:7}),Object(b.jsx)("div",{children:Object(b.jsx)(Ae,{icon:Object(b.jsx)(Ce.a,{}),info:"machingclee@gmail.com"})})]})},Ie=a(179),Ee=a.n(Ie),Ne=a(261),De=function(e){var t,a=new Date(e);return{date:a.getDate(),month:(t=a.getMonth(),{0:"January",1:"February",2:"March",3:"April",4:"May",5:"June",6:"July",7:"August",8:"September",9:"October",10:"November",11:"December"}[String(t)]),year:a.getFullYear()}},Pe=function(e){return encodeURIComponent(e)},ze=function(e){return decodeURIComponent(e)},Le=function(e){return e.replace(/\W+/g,"-")},Fe=function(e){return e.replace(/\$/g,"!@!@!").replace(/\s/g,"-").replace(/\!@\!@\!/g,"$")},Me=a(262),Be=a.n(Me);var Ue=function(e){return e.replace(/\d+/g,"")},We=function(e){return!("boolean"==typeof e?e:!!/(true|yes)/.test(e))},qe=function(){var e=Object(Ne.a)(Ee.a.mark((function e(t){var a,i,n,o,r,s,l;return Ee.a.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return a=t,a="https://raw.githubusercontent.com/machingclee/machingclee.github.io/main"+t,e.next=4,fetch(a);case 4:return i=e.sent,e.next=7,i.text();case 7:return n=e.sent,o=Be()(n),r=o.content,s=o.data,l=s,e.abrupt("return",{content:r,data:l});case 11:case"end":return e.stop()}}),e)})));return function(t){return e.apply(this,arguments)}}();var Ge=a(1073),He=a(67),Ve=a.n(He),Ye=Object(R.a)({returnButton:{textTransform:"capitalize",fontVariant:"small-caps",minWidth:"unset",padding:"2px 12px",paddingLeft:0,fontSize:"16px",color:"#394852","& svg":{color:"#394852",fontSize:"30px!important"},marginLeft:"-8px"}},{name:"return-button-modified"}),Xe=function(e){var t=e.returnCallback,a=Ye(),i=Object(l.f)();return Object(b.jsxs)(k.a,{onClick:function(){return t?t():i.goBack()},className:a.returnButton,children:[Object(b.jsx)(Ve.a,{})," Return"]})},Ke=a(263),Ze=a.n(Ke),Je=a(264),Qe=a.n(Je),et=a(265),tt=a.n(et),at=a(266),it=a.n(at),nt=(a(515),a(1077)),ot=a(271),rt=a(60),st=a.n(rt),lt={jax:["input/TeX","output/HTML-CSS"],tex2jax:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]},"HTML-CSS":{availableFonts:["TeX"],linebreaks:{automatic:!0,width:"container"},minScaleAdjust:100},TeX:{}},ct=function(e){var t=e.tex,a=e.inline,i=void 0===a||a;return Object(b.jsx)(st.a.Node,{formula:t,inline:i})},dt=a(20),pt=a.n(dt),mt=a(272),ut=a.n(mt),ht=["inline","className","children"],ft=Object(R.a)({example:{paddingLeft:"calc(2em - 3px)",marginTop:40,marginBottom:60,borderLeft:"3px solid rgba(123,185,209,0.3)"},proof:{"& > p:nth-child(1)":{textIndent:"calc(3px - 2em) !important"},paddingLeft:"calc(2em - 3px)",marginTop:20,marginBottom:20},codeBlock:{"& .react-syntax-highlighter-line-number":{minWidth:"2.5em !important"},"& pre":{paddingTop:"0.95em !important",background:"#F5F5F5!important",border:"1px solid #E0E0E0",borderRadius:2},"& .no-bg":{background:"transparent !important","& code":{background:"transparent !important"}}}},{name:"custom-code-style"}),gt=function e(t){var a=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"",i=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=a;return null===t||void 0===t||t.forEach((function(t){if("string"===typeof t)n+=i?"$".concat(t,"$"):"".concat(t);else{var a=t.props;if(null===a||void 0===a?void 0:a.children){var o,r=null===a||void 0===a||null===(o=a.className)||void 0===o?void 0:o.includes("math-inline");n=e(null===a||void 0===a?void 0:a.children,n,r)}}})),n},_t={leftBlockClassname:""},bt={leftBlockClassname:"h4-left-block"},yt={leftBlockClassname:"h5-left-block"},wt={leftBlockClassname:"h6-left-block"},xt=function(e){return function(t){var a=t.node,i=t.children,n=a.tagName,r=gt(i,"");return Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)("a",{className:"md-title",id:r.replace(/\$/g,"!@!@!").replace(/\s/g,"-").replace(/\!@\!@\!/g,"$")}),o.a.createElement(n,{},Object(b.jsxs)(b.Fragment,{children:[e.leftBlockClassname&&Object(b.jsx)("div",{className:e.leftBlockClassname,style:{height:"100%",display:"inline"}}),i]}))]})}},vt={code:function(e){var t=e.inline,a=e.className,i=e.children,n=Object(N.a)(e,ht),o=ft(),r=/language-(\w+)-*(\d+)*/.exec(a||""),s=pt.a.get(r,"[1]",""),l=pt.a.get(r,"[2]","");return!t&&r?Object(b.jsx)("div",{className:v()(o.codeBlock),children:Object(b.jsx)(nt.a,Object(I.a)({style:Object(I.a)({},ot.a),className:"none"===s?"no-bg":"",showLineNumbers:!!l,lineNumberStyle:{opacity:.35},startingLineNumber:l?parseInt(l):void 0,language:s,children:String(i).replace(/\n$/,"")},n))}):Object(b.jsx)("code",Object(I.a)(Object(I.a)({className:a},n),{},{children:i}))},example:function(e){e.className;var t=e.children,a=ft();return Object(b.jsx)("div",{className:a.example,children:t})},proof:function(e){e.className;var t=e.children,a=ft();return Object(b.jsx)(b.Fragment,{children:Object(b.jsxs)("div",{className:a.proof,children:[t,Object(b.jsx)("div",{style:{display:"flex",justifyContent:"flex-end"},children:Object(b.jsx)(ut.a,{})})]})})},h1:xt(_t),h2:xt(_t),h3:xt(_t),h4:xt(bt),h5:xt(yt),h6:xt(wt),span:function(e){var t=e.className,a=e.children;return/math\-inline/.exec(t||"")?Object(b.jsx)(ct,{tex:String(a).replace(/\n$/,""),inline:!0}):Object(b.jsx)(b.Fragment,{children:a})},div:function(e){var t=e.className,a=e.children;return/math\-display/.exec(t||"")?Object(b.jsx)("div",{className:"display-math",children:Object(b.jsx)(ct,{tex:String(a).replace(/\n$/,""),inline:!1})}):Object(b.jsx)(b.Fragment,{children:a})}};function kt(e,t){return e.children===t.children}var St=Object(n.memo)((function(e){e.style;var t=e.children,a=e.clipDistance,i=(null===t||void 0===t?void 0:t.toString())||"",o=Object(l.g)().pathname;return Object(n.useEffect)((function(){var e=window.MathJax;e&&(e.Hub.Queue(["Typeset",e.Hub]),e.Hub.Config({TeX:{tags:"ams",Macros:{R:"{\\mathbb{R}}",KL:"D_{\\text{KL}}",P:"\\mathbb{P}",C:"\\mathbb{P}",N:"\\mathbb{N}",brac:["\\left(#1\\right)",1],cbrac:["\\left\\{#1\\right\\}",1],sbrac:["\\left[#1\\right]",1],Z:"\\mathbb{Z}",E:"{\\mathbb{E}}",var:"\\mathrm{Var}",qed:"\\tag*{$\\blacksquare$}",bf:"\\boldsymbol",argmax:"\\mathrm{argmax}",argmin:"\\mathrm{argmin}",Lip:"\\mathrm{Lip}",spt:"\\mathrm{spt}",pd:"\\partial",dis:"\\displaystyle",id:"\\mathrm{id}",inner:["\\left\\langle#1\\right\\rangle",1],ol:"\\overline",olw:["\\overline{#1}^{w}",1],dom:"\\mathrm{dom}\\,",epi:"\\mathrm{epi}\\,",limi:"\\mathop{\\underline{\\lim}}",lims:"\\mathop{\\overline{\\lim}}",w:"w\\,\\text{-}\\!",conv:"\\mathrm{conv}",aff:"\\mathrm{aff}",nn:"\\mathcal{N}",limn:"\\lim_{n\\to\\infty}",limsn:"\\lims_{n\\to\\infty}",limin:"\\limi_{n\\to\\infty}",wto:"\\rightharpoonup"},extensions:["AMSmath.js","boldsymbol.js ","AMSsymbols.js","extpfeil.js"]}}))}),[o]),Object(b.jsx)(Ze.a,{components:vt,linkTarget:"_blank",transformLinkUri:function(e){return(null===e||void 0===e?void 0:e.startsWith("/"))?"".concat("").concat(e):e},transformImageUri:function(e){return(null===e||void 0===e?void 0:e.startsWith("/"))?"".concat("").concat(e):e},remarkPlugins:[it.a,Qe.a],rehypePlugins:[tt.a],children:void 0!==a?i.substring(0,a)+"...":i})}),kt),jt=a(23),Tt=(a(751),Object(R.a)({"@keyframes fadein":{"0%":{opacity:0},"100%":{opacity:1}},customFadin:{"&.fade-in":{animation:"$fadein .3s ease-in-out"}}},{name:"fade-in",classNamePrefix:"fade-in"}));function Ct(e){var t=e.children,a=e.dependencies,i=void 0===a?[]:a,o=e.style,r=void 0===o?{}:o,s=Object(n.useState)(!1),l=Object(E.a)(s,2),c=l[0],d=l[1],p=Tt();return Object(n.useEffect)((function(){d(!1),setTimeout((function(){d(!0)}),1)}),Object(jt.a)(i)),Object(b.jsx)("div",{style:Object(I.a)({opacity:c?1:0},r),children:Object(b.jsx)("div",{className:v()(p.customFadin,c?"fade-in":""),children:Object(b.jsx)(b.Fragment,{children:t})})})}var $t=a(131),Rt=a.n($t),At=a(98),Ot=a.n(At),It=Object(R.a)({nextPrev:{opacity:.65,fontWeight:600,marginBottom:10,display:"flex",alignItems:"center","&.active":{opacity:.8}},projects:{"& img, & iframe":{boxShadow:"rgba(0, 0, 0, 0.1) 0px 0px 5px 0px, rgba(0, 0, 0, 0.1) 0px 0px 1px 0px"},"& a":{color:"#2a718d !important"},fontSize:"110%","& center":{marginTop:20,marginBottom:20}},thumbTitle:{padding:20,paddingTop:10,paddingBottom:10,textAlign:"center",width:"100%",pointerEvents:"none",fontSize:18,fontWeight:600,position:"absolute",zIndex:3,left:0,top:"50%",transform:"translateY(-50%)"},thumbnailBtn:{backdropFilter:"saturate(80%) blur(2px)",backgroundColor:"rgba(255, 255, 255, 0.6)",borderRadius:0,position:"absolute",top:0,left:0,width:"100%",height:"100%",zIndex:2},thumbnail:{border:"1px solid rgba(0,0,0,0.02)",background:"rgba(".concat(240,",").concat(240,",").concat(240,",1)"),position:"relative",height:180,overflow:"hidden","& .title-date":{fontSize:12,fontWeight:400,display:"flex",justifyContent:"flex-end"},"& .title-container":{borderRadius:10,marginLeft:240,boxShadow:"box-shadow: rgba(99, 99, 99, 0.2) 0px 2px 8px 0px;",background:"rgba(255,255,255, 1)",border:"1px dotted rgba(0, 0, 0, 0.3)",padding:15,backdropFilter:"blur(6px)","& .title-inner":{},"& .title-tech":{fontSize:15,fontWeight:400,"& .item":{display:"flex",justifyContent:"flex-start",alignItems:"center"}}},"&:hover":{boxShadow:"rgba(100, 100, 111, 0.3) 0px 7px 29px 0px;",zIndex:4,"& .thumbnail-btn":{background:"none",backdropFilter:"blur(0px)"},"& .thumb-title":{}},"& .thumb-title":{transition:"opacity 0.15s ease-in-out, backdropFilter 0.3s ease-in-out"},"& img":{objectFit:"cover"}},portfolio:{"& h1, & h2, & h4, & h5, & h6":{marginTop:30,marginBottom:20},maxWidth:740,"& img, & iframe":{borderRadius:4}}},{name:"PortfolioCard"}),Et=function(){return function(e){var t=[];return e.keys().map((function(a,i){t.push(e(a))})),t}(a(752))},Nt=function(e){var t,a=e.portfolio,i=e.cardStyle,n=void 0===i?{borderRadius:4}:i,o=e.decorator,r=Object(l.f)(),s=It(),c=new Date(a.date),d=c.getFullYear(),p=c.getMonth().toString().padStart(2,"0"),m=c.getDate().toString().padStart(2,"0"),u="".concat(d,"-").concat(p,"-").concat(m);return Object(b.jsxs)("div",{className:s.thumbnail,style:n,children:[Object(b.jsxs)("div",{className:v()(s.thumbTitle,"thumb-title"),children:[o&&Object(b.jsx)("div",{style:{top:"50%",left:40,position:"absolute",zIndex:4,transform:"translateY(-50%)",pointerEvents:"none"},children:o}),Object(b.jsx)("div",{className:"title-container",children:Object(b.jsxs)("div",{className:"title-inner",children:[Object(b.jsx)("div",{children:a.title}),Object(b.jsx)(y,{height:6}),Object(b.jsx)(pe,{thickness:1}),Object(b.jsx)(y,{height:15}),Object(b.jsx)("div",{className:"title-tech",children:Object(b.jsxs)(Ge.a,{container:!0,children:[Object(b.jsx)(Ge.a,{sm:2,item:!0,style:{textAlign:"end"},children:Object(b.jsx)("span",{style:{display:"inline-block",border:"1px solid #7bb9d1",color:"white",backgroundColor:"#7bb9d1",fontWeight:600,padding:2,fontSize:14,marginRight:4,borderRadius:5,paddingLeft:6,paddingRight:6},children:"Tech"})}),Object(b.jsx)(Ge.a,{sm:5,item:!0,style:{paddingLeft:20},children:a.techs.map((function(e){return Object(b.jsxs)("div",{className:"item",children:[Object(b.jsx)(Rt.a,{fontSize:"small",style:{fontSize:6,marginRight:4}}),e]},e)}))}),Object(b.jsx)(Ge.a,{sm:5,children:Object(b.jsx)("div",{className:"title-date",children:Object(b.jsxs)("span",{style:{borderRadius:4,padding:"2px 4px",border:"1px solid rgba(0,0,0,0.2)"},children:["Project Date ",Object(b.jsx)(ct,{tex:"\\approx\\,",inline:!0}),u]})})})]})})]})})]}),Object(b.jsxs)("div",{style:{position:"absolute",width:240,overflow:"hidden"},children:[Object(b.jsx)("div",{style:{width:"100%",height:"100%",position:"absolute",top:0,left:0,zIndex:2}}),Object(b.jsx)("img",{src:"".concat(a.thumbnail),style:{width:a.thumbWidth,transform:"translate(".concat(a.thumbTransX,"px, ").concat(a.thumbTransY,"px)")}})]}),Object(b.jsx)(k.a,{className:v()(s.thumbnailBtn,"thumbnail-btn"),onClick:(t=a.title,function(){var e=Fe(t),a="".concat(_.PORTFOLIO.path,"/").concat(e);r.push(a)})})]})},Dt=function(){var e=T((function(e){return e.app.articles.portfolios})),t=Object(l.g)().pathname;e.length;return Object(b.jsx)(Ct,{dependencies:[t],children:Object(b.jsx)("div",{children:Object(dt.cloneDeep)(e).sort((function(e,t){var a=parseInt(e.id.replace(/\D/g,""));return parseInt(t.id.replace(/\D/g,""))-a})).map((function(e,t){return Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)(Nt,{portfolio:e},e.id),Object(b.jsx)(y,{height:12})]})}))})})},Pt=function(e){var t=e.portfolio,a=e.prevPortfolio,i=e.nextPortfolio,n=t.content,o=(t.id,t.intro,t.thumbnail,t.title),r=Object(l.g)().pathname,s=Object(l.f)(),c=It();return Object(b.jsxs)(Ct,{dependencies:[r],children:[Object(b.jsxs)("div",{className:c.portfolio,children:[Object(b.jsx)("div",{children:Object(b.jsx)(Xe,{returnCallback:function(){s.push(_.PORTFOLIO.path)}})}),Object(b.jsx)(y,{height:20}),Object(b.jsx)("h3",{style:{marginBottom:40},children:o}),Object(b.jsx)(St,{children:n||""})]}),Object(b.jsx)(y,{height:100}),Object(b.jsx)(y,{}),Object(b.jsxs)("div",{children:[Object(b.jsx)("div",{style:{position:"relative"},children:a&&Object(b.jsxs)(b.Fragment,{children:[Object(b.jsxs)("div",{className:c.nextPrev,children:[Object(b.jsx)(Ve.a,{style:{marginBottom:-1}})," Newer"]}),Object(b.jsx)(Nt,{portfolio:a,decorator:Object(b.jsx)(Ve.a,{style:{opacity:.2,transform:"scale(5)"}})})]})}),Object(b.jsx)(y,{}),Object(b.jsx)("div",{style:{position:"relative"},children:i&&Object(b.jsxs)(b.Fragment,{children:[Object(b.jsxs)("div",{className:c.nextPrev,children:["Older ",Object(b.jsx)(Ot.a,{style:{marginBottom:-1}})]}),Object(b.jsx)(Nt,{portfolio:i,decorator:Object(b.jsx)(Ot.a,{style:{opacity:.2,transform:"scale(5)"}})})]})})]})]})};var zt,Lt=a.p+"static/media/WorkshopinAnalysis.402ecd08.pdf",Ft=a.p+"static/media/Combine_Notes_Linear_Algebra.6aed7a5f.pdf",Mt=a.p+"static/media/Question_and_Answer.028118b5.pdf",Bt=function(e){var t=e.show,a=void 0!==t&&t,i=e.children;return Object(b.jsx)("div",{style:{transition:"opacity 0.35s ease-in-out",opacity:a?1:0},children:Object(b.jsx)(b.Fragment,{children:i})})},Ut=function(e){var t=e.stage,a=e.activeStage,i=e.children,n=a===t;return Object(b.jsx)(Bt,{show:n,children:Object(b.jsx)(P.a,{in:n,timeout:500,children:Object(b.jsx)(b.Fragment,{children:i})})})},Wt=a.p+"static/media/TN_01.c90e2df3.pdf",qt=a.p+"static/media/TN_02.337c3b08.pdf",Gt=a.p+"static/media/TN_03.9205b858.pdf",Ht=a.p+"static/media/TN_04.6dad924e.pdf",Vt=a.p+"static/media/TN_045.2068ad93.pdf",Yt=a.p+"static/media/TN_05.cfa186c2.pdf",Xt=a.p+"static/media/TN_06.4f1cfb3b.pdf",Kt=a.p+"static/media/TN_07.9edb624e.pdf",Zt=a.p+"static/media/TN_08.2bd9ac95.pdf",Jt=a.p+"static/media/TN_09.b1556ebc.pdf",Qt=a.p+"static/media/TN_10.fce3e5cf.pdf",ea=a.p+"static/media/Midterm_Review_limsupinf.1e2193c7.pdf",ta=[{title:"Review of Linear Algebra and Multivariable Calculus ",link:Wt},{title:Object(b.jsxs)(b.Fragment,{children:["Differentiability and ",Object(b.jsx)(ct,{tex:"C^1"})," Theorem"]}),link:qt},{title:"Inverse and Implicit Function Theorem ",link:Gt},{title:"Basic Concept and Properties of Limit Inferior and Limit Superior",link:Ht},{title:"More Examples of Limit Superior and Limit Inferior",link:Vt},{title:Object(b.jsxs)(b.Fragment,{children:["Midterm Review on ",Object(b.jsx)(ct,{tex:"\\limsup"})," and ",Object(b.jsx)(ct,{tex:"\\liminf"})," "]}),link:ea},{title:"Sequence and Series of Functions; Uniform and Nonuniform Convergence",link:Yt},{title:"Consequence of Uniform Convergence",link:Xt},{title:Object(b.jsxs)(b.Fragment,{children:["Set Operations with Functions; Topology on ",Object(b.jsx)(ct,{tex:"\\mathbb{R}"})," Openness, Closedness and Compactness"]}),link:Kt},{title:"Lebesgue Inner, Outer Measures and Lebesgue Measure",link:Zt},{title:"Lebesgue Measurable Functions",link:Jt},{title:"Lebesgue Integration and Convergence Theorems",link:Qt}],aa=a.p+"static/media/TutorialNote01.6e54e0cb.pdf",ia=a.p+"static/media/TutorialNote02.b3893a4a.pdf",na=a.p+"static/media/TutorialNote03.eb578e97.pdf",oa=a.p+"static/media/TutorialNote04.c4a50fec.pdf",ra=a.p+"static/media/TutorialNote05.4c1b94b1.pdf",sa=a.p+"static/media/TutorialNote06.f2953336.pdf",la=a.p+"static/media/TutorialNote07.6adb08de.pdf",ca=a.p+"static/media/TutorialNote08.3e13cb05.pdf",da=a.p+"static/media/TutorialNote09.c6fe4b65.pdf",pa=a.p+"static/media/TutorialNote10.2941efe7.pdf",ma=[{title:"Linear Systems; Echelon and Reduced Echelon Forms; Basic and Free Variables",link:aa},{title:"Vector Spaces, Linear Span and Linear Independence",link:ia},{title:"Linear Transformations",link:na},{title:"Matrix Operations; Inverse, Transpose and Elementary Matrices; Determinants",link:oa},{title:"Vector Spaces Again, Bases and Dimensions",link:ra},{title:Object(b.jsxs)(b.Fragment,{children:["Important Subspaces of ",Object(b.jsx)(ct,{tex:"\\mathbb R^n"}),", Row, Column Spaces; Rank-Nullity Theorem"]}),link:sa},{title:"Coordinates and Matrix Representations of Linear Transformations; Generalized Nullity-Rank Theorem",link:la},{title:"Eigenvalues and Eigenvectors for Square Matrices; Algebraic and Geometric Multiplicity of Eigenvalues; Diagonalization",link:ca},{title:'Inner Product, "Norm" (i.e., "Length"); Orthogonal and Orthonormal Sets, Orthogonal Complement; Gram-Schmidt Orthogonalization Process',link:da},{title:"Orthogonal Projection onto Finite Dimensional Subspaces; Minimizer Obtained by Orthogonal Projections",link:pa}],ua=a.p+"static/media/TN_00.51d1ba8e.pdf",ha=a.p+"static/media/TN_01.716a26e2.pdf",fa=a.p+"static/media/TN_02.009428e5.pdf",ga=a.p+"static/media/TN_03.479dda15.pdf",_a=a.p+"static/media/TN_04.ac918127.pdf",ba=a.p+"static/media/TN_05.e6aeca59.pdf",ya=a.p+"static/media/TN_06.83510eba.pdf",wa=a.p+"static/media/TN_07.566a16bc.pdf",xa=a.p+"static/media/TN_08.b7a801ee.pdf",va=a.p+"static/media/TN_09.f2722170.pdf",ka=a.p+"static/media/TN_10.6d1f38b4.pdf",Sa=[{title:"Mathematical Induction and Inequalities",link:ua},{title:"Informal Logic, Sets and Functions",link:ha},{title:"Countability",link:fa},{title:"Infimum and Supremum",link:ga},{title:"Sequences",link:_a},{title:"Midterm-Review",link:a.p+"static/media/review.c9c071ab.pdf"},{title:"Continuous Functions",link:ba},{title:"Differentiation (Part I): L'Hopital's Rule & Mean-Value Theorem",link:ya},{title:"Differentiation (Part II): Taylor Series (Skipped in this course)",link:wa},{title:"Riemann Integral (Part I): Riemannn Integrability",link:xa},{title:"Riemann Integral (Part II): Miscellaneous",link:va},{title:"Riemann Integral (Part III): Improper Integral (Skipped in this course)",link:ka}];a(765);!function(e){e.LINEAR_ALGEBRA="Linear Algebra",e.MATHEMATICAL_ANALYSIS="Mathematical Analysis",e.REAL_ANALYSIS="Real Analysis"}(zt||(zt={}));var ja=[{category:zt.LINEAR_ALGEBRA,list:ma},{category:zt.MATHEMATICAL_ANALYSIS,list:Sa},{category:zt.REAL_ANALYSIS,list:ta}],Ta=(a(766),a(273)),Ca=a(134),$a=function(e){var t=e.split(","),a=Object(E.a)(t,3),i=a[0],n=a[1],o=a[2];return{src:i,width:Number(n),height:Number(o)}},Ra=(a(830),a(7)),Aa=a(277),Oa=a.n(Aa),Ia=(a(831),a(275)),Ea=a.n(Ia),Na=a(278),Da=a.n(Na),Pa=a(276),za=function(){return{scrollContainer:document.querySelector(".os-viewport")}};function La(e){var t=e.width,a=void 0===t?10:t;return Object(b.jsx)("div",{style:{width:a,minWidth:a,userSelect:"none"}})}var Fa=a(285),Ma=function(e){var t=e.id,a=void 0===t?"":t,i=e.activeArticle,n=e.className,o=void 0===n?"":n,r=e.ref,s=void 0===r?void 0:r;return Object(b.jsxs)(w.a,{ref:s,id:a,className:v()(o),options:{scrollbars:{autoHide:"leave",autoHideDelay:100}},children:[Object(b.jsxs)("div",{className:"content-row",children:[Object(b.jsx)("h5",{children:" Contents "}),Object(b.jsx)(y,{height:10}),Object(b.jsx)(pe,{style:{marginTop:-12}}),Object(b.jsx)("p",{})]}),Object(b.jsx)("div",{className:"TOC-heading",children:Object(b.jsx)(Fa.a,{markdown:(null===i||void 0===i?void 0:i.content)||"",hyperlink:!0})}),Object(b.jsx)(pe,{}),Object(b.jsx)(y,{})]})},Ba=Object(R.a)((function(e){return{tagsRow:function(e){return{display:"flex",flexWrap:"wrap",listStyle:"none",paddingLeft:"0 !important",paddingRight:20,marginBottom:6,"& div":{"& code":{pointerEvents:e.clickable?"auto":"none",padding:"2px 5px",borderRadius:3,color:"white",backgroundColor:"#3b62914c !important",marginRight:10,transition:"opacity 0.1s ease-in-out",cursor:"pointer",userSelect:"none","&:hover":{opacity:.5}}}}}}}));function Ua(e){var t=e.tags,a=e.styles,i=void 0===a?{}:a,n=e.clickable,o=void 0===n||n,r=e.onClick,s=void 0===r?function(){}:r,c=Object(l.f)(),d=Ba({clickable:o});return Object(b.jsx)("div",{className:v()(d.tagsRow,"tags-container"),style:Object(I.a)({},i),children:Object(jt.a)(t).sort((function(e,t){return e.localeCompare(t)})).map((function(e,t){return Object(b.jsx)("div",{onClick:function(){c.push("".concat(_.BLOG.path,"/category/").concat(e)),s()},children:Object(b.jsx)(St,{children:"`".concat(e,"`")})},t)}))})}var Wa=Object(R.a)({articleStyle:{"& .mtext, & .mjx-char":{fontSize:"1.15rem"},"& h4":{"& ~ h5":{marginTop:40}},"& h5":{"& ~ h6":{marginTop:40}},"& a":{color:"#2a718d !important"}},codeStyle:{"& code":{fontSize:"0.85em !important",background:"#F5F5F5!important",border:"1px solid rgba(0,0,0,0.03)",borderRadius:4},"& pre":{margin:"1em 0","& code":{fontSize:"0.85em !important",border:"none"}}},floatingTitle:{display:"block",paddingRight:10,borderRight:"6px solid transparent"},activeFloatingTitle:{borderRight:"6px solid #6fa8cd",transition:"border-color 0.2s ease-in-out"},inactiveFloatingTitle:{color:"inherit",borderRight:"6px solid transparent",backgroundColor:"inherit"},floatingTOC:Object(Ra.a)({"& .TOC-heading":{fontSize:16},paddingTop:0,"& .content-row":{position:"sticky",paddingTop:20,top:0,backdropFilter:"blur(8px)",background:"rgba(255,255,255,0.7)"},"& h5":{marginTop:0},overflowY:"scroll",maxHeight:650,transform:"translate(-500px,-30px)",borderRadius:6,padding:20,backgroundColor:"white",border:"1px solid rgba(0,0,0,0.1)",width:360,transition:"top 0.1s linear",position:"sticky",top:70,left:50,opacity:0,pointersEvent:"none","&.open":{opacity:1,transition:"opacity 0.1s linear",pointersEvent:"auto"}},"@media (max-width:1700px)",{opacity:"0 !important",pointersEvent:"none !important"})},{name:"custom-articles"}),qa=function(e){return e.length>25?e.substring(0,25)+" ...":e},Ga=function(){var e=j(),t=T((function(e){return e.app.articles.archive.open}));return Object(b.jsxs)(k.a,{onClick:function(){e(be.actions.updateArchive({open:!t}))},style:{display:"flex",alignItems:"center"},children:[Object(b.jsx)(Oa.a,{style:{marginRight:8},fontSize:"small"})," Archive"]})};function Ha(e){var t=e.showComment,a=e.showTOC,i=e.articles,o=void 0===i?[]:i,r=e.activeArticle,s=e.navToArticle,c=(e.style,e.className),d=void 0===c?"":c,p=Wa(),m=j(),u=Object(l.g)().pathname,f=Object(l.f)(),g=Object(n.useRef)(null),w=Object(n.useRef)(""),x=T((function(e){return e.app.leftMenuIsOpen})),S=o.length,C=o.findIndex((function(e){return e.title===(null===r||void 0===r?void 0:r.title)})),$=o[Math.max(C-1,0)],R=o[Math.min(C+1,S-1)],A=null===r||void 0===r?void 0:r.date,O=Object(n.useState)(null),I=Object(E.a)(O,2),N=I[0],D=I[1],P=document.querySelector(".cross-sensor"),z=Object(n.useCallback)((function(){var e=document.querySelectorAll(".md-title"),t=e.length;if(t>0){var a=0;for(var i in Array.from(e)){if(!(e[parseInt(i)].getBoundingClientRect().top<140)){a=Math.max(parseInt(i)-1,0);break}parseInt(i)===t-1&&(a=t-1)}var n=Array.from(document.querySelectorAll(".".concat(p.floatingTOC," a")));e.forEach((function(e,t){var i=e.id,o=n.find((function(e){return decodeURI(e.href).includes("#".concat(i))}));if(o)if(t===a){var r=document.querySelector("#floating-toc .os-viewport"),s=o.offsetTop;null===r||void 0===r||r.scrollTo({left:0,top:s-400,behavior:"smooth"}),o.className=v()(p.floatingTitle,p.activeFloatingTitle)}else o.className=v()(p.floatingTitle,p.inactiveFloatingTitle)}))}}),[r]),L=Object(n.useCallback)(Object(dt.debounce)((function(){P&&D(P.getBoundingClientRect().top),z()}),100),[P,r,u]),F=za().scrollContainer,M=T((function(e){return e.app.articles.defaultTocCrossed}));return Object(n.useEffect)((function(){var e,a=w.current===(null===r||void 0===r?void 0:r.id);r&&r.id&&!a&&t&&(console.log("disqus reloaded","https://machingclee.github.io/#!"+r.id,r.title,r.id),null===(e=Window.DISQUS)||void 0===e||e.reset({reload:!0,config:function(){this.page.identifier="https://machingclee.github.io/#!"+r.id,this.page.url="https://machingclee.github.io/#!"+r.id}})((function(){var e=document,t=e.createElement("script");t.src="https://c-c-lee-blog.disqus.com/embed.js",t.async=!0,t.setAttribute("data-timestamp",new Date+""),(e.head||e.body).appendChild(t)}))(),w.current=r.id)}),[r]),Object(n.useEffect)((function(){return P?(L(),F.addEventListener("scroll",L)):D(300),function(){P&&F.removeEventListener("scroll",L)}}),[P]),Object(n.useEffect)((function(){null!=N&&m(N<300?be.actions.setDefaultTocCrossed(!0):be.actions.setDefaultTocCrossed(!1))}),[N]),r?Object(b.jsx)(st.a.Provider,{options:lt,children:Object(b.jsx)("div",{className:v()("articles",d,x?"":"expanded"),style:{position:"relative"},children:Object(b.jsx)("div",{className:"articles-container",children:Object(b.jsx)(h.a,{children:Object(b.jsx)("div",{style:{display:"flex",justifyContent:"center",width:"100%",margin:0},children:Object(b.jsxs)("div",{className:v()("content",p.codeStyle),children:[r&&r.hasToc&&Object(b.jsx)("div",{style:{position:"absolute",height:"100%"},children:Object(b.jsx)(Ma,{id:"floating-toc",className:v()(p.floatingTOC,M?"open":""),activeArticle:r})}),Object(b.jsx)("div",{className:v()("article",p.articleStyle),children:r&&Object(b.jsxs)(Ct,{dependencies:[u],children:[Object(b.jsxs)("div",{children:[" ",Object(b.jsx)(Xe,{returnCallback:function(){localStorage.getItem("scroll")?f.goBack():f.push(_.BLOG.path)}})," "]}),Object(b.jsx)(y,{height:20}),Object(b.jsxs)("div",{ref:g,children:[Object(b.jsxs)("div",{className:"first-row",children:[Object(b.jsx)("div",{className:"date",children:"".concat(null===A||void 0===A?void 0:A.month," ").concat(null===A||void 0===A?void 0:A.date,", ").concat(null===A||void 0===A?void 0:A.year)}),Object(b.jsxs)("div",{className:"archive-button-row",children:[Object(b.jsx)(Ea.a,{documentTitle:r.title,pageStyle:function(){return"\n          @page {\n            margin: 30mm;\n    }\n          @media print {\n            html, body {\n            font-color: black;\n          font-size: 130%;\n          height: initial !important;\n          overflow: initial !important;\n      }\n          body .archive-button-row {\n            display: none;\n\n      }\n    }\n  "},trigger:function(){return Object(b.jsxs)(k.a,{children:[Object(b.jsx)(Da.a,{style:{transform:"scale(0.9)"}}),Object(b.jsx)(La,{width:7}),"Print"]})},content:function(){return g.current}}),Object(b.jsx)(La,{width:5}),Object(b.jsx)(Ga,{})]})]}),Object(b.jsx)(y,{}),Object(b.jsx)(pe,{thickness:3}),Object(b.jsx)(y,{height:20}),Object(b.jsx)("h3",{style:{marginBottom:10},children:r.title}),Object(b.jsx)(Ua,{tags:r.tags,clickable:!1,styles:{marginBottom:r.hasToc?0:30}}),a&&r.hasToc&&Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)(Ma,{activeArticle:r}),Object(b.jsx)("div",{className:"cross-sensor"})]}),Object(b.jsx)(St,{children:(null===r||void 0===r?void 0:r.content)||""}),Object(b.jsx)(y,{}),Object(b.jsx)(pe,{thickness:3}),Object(b.jsx)(y,{height:10})]})]})}),r&&r.id&&t&&Object(b.jsxs)("div",{style:{userSelect:"none"},children:[Object(b.jsx)(y,{height:100}),Object(b.jsx)(Pa.DiscussionEmbed,{shortname:"c-c-lee-blog",config:{url:"https://machingclee.github.io/#!"+r.id,identifier:"https://machingclee.github.io/#!"+r.id,title:r.title,language:"en_us"}})]}),r&&Object(b.jsx)(b.Fragment,{children:Object(b.jsxs)("div",{className:"prev-next-article-buttons-row",children:[Object(b.jsx)("div",{children:C>0&&Object(b.jsxs)(k.a,{onClick:function(e){s&&s($.articleRouteId,e)},title:$.title,children:[Object(b.jsx)(Ve.a,{})," ",qa($.title)]})}),Object(b.jsx)("div",{children:C<o.length-1&&Object(b.jsxs)(k.a,{onClick:function(e){s&&s(R.articleRouteId,e)},title:R.title,children:[qa(R.title)," ",Object(b.jsx)(Ot.a,{})]})})]})})]})})})})})}):null}a(836),a(837);var Va=a(1076),Ya=a(1081),Xa=["open","anchor","container","children","drawerWidth"],Ka=Object(R.a)((function(e){return Object(Ya.a)({root:{display:function(e){return e.drawerWidth}},appBar:{transition:e.transitions.create(["margin","width"],{easing:e.transitions.easing.sharp,duration:e.transitions.duration.leavingScreen})},appBarShift:{width:function(e){return"calc(100% - ".concat(e.drawerWidth,"px)")},transition:e.transitions.create(["margin","width"],{easing:e.transitions.easing.easeOut,duration:e.transitions.duration.enteringScreen}),marginRight:function(e){return e.drawerWidth}},title:{flexGrow:1},hide:{display:"none"},drawer:{width:function(e){return e.drawerWidth},flexShrink:0},drawerPaper:{width:function(e){return e.drawerWidth}},drawerHeader:Object(I.a)(Object(I.a)({display:"flex",alignItems:"center",padding:e.spacing(0,1)},e.mixins.toolbar),{},{justifyContent:"flex-start"}),content:{flexGrow:1,padding:e.spacing(3),transition:e.transitions.create("margin",{easing:e.transitions.easing.sharp,duration:e.transitions.duration.leavingScreen}),marginRight:function(e){return-e.drawerWidth}},contentShift:{transition:e.transitions.create("margin",{easing:e.transitions.easing.easeOut,duration:e.transitions.duration.enteringScreen}),marginRight:0}})})),Za=function(e){var t=e.open,a=e.anchor,i=void 0===a?"right":a,n=(e.container,e.children),o=e.drawerWidth,r=void 0===o?400:o,s=Object(N.a)(e,Xa),l=Ka({drawerWidth:r});return Object(b.jsx)(Va.a,Object(I.a)(Object(I.a)({},s),{},{variant:"temporary",anchor:i,open:t,classes:{paper:l.drawerPaper},ModalProps:{keepMounted:!0},children:Object(b.jsx)("div",{children:n})}))};a(838);function Ja(e){var t=e.open,a=e.toggleOpen,i=e.activeArticleId,n=e.articles,o=e.navToArticle;return Object(b.jsx)(Za,{onClose:a,open:t,drawerWidth:600,children:Object(b.jsxs)("div",{className:"achive-drawer",children:[Object(b.jsx)(y,{}),Object(b.jsx)("div",{children:n.map((function(e){var t=e.date,n=e.title,r=(e.intro,e.articleRouteId),s=r===i,l=t.date,c=t.month,d=t.year;return Object(b.jsxs)("div",{className:v()("toc-home-row",s?"active":""),children:[Object(b.jsx)("div",{className:"toc-home-title",children:Object(b.jsx)("span",{className:"toc-home-title-text",onClick:function(){o(r),a()},children:Object(b.jsx)(St,{children:n})})}),Object(b.jsx)(pe,{}),Object(b.jsx)("div",{className:"toc-home-date",children:Object(b.jsx)("i",{children:"".concat(c," ").concat(l,", ").concat(d)})})]},n)}))})]})})}var Qa=function(){var e=T((function(e){return e.app.articles.archive.open})),t=Object(l.f)(),a=T((function(e){return e.app.articles.blogs})),i=j(),n=T((function(e){return e.app.articles.activeBlogArticleId}));return Object(b.jsx)(Ja,{articles:a,open:e,toggleOpen:function(){i(be.actions.updateArchive({open:!e}))},activeArticleId:n,navToArticle:function(e){t.push(g(Pe(e)))}})},ei=a(279),ti=a.n(ei),ai=(a(839),Object(R.a)({anchorContainer:{paddingTop:40},anchorWithoutAnchorBehaviour:{textDecoration:"none !important","&:hover":{color:"inherit !important"}},byTopicTOC:{"& .tag":{textTransform:"capitalize",fontVariant:"small-caps",fontSize:"1.4rem",fontWeight:550,color:"#394852"},"& .entity":{transition:"opacity 0.1s ease-in-out",marginBottom:15},"& .title":{"&:hover":{opacity:.5},display:"flex","& .bullet":{transform:"translateY(1px)",display:"flex",justifyContent:"center",alignItems:"center",width:30,"& svg":{fontSize:8}},"& .content":{fontWeight:500}},"& .intro":{marginLeft:30,fontSize:15,opacity:.85},"& p":{marginBottom:2}}},{name:"TOC-articles"}));function ii(e){var t=e.articles,a=e.sortPostBy,i=e.navToArticle,n=void 0===i?function(){}:i,r=(e.navToArticleNewTab,e.activeTag),s=void 0===r?"":r,l=ai(),c=function(){var e=new Set;return t.map((function(t){return t.tags.forEach((function(t){return e.add(t)}))})),Array.from(e).sort((function(e,t){return e.localeCompare(t)}))}();return Object(b.jsxs)("div",{className:"toc-home",children:["topic"===a&&Object(b.jsx)("div",{className:l.byTopicTOC,children:c.map((function(e){return Object(b.jsx)(o.a.Fragment,{children:Object(b.jsx)(Ct,{children:Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)("div",{className:l.anchorContainer,children:Object(b.jsx)("a",{id:e},"anchor-".concat(e))}),Object(b.jsxs)("div",{children:[Object(b.jsx)("div",{className:"tag",children:e}),Object(b.jsx)(y,{height:8}),Object(b.jsx)(pe,{}),Object(b.jsx)(y,{height:16}),Object(b.jsx)("div",{children:t.filter((function(t){return t.tags.indexOf(e)>-1})).map((function(e){return Object(b.jsxs)("div",{className:"entity",children:[Object(b.jsx)("a",{className:l.anchorWithoutAnchorBehaviour,href:g(e.articleRouteId),onClick:function(t){n(e.articleRouteId,t)},children:Object(b.jsxs)("div",{className:"title",children:[Object(b.jsx)("div",{className:"bullet",children:Object(b.jsx)(Rt.a,{})}),Object(b.jsx)("div",{className:"content",children:e.title})]})}),Object(b.jsx)("div",{className:"intro",children:Object(b.jsx)(St,{children:e.intro})})]},e.articleRouteId)}))})]},e)]})})},e)}))}),"date"===a&&Object(b.jsx)(b.Fragment,{children:t.map((function(e){var t=e.date,a=e.title,i=e.intro,o=e.articleRouteId,r=e.edited,c=(e.tags,t.date),d=t.month,p=t.year,m=function(){return""===s||!!e.tags.includes(s)};return Object(b.jsxs)(P.a,{in:m(),children:[Object(b.jsx)(y,{height:10}),Object(b.jsx)(D.a,{in:m(),children:Object(b.jsxs)("div",{className:"toc-home-row",children:[Object(b.jsxs)("div",{className:"toc-home-date",children:[Object(b.jsx)("div",{children:Object(b.jsxs)("i",{children:["".concat(d," ").concat(c),","]})}),Object(b.jsxs)("div",{children:[" ",Object(b.jsx)("i",{children:p})," "]}),r&&Object(b.jsxs)("div",{className:"edited",children:[Object(b.jsx)(y,{height:5}),Object(b.jsx)("div",{children:Object(b.jsx)("i",{children:"Last Edit:"})}),Object(b.jsx)("div",{children:Object(b.jsxs)("i",{children:[" `$",r.month," $",r.date,"`"]})}),Object(b.jsxs)("div",{children:[" ",Object(b.jsx)("i",{children:r.year})," "]})]})]}),Object(b.jsxs)("div",{className:"toc-home-title",children:[Object(b.jsx)("div",{className:"toc-home-title-text",children:Object(b.jsx)("div",{className:"toc-home-text-container",children:Object(b.jsx)("a",{className:l.anchorWithoutAnchorBehaviour,href:g(o),onClick:function(e){return n(o,e)},children:Object(b.jsx)(St,{children:a})})})}),Object(b.jsx)(y,{height:8}),Object(b.jsx)(Ua,{tags:e.tags}),Object(b.jsx)(y,{height:12}),Object(b.jsx)("div",{className:"toc-home-intro",children:Object(b.jsx)(St,{children:i})}),Object(b.jsx)(y,{height:5})]})]},a)})]},e.articleRouteId)}))})]})}var ni=a(282),oi=a.n(ni),ri=a(283),si=a.n(ri),li=a(132),ci=a(181),di=a(284),pi=a(280),mi=["iconContainerHeight","className","inputRef"],ui="#F3F3F3",hi="rgba(0,0,0,0)",fi=Object(R.a)({searchInput:{backgroundColor:ui,outline:"none",padding:10,borderRadius:80,border:"1px solid ".concat(hi),borderLeft:"none","&:focus-visible":{}}});function gi(e){var t=e.iconContainerHeight,a=void 0===t?35:t,i=e.className,n=e.inputRef,o=Object(N.a)(e,mi),r=fi();return Object(b.jsxs)("div",{style:{display:"flex"},children:[Object(b.jsxs)("div",{style:{maxHeight:a,padding:22,display:"flex",justifyContent:"center",alignItems:"center",backgroundColor:ui,borderTopLeftRadius:80,border:"1px solid ".concat(hi),borderRight:"none",borderBottomLeftRadius:80},children:[" ",Object(b.jsx)(K.b,{})," "]}),Object(b.jsx)("input",Object(I.a)(Object(I.a)({ref:n},o),{},{style:{flex:1,borderTopLeftRadius:0,borderBottomLeftRadius:0},className:v()(i||"",r.searchInput)}))]})}var _i=a(281),bi=function(){var e=T((function(e){return e.app.articles.blogs})),t=T((function(e){return e.app.articles.activeBlogArticleId})),a=e.find((function(e){return e.articleRouteId===t})),i=function(){var t={};return e.forEach((function(e){e.tags.forEach((function(e){var a=pt.a.get(t,e,0)+1;pt.a.set(t,e,a)}))})),t}();return{articles:e,activeBlogArticleId:t,activeArticle:a,tagAndCounts:i}},yi=a(1074),wi=Object(R.a)({searchItem:{"& .tags":{display:"flex",justifyContent:"flex-end","& div":{marginBottom:6,display:"flex",justifyContent:"flex-end","&:not(.tags-container)":{height:21}}},"& p":{marginBottom:0},"& .info":{padding:10},"& .info:hover":{backgroundColor:"rgba(220,220,220)"}}});function xi(){var e=Object(n.useState)([]),t=Object(E.a)(e,2),a=t[0],i=t[1],o=Object(n.useState)(""),r=Object(E.a)(o,2),s=r[0],c=r[1],d=Object(n.useRef)(null),p=Object(l.f)(),h=wi(),f=Object(n.useRef)(new _i.a(pi,{keys:["content","tag","tags","title","intro"],threshold:m})),_=Object(dt.debounce)((function(e){c(e.target.value);var t=e.target.value;if(t){var a=f.current.search(t);i(a.map((function(e){var t=e.item;return{title:t.title,intro:t.intro,tag:t.tag||"",tags:t.tags||""}})))}else i([])}),300),x=Object(n.useRef)(null),v=Object(n.useRef)(null),k=Object(n.useState)(!0),S=Object(E.a)(k,2),j=S[0],T=S[1],C=Object(n.useRef)(null),$=function(e){var t=e.ref,a=Object(n.useState)(!0),i=Object(E.a)(a,2),o=i[0],r=i[1];return Object(n.useEffect)((function(){function e(e){t.current&&!t.current.contains(e.target)?r(!0):r(!1)}return document.addEventListener("mousedown",e),function(){document.removeEventListener("mousedown",e)}}),[]),{outsideClicked:o,setOutsideClicked:r}}({ref:v}),R=$.outsideClicked,A=$.setOutsideClicked,O=bi().tagAndCounts,I=Object.keys(O).filter((function(e){return e.indexOf(s)>-1}));return Object(n.useEffect)((function(){R?setTimeout((function(){T(!1)}),300):T(!0)}),[R]),Object(b.jsxs)(b.Fragment,{children:[Object(b.jsxs)("div",{style:{position:"relative"},ref:v,children:[Object(b.jsx)(gi,{placeholder:"Tag, title or content",onChange:_,inputRef:C}),!R&&Object(b.jsx)("div",{onClick:function(){i([]),c(""),C.current&&(A(!1),C.current.value="")},style:{position:"absolute",top:"50%",right:10,transform:"translateY(-50%)",cursor:"pointer"},children:Object(b.jsx)(K.c,{style:{fontSize:26}})}),j&&Object(b.jsx)("div",{ref:d,children:Object(b.jsx)(yi.a,{in:!R,style:{transformOrigin:"top center"},children:Object(b.jsx)(w.a,{ref:x,style:{transition:"height 0.3s ease-in-out",marginTop:5,borderRadius:4,maxHeight:u,position:"absolute",zIndex:1,width:"100%",overflowY:"scroll",backgroundColor:"white",padding:10,boxShadow:ne},options:{scrollbars:{autoHide:"leave",autoHideDelay:100}},children:Object(b.jsxs)("div",{style:{margin:10,overflow:"hidden"},children:[I.length>0&&Object(b.jsx)(b.Fragment,{children:Object(b.jsxs)("div",{style:{marginLeft:20,display:"flex",opacity:.6},children:[Object(b.jsx)("div",{style:{width:144},children:" Searchable Tags "}),Object(b.jsx)("div",{style:{flex:1,borderLeft:"4px solid #3b629126",paddingLeft:20},children:I.sort((function(e,t){return e.localeCompare(t)})).join(", ")})]})}),0==a.length&&s&&Object(b.jsx)("div",{children:"No result was found."}),a.length>0&&a.map((function(e){var t=e.intro,a=e.title,i=e.tag,n=e.tags,o=(i||n).split(",").map((function(e){return e.trim()}));return Object(b.jsx)("div",{className:h.searchItem,style:{padding:"10px 10px",cursor:"pointer"},children:Object(b.jsxs)("div",{style:{display:"flex"},children:[Object(b.jsxs)("div",{style:{flex:1},className:"info",onClick:function(){p.push(g(Le(a))),A(!0)},children:[Object(b.jsx)("div",{style:{color:"##394852",fontWeight:600,fontSize:"1.04rem"},children:Object(b.jsx)(St,{children:a})}),Object(b.jsx)("div",{style:{color:"##394852",fontSize:"0.86rem",opacity:.7},children:Object(b.jsx)(St,{children:t})})]}),Object(b.jsx)("div",{className:"tags",style:{width:180,paddingTop:10,paddingLeft:20,display:"flex",justifyContent:"flex-end"},children:Object(b.jsx)(Ua,{tags:o,onClick:function(){return A(!0)}})})]})})}))]})})})})]}),Object(b.jsx)(y,{height:20})]})}var vi=Object(b.jsx)(La,{width:4}),ki={web:Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)(oi.a,{style:{transform:"scale(0.8)",marginLeft:-5}}),vi]}),math:Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)(si.a,{style:{transform:"scale(0.85)",marginLeft:-7}}),vi]}),coding:Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)(te.a,{style:{transform:"scale(1.2)"}}),vi,vi]}),"deep-learning":Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)(li.b,{style:{transform:"scale(1)",marginLeft:0}}),vi,vi]}),react:Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)(li.a,{}),vi,vi]}),python:Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)(ci.b,{style:{transform:"scale(1.1)"}}),vi,vi]}),javascript:Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)(ci.a,{}),vi,vi]}),all:Object(b.jsxs)(b.Fragment,{children:[Object(b.jsx)(di.a,{})," ",vi,vi]})},Si=Object(R.a)({sortRow:{display:"flex","& button":{fontWeight:500,letterSpacing:"unset",fontSize:14,transition:"opacity 0.1s ease-in-out","&.active":{opacity:1,background:"#3b62918c",color:"white",borderColor:"transparent"},"&:hover":{opacity:.87},opacity:.5,border:"1px solid #31343a",fontVariant:"small-caps",width:140,padding:"3px 14px",textTransform:"none"}}},{name:"my-blog"}),ji=function(e){return Object.keys(ki).includes(e)?ki[e]:Object(b.jsx)(b.Fragment,{})},Ti=function(){return function(e){var t=[];return e.keys().map((function(a,i){t.push(e(a))})),t}(a(840))};var Ci=[{path:_.ROOT.path,component:function(){var e=Object(l.g)().pathname,t=T((function(e){return e.app.articles.defaultTocCrossed})),a=Ci.find((function(e){return e.path===_.ROOT.path})),i=T((function(e){return e.app.leftMenuIsOpen})),o=T((function(e){return e.app.activeNavRoute})),r=Object(n.useRef)(null),s=A(),m=Object(n.useRef)(""),u=function(){return document.querySelector(".os-viewport").scrollTo(0,0)};return Object(n.useEffect)((function(){var t=O(m.current||""),a=O(e||""),i=/\/blog\/article\//g.test(e||"");t&&a?i&&u():u(),m.current=e}),[e]),Object(b.jsx)(b.Fragment,{children:Object(b.jsxs)("div",{style:{height:"100vh",overflowY:"hidden"},className:v()(s.higherContainer,t?"has-article":""),children:[Object(b.jsx)($,{}),Object(b.jsx)(w.a,{id:"top-overlayscroll-container",ref:r,className:"main-content",style:{height:"calc(100vh - ".concat(o===_.LANG_STIDY_FF14.path?0:p,"px)"),width:"100%",overflow:"hidden"},children:Object(b.jsx)(h.a,{className:v()("total-container"),children:Object(b.jsxs)("div",{className:"right",style:{paddingLeft:i?c:d},children:[Object(b.jsx)(y,{height:40}),a&&a.routes&&Object(b.jsx)(l.c,{children:a.routes.map((function(e,t){return Object(b.jsx)(l.a,{path:e.path,component:e.component,exact:e.exact},e.path)}))}),Object(b.jsx)(y,{}),Object(b.jsx)(y,{}),Object(b.jsx)(y,{}),Object(b.jsx)(y,{}),Object(b.jsx)(y,{})]})})})]})})},title:"",routes:[{title:_.ABOUT.title,path:_.ABOUT.path,component:function(){return Object(b.jsxs)("div",{className:"about-me",children:[Object(b.jsxs)(B,{children:[Object(b.jsxs)("div",{children:[Object(b.jsx)("h4",{children:"About Me "}),Object(b.jsx)(pe,{}),Object(b.jsx)(y,{}),Object(b.jsxs)("div",{className:"self-intro",children:[Object(b.jsxs)("div",{className:"avatar-row left",children:[Object(b.jsx)("div",{style:{width:"100%",display:"flex",justifyContent:"center"},children:Object(b.jsx)(xe,{height:140,children:Object(b.jsx)("img",{src:we})})}),Object(b.jsx)(y,{}),Object(b.jsx)("div",{className:"name-row",children:"Ching-Cheong Lee"}),Object(b.jsx)(y,{height:10}),Object(b.jsxs)("div",{className:"job-title-row",children:[Object(b.jsx)("span",{className:"software-engineer",children:"Software Engineer"})," in ",Object(b.jsx)("br",{})," Hong Kong"]}),Object(b.jsx)(y,{height:15}),Object(b.jsx)(Oe,{})]}),Object(b.jsxs)("div",{className:"right custom-indent",style:{borderLeft:"1px solid rgba(0,0,0,0.1)",paddingLeft:40},children:[Object(b.jsx)("p",{children:"Graduated from HKUST with an M.Phil degree in Mathematics, worked in both academic and nonacademic fields, and with solid knowledge in frontend programming."}),Object(b.jsx)("p",{children:"I have strong interest in deep learning, studying various algorithms and studying how to train a model have become my hobby."}),Object(b.jsx)("p",{children:"I have been a game artist as well in mainland China. Having consideration in financial aspect, my career path leans towards IT industry. But anyway painting has been being my hobby, my art portfolio can be found in"}),Object(b.jsx)("a",{href:"https://www.artstation.com/checkercc",target:"_blank",style:{borderRadius:8,padding:"2px 10px 5px 10px",boxShadow:oe},children:"https://www.artstation.com/checkercc"})]})]}),Object(b.jsx)(y,{}),Object(b.jsx)(y,{}),Object(b.jsx)("h4",{children:"Education"}),Object(b.jsx)(pe,{}),Object(b.jsx)(y,{}),Object(b.jsx)(re,{items:[{title:"MicroMaster in A.I. and Programming",date:"03/2019 - 06/2019",company:"Tecky Academy, Hong Kong"},{title:"M.Phil. in Mathematics",date:"09/2012 - 08/2014",company:"The Hong Kong University of Science and Technology, Hong Kong",description:Object(b.jsxs)("div",{children:["Thesis: ",Object(b.jsx)(ce,{href:"https://lbezone.ust.hk/pdfviewer/web/viewer.php?file=aHR0cHM6Ly9sYmV6b25lLnVzdC5oay9vYmovMS9vL2IxMzM0NDg0L2IxMzM0NDg0LnBkZg==#page=1",title:"Interpolating Sequences and Extremal Problems"})]})},{title:"B.Sc. in Mathematics (First Class Honor)",date:"09/2009 - 06/2012",company:"The Hong Kong University of Science and Technology, Hong Kong"}]}),Object(b.jsx)(y,{}),Object(b.jsx)(ye,{}),Object(b.jsx)(y,{}),Object(b.jsx)(ue,{})]}),Object(b.jsx)(y,{}),Object(b.jsx)(y,{}),Object(b.jsx)("h4",{children:"Computer Set-up for Deep-Learning"}),Object(b.jsx)(pe,{}),Object(b.jsx)(y,{}),Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:" OS: Windows 10 Professional"}),Object(b.jsx)("li",{children:"CPU: Intel(R) Core(TM) i9-10900 CPU @ 2.80GHz  2.81 GHz"}),Object(b.jsx)("li",{children:"Graphic Card: NVIDIA GeForce RTX 3090"}),Object(b.jsx)("li",{children:"RAM: 32.0GB"}),Object(b.jsx)("li",{children:" Many Others ..."})]})]}),Object(b.jsx)(y,{}),Object(b.jsx)(y,{}),Object(b.jsx)(y,{}),Object(b.jsx)(y,{})]})},exact:!0},{title:_.PORTFOLIO.title,path:_.PORTFOLIO.path,component:function(){var e=It(),t=j(),a=Object(l.h)("".concat(_.PORTFOLIO.path,"/:matchedPortfolioTitle")),i=T((function(e){return e.app.articles.activePortfolioTitle})),o=T((function(e){return e.app.articles.portfolios})),r=o.findIndex((function(e){return Fe(e.title)===i})),s=o[r],c=Object(dt.get)(o,"[".concat(r-1,"]"),null),d=Object(dt.get)(o,"[".concat(r+1,"]"),null),p=((null===a||void 0===a?void 0:a.params)||{matchedPortfolioTitle:""}).matchedPortfolioTitle,m=function(e){t(be.actions.updateArticles({portfolios:e}))};return Object(n.useState)((function(){!function(e){for(var t=e.getArticles,a=e.setArticles,i=t(),n=[],o=[],r=0;r<i.length;r++)o.push(qe(i[r].default).then((function(e){var t=e.content,a=e.data;if(!a.id)throw new Error("id was not found on article: ".concat(a.title,"."));if(!a.wip){var i=a.tech;if(i){var o=null===i||void 0===i?void 0:i.split(",").map((function(e){return e.replace(/^\s*/g,"")})).map((function(e){return e.replace(/\s*$/g,"")}));a.techs=o}n.push(Object(I.a)({content:t},a))}})).catch((function(e){console.log(e)})));Promise.all(o).then((function(){var e=n.sort((function(e,t){var a=parseInt(e.id.replace(/\D+/g,""));return parseInt(t.id.replace(/\D+/g,""))-a}));a(e)}))}({getArticles:Et,setArticles:m})})),Object(n.useEffect)((function(){t(be.actions.updateArticles({activePortfolioTitle:p}))}),[p]),Object(n.useEffect)((function(){return function(){t(be.actions.updateArticles({activePortfolioTitle:""}))}}),[]),Object(b.jsxs)(B,{className:e.projects,title:"Projects",children:[Object(b.jsx)(y,{height:20}),Object(b.jsx)("div",{style:{padding:50,paddingTop:10},children:Object(b.jsxs)(st.a.Provider,{options:lt,children:[""==i&&Object(b.jsx)(Dt,{}),""!==i&&s&&Object(b.jsx)(Pt,{portfolio:s,prevPortfolio:c,nextPortfolio:d})]})})]})},exact:!1},{title:_.ARTWORKS.title,path:_.ARTWORKS.path,component:function(){var e=["https://cdnb.artstation.com/p/assets/images/images/011/689/703/medium/ching-cheong-lee-6.jpg?1530874176,6,3","https://cdna.artstation.com/p/assets/images/images/010/280/502/medium/ching-cheong-lee-47-artstation.jpg?1523588913,1,1","https://cdna.artstation.com/p/assets/images/images/010/280/394/large/ching-cheong-lee-000000preview.jpg?1523565280,4,5","https://cdna.artstation.com/p/assets/images/images/014/349/930/medium/ching-cheong-lee-2.jpg?1543595516,4,4","https://cdna.artstation.com/p/assets/images/images/006/007/422/medium/ching-cheong-lee-10.jpg?1495381736,4,3","https://cdna.artstation.com/p/assets/images/images/033/246/420/medium/ching-cheong-lee-18.jpg?1608905010,1,1","https://cdna.artstation.com/p/assets/images/images/005/924/540/large/ching-cheong-lee-treepractice.jpg?1494750047,1,1","https://cdna.artstation.com/p/assets/images/images/011/579/872/large/ching-cheong-lee-27.jpg?1530305575,3,1","https://cdnb.artstation.com/p/assets/images/images/007/711/521/medium/ching-cheong-lee-7.jpg?1508008886,5,2","https://cdnb.artstation.com/p/assets/images/images/005/069/419/medium/ching-cheong-lee-22.jpg?1488275626,3,2","https://cdna.artstation.com/p/assets/images/images/005/069/204/large/ching-cheong-lee-12.jpg?1488274783,3,2"],t=Object(n.useState)(0),a=Object(E.a)(t,2),i=a[0],o=a[1],r=Object(n.useState)(!1),s=Object(E.a)(r,2),l=s[0],c=s[1],d=Object(n.useCallback)((function(e,t){t.photo;var a=t.index;o(a),c(!0)}),[]);return Object(b.jsx)("div",{className:"artworks",children:Object(b.jsxs)(B,{title:"Selective Artworks",children:[Object(b.jsxs)("p",{children:["A full list of my artworks can be found in my arstation page: ",Object(b.jsx)("a",{target:"_blank",href:"https://www.artstation.com/checkercc",children:"https://www.artstation.com/checkercc"}),"."]}),Object(b.jsx)(y,{}),Object(b.jsx)(Ta.a,{photos:e.map((function(e){return $a(e)})),columns:2,onClick:d}),Object(b.jsx)(Ca.b,{children:l?Object(b.jsx)(Ca.a,{onClose:function(){o(0),c(!1)},children:Object(b.jsx)(Ca.c,{isFullscreen:!1,currentIndex:i,views:e.map((function(e){$a(e).src;return{source:{regular:$a(e).src}}}))})}):null})]})})},exact:!0},{title:_.MATH_MATERIAL.title,path:_.MATH_MATERIAL.path,component:function(){var e=Object(n.useState)(zt.LINEAR_ALGEBRA),t=Object(E.a)(e,2),a=t[0],i=t[1];return Object(b.jsx)("div",{className:"math-material",children:Object(b.jsx)(B,{title:"Study Notes and Blog",children:Object(b.jsxs)(st.a.Provider,{children:[Object(b.jsx)("h5",{children:"Math Study Notes"}),"The following are study notes I made when I was studying mathematics in my undergraduate career.",Object(b.jsx)(y,{height:8}),Object(b.jsxs)("ul",{children:[Object(b.jsx)("li",{children:Object(b.jsx)(ce,{title:"Linear Algebra",href:Ft})},"linear"),Object(b.jsx)("li",{children:Object(b.jsx)(ce,{title:"Real Analaysis (Specifically on Measure Theory)",href:Lt})},"real"),Object(b.jsx)("li",{children:Object(b.jsx)(ce,{title:"Record of Solved Problems and Solutions ",href:Mt})},"problems")]}),Object(b.jsx)(y,{}),Object(b.jsx)("h5",{children:"Math Tutorial Notes"}),"A list of tutorial notes I made when I was a teaching assistant in HKUST.",Object(b.jsx)(y,{}),Object(b.jsxs)("div",{className:"notes",children:[Object(b.jsx)("div",{className:"button-column",children:Object.values(zt).map((function(e){return Object(b.jsxs)(k.a,{className:v()("topic-button",a===e?"active":""),onClick:function(t){i(e)},children:[" ",e]},e)}))}),Object(b.jsx)("div",{className:"notes-column",children:ja.map((function(e){return Object(b.jsx)(Ut,{activeStage:a,stage:e.category,children:Object(b.jsx)("div",{className:"tutorial-notes",children:e.list.map((function(e,t){return Object(b.jsx)("div",{className:"note",children:Object(b.jsx)("div",{children:Object(b.jsxs)("div",{className:"tutorial-title-row",children:[Object(b.jsx)("div",{className:"topic",children:"Note ".concat(String(t+1).padStart(2,"0"))}),Object(b.jsx)("div",{className:"title",children:Object(b.jsx)(ce,{title:e.title,href:e.link})})]})})},e.link)}))})},e.category)}))})]}),Object(b.jsx)(y,{}),Object(b.jsx)(y,{}),Object(b.jsx)("h5",{children:"Old Blog"}),Object(b.jsx)("p",{children:Object(b.jsx)("a",{href:"https://checkerlee.blogspot.com/",target:"_blank",children:" https://checkerlee.blogspot.com/"})})]})})})},exact:!0},{title:_.CONTACT.title,path:_.CONTACT.path,component:Oe,exact:!0},{title:_.BLOG.title,path:_.BLOG.path,component:function(){var e,t=Si(),a=Object(l.f)(),i=Object(l.g)().pathname,o=function(){var e=Object(n.useState)(0),t=Object(E.a)(e,2),a=t[0],i=t[1],o=za().scrollContainer;return Object(n.useEffect)((function(){var e=Object(dt.debounce)((function(){var e=(null===o||void 0===o?void 0:o.offsetHeight)||0,t=(null===o||void 0===o?void 0:o.scrollTop)||0,a=Math.max(((null===o||void 0===o?void 0:o.scrollHeight)||1)-e,1),n=t;i(Math.min(1,n/a))}),100);return null===o||void 0===o||o.addEventListener("scroll",e),function(){null===o||void 0===o||o.removeEventListener("scroll",e)}}),[o]),{ScrollMeterButton:function(){return Object(b.jsxs)("div",{className:"scroll-top-meter",style:{position:"fixed",right:60,bottom:60,zIndex:2,backgroundColor:"rgba(0,0,0,0.5)",color:"white",padding:"5px 6px",borderRadius:3,fontSize:"0.8rem",userSelect:"none",width:55,height:30},children:[Object(b.jsx)("div",{style:{display:"flex",justifyContent:"center",transform:"scale(1.5, 0.75)",marginTop:-9,marginBottom:-10,opacity:a>.01?1:.2},children:Object(b.jsx)(ti.a,{})}),Object(b.jsx)("div",{style:{display:"flex",justifyContent:"center"},children:Math.ceil(100*a)+"%"}),Object(b.jsx)(k.a,{style:{position:"absolute",top:0,left:0,width:"100%",height:"100%",minWidth:0},onClick:function(){return o.scrollTo({top:0})}})]})}}}().ScrollMeterButton,r=((null===(e=Object(l.h)("".concat(_.BLOG.path,"/category/:activeTag")))||void 0===e?void 0:e.params)||{activeTag:""}).activeTag,s=Object(l.h)("".concat(_.BLOG.path,"/article/:matchedArticleId")),c=ze(((null===s||void 0===s?void 0:s.params)||{matchedArticleId:""}).matchedArticleId),d=T((function(e){return e.app.sortPostBy})),p=bi(),m=p.activeArticle,u=p.activeBlogArticleId,h=p.articles,f=p.tagAndCounts,w=function(e,t){x.current=0;var i=g(Pe(e));if(t&&(t.preventDefault(),!(1===t.button||t.ctrlKey)&&0===t.button))return void a.push(i)};Object(n.useEffect)((function(){C(be.actions.setActiveTag(r))}),[r]);var x=Object(n.useRef)(0),S=h.length,C=j(),$=function(e){C(be.actions.updateArticles({blogs:e}))};Object(n.useState)((function(){!function(e){var t=e.getArticles,a=e.setArticles,i=t(),n=[],o=[],r=j();r(be.actions.setShowLoading({showLoading:!0}));for(var s=0;s<i.length;s++)o.push(qe(i[s].default).then((function(e){var t=e.content,a=e.data,i=a.date,o=a.title,r=a.intro,s=a.tags,l=void 0===s?"":s,c=a.tag,d=void 0===c?"":c,p=a.edited,m=void 0===p?"":p,u=a.id,h=a.wip,f=void 0!==h&&h,g=a.toc,_=void 0===g||g,b=a.update,y=void 0===b?"":b;y&&console.log("typeof update",typeof y," value",y);var w=De(i),x=m?De(m):null,v=(d||l).split(",").map((function(e){return e.trim()}));if(!u)throw new Error("id was not found on article: ".concat(o,"."));We(f)&&n.push({date:w,edited:x,title:o,content:t.trim(),intro:r,articleRouteId:Le(o),tags:v,id:u,hasToc:_})})).catch((function(e){return console.log(e)})));Promise.all(o).then((function(){var e=n.sort((function(e,t){var a="".concat(e.date.date," ").concat(e.date.month," ").concat(e.date.year),i="".concat(t.date.date," ").concat(t.date.month," ").concat(t.date.year),n=new Date(i).getTime()-new Date(a).getTime();return 0!==n?n:function(){var a=Ue(e.id),i=Ue(t.id);return a===i?parseInt(t.id.replace(i,""))-parseInt(e.id.replace(a,"")):1}()}));a(e)})).then((function(){setTimeout((function(){r(be.actions.setShowLoading({showLoading:!1}))}),100)}))}({getArticles:Ti,setArticles:$})}));var R=Object(n.useCallback)((function(e){var t=parseInt(localStorage.getItem("scroll")||"0"),a=e.currentTarget.location.pathname;if(a===_.BLOG.path||a==="".concat(_.BLOG.path,"/"))var i=setInterval((function(){var e=document.querySelector(".os-viewport");e&&(null===e||void 0===e||e.scrollTo({left:0,top:t}),clearInterval(i))}),10)}),[u]),A=Object(n.useCallback)(Object(dt.debounce)((function(e){var t=document.querySelector(".os-viewport"),a=t.scrollTop;a!=x.current&&(x.current=a,!u&&t&&localStorage.setItem("scroll",t.scrollTop+""))}),100),[u]);Object(n.useEffect)((function(){-1===i.indexOf(_.BLOG.path)&&localStorage.setItem("scroll","0")}),[i]),Object(n.useEffect)((function(){var e=document.querySelector(".os-viewport");return window.addEventListener("popstate",R),null===e||void 0===e||e.addEventListener("scroll",A),function(){window.removeEventListener("popstate",R),null===e||void 0===e||e.removeEventListener("scroll",A)}}),[u]),Object(n.useEffect)((function(){c&&h.length>0?C(be.actions.updateArticles({activeBlogArticleId:c})):C(be.actions.updateArticles({activeBlogArticleId:""}))}),[c,h]);var O=function(e){a.push("".concat(_.BLOG.path,"/category/").concat(e))};return Object(n.useEffect)((function(){d&&C(be.actions.setActiveTag(""))}),[d]),Object(b.jsxs)("div",{className:"blog",children:[Object(b.jsx)(Qa,{}),Object(b.jsx)(o,{}),Object(b.jsxs)(B,{title:c?void 0:Object(b.jsx)(b.Fragment,{children:"Articles"}),children:[!c&&Object(b.jsxs)("div",{className:t.sortRow,children:[Object(b.jsx)(k.a,{className:"date"===d?"active":"",onClick:function(){C(be.actions.setSortPostBy("date"))},children:"Sort by Date"}),Object(b.jsx)(La,{}),Object(b.jsx)(La,{}),Object(b.jsx)(k.a,{className:"topic"===d?"active":"",onClick:function(){C(be.actions.setSortPostBy("topic"))},children:"Sort by Topic"})]}),Object(b.jsx)(y,{}),Object(b.jsx)(xi,{}),Object(b.jsx)(y,{}),!c&&function(){var e=Object(l.f)();return Object(b.jsxs)(b.Fragment,{children:[Object(b.jsxs)("div",{className:"category-row",children:[Object(b.jsxs)("div",{className:"top-row",children:[Object(b.jsx)("div",{className:"cat-left",children:" Categories"}),Object(b.jsxs)("div",{className:"cat-right",children:[" ",Object(b.jsx)(Ga,{})," "]})]}),Object(b.jsx)(y,{height:10}),Object(b.jsx)("div",{children:Object(b.jsx)("ul",{children:Object(b.jsxs)("li",{className:v()("cat-button",""===r?"active":""),children:["All",Object(b.jsx)("sup",{children:S}),Object(b.jsx)(k.a,{onClick:function(){return e.push(_.BLOG.path)},style:{position:"absolute",top:0,left:0,width:"100%",height:"100%",minWidth:0}})]})})}),Object(b.jsx)("div",{children:Object(b.jsx)("ul",{children:Object.entries(f).sort((function(e,t){var a=Object(E.a)(e,2),i=a[0],n=(a[1],Object(E.a)(t,2)),o=n[0];n[1];return i.localeCompare(o)})).map((function(e){var t=Object(E.a)(e,2),a=t[0],i=t[1],n=r===a;return Object(b.jsxs)("li",{className:v()("cat-button",n?"active":""),children:[ji(a),a,Object(b.jsx)("sup",{children:i}),Object(b.jsx)(k.a,{onClick:function(){if("date"===d)O(a);else{var e=document.getElementById(a),t=document.querySelector(".os-viewport");console.log(e,t),e&&t&&t.scrollTo({left:0,top:e.offsetTop})}},style:{position:"absolute",top:0,left:0,width:"100%",height:"100%",minWidth:0}})]},a)}))})})]}),"date"===d&&Object(b.jsx)(y,{})]})}(),!m&&Object(b.jsx)(ii,{activeTag:r,sortPostBy:d,articles:h,navToArticle:w}),c&&Object(b.jsx)(Ha,{className:"tech-blog",showComment:!0,showTOC:!0,articles:h,activeArticle:m,navToArticle:w})]})]})},exact:!1}]}],$i=a(1075);a(1020);function Ri(){var e=T((function(e){return e.app.showLoading}));return Object(b.jsx)(b.Fragment,{children:e&&Object(b.jsx)("div",{className:"custom-loading-overlay",children:Object(b.jsx)($i.a,{className:"loading-svg",style:{color:"rgba(0,0,0,0.2)"}})})})}var Ai=function(){var e=j(),t=Object(l.g)().pathname,a=Ci.find((function(e){return e.path===_.ROOT.path})),o=T((function(e){return e.app.articles.blogs})),r=T((function(e){return e.app.articles.activeBlogArticleId})),s=o.find((function(e){return e.articleRouteId===r})),c=T((function(e){return e.app.activeNavRoute}));return Object(n.useEffect)((function(){var n=a.routes.map((function(e){return e.path})).reduce((function(e,a){return new RegExp(a).test(t)?a:e}),i.NONE);n!==c&&(e(be.actions.changeActiveNavRoute(n)),n!==i.BLOG&&e(be.actions.updateArticles({activeBlogArticleId:"",defaultTocCrossed:!1})))}),[t,c]),Object(n.useEffect)((function(){var e=a.routes,i=e.reduce((function(e,a){return new RegExp(a.path).test(t)?a:e}),e[0]);document.title="CCLee / ".concat(null===i||void 0===i?void 0:i.title),r&&(document.title="CCLee / ".concat(null===i||void 0===i?void 0:i.title," / ").concat(null===s||void 0===s?void 0:s.title))}),[t,s]),Object(b.jsxs)("div",{className:"App",children:[Object(b.jsx)(Ri,{}),Object(b.jsx)("div",{className:"main",style:{backgroundRepeat:"no-repeat",backgroundSize:"cover",backgroundImage:'url("'.concat("",'/assets/images/background_img.jpg")')},children:Object(b.jsx)(l.c,{children:Ci.map((function(e){return Object(b.jsx)(l.a,{path:e.path,component:e.component,exact:e.exact},e.path)}))})})]})},Oi=function(e){e&&e instanceof Function&&a.e(3).then(a.bind(null,1082)).then((function(t){var a=t.getCLS,i=t.getFID,n=t.getFCP,o=t.getLCP,r=t.getTTFB;a(e),i(e),n(e),o(e),r(e)}))},Ii={selectedNoteId:"",selectedPageId:"",selectedVocabId:"",imageText:"",history:[],searchText:""},Ei=Object(ge.b)({name:"dict",initialState:Ii,reducers:{setSearchText:function(e,t){e.searchText=t.payload},setSelectedNoteId:function(e,t){e.selectedNoteId=t.payload},setSelectedPageId:function(e,t){e.selectedPageId=t.payload},setSelectedVocabId:function(e,t){e.selectedVocabId=t.payload},clearState:function(e,t){Object.entries(Ii).forEach((function(t){var a=Object(E.a)(t,2),i=a[0];a[1];e[i]=Ii[i]}))}}}),Ni=Object(ge.a)({reducer:{dict:Ei.reducer,app:be.reducer},devTools:!0}),Di=(a(1021),a(1022),a(93));localStorage.setItem("scroll","0"),s.a.render(Object(b.jsx)(S.a,{store:Ni,children:Object(b.jsx)(Di.a,{basename:"/",children:Object(b.jsx)(Ai,{})})}),document.getElementById("root")),Oi()}]),[[1023,1,2]]]);